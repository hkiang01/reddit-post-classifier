This is a reference implementation of a basic reinforcement learning environment. It is intended as a playground for anyone interested in this field. My goal is to provide a minimal and clean implementation of the main concepts, so you can: This package exports a function that provides the environment you'll need to try your own problems. There are three components required for the learning to start: The environment provides helpful methods to set those up, train an agent and replay its findings within your game. This example shows a basic usage of this package, and each step will be explained in its own section in order. sets up the game for the learning environment It takes the game implementation as its first argument and the game initial state as the second one. This initial game state will be used in all game simulations and can only be changed by calling this method again. The game implementation should be implemented by you following this interface: This is generally all you need to implement in order to use this package. That's not to say you shouldn't mess around anywhere else if you feel like it. sets up the memory for the learning environment This method is analogous to . It takes the memory implementation as its first argument and the memory initial state as the second one. This package provides a basic implementation for the memory that can be used out of the box. It includes both required functions and an extra one, that returns an empty memory state. The function takes a number to be used as the initial value for all unset state-action pairs. This memory implementation relies on toString method to compare your game states. This means that for this memory to work correctly, you need to make sure the string returned by for your game state really represents it. sets up the policies to be followed by the agent in the learning environment It takes one required policy (move) and two optional ones (learn and play). If one policy is omitted, it is defaulted to the previous one. The policies are used by the agent as follows: This package provides the implementation of the most popular policies used in this type of learning algorithm. trains an agent using the game, memory and policies previously set It takes the number of episode sessions to train your agent for as its first argument. The second and third ones are the learning rate and discount factor parameters. This method will mutate the environment's memory to reflect the agent's learning. How long it takes for this method to run will depend both on your game's episode length and agent's performance. Meaning it will not take forever unless your agent is both really stubborn and really disciplined. The only parameter taken by this function is a callback to pass the resulting episode. An episode is a javascript iterator of . Not satisfied with the included memory implementation? Want to try out a custom policy? This training environment is too simple for you? This section will expose the main data types and abstractions adopted in this package. Let me know if you code something awesome with them. The included memory implementation is supposed to be basic and easy to understand. Other implementations might focus on performance or even new functionality. If you want to implement your own, here's what you need to code: If you want to implement your own policies, it is just as easy as writing a simple function. You probably won't need to, since the ones included should get you covered. I sure won't stop you though, so here is the expected signature: In order to implement the learning environment I found useful to code these two primitives: You might find these functions useful to code your own extensions, so here are their signatures: Seriously, for reading this whole doc.