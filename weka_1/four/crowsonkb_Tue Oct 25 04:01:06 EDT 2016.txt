[Original Python code for image synthesis](https://gist.github.com/crowsonkb/550fe172cd277bb2f057574f2e75aff4) - lots of image-specific things here.

[Torch code that I tried training neural networks with](https://gist.github.com/crowsonkb/8da6cc4bfc5e99565ea7f897700a0bc0) - adapted from the Python code with help from [optim/lbfgs.lua](https://github.com/torch/optim/blob/master/lbfgs.lua).

It was originally for stylized image synthesis by inverting CNNs ([neural style](https://github.com/crowsonkb/style_transfer) i.e. Gatys et al.). The usual way of doing this is to start from a white noise image and apply gradient descent or L-BFGS to it, using gradients from the backward pass of the CNN. It isn't a stochastic problem and ordinary L-BFGS works well on it.

Since momentum helps so much with gradient descent on this problem, I got ideas about incorporating momentum into L-BFGS (Nesterov ofc.). I already had custom L-BFGS code that I wrote to have a line search capable of dealing with the weird way I was normalizing the gradients. I made some fortuitous discoveries along the way. I ended up applying damping as well (forming the L-BFGS y vectors as a linear combination of the original y and the step), and learning how to modify the y vectors to produce a Hessian estimation that scaled the learning rates per-parameter. For stability's sake, and because I needed learning rate decay anyway, I scaled the learning rates with the Adagrad scaling matrix (per-parameter L2 norm of gradients seen so far). To my surprise the result behaved in a stable manner without a line search and actually worked better than Adam or L-BFGS on my image synthesis problem. Since I had modified the L-BFGS quasi-Newton algorithm with damping, momentum, and scaling, I started calling the result DMSQN.

I then reimplemented it in Torch just to see what would happen when I tried this weird thing on neural network training instead of the problem it was originally adapted for. To my great surprise, it actually performed well and didn't blow up numerically. There doesn't seem a reason to recommend its use over SGD or Adam, but it at least wasn't worse. The lack of a line search, along with the previously-mentioned damping, momentum, and scaling, probably account for why it worked in the stochastic regime even though it wasn't originally designed for it. (oLBFGS for instance does not use a line search and uses a similar form of damping; adaQN repurposes the Adagrad scaling matrix but IMO in not as stability-promoting a way as I used it)

Has anyone experimented with incorporating momentum into L-BFGS or other second-order methods, at all? There's a lot of prior work on damping for instance.