Hi there!

First time here, please be gentle.

I'm currently working on a project - "Using machine learning in safety and reliability".

**Some background:**

If you have a piece of equipment it and operate it, it will eventually fail. So you note down the failure time (operating hours) and replace it.After some time (especially if you have the identical equipment several times) you collect a collection of failure times.

The usual approach is to use Minitab or R or whatever software you fancy and fit a model to the data (Exponential, Weibull, Gamma, etc.). So ideally you find a model which fits rather nice and then for the future you can describe the behaviour of your equipment with just one or two parameters.

This is often displayed in form of a Cumulative Failure Probability plot. See for example here:

http://support.minitab.com/en-us/minitab/17/cum_failure_plot_def.png

At time 0 you have a 0% chance of being failed. It increases in a certain shape and will ultimately reach 100%.


**Task:**

Reality is different. Equipment does not follow a model perfectly. But thanks to the modern world of Machine Learning, that approach might not be necessary.

My current task is:

* Have a bunch of failure dates (randomly generated and by some real life data)
* Feed that into a machine learning algorithm without implying any failure distribution model
* Get a model which will predict the percentage of being failed at a given time.


**Where I am now:**

So far I have used python and scikit-learn to built my very first python program ever (learning python AND machine learning at the same time here!).

I randomly draw failure dates which perfectly follow a Weibull distribution so far. Later I want to implement some randomness and noise, but for the start, I want to keep to "pure" values so I know what to expect.

I feed it my failure times and it creates a failure distribution curve. The more samples I use, the better it fits (obviously).

See following pictures:
Weibull with Alpha = 1.2 and Lambda = 0.0002
Blue line = The Weibull curve
Red Dots = The random data points
Red Line = The fitting line of my machine learning algorithm

First picture with 200 samples, second one with 20 samples.

See http://imgur.com/a/Ln34X for illustration of where I am now.

Till now I'm fairly happy. I actually managed to write two programs: One to generate a CSV with random Weibull values and one with the actual machine learning routine included. I used the RandomForestRegressor model provided by scikit-learn.

**Future tasks:**

* Implement proper data preprocessing (normalize)
* Compare to traditional parametric model fitting
* Include some covariates ("My device is running at temperature X and at load level Y, what are the chances of it being failed at time Z?")

**My Questions:**

Do you have any general advices on such a "simple" machine learning task? Is my approach so far reasonable or am I missing something obvious?

In reality you don't have hundreds of data points, just in the range 10-60. Are there any advices with handling "Little Data"?

The cumulative failure probability curve is per definition always rising. The probability to be failed at time X+1 is always higher than at X. How can I tell that to my machine learning routine?

I am grateful for every input you can give as I am eager to learn more - both in python/numpy/scipy/scikit-learn and machine learning theory in general.