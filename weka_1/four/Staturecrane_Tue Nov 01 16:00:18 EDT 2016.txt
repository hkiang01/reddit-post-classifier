Full code for the model can be found here Stumbling onto Alec Radford's deep convolutional generative adversarial network, or DCGAN, was one of the few genuinely jaw-dropping moments I've experienced in my life. It needs no further introduction; visit the webpage and see the madness. Or check it out right here: Those images, clearly of bedrooms, were not captured by cameras, but by statistics. They are generated from what you might call the "probability space" of bedroom pictures; the statistical distribution that contains all the various features you would expect to find in a picture of a bedroom, neatly separated so that a random sample from said distribution can be "morphed" into something that resembles a photograph. The way this is done is both shockingly simple and dreadfully complicated. It's simple in that it doesn't require a ton of code, appears to work on various kinds of datasets, and can be run on a medium-grade GPU to start producing interesting results in a few hours. It's complicated because modifying, debugging or adding to the model requires understanding the myriad kinks of generative adversarial networks, which are slightly odd beasts. I have stitched together the Torch implementation of DCGAN and Kaixhin's variational autoencoder, the latter of which performs variational inference on complicated integrals like the ones that define photographic probability distributions and can be trained with vanilla stochastic gradient descent. By tinkering with the heuristics, I was able to successfully generate fine-art-like images from the wikiart.org dataset, which was compiled by Small Yellow Duck and hosted on Kaggle. The examples below are picked from thousands upon thousands of samples generated by the model and do not exist in the training data. While the base of the DCGAN remains the same, I have changed a few things, as well as added the variational autoencoder. Compelled by a post on stabilizing GANs, I have both set limits on the generator and discriminator, so they only update when they or their adversary is performing either particularly well or particularly poorly, as well as added noise to the inputs feeding into the discriminator. Simulated annealing then lowers the amount of that noise as training progresses to encourage convergence of the GAN. The variational autoencoder uses a modified version of the discriminator to produce the latent variables, which then feed into the generator, so the entire model is convolutional. All three networks contribute equally to the loss, although the generator and discriminator are not always updating their gradients (due to the balancing act that must be played to keep them from outperforming one another). All these tricks combined seem to work consistently and stably for up to 64x64 color images, and although I suspect convergence is possible for larger dimensions, I have yet to successfully do it myself.