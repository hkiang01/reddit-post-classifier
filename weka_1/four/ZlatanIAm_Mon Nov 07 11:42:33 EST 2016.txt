Assumptions: you have a TX1 with a fresh install of JetPack 2.3 L4T. First things first. We need to remove all the fat from the install. There are tons of optimized libraries in JetPack, but so much of it takes up the valuable memory space we need to get the facial recognition app up and running. Thankfully, others have paved the way and made these steps pretty much a walk in the park. Thank you to StackOverflow user, Dwight Crowe for his stellar post on how to get Tensorflow R0.9 working on a TX1. I'm literally just going to post his exact methodology. Here we need to make an edit so that the bazel build will recognize aarch64 as ARM Now it's time to compile bazel. Now we install tensorflow R0.9. Any higher than R0.9 and it requires bazel 0.3.0, which we didn't install. You will build tensorflow once and it will fail. But by building it with the failure, it gives you the bazel .cache dir you need to place updated config.guess and config.sub files necessary for the full installation. Download the proper config files and update the .cache dir Here is where things get a bit tricky. As Dwight suggests, you'll have to change a few files so that tensorflow compiles correctly. TX1 can't do fancy constructors in cwise_op_gpu_select.cu.cc or sparse_tensor_dense_matmul_op_gpu.cu.cc Running with CUDA 8.0 requires new macros for FP16. Dwight throws some thanks to Kashif/Mrry for pointing out the fix, so I'm throwing some thanks to whoever those people are as well. And lastly, ARM has no NUMA nodes so this needs to be added or you will get an immediate crash on starting tf.Session() So I ran into strange errors that were solved by accident. After running the above commands, bazel fails in weird places. Sometimes at a random op. Sometimes a 'cross_tool' error. Truth be told, I accidently reran the command with a different job number and the op that it had failed on previously ended up compiling just fine. And that was it. Just changing the job number. I switched between 3 and 4 a few times and it compiled just fine. Very weird. But whatever. It works. Just to verify it, repeated this process on a few devices and it always works. Now that Tensorflow is installed, remove bazel and all of bazel's caches that eat memory. All we need here are the image reading and displaying opts. Nothing else. So the compile is small and takes up minimal space. Head over to David Sandberg's tensorflow implementation of OpenFace and download the resnet model weights in the Pre-trained model section. Then download the dlib facedetector from dlib.net. BOOM! Finished with downloads and installation. Now it's time to build our embedded face detector. First thing we need to do is copy align_dlib.py from here and make some quick changes. In the 'stock' version, it looks for the 'biggest' bounding box and only processes that one. But we're going to augment it so that it will classify all bounding boxes that it finds; eg every face will classified rather than just the largest. Also, we're going to make another quick change to the face detector based on the issues from this thread, whereby the detector shears the faces and warps them slightly. It should be noted that while David Sandberg uses a version of MTCNN to detect faces, we have to use the augmented dlib version. This is done so that when the final detection system is running, the memory profile doesn't get out of wack and spontaneously kill our processes. By changing the face detector, there will be an effect on the overall detection accuracy of our system, but the difference will be minimal. The second thing we need to do is build a scanner to identify the faces you actually want to classify. One thing to note is that with the Jetson, using the camera with OpenCV can be tricky. We need to make sure open the video with this prompt in our call via OpenCV: "" Here's our script called scan.py. Most open source facial recognition libraries like OpenFace, home_surveillance, facenet, etc. use the model similar to the one outlined in the FaceNet paper written by Florian Schroff, Dmitry Kalenichenko, James Philbin. Here' we're no different and will be using the model that David Sandberg's facenet.uses. The model works to take an image of an individuals face and pass it through a network (the model David uses is a variant of Inception-Resnet). The goal is to make the network embed the image in a feature space so that the squared distance between images of the same identity is small and the squared distance between a pair of images from different identities is large. This is done using something called a Triplet Loss. It's probably the one of the single-most important feature of the model's structure. Rather than break down the entire model, I just want to mention what made this model stick out for me: The goal here is to promote an embedding scheme that enforces a margin between each face pair of one identity to that of all other identities. In order to ensure that the network learns properly, triplets are selected in such a way that during the forward pass, negative samples are selected in an online fashion from the current minibatch. The authors note that selecting very distant negatives can lead to a bad local minima early on, so they instead select negatives so that their distance is further away from the image's embedding than the positive example, but are still meaningful because the squared distance is close to the anchor positive distance. Thus resulting in negatives that lie inside the margin alpha and help avoid a collapsed model. TL;DR: Read the paper. It's worth it. After you've scanned the faces you want to via the TX1's camera, we're going to want to put something together to actually classify faces. This script takes on concepts from openface's web-demo as well as facenet's validate_on_lfw.py. So what's going on here? Welp, we want to train a model from all images within our training set from scan.py, then use the facenet model to build a representation of each image. After each image has been processed via the network, we train an SVM on their representations and we teach it to classify a person's processed image correctly. That trained SVM is then used to classify all the faces that the camera sees. Boomshakalaka. And there you have it. A few simple scripts, and you have an embedded detector up and running. Because everyone likes a demo in camera vertical (...a little lag due to tunneling X over ssh)