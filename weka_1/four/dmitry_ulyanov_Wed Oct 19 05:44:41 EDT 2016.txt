This is a multicore modification of Barnes-Hut t-SNE by L. Van der Maaten with python and Torch CFFI-based wrappers. This code also works faster than sklearn.TSNE on 1 core. Barnes-Hut t-SNE is done in two steps. First step: an efficient data structure for nearest neighbours search is built and used to compute probabilities. This can be done in parallel for each point in the dataset, this is why we can expect a good speed-up by using more cores. Second step: the embedding is optimized using gradient descent. This part is essentially consecutive so we can only optimize within iteration. In fact some parts can be parallelized effectively, but not all of them a parallelized for now. That is why second step speed-up will not be that significant as first step sepeed-up but there is still room for improvement. So when can you benefit from parallelization? It is almost true, that the second step computation time is constant of and depends mostly on . The first part's time depends on a lot, so for small , for large . As we are only good at parallelizing step 1 we will benefit most when is large enough (MNIST's is large, even for is not so much). I wrote multicore modification originally for Springleaf competition, where my data table was about and only several days left till the end of the competition so any speed-up was handy. Interestingly, that this code beats other implementations. We compare to (Barnes-Hut of course), L. Van der Maaten's bhtsne, py_bh_tsne repo (cython wrapper for bhtsne with QuadTree). for every run. In fact py_bh_tsne repo works at the same speed as this code when using more optimization flags for compiler. This is a benchmark for MNIST data: I did my best to find what is wrong with sklearn numbers, but it is the best benchmark I could do (you can find test script in folder). This table shows a relative to 1 core speed-up when using cores. Python and torch wrappers are available. Make sure is installed on your system and install python prerequisites: Pip installation does not copy file for some reason (experts wanted). Tested with both Python 2.7 and 3.5 (conda) and Ubuntu 14.04. Never tested on MacOS, something similar to this should be done for successful compilation. Also read this issue. You can use it as a drop-in replacement for sklearn.manifold.TSNE. Please refer to sklearn TSNE manual for parameters explanation. Only double arrays are supported for now. For this implementation is fixed to , which is the most common case (use Barnes-Hut t-SNE or sklearn otherwise). Also note that some of the parameters will be ignored for sklearn compatibility. Only these parameters are used (and they are the most important ones): You can test it on MNIST dataset with the following command: To make the computation log visible in jupyter please install () and execute this line in any cell beforehand: Memory leakages are possible if you interrupt the process. Should be OK if you let it run until the end. To install execute the following command from repository folder: You can run t-SNE like that: type only supported for now. Please cite this repository if it was useful for your research: Of course, do not forget to cite L. Van der Maaten's paper