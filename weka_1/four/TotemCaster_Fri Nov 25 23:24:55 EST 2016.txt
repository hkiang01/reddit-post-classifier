Torch implementation for learning a mapping from input images to output images, for example: On some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). However, for harder problems it may be important to train on far larger datasets, and for many hours or even days. The test results will be saved to an html file here: . Switch to to train translation in opposite direction. Models are saved to (can be changed by passing in train.lua). See in train.lua for additional training options. This will run the model named in direction on all images in . Result images, and a webpage to view them, are saved to (can be changed by passing in test.lua). See in test.lua for additional testing options. Download the datasets using the following script (more datasets are coming soon!): We require training data in the form of pairs of images {A,B}, where A and B are two different depicitions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A: Create folder with subfolders and . and should each have their own subfolders , , , etc. In , put training images in style A. In , put the corresponding images in style B. Repeat same for other data splits (, , etc). Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g. is considered to correspond to . Once the data is formatted this way, call: This will combine each pair of images (A,B) into a single image file, ready for training. Optionally, for displaying images during training and test, use the display package. By default, the server listens on localhost. Pass to allow external connections on any interface: Then open in your browser to load the remote desktop. If you use this code for your research, please cite our paper Image-to-Image Translation Using Conditional Adversarial Networks: Code borrows heavily from DCGAN. The data loader is modified from DCGAN and Context-Encoder.