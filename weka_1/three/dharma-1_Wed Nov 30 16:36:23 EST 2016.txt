 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.06440 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs stat stat.ML References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Pavlo Molchanov Stephen Tyree Tero Karras Timo Aila Jan Kautz Bookmark (what is this?) Computer Science > Learning Title: Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning Authors: Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz (Submitted on 19 Nov 2016) Abstract: We propose a new framework for pruning convolutional kernels in neural networks to enable efficient inference, focusing on transfer learning where large and potentially unwieldy pretrained networks are adapted to specialized tasks. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on an efficient first-order Taylor expansion to approximate the absolute change in training cost induced by pruning a network component. After normalization, the proposed criterion scales appropriately across all layers of a deep CNN, eliminating the need for per-layer sensitivity analysis. The proposed criterion demonstrates superior performance compared to other criteria, such as the norm of kernel weights or average feature map activation. Comments: 5 pages, 8 figures, NIPS workshop: The 1st International Workshop on Efficient Methods for Deep Neural Networks Subjects: Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1611.06440 [cs.LG]   (or arXiv:1611.06440v1 [cs.LG] for this version) Submission history From: Pavlo Molchanov [view email] [v1] Sat, 19 Nov 2016 22:48:30 GMT (530kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 