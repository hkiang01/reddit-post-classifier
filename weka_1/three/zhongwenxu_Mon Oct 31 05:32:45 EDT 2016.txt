 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.09027 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Jack W. Rae Jonathan J. Hunt Tim Harley Ivo Danihelka Andrew W. Senior ... Bookmark (what is this?) Computer Science > Learning Title: Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes Authors: Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, Timothy P Lillicrap (Submitted on 27 Oct 2016) Abstract: Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer. Comments: in 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain Subjects: Learning (cs.LG) Cite as: arXiv:1610.09027 [cs.LG]   (or arXiv:1610.09027v1 [cs.LG] for this version) Submission history From: Jonathan Hunt [view email] [v1] Thu, 27 Oct 2016 22:38:05 GMT (2240kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 