Agent interacts with the environment and learns through trial-and-error Agent affects its own observations (no i.i.d.) Learn a policy $\pi$ that maps states to actions to maximise $\mathbb{E}[R]$ (allows bootstrapping instead of Monte Carlo estimates) If we had $Q^*$, $\pi^*(\mathbf{s}_t) = \arg\!\max\limits_{\mathbf{a}}Q^*(\mathbf{s}_t, \mathbf{a})$ Therefore, $Q^\pi$ can be improved by bootstrapping Learn from experience: $Q'(\mathbf{s}_t, \mathbf{a}_t) = Q(\mathbf{s}_t, \mathbf{a}_t) + \alpha \delta$,where $\alpha$ is the learning rate and $\delta$ is the TD-error $Y$ is reward received + discounted max Q-value of next state Loss is Mean Squared Error (over batch): $\mathcal{L}(\delta) = \frac{1}{N}\sum\limits_{n=1}^{N}(\delta_n)^2$ DL Note: RL updates are usually formulated for gradient ascent