 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.06080 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression Authors: Quang Minh Hoang, Trong Nghia Hoang, Kian Hsiang Low (Submitted on 18 Nov 2016) Abstract: While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models. Comments: 31st AAAI Conference on Artificial Intelligence (AAAI 2017), Extended version with proofs, 11 pages Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1611.06080 [stat.ML]   (or arXiv:1611.06080v1 [stat.ML] for this version) Submission history From: Kian Hsiang Low [view email] [v1] Fri, 18 Nov 2016 14:00:48 GMT (111kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 