 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.06258 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG cs.NE stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Using Fast Weights to Attend to the Recent Past Authors: Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu (Submitted on 20 Oct 2016 (v1), last revised 27 Oct 2016 (this version, v2)) Abstract: Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. Subjects: Machine Learning (stat.ML); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.06258 [stat.ML]   (or arXiv:1610.06258v2 [stat.ML] for this version) Submission history From: Jimmy Ba [view email] [v1] Thu, 20 Oct 2016 01:03:20 GMT (153kb,D) [v2] Thu, 27 Oct 2016 19:53:07 GMT (154kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 