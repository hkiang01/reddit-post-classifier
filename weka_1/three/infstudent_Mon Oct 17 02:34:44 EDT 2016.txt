 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.04161 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.NE References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Shiyu Liang R. Srikant Bookmark (what is this?) Computer Science > Learning Title: Why Deep Neural Networks? Authors: Shiyu Liang, R. Srikant (Submitted on 13 Oct 2016) Abstract: Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\varepsilon$) require $\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\varepsilon$) require $\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a "small" number of neurons. Subjects: Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.04161 [cs.LG]   (or arXiv:1610.04161v1 [cs.LG] for this version) Submission history From: Shiyu Liang [view email] [v1] Thu, 13 Oct 2016 16:34:30 GMT (340kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 