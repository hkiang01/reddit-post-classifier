 Texture Enhancement via High-Resolution Style Transfer for Single-Image Super-Resolution Il Jun Ahn† and Woo Hyun Nam†* Abstract — Recently, various deep-neural-network (DNN)- based approaches have been proposed for single-image super- resolution (SISR). Despite their promising results on major structure regions such as edges and lines, they still suffer from limited performance on texture regions that consist of very complex and fine patterns. This is because, during the acquisition of a low-resolution (LR) image via down-sampling, these regions lose most of the high frequency information necessary to represent the texture details. In this paper, we present a novel texture enhancement framework for SISR to effectively improve the spatial resolution in the texture regions as well as edges and lines. We call our method, high- resolution (HR) style transfer algorithm. Our framework consists of three steps: (i) generate an initial HR image from an interpolated LR image via an SISR algorithm, (ii) generate an HR style image from the initial HR image via down-scaling and tiling, and (iii) combine the HR style image with the initial HR image via a customized style transfer algorithm. Here, the HR style image is obtained by down-scaling the initial HR image and then repetitively tiling it into an image of the same size as the HR image. This down-scaling and tiling process comes from the idea that texture regions are often composed of small regions that similar in appearance albeit sometimes different in scale. This process creates an HR style image that is rich in details, which can be used to restore high-frequency texture details back into the initial HR image via the style transfer algorithm. Experimental results on a number of texture datasets show that our proposed HR style transfer algorithm provides more visually pleasing results compared with competitive methods.1 Index Terms — Single-image super-resolution, texture enhancement, high-resolution style, style transfer, down-scaling and tiling. I. INTRODUCTION The aim of single-image super-resolution (SISR) algorithm is to recover a high-resolution (HR) image from a single low- resolution (LR) image [1]. Although the SISR problem inherently ill-posed, many valuable algorithms have been presented for computer vision and image processing applications such as surveillance imaging, medical imaging, or 1 The authors thank to Mr. Kiheum Cho, Dr. Yongsup Park, and Ms. Tammy Lee in the Digital Media & Communications R&D Center, Samsung Electronics, Seoul, Korea for the helpful discussions and collaboration. Il Jun Ahn and Woo Hyun Nam are with the Digital Media & Communications R&D Center, Samsung Electronics, Seoul, Korea. (*Corresponding author: W. H. Nam, E-mail: woohyun.nam@samsung.com) †Both authors contributed equally to this work. (a) Original (b) Bicubic (c) VDSR (d) HRST (proposed) Fig. 1. Our high-resolution style transfer (HRST) based SISR method compares favorably with a representative related work [19] on texture region (up-sampling factor is 4.). ultra-high-definition (UHD) image generation where more image details are required. Early methods include simple and fast interpolation-based scheme with bicubic or Lanzcos filter [2]. For better performance, more advanced schemes using statistical image priors [3]-[7] or internal patch recurrence [8], [9] were also introduced. Meanwhile, sophisticated machine learning based schemes have been widely used to learn the relationship from LR to HR patches. Neighborhood embedding approaches [10], [11] up- sample a given LR image patch by finding similar LR training patches in a low dimensional manifold and combining their corresponding HR patches for reconstruction. Sparse-coding (or dictionary learning) approaches [12]-[14] use a learned compact dictionary on the basis that natural patches can be represented using sparse activations of dictionary atoms. Random forests approaches [15] directly formulate SISR as a regression problem, which can avoid complex and time- consuming training of a sparse dictionary. Recently, various deep learning-based approaches via convolutional neural networks (CNN) were proposed with excellent performance. Dong et al. [16], [17] showed that CNN could be successfully applied for SISR. This CNN method, which we call SRCNN, used a three layer convolutional network and trained in an end-to-end manner to learn a mapping from interpolated LR image to original HR mailto:woohyun.nam@samsung.com (c) Original x0.2x0.4x0.6x0.8x1.0 (b) Down-scaling and tiling results(a) Initial HR Fig. 2. (a) Initial HR image enhanced from the interpolated LR image via a SISR, (b) various results obtained by down-scaling and tiling the initial HR image based on scaling factors ranging from 0.2 to 0.8, and (c) original HR image. In terms of texture detail representation, the result with scaling factor 0.4 is most similar to the original image. The yellow boxes in (b) denote down-scaled ones of initial HR image. image. To further improve the performance on both accuracy and speed, the authors extended their work to enable the network to learn the mapping from LR to HR image directly, rather than from the interpolated LR image [18]. Since up- sampling is only performed in the last layer of the network, the method can avoid expensive computations in the HR dimension. Kim et al. [19] presented a highly performant architecture which consists of very deep convolutional network of 20 layers. Since the deep networks lead to enlargement of receptive fields that can take a large image context into account, the method achieved state-of-the art performance with a large margin. They also presented a novel residual learning approach and showed that it is more favorable in training the deep layers than a non-residual based one. Meanwhile, to reduce the number of convolutional parameters while keeping the large receptive fields, the authors proposed a different architecture based on deeply recursive convolutional network [20], which showed comparable performance to [19]. Despite the promising results of the recent SISR algorithms, compared with original HR image, they still show overly smoothed results and/or lack of high-frequency details, especially on texture regions (see Fig. 1(a)-(c).). In an attempt to resolve this problem, Johnson et al. [21] suggested using perceptual loss, instead of conventional mean squared reconstruction error (MSE), and Ledig et al. [22] proposed a notable SR framework combined with generative adversarial network (SRGAN). Even though quantitative performance of these methods, such as peak signal-to-noise-ratio (PSNR) or structural similarity (SSIM), is inferior to competitive methods, they delivered visually improved HR images. Meanwhile, Gatys el al. [23] presented a very interesting approach on artistic image generation, called style transfer algorithm. Based on high-level feature maps extracted via pre- trained VGG networks [24], this algorithm synthesizes a style of an artwork to a content image, of an arbitrary photograph, while preserving the structure of the content image. Inspired by this artistic image generation, we were wondering if we might apply this style transfer algorithm to generate the texture-enhanced image. In other words, if we can obtain a satisfactory level of HR texture image, even if the image is different from the original HR image, we can regard the obtained image as a style image and combine it with the input content image to generate a texture-enhanced HR image. Based on this motivation, we present a novel texture enhancement framework for SISR. We call our proposed method, HR style transfer (HRST) algorithm. As shown in Fig.1, our proposed HRST algorithm provides more visually pleasing results compared to the representative state-of-the-art SISR method [19]. The remainder of this paper is organized as follows. We introduce the observation of our method in Section II. Section III describes the proposed HRST-based SR framework in detail. In Section IV, we provide experimental results with qualitative and quantitative analyses on 100 texture images. We then discuss the robustness of the proposed method and the detailed method for 4K image SR in Section V. Finally, we draw the conclusion in Section VI. II. OBSERVATION It is a very challenging problem to recover the finer details of texture regions when we super-resolve at a large up- sampling factor (over × 4). As shown in Fig. 1(a)-(c), the current algorithm only sharpens the lines and edges present in the interpolated LR image. However, it cannot effectively restore the high-frequency details lost by the down-sampling used to create the LR image. To address this issue, we use the observation that texture regions tend to be comprised of visually very similar patches of various sizes. Based on this idea, we down-scale and repetitively tile the input image. This process creates a texture map we call an HR style image that is very similar in feel to the original HR image (See Fig. 2.). Here, to better correlate the image obtained via down-scaling and tiling with the original HR image, SR version of the interpolated LR image via a SISR method, namely initial HR image, may be utilized as an input image, instead of the interpolated LR image itself. Then, we take the HR style image and combine it feature- wise with the initial HR image to generate a texture-enhanced HR image. This is different from a simple texture mapping process which just overlays one image onto a different image. Instead, our method searches both the initial HR image and the HR style image from low-level to high-level feature space for similar features. If matches are found, it strengthens them. These features are similar in terms of the correlation in feature space that is invariant to the spatial location, scale or rotation. Convolution Pooling … Convolution Pooling … 3. Texture enhanced HR image generation via style transferInitial enhancement Scaling Tiling Interpolated LR image HR styleInitial HR image Final HR image 2. HR style generation 1. Initial enhancement Content analysis Style analysis L-BFGS Convolution Pooling … Scaling factor estimation Convolution … Fig. 3. Overall diagram of the proposed texture enhancement framework. III. PROPOSED ALGORITHM Based on the observation detailed above, we propose a texture enhancement framework for SISR, HRST algorithm. As shown in Fig. 3, the proposed framework consists of three steps: (i) initial enhancement to generate an initial HR image from an interpolated LR image via an SISR algorithm, (ii) HR style image generation via down-scaling and tiling, and (iii) texture enhanced HR image generation by combining the HR style image with the initial HR image via a customized style transfer algorithm. In the initial enhancement step, an existing state-of-the art SISR algorithm [19] is adopted to obtain the best initial HR image. The detail procedures based on the interpolated LR and initial HR images are presented in the following subsections. A. HR Style Generation As shown in Fig. 3, the HR style image is generated by down-scaling the initial HR image and then repetitively tiling it into an image of the same size as the HR image. In the down-scaling part, scaling factor selection is important because the scaling factor is closely related to how much the image should be improved. In general, the smaller scaling factor leads to the more HR details. However, if the selected scaling factor is enough small to exceed the detail representation of the original HR image, it may cause unnatural or overly enhanced results. We thus introduce the process of deriving a formula that determines the proper scaling factor. In this process, we utilize 100 original HR images of 256 × 256 pixels, which are randomly cropped from 20 4K texture images, as a training image set. For each HR image, by applying the bicubic down- sampling with a factor of 0.25, we obtain a LR image. Based on the obtained LR image, we first prepare a bicubic- interpolated LR image and an initial HR image via the SISR method [19]. Using the scale factors in the range of 0.2 to 0.9 with step size of 0.025, we generate a number of HR style images. We then obtain the final HR images corresponding to each scaling factor via a style transfer. By comparing the final HR images with the original HR image, we find the optimal scaling factor that provides the best matched result in terms of texture detail representation. For this comparison, we adopt the mean mutual information (MMI) [25], [26] to quantify a structural complexity of the texture image. The MMI of the image was calculated as BNlog3 JEME4MMI −= , (1) where JE represents the joint entropy among neighbor pixels, and ME represents the marginal entropy of the joint distribution. NB is the number of histogram bins. The MMI has unity value for uniform patterns and is zero for random ones. Based on the comparison results, we visualize the tendency of the selected scaling factors, ϕ depending on the MMI difference between the original and the initial HR image, δ in Fig. 4(a). It should be noted that if δ gets larger, the lower scaling factor is selected as optimum, and the relationship is nearly inverse-linear. During the SR process, since we usually cannot use the original HR image, to estimate the ϕ without using the original HR image, we further investigate the correlation between δ and ∆ , the MMI improvement from the interpolated LR to the initial HR image. As shown in Fig. 4(b), since the relationship can be approximately modeled linearly, we can y = 2.327x + 0.1009 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 y = -1.988x + 0.993 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 | MMI (Initial HR) - MMI (Interpolated LR) |, ∆ | M M I ( O rig in al H R ) – M M I ( In iti al H R ) | , δ Se le ct ed o pt im al s ca lin g fa ct or , ϕ (a) (b) | MMI (Original HR) – MMI (Initial HR) |, δ 1009.0327.2 +∆−=δ 993.0988.1 +δ−=ϕ Fig.4. (a) Optimal scaling factors depending on the MMI deference between the original HR and the initial HR images, and (b) correlation between MMI improvement from the interpolated LR to Initial HR image and the MMI difference in (a). indirectly determine the ϕ based on the ∆ by combining two linear relationships, this is summarized as, 792.0626.4 +∆−=ϕ . (2) To obtain the scaling factor with a unit of 0.025, (2) is reformulated as below, 025.0 025.0 0125.0ˆ ×    +ϕ≡ϕ , (3) where ϕ̂ represents the selected optimal scaling factor. Here,   denotes a floor operation. Note that since ∆ can vary depending on the problem condition such as up-sampling factor, or initial enhancement method etc., (2) and (3) should be recalculated if the problem condition is changed. The down-scaled image generated by the selected scaling factor is then extended to the HR image size via tiling process. In this process, to avoid an abrupt discontinuity at the tiling boundary, a boundary filtering or mirroring scheme could be applied. B. Texture Enhanced HR Image Generation By using the HR style obtained in subsection III.A, we generate the final HR image with improved HR texture details, while maintaining the global characteristics of the initial HR image such as location and shape of major structures. To realize this, we adopt the style transfer algorithm [23], and customize it for better performance. In the customized style transfer algorithm, we mainly perform two adjustments: (i) increase the number of layers used for content loss calculation, and (ii) utilize the initial HR image as an initial estimate for the final HR image. As in [23], we performs the joint minimization of style and content losses in feature space to obtain the final HR image, x̂ by combining the HR style image with the initial HR image, which can be written as, ( ) ( )cxsxx x ,,minˆ contentstyle LL β+α= . (4) Here, α and β are the weighting factors of style and content losses, styleL and contentL , respectively. s and c are the HR style and the content image respectively for styleL and contentL calculation. x denotes the intermediate result image for the final HR image. For clear description of styleL and contentL in feature space, we define the feature maps in the l-th layer from an image, z as ll NCl RF ×∈z . Here, Cl is the number of feature maps and N l is the size of a feature map. l ikF ,z (or l jkF ,z ) denotes the i-th (or j- th) feature map at a pixel position k for image z. To extract the feature maps, we utilize the pre-trained VGG-16 network [24], which consists of 13 convolutional and 5 pooling layers. In styleL , to analyze the style of z in feature space, we utilize the Gram matrix, which can measure correlations between two arbitrary feature maps at a certain layer, written as ∑= k l jk l ik l ij FFG ,,, zzz . (5) To force the Gram matrix of x, l ijG ,x similar to that of s, l ijG ,s , the energy functional for the l-th layer can be formulated as below, ( )∑ −= ji l ij l ij ll l GGNC E , 2 ,,22style, 4 1 sx . (6) Since style,lE compares l ijG ,x and l ijG ,s , unlike l ikF ,x and l ikF ,s , it can allow l ikF ,x (or l jkF ,x ) to be different to l ikF ,s (or l jkF ,s ), while making only the correlation between l ikF ,x and l jkF ,x similar to that between l ikF ,s and l jkF ,s . This property can be advantageously exploited in a way that the HR style image obtained through the down-scaling and tiling process is not spatially consistent with the original HR image. Since style,lE does not make l ikF ,x (or l jkF ,x ) similar to l ikF ,s (or l jkF ,s ), it can implicitly prevent to transfer spatially-corresponding but unwanted image patterns of the s to the x. Instead, the style,lE (a) Original (b) Bicubic (e) VDSR (f) HRST (c) SRCNN (d) SCN (g) HR style (Scaling factor of 0.4) PSNR/ SSIM/ ∆BRISQUE 21.495/ 0.2758/ 27.019 21.664/ 0.3321/ 20.727 21.640/ 0.3365/ 17.386 21.691/ 0.3369/ 19.627 18.034/ 0.1074/ 10.792 Fig. 5. SR results on a grassplot image with up-sampling factor of 4. (a) The original image, and resultant images (b-g) via (b) bicubic interpolation, (c) Dong, et al.’s [16], [17], (d) Wang, et al. [30].’s, (e) Kim et al.’s [19], (f) the proposed HRST based SR algorithm. (g) is the HR style image at a scaling factor of 0.4 used for (f). (a) Original (b) Bicubic (e) VDSR (c) SRCNN (d) SCN (g) HR style (Scaling factor of 0.5) PSNR/ SSIM/ ∆BRISQUE 22.430/ 0.3854/ 46.326 22.713/ 0.4455/ 37.712 22.697/ 0.4509/ 35.624 22.771/ 0.4520/ 36.497 (f) HRST 19.319/ 0.2614/ 0.545 Fig. 6. SR results on a forest image with up-sampling factor of 4. (a) The original image, and resultant images (b-g) via (b) bicubic interpolation, (c) Dong, et al.’s [16], [17], (d) Wang, et al. [30].’s, (e) Kim et al.’s [19], (f) the proposed HRST based SR algorithm. (g) the HR style image at a scaling factor of 0.5 used for (f). makes it possible to enhance the corresponding features of x if only similar correlations in feature space, irrespective of spatial location, scale or rotation, are found in both x and s. To reflect all the texture information of the s from low-level to high-level feature space, we measure the style,lE for each layer, and take the weighted summation of the energy functionals for styleL . This is summarized as, ( ) ∑ Ω∈ = style style,style,style , l ll EwL sx . (7) Here, wl,style is the weight factor to adjust contribution of each layer to the style loss, and styleΩ is set of layers for style loss calculation. Meanwhile, to explicitly prevent that x becomes quite different from c, which is the initial HR image, the energy (a) Original (b) Bicubic (e) VDSR (f) HRST (c) SRCNN (d) SCN (g) HR style (Scaling factor of 0.6) 24.019/ 0.6402/ 55.675 24.827/ 0.7036/ 44.498 25.094/ 0.7147/ 40.106 25.280/ 0.7253/ 41.011 22.038/ 0.6209/ 23.099 PSNR/ SSIM/ ∆BRISQUE Fig. 7. SR results on a snow mountain image with up-sampling factor of 4. (a) The original image, and resultant images (b-g) via (b) bicubic interpolation, (c) Dong, et al.’s [16], [17], (d) Wang, et al. [30].’s, (e) Kim et al.’s [19], (f) the proposed HRST based SR algorithm. (g) the HR style image at a scaling factor of 0.6 used for (f). (a) Original (b) Bicubic (e) VDSR (f) HRST (c) SRCNN (d) SCN (f) HR style (Scaling factor of 0.7) 34.324/ 0.8811/ 31.232 35.559/ 0.9070/ 24.058 35.786/ 0.9111/ 22.152 35.833/ 0.9109/ 21.818 31.429/ 0.8279/ 12.556 PSNR/ SSIM/ ∆BRISQUE Fig. 8. SR results on a cloud image with up-sampling factor of 4. (a) The original image, and resultant images (b-g) via (b) bicubic interpolation, (c) Dong, et al.’s [16], [17], (d) Wang, et al. [30].’s, (e) Kim et al.’s [19], (f) the proposed HRST based SR algorithm. (g) the HR style image at a scaling factor of 0.7 used for (f). functional for l-th layer can be formulated as below, ( )∑ −= ki l ik l ikl FFE , 2 ,,content, 2 1 cx , (8) Here, styleΩ denotes set of layers for content loss calculation. To extend the constraints to all ranges from mid-level to high-level feature space, instead of using (8) for contentL as in [23], we define contentL as below, ( ) ∑ Ω∈ = content content,content,content ;, l ll EwlL hx . (9) Here, wl,content is the weight factor to adjust contribution of each layer to the content loss, and contentΩ is set of layers for content loss calculation. To find the optimum solution, x̂ in (4), we adopt a representative gradient-based optimization, L-BFGS method [27], which provides the solution with a fast convergence. In this method, the gradient of (4) with respect to x can be determined based on x∂∂ /styleL and x∂∂ /contentL obtained via a standard error back-propagation [28]. Meanwhile, it should be emphasized that we initialize x with c, instead of Gaussian random noise that is used in the existing style transfer algorithm [23]. This is an important detail not only to prevent the final HR image from being generated differently each time but also to help to preserve the original structure better. Furthermore, it leads to fast convergence. IV. EXPERIMENTAL RESULTS We first describe the parameter settings used to obtain the proposed results. The results are obtained by utilizing the style feature maps on 5 convolutional layers, ‘conv1’, ‘conv3’, ‘conv5’, ‘conv8’, and ‘conv11’ (wl,style = 1/5 for those layers), while utilizing the content feature maps on 3 convolutional layers, ‘conv7’, ‘conv10’, and ‘conv13’ (wl,content = 1/3 for those layers). Using intermediate and higher level layers for the content feature maps helps to maintain the global and apparent structures, while allowing fine structures to be enhanced by the style feature map. The ratio βα / and the number of iterations are set to 1× 104 and 300, respectively. To verify the performance of the proposed texture enhancement algorithm, we prepared 100 texture images. These images were cropped to a size of 256× 256 pixels from 4K images. The scaling factors determined by (3) ranged from 0.4 to 0.75. We should mention that the images in this section are not same to those used in the subsection III.A. All experiments are performed with a down- sampling factor of 4. For performance comparison, we emphasize that the goal of this work is not to replicate the results of state-of-the-art PSNR or SSIM, but instead to demonstrate the perceptually improved visual quality. To quantify the visual improvement, we measure the difference of BRISQUE [29] metric compared with original HR image, notated as ∆ BRISQUE, the metric, which is known to have a high correlation with human subjective evaluation. We compared the performance of the proposed HRST method to the bicubic-interpolation and the three different methods: the super-resolution CNN (SRCNN) [18], deep networks for super-resolution with sparse prior (SCN) [30], and very deep CNN-based super-resolution (VDSR) [19], which are currently the best performing CNN-based approaches, among the algorithms that have publically released the available code. In addition, for reference, we show the generated HR style images used for the proposed HRST. Visual comparison of the super-resolved images is given in Figs. 5-9. For the images, the selected scaling factors were 0.4, 0.5, 0.6, and 0.7, respectively. We can note that the existing SISR algorithms poorly restore fine and detail textures, and TABLE I QUANTITATIVE EVALUATION RESULTS ON AVERAGE FOR 100 TEXTURE IMAGES. THE NUMBER IN PARENTHESES DENOTES THE STANDARD DEVIATION, AND BOLD INDICATES THE BEST RESULT. Selected Scaling factor Methods PSNR SSIM ∆BRISQUE [x0.4, x0.475] (13 images) Bicubic 21.63 (3.45) 0.3787 (0.1161) 27.63 (16.80) SRCNN 21.91 (3.49) 0.4371 (0.1097) 22.75 (13.96) SCN 21.90 (3.49) 0.4407 (0.1104) 23.90 (12.15) VDSR 21.96 (3.50) 0.4435 (0.1103) 24.19 (13.23) HRST 19.14 (3.02) 0.2803 (0.0890) 13.26 (12.82) [x0.5, x0.575] (23 images) Bicubic 20.76 (2.01) 0.4122 (0.0796) 39.15 (7.86) SRCNN 21.19 (2.08) 0.4827 (0.0810) 31.24 (9.88) SCN 21.19 (2.11) 0.4871 (0.0817) 33.47 (5.39) VDSR 21.28 (2.13) 0.4919 (0.0828) 33.24 (10.91) HRST 18.46 (1.75) 0.3266 (0.0718) 10.50 (9.76) [x0.6, x0.675] (40 images) Bicubic 21.29 (2.66) 0.4551 (0.0941) 41.14 (9.04) SRCNN 21.82 (2.72) 0.5320 (0.0868) 32.23 (8.80) SCN 21.83 (2.73) 0.5378 (0.0882) 37.40 (14.31) VDSR 21.94 (2.75) 0.5443 (0.0888) 44.11 (20.02) HRST 19.46 (2.43) 0.4024 (0.0938) 15.35 (8.91) [x0.7, x0.75] (24 images) Bicubic 23.01 (4.57) 0.5843 (0.1314) 36.16 (13.27) SRCNN 24.03 (4.62) 0.6652 (0.1106) 31.14 (8.35) SCN 24.18 (4.71) 0.6752 (0.1087) 35.36 (13.02) VDSR 24.33 (4.62) 0.6852 (0.1061) 47.64 (23.74) HRST 21.42 (3.56) 0.58044 (0.1007) 16.33 (8.31) generally provide overly-smoothed results, although they successfully enhance coarse and apparent structures. In contrast, the proposed algorithm provides finer and sharper texture representations without introducing noticeable artifacts. (a) Original (b) x0.65 (c) x0.675 (d) x0.7 (e) x0.725 (f) x0.75 Fig. 9. Comparison of HRST results with respect to the scaling factor variation ranging from 0.65 to 0.75 with a step size of 0.025. In addition to the subjective visual comparison, a quantitative comparison results performed using the metrics of PSNR, SSIM, and ∆ BRISQUE, are summarized in Table I. We note that, the proposed method outperforms all existing SISR algorithms with a large margin in terms of ∆ BRISQUE, which has an important role in perceptual image quality measurement, although it provides inferior performance in terms of PSNR and SSIM. V. DISCUSSIONS The scaling factor for HR style may be critical in the proposed framework. To find the optimal scale factor, we introduced the scaling factor estimation scheme in subsection III.A. In the scheme, we approximate the correlation between the optimal scaling factor and the MMI improvement from the interpolated LR to the initial HR image with a simple linear model. This approximation may induce some estimation error; however, the scaling factor is robust to slight variations. In other words, although the final HR result may become somewhat different, it is within the observer’s preference as shown in Fig. 9. Due to the increasing use of 4K (or UHD) TV, the importance of SR for 4K images is emerging. We thus describe how to apply the proposed algorithm to real 4K images. To alleviate the hardware burden we divide a 4K image into 46× 25 patches with size of 240× 240. Here, we make the patches overlapped with 30% to prevent the abrupt change in the down-scaling factor. We then obtain texture enhanced HR patches independently by using the proposed HRST algorithm. Finally, we complete the final 4K image by sticking the patches with blending the overlap of each patch. As shown in Figs 10 and 11, the proposed algorithm provides vivid and realistic 4K image with finer and shaper texture details without undesirable artifacts compared to the existing SR algorithms. As shown in the evaluation results using PSNR and SSIM in Table I, the proposed HRST needs to be further improved to achieve perfect restoration of the original image. Nonetheless, the HRST can constrain such that the generated image retains the global structures of the content image. It thereby delivers a (b) Original (c) Bicubic (e) SCN (g) HRST (d) SRCNN (f) VDSR (a) Proposed SR result via HRST from quarter HD to 4K image resolution Fig. 10. SR results from quarter HD to 4K image resolution: Palm tree mage. The red box denotes the cropped position of the magnified images. sufficiently plausible image, although it may be somewhat different from the original image. Since the current HRST framework is based on external optimization process, it can take up to few hours for a 4K image SR on a NVIDIA K40 GPU environment. To improve the practical applicability of the HRST, we will extend the current HRST framework to be end-to-end trainable instead of being externally optimized. We will also improve the framework to be applicable to video sequences as a future work. VI. CONCLUSION In this paper, we present a novel texture enhancement framework for SISR via HR style transfer algorithm. We effectively improve the spatial resolution on the texture regions as well as edge and line regions, which is yet unresolved by existing state-of-the-art SISR algorithms. For the texture enhancement, we first obtain an initial HR image from the interpolated LR image, and then generate the HR style image from the initial HR image via down-scaling and tiling process. By properly combining semantic information of both the HR style and the initial HR images via the customized style transfer algorithm, we finally generate the texture- enhanced HR image. Experimental results demonstrate that the proposed algorithm can provide realistic and more visually pleasing SR images with finer and sharper textures, compared to the existing SR algorithms, without introducing undesirable artifacts. (b) Original (c) Bicubic (e) SCN (d) SRCNN (f) VDSR (a) Proposed SR result via HRST from quarter HD to 4K image resolution (g) HRST Fig. 11. SR results from quarter HD to 4K image resolution: Rock mountain image. The red box denotes the cropped position of the magnified images. REFERENCES [1] G. Freedman and R. Fattal, “Image and video upscaling from local self- examples,” ACM Trans. Graph., vol. 30, no.2, pp. 12.1-12.11, 2011. [2] C. E. Duchon, “Lanczos filtering in one and two dimensions,” J. Appl. Meteorol., vol. 18, no. 89, pp. 1016-1022, 1979. [3] D. Dai, R. Timofte, and L. Van Gool, “Jointly optimized regressors for image super-resolution,” Eurographics, vol. 34, no. 2, pp. 95-104, 2015. [4] W. T. Freeman, E. C. Pasztor, and O. T. Carmichael, “Learning low- level vision,” Int. J. Comput. Vis., vol. 40, no. 11, pp. 25-47, 2000. [5] S. Schulter, C. Leistner, and H. Bischof, “Fast and accurate image upscaling with super-resolution forests,” in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2015, pp. 3791-3799. [6] R. Timofte, V. De Smet, and L. Van Gool, “Anchored neighborhood regression for fast example-based super- resolution,” in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2013, pp. 1920-1927. [7] J. Yang, Z. Wang, Z. Lin, S. Cohen, and T. Huang, “Coupled dictionary training for image super-resolution,” IEEE Trans. Image Process., vol. 21, no. 11, pp. 3467-3478, 2012. [8] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a single image,” in Proc. IEEE Int. Conf. Comput. Vis., 2009, pp. 349-356. [9] C. Y. Yang, J. B. Huang, and M. H. Yang, “Exploiting self-similarities for single frame super-resolution,” in Proc. IEEE Asia Conf. Comput. Vis., 2010, pp. 497-510. [10] H. Chang, D.-Y. Yeung, and Y. Xiong, “Super-resolution through neighbor embedding, in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2004, pp. 275-282. [11] M. Bevilacqua, A. Roumy and M.-L. A. Morel. Low- complexity single- image super-resolution based on nonnegative neighbor embedding,” in proc. British Mach. Vis. Conf., 2012, pp. 135.1-135.10. [12] R. Timofte, V. De Smet, and L. Van Gool., “A+: Adjusted anchored neighborhood regression for fast super-resolution,” in Proc. IEEE Asia Conf. Comput. Vis., 2014, pp. 111-126. [13] J. Yang, J. Wright, T. S. Huang, and Y. Ma., “Image superresolution via sparse representation,” IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861-2873, 2010. [14] R. Zeyde, M. Elad, and M. Protter, “On single image scale-up using sparse-representations,” in proc. Int. Conf. Curves Surf., 2012, pp. 711- 730. [15] S. Schulter, C. Leistner, and H. Bischof, “Fast and accurate image upscaling with super-resolution forests,” in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2015, pp. 3791-3799. [16] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolutional network for image super-resolution,” in Proc. Eur. Conf. Comput. Vis., 2014, pp. 184-199. [17] C. Dong, C. C. Loy, K. He, and X. Tang, “Image superresolution using deep convolutional networks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2, pp. 295-307, 2015. [18] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang, “Real-Time Single Image and Video Super- Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,” in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2016, pp. 1874-1883. [19] J. Kim, K. Lee, and K. M. Lee, “Accurate image super-resolution using very deep convolutional networks,” in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2016, pp. 1646-1654. [20] J. Kim, K. Lee, and K. M. Lee. “Deeply-recursive convolutional network for image super-resolution,” arXiv preprint arXiv:1511.04491, 2015. [21] J. Johnson, A. Alahi, and F. Li, “Perceptual losses for real-time style transfer and super- resolution,” arXiv preprint arXiv:1603.08155, 2016. [22] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,” arXiv preprint arXiv:1609.04802, 2016. [23] L. A. Gays, A. S. Ecker, and M Bethge. “Image style transfer using convolutional neural networks, in proc. IEEE Conf. Comput. Vis. Pattern Recog., 2016, pp. 2414-2423. [24] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014. [25] R. Proulx, L. Parrott, “Measures of structural complexity in digital images for monitoring the ecological signature of an old-growth forest ecosystem,” Ecological Indicators, vol. 8, no. 3, pp. 270-284, 2008. [26] R. Wackerbauer, A. Witt, H. Atmanspacher, J. Kurths, and H. Scheingraber, “A comparative classification of complexity measures,” Chaos Soliton Frac., vol. 4, no. 1, pp. 133-173, 1994. [27] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal, “L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization,” ACM Trans. on Math. Softw., vol. 23, no. 4, pp. 550-560, 1997. [28] Y. A. LeCun, L. Bottou, G. B. Orr, and K. R. Muller, “Efficient backprop,” Neural networks: Tricks of the trade, 2012. [29] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality assessment in the spatial domain,” IEEE Trans. Image Process., vol. 21, no. 12, pp.4695-4708, 2012. [30] Z. Wang, D. Liu, J. Yang, W. Han, T. Huang, “Deep networks for image super-resolution with sparse prior,” in proc IEEE International Conference on Computer Vision , 2015, pp. 370-378. BIOGRAPHIES Il Jun Ahn received the Ph.D. degree in electrical engineering from the Korea Advanced Institute of Science and Technology (KAIST), Daejeon, in 2016. Since 2016, he has been with the Digital Media & Communications R&D Center, Samsung Electronics, where he is currently a Senior engineer. His research interests are digital image processing, video signal processing, and medical imaging. Woo Hyun Nam received the Ph.D. degree in electrical engineering from the Korea Advanced Institute of Science and Technology (KAIST), Daejeon, in 2013. Since 2013, he has been with the Digital Media & Communications R&D Center, Samsung Electronics, where he is currently a Senior engineer. His research interests are medical image processing and video signal processing. He is currently focused on deep learning based image/video visual quality improvement. I. Introduction II. Observation III. Proposed algorithm A. HR Style Generation B. Texture Enhanced HR Image Generation IV. Experimental results V. Discussions VI. Conclusion References 