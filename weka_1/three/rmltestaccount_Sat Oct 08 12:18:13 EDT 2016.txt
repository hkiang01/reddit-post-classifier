 Google Research Blog The latest news from Research at Google Equality of Opportunity in Machine Learning Friday, October 07, 2016 Posted by Moritz Hardt, Research Scientist, Google Brain Team As machine learning technology progresses rapidly, there is much interest in understanding its societal impact. A particularly successful branch of machine learning is supervised learning. With enough past data and computational resources, learning algorithms often produce surprisingly effective predictors of future events. To take one hypothetical example: an algorithm could, for example, be used to predict with high accuracy who will pay back their loan. Lenders might then use such a predictor as an aid in deciding who should receive a loan in the first place. Decisions based on machine learning can be both incredibly useful and have a profound impact on our lives. Even the best predictors make mistakes. Although machine learning aims to minimize the chance of a mistake, how do we prevent certain groups from experiencing a disproportionate share of these mistakes? Consider the case of a group that we have relatively little data on and whose characteristics differ from those of the general population in ways that are relevant to the prediction task. As prediction accuracy is generally correlated with the amount of data available for training, it is likely that incorrect predictions will be more common in this group. A predictor might, for example, end up flagging too many individuals in this group as ‘high risk of default’ even though they pay back their loan. When group membership coincides with a sensitive attribute, such as race, gender, disability, or religion, this situation can lead to unjust or prejudicial outcomes. Despite the need, a vetted methodology in machine learning for preventing this kind of discrimination based on sensitive attributes has been lacking. A naive approach might require a set of sensitive attributes to be removed from the data before doing anything else with it. This idea of “fairness through unawareness,” however, fails due to the existence of “redundant encodings.” Even if a particular attribute is not present in the data, combinations of other attributes can act as a proxy. Another common approach, called demographic parity, asks that the prediction must be uncorrelated with the sensitive attribute. This might sound intuitively desirable, but the outcome itself is often correlated with the sensitive attribute. For example, the incidence of heart failure is substantially more common in men than in women. When predicting such a medical condition, it is therefore neither realistic nor desirable to prevent all correlation between the predicted outcome and group membership. Equal Opportunity Taking these conceptual difficulties into account, we’ve proposed a methodology for measuring and preventing discrimination based on a set of sensitive attributes. Our framework not only helps to scrutinize predictors to discover possible concerns. We also show how to adjust a given predictor so as to strike a better tradeoff between classification accuracy and non-discrimination if need be. At the heart of our approach is the idea that individuals who qualify for a desirable outcome should have an equal chance of being correctly classified for this outcome. In our fictional loan example, it means the rate of ‘low risk’ predictions among people who actually pay back their loan should not depend on a sensitive attribute like race or gender. We call this principle equality of opportunity in supervised learning. When implemented, our framework also improves incentives by shifting the cost of poor predictions from the individual to the decision maker, who can respond by investing in improved prediction accuracy. Perfect predictors always satisfy our notion, showing that the central goal of building more accurate predictors is well aligned with the goal of avoiding discrimination. Learn more To explore the ideas in this blog post on your own, our Big Picture team created a beautiful interactive visualization of the different concepts and tradeoffs. So, head on over to their page to learn more. Once you’ve walked through the demo, please check out the full version of our paper, a joint work with Eric Price (UT Austin) and Nati Srebro (TTI Chicago). We’ll present the paper at this year’s Conference on Neural Information Processing Systems (NIPS) in Barcelona. So, if you’re around, be sure to stop by and chat with one of us. Our paper is by no means the final word on this important and complex topic. It joins an ongoing conversation with a multidisciplinary focus of research. We hope to inspire future research that will sharpen the discussion of the different achievable tradeoffs surrounding discrimination and machine learning, as well as the development of tools that will help practitioners address these challenges. Google Labels: Google Brain , Machine Learning , Publications , Supervised Learning    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms 