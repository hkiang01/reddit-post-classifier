 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01491 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cond-mat cond-mat.dis-nn cs cs.AI cs.CC stat stat.ML References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Raman Arora Amitabh Basu Poorya Mianjy Anirbit Mukherjee Bookmark (what is this?) Computer Science > Learning Title: Understanding Deep Neural Networks with Rectified Linear Units Authors: Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee (Submitted on 4 Nov 2016 (v1), last revised 26 Nov 2016 (this version, v3)) Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train a ReLU DNN with one hidden layer to {\em global optimality}. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\R^n \to \R$ function is representable by a ReLU DNN {\em if and only if} it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN. We follow up with gap results showing that there is a smoothly parameterized family of $\R\to \R$ "hard" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\R^n\to \R$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of "hard" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory. Subjects: Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Computational Complexity (cs.CC); Machine Learning (stat.ML) Cite as: arXiv:1611.01491 [cs.LG]   (or arXiv:1611.01491v3 [cs.LG] for this version) Submission history From: Poorya Mianjy [view email] [v1] Fri, 4 Nov 2016 18:54:50 GMT (545kb,D) [v2] Fri, 11 Nov 2016 20:25:56 GMT (546kb,D) [v3] Sat, 26 Nov 2016 17:38:11 GMT (545kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 