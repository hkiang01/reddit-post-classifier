 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01462 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.CL stat stat.ML References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Hakan Inan Khashayar Khosravi Richard Socher Bookmark (what is this?) Computer Science > Learning Title: Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Authors: Hakan Inan, Khashayar Khosravi, Richard Socher (Submitted on 4 Nov 2016) Abstract: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5. Comments: 10 pages, 2 figures, 3 tables Subjects: Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML) Cite as: arXiv:1611.01462 [cs.LG]   (or arXiv:1611.01462v1 [cs.LG] for this version) Submission history From: Hakan Inan [view email] [v1] Fri, 4 Nov 2016 17:36:20 GMT (303kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 