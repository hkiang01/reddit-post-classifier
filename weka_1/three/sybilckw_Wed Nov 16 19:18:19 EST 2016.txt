 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.04581 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Peter H. Jin Qiaochu Yuan Forrest N. Iandola Kurt Keutzer Bookmark (what is this?) Computer Science > Learning Title: How to scale distributed deep learning? Authors: Peter H. Jin, Qiaochu Yuan, Forrest Iandola, Kurt Keutzer (Submitted on 14 Nov 2016) Abstract: Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes). Comments: Extended version of paper accepted at ML Sys 2016 (at NIPS 2016) Subjects: Learning (cs.LG) Cite as: arXiv:1611.04581 [cs.LG]   (or arXiv:1611.04581v1 [cs.LG] for this version) Submission history From: Peter Jin [view email] [v1] Mon, 14 Nov 2016 20:59:54 GMT (670kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 