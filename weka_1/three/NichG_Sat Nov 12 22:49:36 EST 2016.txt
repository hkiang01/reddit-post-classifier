This post summarizes a bunch of connected tricks and methods I explored with the help of my co-authors. Following the previous post, above the stability properties of GANs, the overall aim was to improve our ability to train generative models stably and accurately, but we went through a lot of variations and experiments with different methods on the way. I’ll try to explain why I think these things worked, but we’re still exploring it ourselves as well. The basic problem is that generative neural network models seem to either be stable but fail to properly capture higher-order correlations in the data distribution (which manifests as blurriness in the image domain), or they are very unstable to train due to having to learn both the distribution and the loss function at the same time, leading to issues like non-stationarity and positive feedbacks. The way GANs capture higher order correlations is to say ‘if there’s any distinguishable statistic from real examples, the discriminator will exploit that’. That is, they try to make things individually indistinguishable from real examples, rather than in the aggregate. The cost of that is the instability arising from not having a joint loss function – the discriminator can make a move that disproportionately harms the generator, and vice versa. Other methods are stable, but have difficulty in data spaces dominated by higher-order correlations. Variational autoencoders are an example of this. By imposing a certain distribution on the latent space, they provide a principled way to generate new samples from the data distribution which occur at the correct relative probabilities. VAEs tend to suffer blurriness in the output, however, especially when the entropy is high. This is because the user must specify a similarity metric in the data space by hand. That is to say, when training a variational autoencoder, one must say ‘this image is closer to the target than that one’, and that definition of closeness is usually taken to be per-pixel mean-squared-error (MSE). The problem with this is that it implies that the distribution of images should be modeled by a set of independent gaussian distributions for each pixel, whereas perceptually it’s the higher-order correlation between pixels that is more important to a human observer. For example, an image of a circle can be shifted by one pixel to the left and still appear as the same circle, but in terms of per-pixel MSE, this corresponds to a huge fluctuation in the pixels at the edge. Because of this, methods using functional or perceptual information in place of MSE can be used to improve the quality of images generated by a non-adversarial setup (paper). Alex Champandard (@alexjc) has made a number of investigations along these lines as well. We poked around with these methods, but when used on any long-term or detail-sensitive task, it seemed like the GANs would work well for a bit and then crash or become unstable, while the variational autoencoder would train in a stable manner but wouldn’t converge to the level of accuracy in the generative model to allow us to use the output for followup purposes. We started thinking about other methods outside of modern neural networks and how they worked, and that investigation, combined with a recent idea, led us to something that seemed to work for our purposes. One of the ingredients is described in a recent paper on Monte-Carlo sampling on autoencoders (code). This paper discusses the way in which the actual distribution of latent space encodings for auto-encoders and variational auto-encoders tends to have gaps and other structure, associated with features of the generated images that do not actually vary smoothly (for example, glasses versus no-glasses). The idea in this paper was that one could effectively perform a Monte Carlo sampling procedure over the latent space just by repeatedly decoding and encoding a given example. The result would be that the generated example would tend to slide towards areas of higher probability density, and thus would give more self-consistent and realistic results. In that paper, the autoencoder is trained in the standard way (as a single encoding and decoding pass), but resampling is used to improve the quality of the generated output. The second ingredient has to do with energy-based methods. In energy-based generative models, there is some network of links between features in the data which describe for each pair what their average correlation over the dataset is (this can also be modelled conditionally, hidden variables can be used, etc). To generate a sample, one starts with some arbitrary pattern and then performs a Monte-Carlo sampling process to try to relax the mismatch between the expected correlations and the ones in the pattern. It turns out there’s a relationship between these kinds of model and neural networks, going all the way back to Hopfield networks. In a Hopfield network, a process akin to Hebbian learning is used to update the model, with the result that the correlation structure ends up being learned. Furthermore, rather than doing Monte-Carlo sampling to minimize the energy, it turns out you can just repeatedly run a pattern through the network and it will tend to converge on the equivalent result. The Hopfield network is basically just a single layer with a threshold nonlinearity, so there’s a limit to what kinds of generalizations it will learn. To extend past that, Restricted Boltzmann Machines operate under a similar idea but now introduce a separation between a set of hidden variables (the same as the latent space of an autoencoder) and the variables describing the data. Gibbs sampling is used to converge on the equilibrium distribution rather than iterating the state through the network, though. An extra modification is added to make the learning stable – the network is adjusted to move towards correlations in the data, but to move away from what the network generates spontaneously. This prevents the hidden variables from developing their own strong, but meaningless correlations. The pattern we want to make note of here is that there seems to be a very general equivalency between Monte Carlo sampling and recurrently cycling data through a network. This was used in another paper (Conditional Random Fields as Recurrent Neural Networks) to make a neural network version of an algorithm used to learn Conditional Random Fields (yet another kind of energy-based model). Specifically, by replacing the sampling procedure with looping through a neural network, the entire thing can be attached to a bigger neural network pipeline and trained end-to-end with gradient descent. In that sense, an energy-based model ends up being just another kind of layer. The nice thing about energy-based models is they have a single, shared loss function in the form of the energy function, which makes them tend to be very stable to train. In a GAN, the mismatch between generator and discriminator can create oscillatory, chaotic, or crashing behaviors, but in an energy-based model that mismatch is guaranteed to be exactly zero. The problem is that they usually involve Monte Carlo sampling, which at best only approximately differentiable (using something like the re-parameterization trick of VAEs), could take a long time to converge depending on the type of step used, etc. So this equivalency between recurrent networks and Monte Carlo sampling is really cool – it means that we can basically get a differentiable, energy-like model for free, maybe gaining its nice stability properties without the hard work. So why not try that with the Monte Carlo sampling on autoencoders paper, but now instead of just doing a sampling procedure, we actually make the entire auto-encoder into a recurrent network and train not on the single-pass decoding accuracy, but on the N-pass decoding accuracy? The idea is, take the data as input, encode to the latent space, add some noise, decode, encode, add noise, … The loss function is then the mean-squared-error between the input and every subsequent decoded output (1-pass, 2-pass, etc). In addition, rather than do a full-on variational autoencoder, I can just add a penalty term proportional to the norm of the encoding vector. In this case, it seems we can even get away with not having the variational element but still obtain a generative model because of the repeated re-sampling. Normally, we wouldn’t be able to say how to generate random latent variable values to obtain the correct probability distribution in the output as to match the data distribution. However, with the repeated re-sampling and addition of noise, any point we start at in the latent space eventually will converge to the equilibrium distribution over the latent space as defined by the network itself. So we just need to pick a point and iterate long enough, and we’ll get something self-consistent. This is a nice hack in terms of the simplicity of the approach, since explaining and understanding variational autoencoders tends to be… challenging. We gave it a try, and it seems to work. One the one hand, it doesn’t completely solve the blur problem like GANs do. But on the other hand, it seems to learn correlations in the data space better than variational autoencoders. For a test case, we made a simple set of 3D data where the first two dimensions were sampled from IID unit-norm Gaussian distributions, but the third dimension was equal to . The network architecture in each case uses a 6D latent space and has one extra 128-neuron hidden layer before and after the encoding layer (in other words, 3->128->6->128->3). No nonlinearity is applied to the encoding or output layer, though a weak (0.03) penalty is applied to keep the code vectors from becoming arbitrarily large. The networks were trained on 10000 samples, for 400 epochs. A pure autoencoder encodes these samples quite accurately, but if we pick random points in the latent space they seem to have nothing to do with the distribution at all. On the other hand, the reconstruction error isn’t great with the variational autoencoder and while it has some sense of the distribution, its still not quite right. There’s too much weight centered at the vertex, and furthermore the distribution seems to be biased in (probably being pulled towards the mean value of ). A few rounds of Monte-Carlo resampling of the plain autoencoder does seem to make it behave much better as a generative model, but it still isn’t converging to the actual distribution. On the other hand, the recurrent autoencoder ends up learning a very sharp model of the correct distribution. The only differences between the Recurrent Auto-encoder and the Monte Carlo resampled autoencoder is that gradients were propagated across the resampling procedure in the RAE during training and that noise was added to the latent space each time, but it makes a huge difference. We also tested on an artificial multi-modal 2D data set, to see if we could generate complex and highly non-Gaussian output distributions. Again, the recurrent auto-encoder seemed to be able to learn to generate points belonging to each of the clusters, without just collapsing to the mean. We’ve only done some minor tests with image data at this point, but the results were less impressive and still seemed to be blurry. The data set was quite high-entropy though, so a more thorough test using a faces dataset is probably worth doing in the future. Okay, great, so what? Well, it might help to explain why all the fuss about generative models. A few weeks ago at a workshop on Karl Friston’s Free Energy ideas in neuroscience. If you aren’t familiar with them, basically the idea boils down to a hypothesis that ‘brains arrange themselves and take actions to try to minimize their surprise’. However, this is expressed in terms of the joint probability distribution between all the various internal parameters of the brain, sensory states, even actions taken. This means you can write down some potentially interesting objective functions to optimize. For instance, you can ask for things like ‘take actions that minimize the uncertainty of my future predictions of my sensory state’ as easily as you can ask for things like ‘take actions to end up near the goal’ – at least, you can write it down as easily. The problem is, working with the full joint distribution is tough. The way people go about it in neuroscience makes it even tougher – they generally start with an explicit, parameterized model of reality and then do formally correct differential Bayesian inference on everything. This, if I may say so, causes a wealth of horrible implementation difficulties, practical problems, etc: you need to already have an explicit model, you need to keep track of a lot of high-dimensional distributions, or you have to make local Gaussian approximations which at best leave you needing to explicitly calculate Hessians between parameters. This means people generally use these methods on problems with a very, very small number of parameters and a very low dimensional input/output space compared to machine learning. One thing that came out of the workshop was the idea that most of those technical difficulties can be answered in large part by just skipping the formal Bayesian inference procedure and instead heuristically estimating that with a generative neural network to learn the joint distribution over sensory states and action variables and so on, since the energy function can be directly computed from that joint distribution. This is something I’ve been working on with Yen Yu as a way to make Free Energy-based methods scale to much larger systems. But to do this, we needed a good, solid base on the machine learning side to build on as well – that’s going to be our recurrent auto-encoder. If you aren’t into the neuroscience side of it, why care about this Friston Free Energy stuff? Well, being able to use things like your model’s uncertainties in differentiable loss functions and objectives lets you do some cool stuff. As an example, I’m going to talk about the problem of novelty search. One of the fundamentally troubling things about Friston’s surprisal minimization idea is that surprise-minimizing actions could consist of turning off all the lights and shutting down – if you blind yourself, you won’t see such surprising things. The discussion over that (the so-called Dark Room Problem) is pretty involved, and there are a number of ad hoc techniques for addressing it, but it was a bit unsatisfying. Following the workshop, we had an email exchange about this issue, and Lana Sinapayen proposed an ‘Adversarial Free Energy’ which would see the cognitive model trying to minimize its surprisal in terms of the parameters of its predictive processes, but maximize its surprisal in terms of the actions it took. The result would be a sort of proactive explorer – an agent that would try to prove itself wrong at every junction, but would still try to learn the most accurate model of the world that it could despite that. Just as its easy to write down ways to minimize Friston Free Energy given a nicely differentiable, generative model, it’s possible to do the same for Sinapayan’s Adversarial Free Energy. Putting aside issues of action selection and the response of an environment (which we’ll write about shortly), we can also use this as a form of combinatoric novelty search on a static data set. That is to say, we learn a way to efficiently encode the data, and then intentionally go looking for examples that are hard, but not impossible, to code using our learned scheme. The result is a sort of local search in the space of descriptions – a network that has learned to code sine waves efficiently won’t jump immediately to noise patterns because it doesn’t know how to code for them, but instead finds new things which are hard but possible to code for given what the network has already seen. It won’t expand randomly, but instead will stretch the limits of what the network currently understands. Explicitly, the algorithm is as follows: 1) Start with data set 2) Learn autoencoder weights to minimize , where is the input data, is the decoding after the nth pass, and is the latent representation in the nth pass. 3) Pick a set of random points in the latent space, corresponding to some small percentage of the data set size (say, 10%) 4) Compute the gradient where is the decoding of and is the encoding of back in the latent space. Normalize/scale/etc the gradient, and add to 5) Repeat step 4 until convergence/as many times as you want. 6) Replace some of the original data with , and repeat from step 2 Last Friday, Nathaniel Virgo and I sat down and gave this a shot, starting with a data set composed of sine waves and square waves belonging to a narrow band of frequencies. The network is a bit bigger than the one used in the simple parabolic example (a few more layers, 384 hidden layer neurons, 64 dimensional coding space), but the idea is the same. We first trained the autoencoder a bit to get the initial structure of the coding space solidified, then alternated 50 epochs of training with a replacement of 2.5% of the data with novel samples generated according to the above gradient descent procedure. At first, the results aren’t very interesting – the autoencoder is struggling to represent the square waves accurately, etc. Maybe the novel samples are slightly different amplitude, but generally have the same frequencies, shapes, and phase angles as the source data. Then, however, you slowly start to see the introduction of new features. At some point, the process discovers how creates a higher-frequency sinusoid. At some point it discovers shapes that combine a slow square wave and a fast sine wave, things with one frequency over half of the waveform and another over the other half, etc. In each case, the discovered forms have some relationship to what the network has learned to code for previously – jumping straight to raw noise doesn’t seem to happen, because after all it’s very hard for the network to accurately encode and decode the space of noise patterns. So the network finds some balance between novelty and structure. Watching the encoding space (or at least, a 2D projection of it) during training gives an idea of what this kind of exploration looks like. The encoding space of the recurrent auto-encoder has a sort of fixed-point structure, with a number of points acting as attractors for nearby codes. The gradient descent pushes out to the edges of those basins of attraction, but can’t actually push the network very far outside of the data distribution. Those edges get explored, data is added, and the next training pass pulls those higher-density regions inwards to minimize the average norm of the latent code words over the data set. However, the network still has to code for these new patterns accurately, or it will lose out on the MSE part of the loss term, and so these new branches are preserved and explored further during the next novelty pass. In this case, there isn’t really an external reference to ground this novelty aside from the network architecture itself – that is to say, the network is just learning what ‘easy to code’ and ‘hard to code’ mean given the context of how its own mind works. But in general, this kind of novelty search can just as well be tied to processes that pass through some kind of black box environment, leading to the discovery of action patterns that move the agent into a rarely or unexplored region of the problem space. There’s a direct map between this kind of technique (seeking rare points in the latent space) and a technique called umbrella sampling, used for example in computational chemistry to sample rare chemical transitions in molecular dynamics simulations. In both cases, given some kind of representation on which relative probabilities of states can be evaluated, biasing generation towards lower probability states gives a way to efficiently seek out hard-to-find (but still feasible and realistic) configurations and transitions. The sourcecode for these experiments is available here.