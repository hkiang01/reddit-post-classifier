 ARGONNE NATIONAL LABORATORY9700 South Cass AvenueArgonne, Illinois 60439 On Automatic Di�erentiation 1byAndreas GriewankMathematics and Computer Science DivisionPreprint ANL/MCS-P10-1088 November 1988 1This work was supported by the Applied Mathematical Sciences subprogram of the O�ce of EnergyResearch, U.S. Department of Energy, under contracts W-31-109-Eng-38. On Automatic Di�erentiationAndreas GriewankMathematics and Computer Science DivisionArgonne National LaboratoryArgonne, IL 60439, U.S.A.AbstractIn comparison to symbolic di�erentiation and numerical di�erencing, the chain rulebased technique of automatic di�erentiation is shown to evaluate partial derivativesaccurately and cheaply. In particular it is demonstrated that the reverse mode ofautomatic di�erentiation yields any gradient vector at no more than �ve times the costof evaluating the underlying scalar function. After developing the basic mathematicswe describe several software implementations and brie y discuss the rami�cations foroptimization.Key words: gradient evaluation, automatic di�erentiation, symbolic di�erentia-tion, reverse accumulation, analytic derivatives.1 IntroductionIn 1982 Phil Wolfe [31] made the following observation regarding the ratio between the costof evaluating a gradient with n components and the cost of evaluating the underlying scalarfunction.If care is taken in handling quantities which are common to the function andderivatives, the ratio is usually around 1.5, not n+1. [31]The main purpose of this article is to demonstrate that Phil Wolfe's observation is in facta theorem (with the average 1.5 replaced by an upper bound 5) and that care can be takenautomatically. This remarkable result is achieved by one variant of automatic di�erentiation[25], which simply implements the chain rule in a suitable fashion. The same approach canbe used to compute second and higher derivatives. At least since the �fties these techniqueshave been developed by computational scientists in various �elds, and several softwareimplementations are now available. Although a theorem con�rming Wolfe's assertion forrationals was published in 1983 by Baur and Strassen [2], the optimization community tooklittle notice of these developments. This can be partly explained by a lack of clarity in thecustomary terminology.Automatic di�erentiation is often confused with symbolic di�erentiation or even withthe approximation of derivatives by divided di�erences. For algebraically rather simple2 functions, the explicit derivative expressions obtained by symbolic di�erentiation may bereadable to an experienced user and thus provide an extremely useful extension of researchwith pencil and paper. However, for functions of any complexity in more than three vari-ables, the analytic expressions for gradient or Hessian tend to take up several pages and areunlikely to facilitate any insights.In this article we will concentrate on the goal of obtaining numerical derivative valuesat given arguments. The need for e�cient and accurate derivative evaluations arises in par-ticular during the iterative solution of nonlinear problems and the subsequent sensitivityanalysis. Following several other authors, notably Iri [15], we will argue that for thesenumerical purposes the reverse mode of automatic di�erentiation is far supe-rior to symbolic di�erentiation or divided di�erence approximations. The lattertechnique is always less accurate and about as costly as the forward form of automaticdi�erentiation.The paper is organized as follows. The remainder of this Section we brie y discussthe historical development and applications of automatic di�erentiation. In Section 2 weutilize two simple example functions to illustrate the characteristic properties of varioustechniques for evaluating gradients. In Section 3 we develop the two modes of automaticdi�erentiation for the general case and conclude that the cost of evaluating gradients in thereverse mode is additive with respect to function composition. As a corollary we obtainWolfe's assertion with 1.5 replaced by the uniform bound 5. Section 4 describes severalimplementations of automatic di�erentiation that require the user to do little more thanprovide a subroutine for the evaluation of the underlying function. In the �nal Section 5we brie y discuss the implications of automatic di�erentiation on the design and selectionof optimization algorithms.The literature relating to automatic di�erentiation is extensive and very diverse. Themain stream of research and implementation has been concerned with the automatic eval-uation of gradients ( or more generally truncated Taylor series ) in the forward mode. Thise�ort goes back at least to Beda et al [3] in the Soviet Union and Wengert [30] in the UnitedStates. Numerous other references are contained in the paper by Kedem [21], the booksby Rall [25] and Kagiwada et al [19], and the recent report by Fischer [11]. In general theresearchers in this main stream were unaware of the reverse mode or continued to considerit as a somewhat obscure approach of a rather theoretical nature.Mathematically the reverse mode is closely related to adjoint di�erential equations. Nu-clear engineers have long used adjoint sensitivity analysis [4], [5] to evaluate the partialderivatives of certain system responses (e.g. the reactor temperature) with respect to thou-sands of design parameters. This approach yields all sensitivities simultaneously at a costcomparable to only a few reactor simulations. In contrast, thousands of these lengthy calcu-lations would be needed to approximate all sensitivities by divided di�erences. For a recentsurvey on the software and applications in this �eld see the paper by Worley [32]. Similarly,in atmospheric and oceanographic research, adjoints of the governing partial di�erentialequations have been used to obtain the gradients of residual norms with respect to initialconditions and other unknown quantities [29]. Here the residuals represent discrepanciesbetween observed and predicted conditions in the atmosphere or ocean. Even though these3 3D calculations may involve millions of variables, the gradient of the sum of squares canbe obtained at essentially the same cost as an evaluation of the residual vector itself. Inorder to avoid any storage and manipulation of matrices the gradient is then utilized in aconjugate gradient like minimization routine.Apparently the �rst general purpose implementation of the reverse mode was the pre-compiler JAKE due to Speelpenning. In his unpublished thesis [28] Speelpenning showedthat Wolfe's assertion is true, but did not state it formally. His original intention was tooptimize the gradient code generated in the forward mode by sharing common expressions.During this attempt he realized that the optimal gradient code can be obtained directlywithout any optimization by (what we call here) the reverse mode of automatic di�erenti-ation. Several other papers proposing the reverse or top down mode are referenced in thesurvey [17]. This excellent article discusses also the closely related issue of estimating eval-uation errors. Now let us examine various techniques for evaluating gradients on a coupleof simple problems.2 Comparisons on two ExamplesThe use of a cubic equation of state [24] yields the Helmholtz energy of a mixed uid in aunit volume at the absolute temperature T asf(x) = RT nXi=1 xi log xi1� bTx � xTAxp8bTx log 1 + (1 +p2)bTx1 + (1� p2)bTx ;where R is the universal gas constant and0 � x; b 2 Rn ; A = AT 2 Rn�n :During the simulation of an oil reservoir this function and its gradient have to be evaluatedat thousands of points in space and time. Typically the number of uid components n isrestricted to less than 20, but we will include larger values in our comparative timings.2.1 MACSYMAl Results on the Helmholtz EnergyFirst let us examine the results of symbolic di�erentiation with MACSYMA, version309, distributed by Symbolics Inc. After entering f(x) and computing its gradient with thedi� command one may translate the symbolic representations into FORTRAN using thefortran command. On the following page we list the resulting code for the evaluation off(x) and the �rst component of its gradient when n = 5. Actually the original code hadto be modi�ed, mainly because it contained more than the maximum of 19 continuationlines allowed in FORTRAN 77. Due to our familiarity with the function we could break theexpression for the �rst gradient component g(1) in the middle, but in general that would bea rather challenging task. Even after this problem and some type con icts in the originalcode were overcome the results are clearly unimpressive. Just imagine this code segment4 RUTU=DSQRT(2.D0)F=0.0013564*(-(x(5)+x(4)+x(3)+x(2)+x(1))*DLOG(-b(5)*x(5)-b(4)*x(1 4)-b(3)*x(3)-b(2)*x(2)-b(1)*x(1)+1)+x(5)*DLOG(x(5))+x(4)*DLOG2 (x(4))+x(3)*DLOG(x(3))+x(2)*DLOG(x(2))+x(1)*DLOG(x(1)))-(x(53 )*(x(5)*a(5,5)+x(4)*a(5,4)+x(3)*a(5,3)+x(2)*a(5,2)+x(1)*a(4 5,1))+x(4)*(a(4,5)*x(5)+x(4)*a(4,4)+x(3)*a(4,3)+x(2)*a(4,25 )+x(1)*a(4,1))+x(3)*(a(3,5)*x(5)+a(3,4)*x(4)+x(3)*a(3,3)+x6 (2)*a(3,2)+x(1)*a(3,1))+x(2)*(a(2,5)*x(5)+a(2,4)*x(4)+a(2,7 3)*x(3)+x(2)*a(2,2)+x(1)*a(2,1))+x(1)*(a(1,5)*x(5)+a(1,4)*8 x(4)+a(1,3)*x(3)+a(1,2)*x(2)+x(1)*a(1,1)))*DLOG(((RUTU+19 )*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x(1))+1)/(: (1-RUTU)*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*; x(1))+1))/(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x( 1))g(1)=b(1)*(x(5)*(x(5)*a(5,5)+x(4)*a(5,4)+x(3)*a(5,3)+x(2)*a(5,2)1 +x1(1)*a(5,1))+x(4)*(a(4,5)*x(5)+x(4)*a(4,4)+x(3)*a(4,3)+x(2)2 *a(4,2)+x(1)*a(4,1))+x(3)*(a(3,5)*x(5)+a(3,4)*x(4)+x(3)3 *a(3,3)+x(2)*a(3,2)+x(1)*a(3,1))+x(2)*(a(2,5)*x(5)+a(2,4)*x(44 )+a(2,3)*x(3)+x(2)*a(2,2)+x(1)*a(2,1))+x(1)*(a(1,5)*x(5)+a5 (1,4)*x(4)+a(1,3)*x(3)+a(1,2)*x(2)+x(1)*a(1,1)))*DLOG(((RUTU6 +1)*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x(1)7 )+1)/((1-RUTU)*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)8 +b(1)*x(1))+1))/(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b9 (1)*x(1))**2-(x(5)*a(5,1)+a(1,5)*x(5)+x(4)*a(4,1)+a(1,4)*x7 (4)+x(3)*a(3,1)+a(1,3)*x(3)+x(2)*a(2,1)+a(1,2)*x(2)+2*x(1)7 *a(1,1))*DLOG(((RUTU+1)*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b7 (2)*x(2)+b(1)*x(1))+1)/((1-RUTU)*(b(5)*x(5)+b(4)*x(4)+b1 (3)*x(3)+b(2)*x(2)+b(1)*x(1))+1))/(b(5)*x(5)+b(4)*x(4)+b(36 )*x(3)+b(2)*x(2)+b(1)*x(1))g(1)=g(1)+0.0013625*(-DLOG(-b(5)*x(5)-b(47 )*x(4)-b(3)*x(3)-b(2)*x(2)-b(1)*x(1)+1)+DLOG(x(1))+b(1)*(x(7 5)+x(4)+x(3)+x(2)+x(1))/(-b(5)*x(5)-b(4)*x(4)-b(3)*x(3)-b(7 2)*x(2)-b(1)*x(1)+1)+1)-((1-RUTU)*(b(5)*x(5)+b(4)*x(4)+7 b(3)*x(3)+b(2)*x(2)+b(1)*x(1))+1)*((RUTU+1)*b(1)/((1-RUTU7 )*(b(5)*x(5)+b(4)*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x(1))7 +1)-(1-RUTU)*b(1)*((RUTU+1)*(b(5)*x(5)+b(4)*x(4)+b(37 )*x(3)+b(2)*x(2)+b(1)*x(1))+1)/((1-RUTU)*(b(5)*x(5)+b(47 )*x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x(1))+1)**2)*(x(5)*(x(5)*a7 (5,5)+x(4)*a(5,4)+x(3)*a(5,3)+x(2)*a(5,2)+x(1)*a(5,1))+x(47 )*(a(4,5)*x(5)+x(4)*a(4,4)+x(3)*a(4,3)+x(2)*a(4,2)+x(1)*a(7 4,1))+x(3)*(a(3,5)*x(5)+a(3,4)*x(4)+x(3)*a(3,3)+x(2)*a(3,27 )+x(1)*a(3,1))+x(2)*(a(2,5)*x(5)+a(2,4)*x(4)+a(2,3)*x(3)+x7 (2)*a(2,2)+x(1)*a(2,1))+x(1)*(a(1,5)*x(5)+a(1,4)*x(4)+a(1,7 3)*x(3)+a(1,2)*x(2)+x(1)*a(1,1)))/((b(5)*x(5)+b(4)*x(4)+b(7 3)*x(3)+b(2)*x(2)+b(1)*x(1))*((RUTU+1)*(b(5)*x(5)+b(4)*7 x(4)+b(3)*x(3)+b(2)*x(2)+b(1)*x(1))+1))5 had been inserted into a subroutine and subsequently the programmer made a trivial editingerror. Then it would be quite di�cult to determine by inspection whether the segment hadbeen corrupted and nearly impossible to correct it. In other words the code is not onlyine�cient but unmaintainable.While some aspects of MACSYMA's FORTRAN interface are annoying, they are by nomeans the root of our problems. The main culprit is the wrong-headed idea of generatingseparate expressions for the function and each gradient component, directly in terms of theindependent variables. By de�nition this approach eliminates any possibility of utilizingcommon expressions during the evaluation. Instead one should write a program forevaluating the function e�ciently and then generate an extended program thatevaluates the function and gradient simultaneously. As we will see later the extendedprogram can be generated automatically.Everything may be done by hand on our second examplef(x) � nYi=1xi = x1 � x2 � � �xn�1 � xnwhich was already used by Speelpenning [28]. Obviously the i�th component of the gradientrf(x) is given by @f=@xi = Yj 6=i xj = x1 � � �xj�1 � xj+1 � � �xnIf calculated in this form each gradient component involves n � 1 multiplications and isthus almost as expensive to evaluate as the function f itself. Since symbolic di�erentiatorsgenerate separate algebraic expressions for each component of rf(x) they require exactlyn times as many arithmetic operations for evaluating function and gradient jointly as forevaluating the function by itself. Formally we may write qffg = n, whereqffg � workff;rfg=workffg :Since the work ratio qffg is even slightly larger for divided di�erences this may at �rstseem a fair price to pay. However, according to Wolfe's assertion we should be able to do alot better, namely to bound qffg by a constant independent of n.2.2 Automatic Di�erentiation of the Product ExampleIn order to obtain the gradient cheaply one could use the identity@f(x)=@xi = f(x)=xi if xi 6= 0 :Unfortunately, this 'solution' suggests that the e�cient evaluation of gradients involvessome special cancellations, which have to be detected by human inspection and requirenumerical exception handling when certain denominators are zero or small. Fortunately,for this example and other cases, the gradient can be evaluated e�ciently withoutany human intervention or numerical instabilities.6 In order to discuss the alternative methods we have to base the evaluation of the functionand its gradient on sequential programs rather than a set of explicit expressions. Using aninformal programming language we can evaluate y = f(x) by the following code.Evaluation of Productxn+1 = x1For i = n + 2; n+ 3 : : :2nxi = xi�n xi�1y = x2nHere and throughout the paper we will allocate all scalar quantities in a single memoryvector hxiii=1:::m, starting with the independent variables hxiii=1:::n and ending with asingle dependent variable xm. The issue of the storage requirements for actual computerimplementations will be discussed in Section 4.Since the intermediate quantities xn+i; i = 1 : : :n are smooth functions they possessgradients rxn+i; i = 1 : : :n with respect to the independent variables x1; x2; : : : ; xn. Inparticular we have rx2n = g � rf and rxn+1 = e1. Evaluating the intermediate gradientsby the chain rule we obtain the following expanded program.Forward Di�erentiation of Productxn+1 = x1rxn+1 = e1For i = n + 2; n+ 3 : : :2nxi = xi�n xi�1rxi = xi�1 ei�n + xi�n rxi�1y = x2ng = rx2nThis program evaluates both function and gradient simultaneously. It can be generatedin a 'mechanical' fashion and is only about twice as long as the original program becauseeach assignment to an intermediate quantity is simply augmented by the calculation of itsgradient. This forward approach has been developed and advocated by several authors(See e.g. [3], [30], [21], and [25]). Various software implementations will be discussed inSection 4.A simple count reveals that the calculation of our example gradient by the program aboveinvolves 12n2 nontrivial multiplications, so that q ' n=2. In general we must expect that theforward mode of automatic di�erentiation increases the number of arithmetic operations bythe factor n, because each evaluation of an intermediate scalar quantity xi is accompaniedby the calculation of the corresponding gradient vector rxi. Apparently Speelpenning wasthe �rst to notice that, instead of the gradient vector, only another scalar, say �xi, needs tobe associated with each quantity xi; i = 1 � � �2n. In case of the product example one may7 de�ne �xn+i as the product of all xj with i < j � n and then set@f=@xi = �xi � xn+i�1 �xn+i:This calculation is performed by the following extended program.Reverse Di�erentiation of Productxn+1 = x1For i= n+ 2; n+ 3; : : : ; 2nxi = xi�n xi�1 fForward Sweepgy = x2n�x2n = 1For i= 2n; 2n� 1; : : : ; n+ 2�xi�1 = �xi xi�n fReverse Sweepg�xi�n = �xi xi�1�x1 = �xn+1g = h�xiii=1:::nThis algorithm requires 3n � 3 multiplications in order to compute the function and itsgradient, so that now q ' 3. That is 50% more than the number of arithmetic operationsrequired by the method based on cancellations, but now there is no need for any branchingwhen one of the variables is small. The amazing fact is that this apparently tricky algorithmfor the gradient of a product can be obtained by a general, straight-forward transformationfrom the original function evaluation program.2.3 Experimental Comparison on Helmholtz EnergyBefore discussing the details of this transformation in the following sections, let us list someempirically observed values for the work ratio qffg on our �rst example. The numbersin Table 1 represent the ratio between the execution times of an extended program thatevaluates f(x) and rf(x) jointly and of the original program that evaluates only f(x) ata given argument. The entries in the �rst column represent the work ratio for divideddi�erences, namely n + 1 with n being the number of variables. The three numbers in thesecond column were obtained as follows. The Helmholtz energy function f(x) was enteredinto the algebraic manipulation package MAPLE [6] and then di�erentiated symbolicallyusing the grad command. On a Sun 3/140 with 16 megabytes real memory, the symbolicgeneration of the gradient always took several minutes, and when n was set to 30 thedi�erentiation failed after 15 minutes due to a lack of memory space. The time for thisprocess was not included in the listed work ratios, which re ect only the times needed tosubstitute the indeterminates xi by real arguments in the expressions for f(x) and rf(x).For example when n = 20 the substitution took 7:13 and 160 seconds CPU time respectively.The results in the third and forth column were obtained on an IBM XT using theprogramming language PASCAL-SC [22]. Like other modern languages this extension of8 Div. Di�. Symbolic Forward Reverse 1 Reverse 26 2.0 1.5 1.00 6.8011 9.8 2.1 1.66 4.6621 22 3.8 1.94 3.4631 - 5.2 2.04 3.9541 - 7.6 2.67 3.6551 - - 2.88 3.8261 - - - 3.7671 - - - 3.8081 - - - 3.83FORTRAN MAPLE PASCAL-SC PASCAL-SC JAKEFTable 1: Observed work ratios on Helmholtz energy for n = 5; 10; 20; : : : ; 80.standard PASCAL allows the transformation of a program for the evaluation of f(x) intoone that evaluates f(x) and rf(x) by a process called operator overloading. This approachwas �rst implemented by Rall [26],[27] in the forward mode of automatic di�erentiation. Wehave implemented the same approach in the reverse mode as described in Section 4. Againthe entries in the table do not include the compilation times for the original and extendedprograms but represent the ratios of the respective execution times. The �fth column wasobtained in almost the same way, except that the original program was written in FORTANand then extended to the gradient routine by the precompiler JAKEF [14] (an update ofSpeelpennings original version JAKE [28]). The resulting pair of FORTRAN programs wasrun on the Sun 3 so that the execution times were naturally much smaller than those ofthe PASCAL-SC programs on the IBM XT. Nevertheless the comparison between runtimeratios provides some meaningful information.As in the case of divided di�erences, the observed work ratios grow linearly with thenumber of variables n, for both symbolic di�erentiation and the forward mode of auto-matic di�erentiation. However, in the latter case the proportionality factor is only about.2 compared to 1.0 in case of the popular divided di�erences. The reverse mode of au-tomatic di�erentiation in PASCAL-SC is always faster than the corresponding forwardscheme, and the work ratio seems indeed uniformly bounded in n. The same is true for theFORTRAN version of reverse accumulation, though there the ratios are initially somewhatlarger. Due to the limitation to 512K core memory, the forward and reverse implementationin PASCAL-SC can handle the Helmholtz energy function only up to 40 and 50 variablesrespectively. MAPLE exhausts the many times larger memory on the Sun much earlier.On the basis of our experience with MACSYMA and MAPLE we conclude that symbolicmanipulators cannot be considered suitable tools for our purposes. Finally we note that acarefully handcoded routine for evaluating a suitable representations of the �rst four deriva-tive tensors requires only about 1.5 times the computing time of evaluating the Helmholtzenergy by itself. Thus we see that when runtime really counts, some mental e�ort may stillbe worthwhile. 9 3 Automatic Di�erentiation of Composite Functions3.1 Composite Functions and their Computational GraphThroughout this section we consider a function y = f(x) : Rn that is de�ned by a givensequential program of the following form.Original ProgramFor i= n+ 1; n+ 2; : : : ; mxi = fihxjij2Jiy = xmHere the elementary functions fi depend on the already computed quantities xj with jbelonging to the index setsJi � f1; 2; : : : ; i� 1g for i = n + 1; n+ 2; : : : ; mIn other words f is the composition of m � n elementary or library functions fi, whosegradients rfi = h@fi=@xjij2Jiare assumed to be computable at all arguments of interest.For example, this is clearly the case when all fi represent either elementary arithmeticoperations, i.e. + , - , * and / or nonlinear system functions of a single argument, e.g.logarithms, exponentials and trigonometric functions. Almost all scalar functions of practi-cal interest can be represented in this factorable form, which has been used extensively byMcCormick et al. [18]. Rather than restricting ourselves to unary and binary elementaryfunctions we allow for any number of arguments ni � jJij < i; where j � j denotes cardinality.In particular we may trivially interpret any function f(x) as a composition of itself so thatin the program above fn+1 = f and m = n + 1; nm = n. More importantly, this generalframework allows for user de�ned subroutines.Sometimes it is very helpful to visualize the original program as a computational graphwith the vertex set fxig1�i�m. An arc runs from xj to xi exactly if j belongs to Ji.With each arc one may associate the value of the corresponding partial derivative @fi=@xj .Because of the restriction on Ji one obtains an acyclic graph, whose minimal elementsare the independent variables. Usually there are several linear orderings of the xi thatare compatible with the partial ordering induced by the directed graph. Whenever twoelementary functions do not directly or through intermediaries depend on each others result,they can be evaluated in either order or even concurrently on a parallel machine. This aspecthas been examined in [9], but will not be pursued any further here. Also, in contrast to theanalysis in [15], we will not use the graph structure for our complexity bounds.10 x1 x2 x3 x4x5 = f5hx1ix6 = f6hx5; x2i x7 = f7hx6; x3ix8 = f8hx7; x4i6����� AAAAAAAAAK �� ��� JJJJJJJJJJJJJ J]����� SSSSSSSSSSSSS SSSSSSo@f5@x1@f6@x5 @f7@x6 @f8@x7 @f6@x2 @f7@x3 @f8@x4Figure 1: Graph for Product where f5hx1i = x1 and fihxj ; xki = xj � xk for i = 6; 7; 8.For any reasonable measure of computational work on a serial machine we may assumethat workffg = mXi=n+1workffig :In de�ning workffg one may account for the number of certain arithmetic operations aswell as fetches and stores from and to memory. Now let us develop the extended programsfor evaluating the gradient rf jointly with f .3.2 Automatic Di�erentiation with Forward AccumulationAgain denoting byrxi the gradient of xi with respect to the independent variables hxjij=1:::nwe derive from the original program by the chain rule:Forward ExtensionFor i = 1; 2 : : :nrxi = eiFor i = n+ 1; n+ 2; : : :mxi =fihxjij2Jirxi = Pj2Ji @fi@xjrxjy = xmg =rxmwhere ei denotes the i� th Cartesian basis vector in Rn.11 Due to the assumed additivity of the work measure we �nd thatworkff;rfg = mXi=n+1 [workffi;rfig+ nni(mults+ adds)] ;where the extra nni arithmetic operations are needed to form rxi as a linear combinationof the ni gradient vectors rxj with j 2 Ji. Here we have neglected the fact that for jjust above n, the gradient vectors rxj will be sparse so that some arithmetic operationsoperations could theoretically be avoided. However, the added complexity of a suitablesparse implementation is unlikely to be justi�ed by the savings, except in very specialcases. Another possible alternative is to run through the basic loop n times, each timeonly evaluating the partial derivatives @xi=@xj with respect to one particular independentvariable xj . This implementation of forward accumulation is considerably less economical interms of computational e�ort but requires only about twice as much storage as the originalprogram. We will not consider this space saver solution in the remainder of the paper.Now suppose that the evaluation of any library function fi requires at most c ni arith-metic operations, where c is a common positive constant. Then it follows from the lastequation that the work ratio de�ned above satis�es qffg � 1 + n=c. This linear growth inthe number of variables was clearly observed on the Helmholtz example and is not acceptablefor large problems.3.3 Automatic Di�erentiation with Reverse AccumulationIn order to obtain a method with a uniformly bounded work ratio we associate with eachintermediate variable xi the scalar derivative�xi � @xm=@xirather than the gradient vector rxi. By de�nition we have �xm = 1 and for i = 1 : : :n@f(x)=@xi = �xi :As a consequence of the chain rule it can be shown (see e.g. [20]) that these adjoint quantitiessatisfy the relation �xj = Xi2Ij @fi@xj �xi ;where Ij � fi � m : j 2 Jig: Thus we see that �xj can be computed once all �xi with i > j areknown. In terms of the program structure it is slightly more convenient to increment all �xjwith j 2 Ji for a known i by the appropriate contribution �xi @fi=@xj. This mathematicallyequivalent looping leads to the following extended program.12 Reverse ExtensionFor i = n+ 1; n+ 2; :: : : : ; mxi = fihxjij2Ji fForward Sweepg�xi = 0y = xm�xm = h�xiini=1 = �gFor i = m;m� 1; : : : ; n+ 1�xj = �xj + @fi@xj �xi for all j 2 Ji fReverse Sweep gg = h�xiini=1When the initial vector �g is set to zero and equals one, then the resulting vector g issimply the gradient rf . Otherwise we obtain for exactly the same computational e�ort themore general result g = �g + rf(x) :In other words the above program can increment a certain multiple of the gradient rf to agiven vector �g of the same length. This is exactly the operation we have to perform for eachelementary function in the reverse extension. Hence we have additivity of the computationalwork in that workff; �g + rfg = mXi=n+1work ffi; �gi + i rfigfor arbitrary scalars i and vectors �gi of length ni. After division by the last equation ofSubsection 3.1 one �nds by elementary arguments thatQffg � work ff; �g + rfgworkffg � maxn<i�mQffig ;Note that Qffg is slightly larger than the work ratio qffg de�ned in Subsection 2.1. Thismeans that the work ratio for f is bounded above by the worst ratio for any of the libraryfunctions fi, which is clearly independent of the total number of variables n. In other wordsthe set of functions f for which the work ratio Qffg does not exceed a certainbound �Q is closed with respect to composition. This rather surprising result holdsfor a wide range of work functionals, provided memory space is unlimited and free. Howeveras was mentioned above, memory access, i.e. fetches and stores, may be included as costs.Now suppose the fi are restricted to the elementary arithmetic operations and standardunivariate functions on a modern mainframe. For sine and cosine the work ratio lies justabove two, and for all other system functions it is close to 1, because their derivatives comepractically free once the function itself has been evaluated. Assuming that an addition ischeaper than a multiplication and a division costs at least 50% more than a multiplication,one �nds that the largest work ratio is attained for the multiplication function fi(x1; x2) �x1 � x2. Therefore we may use the upper bound�Q � Qfx1 � x2g = 3 mults+ 2 adds+ 5 fetches + 3 stores1 mult+ 2 fetches + 1 store � 5 :13 Thus we can conclude that under quite realistic assumptions the evaluation of a gradientrequires never more than �ve times the e�ort of evaluating the underlyingfunction by itself. Obviously the bound of 5 is somewhat pessimistic and one mightexpect to incur an even smaller penalty for evaluating the gradient in practice. This wasfound to be true in our experiments on the Helmholtz example. On the other hand theextended program may involve communications overhead, e.g. extra subroutine calls, thatis not included in our work measure.While the reverse mode is clearly superior to the forwardmode in terms of computationale�ort, it may require a lot more storage than the latter. As coded in Subsection 3.2 theforward extension associates with each scalar variable of the original program a gradientvector of length n. Hence the storage requirement grows by the predictable factor n + 1.This is true even if some variables are repeatedly updated during the function evaluation.In that case the associated vectors can also be overwritten by the gradient of the latestvalue of the variable. For example in the product program of Subsection 2.2 one wouldnormally not allocate n extra storage locations for the partial products xn+i = x1 : : :xibut instead store them successively in the same place. Similarly all gradients rxn+i in thecorresponding extended program could be stored in a common n-vector.In sharp contrast the reverse accumulation in Subsection 2.2 relies on all n � 1 partialproducts xn+i being still available after the �nal function value x2n has been computed.Nevertheless, for this problem both modes require essentially the same storage, and onthe Helmholtz energy function reverse accumulation uses slightly less space than forwardaccumulation. However, the di�erence in the memory requirement of the two methods canbe much more dramatic.3.4 Relations to Adjoints of Initial Value ProblemsSuppose the evaluation of f(x) involves the numerical solution of an initial value problemy0(t) = F [y(t); t; x] for 0 � t � 1 with y(0) = y0(x) ;where y has r components and y0 is a smooth function of x 2 Rn. For a scheme with �xedstep size h the result yh(1) will be a di�erentiable function of x. Provided f depends in turnsmoothly on the �nal values y(1), the whole evaluation procedure �ts (for each �xed mesh)into our framework. For simplicity let us assume that f(x) = wTy(1) with some �xedweighting vector w 2 Rr. During the numerical integration of the initial value problemwith a p-stage scheme, one only has to store p vectors of length r. In the forward mode theassociated gradients would increase the storage requirement for this part of the program ton p r locations. In the reverse mode we have to keep track of all r=h intermediate values,which represent a discrete approximation of the solution function y(t) for 0 � t � 1.Interestingly enough this is exactly the information one needs to calculate the gradientof rf(x) by solving the so called adjoint di�erential equation [23],z0(t) = �FTy [y(t); t; x]z(t) with z(1) = w;14 where Fy denotes the Jacobian of the right hand side with respect to y. Since the boundaryconditions are terminal and the sign on the right hand side is reversed, this linear systemhas exactly the same stability and sti�ness properties as the original initial value problem.The desired gradient is given byrf(x)T = z(0)T @y0@x + Z 10 z(t)TFx[y(t); t; x]dt ;where Fx denotes the Jacobian of the right hand side with respect to x. Thus we see thatin the limiting continuous case, the evaluation of the gradient involves a de�nite integrationbased on the solution of an additional ODE with the same dimensions as the original initialvalue problem. Consequently the work ratio for appropriate discretizations should be closeto 2 and certainly below 5.In fact we may interpret reverse accumulation simply as a discrete analogof the classical adjoint equations from the calculus of variations and controltheory [10]. Obviously the vector y need not be �nite dimensional, and one can adoptthe theoretical arguments and numerical techniques to more general evolution equations inHilbert spaces.In terms of consistency it is probably preferable to discretize only the forward integrationand then to apply reverse accumulation without explicitly referring to the adjoint di�erentialequation at all. On the other hand separate discretizations of the original and adjointequation allow the usage of standard software, with automatic di�erentiation only beingused to obtain the Jacobian of the right hand side [19]. With the bene�t of hindsight onecould also construct an 'optimal' spline representation of y(t) in order to economize onstorage, especially if the integrator is adaptive and involves many tentative evaluations.Apparently nobody has studied the relative merits and computational performance of thesevarious options.When the di�erential equation is solved using an adaptive grid the actually computedfunction is only piecewise di�erentiable. As for any program that includes branching de-pending on values of variables, automatic di�erentiation will generally yield thederivative of the smooth piece containing the current argument. Obviously thisis the best one can achieve, whereas divided di�erences may yield completely meaninglessresults if taken across a crack of the actually computed function. In transforming the orig-inal program to the extended routine with automatic di�erentiation, all control statementsare left unaltered. In e�ect this means that the form of the loop in the original programmay become dependent on the current argument. As pointed out by Kedem [21] errors mayarise when reals are tested for equality. For example the conditional assignmentif x 6= 0 then y = (1� cosx)=x else y = 0would lead to the derivative @y=@x at x = 0 being automatically evaluated as 0 rather thanthe correct value 1=2. Obviously the original programming leaves something to be desiredin this particular example. In our implementation of the reverse mode in PASCAL-SC testsfor equality involving real variables lead to warning messages.15 3.5 Estimation of the Evaluation ErrorThe adjoint quantities �xi can be utilized to obtain good estimates of the total error inevaluating f(x). Suppose one knows that the actually computed intermediate values ~xisatisfy for each i > n j~xi � fih~xjij2Jij � �xi :Moreover, let us assume that the discrepancies between the actual inputs h~xiii=1:::n and theirideal values hxiii=1:::n are bounded by data tolerances h�xiii=1:::n. Then one can expect thatthe actually computed �nal value ~xm satis�esj~xm � f(x)j � mXi=1 j�xij �xi :As shown by induction in [1] this inequality must hold if all functions fi are linear and theadjoint values �xi are exact. Even though these two assumptions are rather unrealistic theright hand side above was found in [17] to provide a usually somewhat pessimistic upperbound on the total error. In that paper the local error bounds �xi were obtained fromthe machine precision of the computer in question. However, other sources of local error(such as discretizations, the approximation of a transcendental function by rationals or theuncertainty of certain problem parameters) could be accounted for as well.Since the local evaluation errors are rarely correlated and usually unbiased, it makessense to consider them as stochastically independent random variables with zero mean andstandard deviations �xi. This assumption implies that the standard deviation of ~xm� f(x)is simply the l2-norm of the m-vector h�xi �xiii=1:::m rather than the l1 norm occuring onthe right hand side above. Iri et al. found that this error estimate was somewhat tighteron their test problems. Either choice is certainly far superior to the ad hoc guesses thatusers currently have to make in order to specify tolerances for stopping criteria in iterativemethods. Therefore these error estimates could be incorporated into optimization codes, toprovide optimal solution accuracy without inconveniencing the user.3.6 Extension to Higher DerivativesIn the forward mode the Hessian r2xm of xm = f(x) can be obtained by updating fori = n+ 1 : : :m r2xi = Xj2Ji 24 @fi@xjr2xj + Xk2Jirxj @2fi@xj@xk (rxk)T35starting with rxi = ei and r2xi = 0 for i = 1 : : :n. Similar chain rules of di�erentiationapply for third and higher derivative tensors. While the inclusion of these recursive relationsinto the original program provides in principle little di�culty, the resulting computationale�ort is at least of order (m � n)np, where p is the degree of the derivative tensor. Inparticular the evaluation the Hessian matrix in forward mode will usually be roughly n2times as expensive as the function itself. 16 Applying the complexity bound for the reverse mode separately to each component ofthe gradient one �nds thatworkfr2fg � nXi=1work�r� @f@xi�� � �Q nXi=1 work� @f@xi� :After division by workffg we obtain in agreement with the results in [17] and [11]workfr2fgworkffg � �Q Pni=1 workf@f=@xigworkfrfg � workfrfgworkffg � n �Q2 :In terms of powers of n this bound is unfortunately optimal, as one can see on the simpleexample f(x) = :5[xTx + (aTx)2] ; rf(x) = x+ (aTx)a ; r2f(x) = I + aaT :Here the function and gradient involve both 2n multiplication, whereas the accumulationof the Hessian requires certainly :5n2 multiplications.Fortunately, it is often su�cient to calculate derivative vectors of the formr1+pf(x)v1v2 : : : vp = r [rpf(x)v1v2 : : :vp]= vTp (r [rpf(x)v1v2 : : : vp�1])where the n-vectors vj ; j = 1 : : :p are given directions. For example Hessian-vector productsof the form r2f(x) v1 can be used in the conjugate gradient method (See e.g. [8] and [20]).Second and third derivatives of the form r2f(x)v1v2 and r3f(x)v1v2v3 characterize thequadratic and cubic turning points [12] of bifurcation theory. Moreover, the gradientsof these scalars involve terms of the form r3f(x)~v1~v2 and r4f(x)~v1~v2~v3, which need beevaluated during the calculation of the turning points by Newton's method. Selected secondderivatives of the Lagrangian occur in the gradient of smooth exact penalty functions [7]for constrained optimization.According to the second equation above, the desired vector of p + 1 � st derivatives isthe gradient of the dot product between vp and an analogous vector of p � th derivatives.Hence it may be computed recursively using p+1 sweeps of reverse gradient accumulation.This shows that evaluating the left hand side above should only be about 51+p times ascostly as evaluating the scalar function f itself. Thus we have exponential growth in theorder of the derivative p but still no dependence on the number of variables n.4 Computer Implementations of Automatic Di�erentiationSo far we have not really justi�ed the adjective automatic because all program transfor-mations were carried out by hand. Moreover, we can certainly not expect that the scalarfunction f(x) is supplied by the user in form of the Original Program in Section 3.1. Also,our speci�cation of the reverse mode via the extended program in Subsection 3.3 is not com-plete, because the required partial derivatives may be evaluated either during the forwardor the reverse sweep. Either variant has been implemented and yields certain advantages.17 4.1 Immediate versus Delayed Di�erentiationThe �rst variant might be called immediate di�erentiation with reverse accumulation. Pro-vided only �rst derivatives are required, every elementary function is linearized at its currentarguments during the forward sweep, and only the computational graph with the nodes xiand the arc values @fi=@xj needs to be stored in a suitable fashion. Even the nodal valuesxi are no longer required after the forward sweep, and they may be overwritten by thecorresponding adjoint values �xi during the reverse sweep. User de�ned subroutines thatreturn their gradient together with the function value are easily incorporated.Similarly, if there are segments of code that produce only one or two scalar values forthe subsequent calculations, the corresponding gradients can be preaccumulated in a localreverse sweep. In other words, these scalars may be interpreted as super � elementaryfunctions of the variables that enter into the segment, and their gradients can be computedduring the forward sweep. This applies in particular to single assignment statements withcomplicated right hand sides, e.g.x3 = (x1 + 3x2)2 + sin2 x1 exp(:2x2) :Here the the representation of x3 as a factorable function of x1 and x2 involves six unaryfunctions and three binary arithmetic operations. Thus we have originally 12 = 6 + 2 � 3partial derivatives as arc values. Preaccumulation of the partial derivatives @x3=@x1 and@x3=@x2 would cut that number to 2. Another example is the product considered in Section2, which might occur as a super-elementary function in a larger program. Preaccumulatingits gradient would essentially halve the number of arcs, whose origins, destinations andvalues have to be stored until the global reverse sweep.Except in the simple cases mentioned above, the detection of suitable super-elements orfunnels [28] requires some combinatorial analysis of the computational graph. If the samefunction is evaluated over and over such a potentially very large preprocessing e�ort maywell be justi�ed. However, it probably will only be economical when the graph is essentiallystatic, i.e. the control ow of the original program is largely independent of the variablevalues. As far as we know this kind of combinatorial optimization on the graph has not yetbeen implemented.A major disadvantage of immediate di�erentiation is the impossibility of obtaininghigher directional derivatives after the forward sweep has been completed. To this endone has to construct a complete representation of the computational graph at the currentargument, rather than just its linearization. In other words one has to store the type anddata dependence of each elementary function in a suitable symbol table. In a way thisdoubles up the structural information that is already contained in the program.4.2 FORTRAN PrecompilerThere are at least three such implementations, namely JAKEF [14], GRESS [13], andPADRE2 [17]. All three precompilers require the user to supply a source code for the18 evaluation of f(x) in some dialect of FORTRAN. The dependent and independent variablesmust be nominated through explicit declarations or a naming convention. The sourcecode is then fed to the precompiler, which analyses its arithmetic assignment statementsvery much like a normal compiler. As we have mentioned before the control statementsremain unaltered. All calculations involving real variables are broken down into elementaryarithmetic operations and univariate system functions, e.g. exponentials or trigonometricfunctions. For each of these elementary functions fi the precompiler has built in expressionsof the one or two partial derivatives @fi=@xj .Using this 'knowledge' the precompiler can construct an extended FORTRAN programthat evaluates the partial derivatives simultaneously with each elementary function. In theforward mode of GRESS, these local partial derivatives are used immediately to calculatethe full gradient rxi of the intermediate value xi with respect to the independent variablesnominated by the user. In the case of JAKEF and the reverse mode of GRESS, the localpartials are stored as arc values with a suitable encoding of their origin and destination,i.e. the j � th and i � th node respectively. PADRE2 delays the di�erentiation by storinginstead a symbol identifying the elementary function and the current argument, so that its�rst and possibly higher derivatives can be evaluated during the reverse sweep. To e�ectthe reverse sweep the precompilers insert a call to a standard accumulation subroutine atthe end of the program.The resulting extended FORTRAN programs rely on runtime support packages con-taining various standard subroutines and possibly also problem speci�c scratch �les. Theuser then compiles and links the whole suite to obtain an executable code for evaluating thefunction, its gradient, and in the case of PADRE2 also second derivatives or error estimates.As an example the next page displays the FORTRAN subroutine PROD that evaluates theproduct of n independent variables followed by the subroutine PRODJ obtained by precom-piling PROD with JAKEF. The in-line comments on the right were added later and wouldnaturally result in compilation errors.Apart from the �ve subroutines called in the extension PRODJ there are two othersubroutines in the runtime support library of JAKEF. Its total length is less than 150 linesof FORTRAN. When calling PRODJ the user has to provide the integer work arrays IFSand the real work array RFS with a su�ciently large common length LFS. The precompilercannot provide a lower bound on LFS, because the storage requirement is usually a functionof the number of variables and other problem parameters. This di�culty occurs in all reverseimplementations, whereas the storage requirement in the forward mode is predictable.Even though we have had no opportunity to test it, the recently released packageGRESS, developed at Oak Ridge National Laboratory, appears to be the most versatile anduser friendly precompiler for automatic di�erentiation that is currently available. It oper-ates in the forward or reverse mode and allows for user de�ned functions as well as implicitrelationships. PADRE2 is the only precompiler capable of producing second derivatives anderror estimates, but as yet it is only documented in Japanese. JAKEF is quite e�cient butdoes not allow user de�ned subroutines. 19 FORTRAN subroutine for evaluating productSUBROUTINE PROD(N,X,F)INTEGER N,IDOUBLE PRECISION F,GRADDOUBLE PRECISION X(N)CONSTRUCT D(F)/D(X) IN GRAD(N) {Nominate the dependentF = 1.D0 /independent variables }DO 10 I = 1,NF = F*X(I)10 CONTINUERETURNEND Extended FORTRAN program generated by JAKEFSUBROUTINE PRODJ(N,X,F,GRAD,YGRAD,LYGRAD,RFS,IFS,LFS)INTEGER LFS,IFS(LFS) { Lot's of extra storage }DOUBLE PRECISION RFS(LFS),TGRA(543)INTEGER N,I,LQ00,LQ01,LYGRAD,IGRAD,RGRAD,IXDOUBLE PRECISION X(N),F,GRAD(N),YGRAD(LYGRAD)IX = 544CALL DPINIT(IX+N,LYGRAD) { Initialization Routine }CALL DMIT0(1,RFS,IFS,LFS) {Storage of zero arc forF = 1.D0 constant assignment}LQ00 = 1LQ01 = NDO 90001 I = LQ00,LQ01 {Loop logically unaltered}CALL DMIT2(1,X(I),IX+I,F,1,RFS,IFS,LFS) {Storage of two arcs forF = F*X(I) multiplication}90001 CONTINUE90000 CONTINUERGRAD = 0CALL DPGRAD(YGRAD,LYGRAD,1,RGRAD,IGRAD,RFS,IFS,LFS) {AccumulationCALL DPCOPY(GRAD,IGRAD,1,YGRAD(IX+1),N) of gradient}RETURNEND 20 4.3 Operator OverloadingThe use of a precompiler means in e�ect that the original program is compiled twice, witha rather cryptic extended source code being generated as a by product. Hence one may ask,whether it is not possible to saddle the main compiler with the task of issuing the instructionsthat have to be executed in order to evaluate certain derivatives. This is in fact possible by afacility called operator overloading, which is available in most modern computer languages,including hopefully FORTRAN 8X. The key idea here is that the programmer can de�nenew types of variables, whose occurence as arguments of an elementary function triggersthe compiler to issue additional instructions. The source code itself remains essentiallyunchanged.Apparently the �rst implementation of this kind is due to Kedem [21]. Since FORTRANitself does not support overloading, he used the general purpose precompiler AUGMENT,which allowed the user to write the original program in a Taylor made extension of FORTAN.The resulting source code was then precompiled into standard FORTRAN by AUGMENT.Since most of its facilities are more conveniently available in modern computer languages,AUGMENT is no longer supported by its authors or anybody else. Kedem's extension ofFORTRAN enabled the user to compute gradients or truncated Taylor series in the forwardmode of automatic di�erentiation.A few years later Rall [26] achieved a much cleaner implementation of the forward modein the language PASCAL-SC, an extension of PASCAL for PC Compatibles distributed byTeubner and Wiley [22]. The transformation process is extremely simple. Suppose we havea standard PASCAL code for the evaluation of a function in the variables X [1::N ] of typeREAL. Then the X [I ] and all real variables that depend on them are redeclared to be of thenew type GRADIENT, which is completely problem independent. Each variable XJ of typeGRADIENT is a record consisting of a scalar part XJ:F and a vector part XJ:D[1::N ]. Ateach stage of the calculation the vector part represents the gradient of the scalar part withrespect to the independent variables X [1::N ]. The vector part of the independent variableX [I ] is initialized as the i-th Cartesian basis vector. Whenever an argument of type GRA-DIENT occurs in an elementary arithmetic operation, say the assignment Z := X � Y , thecompiler looks for an appropriate overloading of the usual elementary operation on REALs.Therefore Rall supplied small, problem independent operator declarations for every possi-ble combination of arguments, e.g. GRADIENT*GRADIENT, REAL*GRADIENT, andGRADIENT*REAL. In the last case for example, both the scalar and vector part of the�rst variable are multiplies by the second variable, which is of type REAL. UnfortunatelyPASCAL-SC does not allow the overloading of standard functions, so that the de�nitionof SIN(X) cannot be extended to arguments X of type GRADIENT. Instead one has tointroduce a new function GSIN(X) that evaluates and di�erentiates the sine for argumentsof type GRADIENT. This and some other limitations of PASCAL-SC require minor mod-i�cations of the program body. Any such changes could be avoided in a more powerfulprogramming language such as C++.The reverse mode of automatic di�erentiation can be implemented in a very similar way.Instead of GRADIENT we de�ne a new type VAREAL that represents a record consisting21 of one REAL value and two pointers to other VAREALs. In contrast to the length of thevector part in GRADIENT, the size of each record of type VAREAL does not depend on thetotal number of independent variables. At execution time the extended program generatesa doubly linked list of such records to represent the linearization of the computationalgraph at the current argument. Since they have to manipulate this data structure theoverloaded operators for arguments of type VAREAL are logically more complicated thanthose for arguments of type GRADIENT in Rall's implementation. However, according tocolumns 2 and 3 of Table 1 in Subsection 2.3 the reverse mode is always faster than theforward mode, even when the number of variables and hence the di�erence in the numberof arithmetic operations is small. This may partly be due to the lack of a mathematicalcoprocessor or oating point accelerator on the IBM PC in use. On systems with suchdevices the generation and manipulation of the doubly linked list might be relatively moreexpensive and thus shift the balance a bit in favor of the forward mode. Possibly for thesame reason, it was found that recreating the list during each of several function evaluationsis no more expensive than reusing the pointers from the �rst evaluations during subsequentcalls. Overloading as such has no bearing on the execution time, because the type dependentdecision which declaration of an operator applies at a particular occurence in the code isalready made during the compilation. Again using the product example, we have listedon the next page the original evaluation program in PASCAL-SC and its modi�cation forreverse di�erentiation via operator overloading. The program on the left simply reads in thenine variable values and prints out their product. The program on the right does exactlythe same and then prints out the nine components of the gradient at the given argument.The central sections of both codes are almost identical, except that the one on the rightneeds the conversion function VARY in assigning real values to variables of the new typeVAREAL. Conversely the function EVAL extracts the real value from a VAREAL, which isneeded in particular for output operations. The type VAREAL, the functions VARY andEVAL, the gradient accumulation procedure ACCUMULATE, the multiplication operator* between VAREALs, and the two pointer variables TAIL and SPARE are all de�nedin the problem independent header �le VHEAD.SRC occuring in the compiler directive$INCLUDE right at the top. The explicit initialization of TAIL and SPARE, and the twoconversion functions could be avoided in a programming language like C++, where theassignment operator can also be overloaded. Here, any oversight in making the requiredmodi�cations will result in compile or run time errors. If the independent variables aredeclared as VAREALs and program executes normally, then the gradient values should becorrect.Compared to precompilation overloading probably requires more user sophistication buton the other hand it clearly o�ers more exibility. Provided all subprograms are compiledtogether, either mode of automatic di�erentiation in PASCAL-SC can deal with user de-�ned functions and even recursive procedure calls. This does not require any extension ormodi�cation of the header �le. Higher derivatives and some optimization of the computa-tional graph can also be implemented by overloading. The forward evaluation of general andstructured Hessians in the advanced language ADA is discussed by Dixon and Mohseniniain [8]. When the currently proposed standard for FORTRAN 8X is actually implementedone of the major objections to operator overloading will be removed.22 Reverse Automatic Di�erentiation by Operator Overloading in PASCAL-SCPROGRAM PROD(INPUT,OUTPUT); PROGRAM PROD(INPUT,OUTPUT);$INCLUDE VHEAD.SRCVAR X : ARRAY[1..9] OF REAL; VAR X : ARRAY[1..9] OF VAREAL;Y,T : REAL; Y : VAREAL; T : REAL;I,N : INTEGER; I,N : INTEGER;BEGIN BEGINTAIL := NIL; SPARE := NIL;N := 5; N := 5;Y := 1; Y := VARY(1);FOR I := 1 TO N DO FOR I := 1 TO N DOBEGIN BEGINREAD(T); READ(T);X[I] := T; X[I] := VARY(T);Y := Y*X[I] Y := Y*X[I]END; END;WRITELN(Y); WRITELN(EVAL(Y));ACCUMULATE(Y);FOR I := 1 TO N DOWRITELN(EVAL(X[I]));END. END.Program for Product Example Extension with Reverse Di�erentiation 23 5 Conclusions and DiscussionLike several previous authors we conclude that in theory and practice the gradients of allfunctions de�ned by computer programs can be evaluated cheaply and automatically. Thisobservation suggests the reexamination of the many arguments in the optimization litera-ture, that rely at least implicitly on the seemingly reasonable assumption, that gradientscodes are often hard to to come by and run typically much slower than the correspondingfunction routine.Since truly derivative-free algorithms rarely have worked for more than a handful ofvariables, many researchers recommend the approximation of gradients by central or one-sided di�erences. Whenever this classical technique can be applied at all, we must have areasonably accurate evaluation algorithm, in which case automatic di�erentiation providesa far superior alternative. Provided there is enough storage, reverse accumulation yieldstruncation error free gradient values at less than 5=n times the computing time of divideddi�erences. This technique has been successfully implemented on problems in nuclear engi-neering and oceanography with thousands or even millions of variables. Should the functionevaluation be so lengthy that the storage of all intermediate results is impossible, then onecan still employ the forward mode to achieve better accuracy at essentially the same costas divided di�erences.Many line search procedures avoid the evaluation of the gradient at trial points beforethese have been accepted as the next main iterate. This strategy could still make sense,since we found that the gradient may well be four or �ve times more expensive to evaluatethen the function. Also, the cubic interpolation made possible by the value of the direc-tional derivative at the trial point destroys the simplicity of usual quadratic interpolation.Moreover the improved accuracy of the cubic interpolants rarely leads to a signi�cant re-duction in the overall number of evaluations or iterations. On the other hand, keeping twoevaluation routines ( one without and one with the gradient) and calling them successivelyat all main iterates does not seem that economical either.Penalty functions have long been used to convert constrained optimization problemsinto unconstrained problems. If one wants the penalty functions to be exact, i.e. attainlocal minima right at the solutions of the constrained problem, then there are basically twochoices. Either the penalty function is nonsmooth or it depends explicitly on the gradi-ents of the objective and constraint functions [7]. In the latter case the resulting gradientand Hessian depend on second and third derivatives of the original problem functions re-spectively. Since this additional level of di�erentiation was thought to be unacceptable,nonsmooth penalty functions have generally been preferred. However, automatic di�eren-tiation can produce the restricted second derivative terms in the gradient of smooth exactpenalty functions at a reasonable cost, namely a �xed multiple of evaluating the objec-tive and constraint functions. Therefore a suitable implementation of unconstrained BFGScould be both user friendly and e�cient, especially since the troublesome Maratos e�ect ofnonsmooth penalty functions cannot occur here.The combination of automatic di�erentiation with the variable metric method BFGS24 recommended above may seem a strange mixture. Indeed, some researchers in automaticdi�erentiation feel that the development of quasi-Newton methods was an emergency mea-sure, which is outdated now that we can obtain the Hessian automatically. This seems tous a rather premature assessment. As we have seen in Subsection 3.6 the evaluation of aHessian-vector product by either mode of automatic di�erentiation may be up to 5n times asexpensive as that of the gradient. Thus we must expect that sometimes an exact or inexactNewton method based on automatic di�erentiation of the gradient will be less e�cient thanthe corresponding �nite di�erence version. In view of the trouble with negative curvatureone may then prefer the simple and usually quite e�cient BFGS method with line-search.In any event automatic di�erentiation should allow the design of an optimization packagethat requires the user only to supply source code for the evaluation of the objective andconstraint functions. The generation of the corresponding gradient codes, the detectionof sparsity, and the determination of the maximal achievable solution accuracy, could allbe done automatically. Ideally, the selection of a suitable linear equation solver for thecomputation of steps on large structured problems could also be left to the package.In nonlinear least squares it is usually assumed that the calculating the gradient of theresidual norm requires the evaluation of the full Jacobian. Hence, the argument goes, wemight as well fully utilize this derivative information by employing a Gauss-Newton likeprocedure. However, as is the case for certain inverse problems [29], the Jacobian matrixmay be huge and dense, whereas reverse accumulation always yields the gradient cheaply.Then nonlinear conjugate gradients or a variable metric method with limited memory isclearly the only choice. On the other hand, there are many problems, where the Jacobianis of moderate size and costs little more than the residual vector to evaluate.Throughout this paper we have restricted our attention to a scalar valued function f(x)in n variables. Naturally all results and techniques can be separately applied to the mcomponents of a vector valued function F (x). However, this approach may be far fromoptimal if the component functions are closely related, i.e. have many common expressions.Also, if m is signi�cantly larger than n the forward mode of automatic di�erentiation islikely to be cheaper. Currently there appears to be no clearly superior strategy for theevaluation of derivative matrices (rather than vectors).Even though the underlying mathematics are straight forward much remains to donein the �eld of Automatic Di�erentiation. With regards to general purpose di�erentiationsoftware for various machine architectures, the problems are mainly of a computer sciencenature. However, some combinatorial analysis of the graph structure might be bene�cial forthe optimal evaluation of derivative matrices and the local preaccumulation of gradients,which was brie y mentioned in Subsection 4.1. Also, as in the case of evolution equationsdiscussed in Subsection 3.4, there are probably other problem classes in which the reversesweep has a natural interpretation and can be implemented in various ways. Finally, au-tomatic di�erentiation could and should be integrated into numerical packages for specialpurposes, such as optimization, sti� di�erential equation, boundary value problems, optimalcontrol, and path-following with bifurcation analysis. This process would be a lot simplerand more widely acceptable, if the next FORTRAN standard were to allow user-de�nedtypes with function and operator overloading.25 6 AcknowledgementsIn preparing this paper the author was aided by the incisive comments of Jorge Mor�e,Steven Wright, and Rob Womersley.References[1] F.L. Bauer (1974). "Computational Graphs and Rounding Errors", SINUM, Vol.11,No.1, pp.87-96 .[2] W. Baur and V. Strassen (1983). "The Complexity of Partial Derivatives", TheoreticalComputer Science, Vol. 22, pp.317-330.[3] L.M. Beda et al (1959). "Programs for Automatic Di�erentiation for the MachineBESM", Inst. Precise Mechanics and Computation Techniques, Academy of Science,Moscow.[4] D.G. Cacuci (1981). "Sensitivity Theory for Nonlinear Systems. I. Nonlinear FunctionalAnalysis Approach", Journal of Mathematical Physics, Vol.22, No.12, pp.2794-2802.[5] D.G. Cacuci (1981). "Sensitivity Theory for Nonlinear Systems. II. Extension to Addi-tional Classes of Responses", Journal of Mathematical Physics, Vol.22, No.12, pp.2803-2812.[6] B.W.Char, K.O.Geddes, G.H.Gonnet, M.B.Monegan, and S.M.Watt (1988). "MAPLEReference Manual, Fifth Edition", Symbolic Computation Group, Department of Com-puter Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1.[7] G. Di Pillo and L. Grippo (1986). "An Exact Penalty Method with Global ConvergenceProperties for Nonlinear Programming Problems", SIAM J. Control Optim. Vol.23,pp.72-84.[8] L.C.W.Dixon and M.Mohseninia (1987). "The Use of the Extended Operations Set ofADA with Automatic Di�erentiation and the Truncated Newton Method", TechnicalReport No.176, The Hat�eld Polytechnic, Hat�eld, U.K.[9] L.C.W.Dixon (1987). "Automatic Di�erentiation and Parallel Processing in Optimisa-tion", Technical Report No.180, The Hat�eld Polytechnic, Hat�eld, U.K.[10] Iu.G.Evtushenko (1982) . "Metody resheniia ekstremal'nykh zadach ikh primenenie vsistemakh optimizatsii", Nauka Publishers, Moskow[11] H. Fischer (1987). "Automatic Di�erentiation: How to compute the Hessian matrix",Technical Report #104A, Technische Universitat Munchen, Institut fur AngewandteMathematik und Statistik. 26 [12] A. Griewank and G.W. Reddien (1988). "Computation of Cusp Singularities for Opera-tor Equations and their Discretizations", Technical Memorandum ANL/MCS-TM-115,Mathematics and Computer Science Division, Argonne National Laboratory, ArgonneIL 60439. To appear in the special issue on Continuation Techniques and BifurcationProblems of the Journal of Computational and Applied Mathematics.[13] J.E. Horwedel, B.A. Worley, E.M. Oblow, and F.G. Pin (1988). "GRESS Version 0.0Users Manual" ORNL/TM 10835, Oak Ridge National Laboratory, Oak Ridge, Ten-nessee 37830, U.S.A.[14] K.E. Hillstrom (1985). "Users Guide for JAKEF", Technical Memorandum ANL/MCS-TM-16, Mathematics and Computer Science Division, Argonne National Laboratory,Argonne IL 60439.[15] M. Iri (1984). "Simultaneous Computations of Functions, Partial Derivatives and Es-timates of Rounding Errors - Complexity and Practicality", Japan Journal of AppliedMathematics, Vol.1, No.2 pp.223-252.[16] M. Iri, T. Tsuchiya, and M. Hoshi (1985). "Automatic Computation of Partial Deriva-tives and Rounding Error Estimates with Application to Large-Scale Systems of Nonlin-ear Equations" (in Japanese), Journal of the Information Processing Society of Japan,Vol. 27, No.4, pp.389-396.[17] M. Iri, and K. Kubota (1987). "Methods of Fast Automatic Di�erentiation and Appli-cations", Research memorandum RMI 87-0, Department of Mathematical Engineeringand Instrumentation Physics, Faculty of Engineering, University of Tokyo.[18] R.H.F. Jackson, and G.P. McCormick (1988). "Second order Sensitivity Analysis inFactorable Programming: Theory and Applications", Mathematical Programming,Vol.41, No.1, pp.1-28.[19] H. Kagiwada, R. Kalaba, N.Rosakhoo, and Karl Spingarn (1986). "Numerical Deriva-tives and Nonlinear Analysis", Vol. 31 of Mathematical Concepts and Methodsin Science and Engineering Edt. A.Miele, Plenum Press, New York and London[20] K.V. Kim, Iu.E. Nesterov, V.A. Skokov, and B.V. Cherkasskii (1984). "An e�cientAlgorithm for Computing Derivatives and extremal Problems" English translation of"E�ektivnyi algoritm vychisleniia proizvodnykh i ekstremal'nye zaduchi", Ekonomikai matematicheskie metody, Vol.20, No.2, pp.309-318.[21] G. Kedem (1980). "Automatic Di�erentiation of Computer Programs", ACM TOMS,Vol.6, No.2, pp.150-165.[22] U. Kulisch et al (1987). PASCAL-SC, A PASCAL Extension for Scienti�c Computa-tion, Information Manual and Floppy Disk, B.G. Teubner, Stuttgart, and John Wiley& Sons, New York.[23] G. Leitmann (1981). "The Calculus of Variations and Optimal Control" Vol.20 ofMathematical Concepts and Methods in Science and Engineering Edt.A.Miele, Plenum Press, New York and London27 [24] D.Y. Peng and D.B. Robinson (1976). "A new two-constant Equation of State", Ind.Eng. Chem. Fundamentals, Vol.15, pp.59-64.[25] L.B. Rall (1981). "Automatic Di�erentiation - Techniques and Applications", SpringerLecture Notes in Computer Science, Vol.120 .[26] L.B. Rall (1984). "Di�erentiation in PASCAL-SC: Type GRADIENT", ACM TOMSVol.10,pp.161-184.[27] L.B. Rall (1987). "Optimal Implementation of Di�erentiation Arithmetic", in Com-puter Arithmetic, Scienti�c Computation and Programming Languages, ed. U. Kulisch,Teubner, Stuttgart.[28] B.Speelpenning (1980). "Compiling fast Partial Derivatives of Functions given by Al-gorithms", Ph.D. Dissertation, Department of Computer Science, University of Illinoisat Urbana-Champaign, Urbana, IL 61801.[29] W.C. Thacker and R.B. Long (1988). "Fitting Dynamics to Data", Journal of Geo-physical Research, Vol.93, No.C2, pp.1227-1240.[30] R.E. Wengert (1964). "A simple Automatic Derivative Evaluation Program". Com.ACM, Vol. 7,pp.463-464 .[31] P.Wolfe (1982) ."Checking the Calculation of Gradients", ACM TOMS, Vol.6, No.4,pp. 337-343.[32] B.A. Worley et al (1989). "Deterministic Sensitivity, and Uncertainty Analysis in LargeScale Computer Models", Proceedings of 10th Annual DOE low level Waste Manage-ment Conference in Denver, Aug.30 -Sept.1, 1988. 28 