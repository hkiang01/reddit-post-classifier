 Under review as a conference paper at ICLR 2017 LEARNING GRAPHICAL STATE TRANSITIONS Daniel D. Johnson Department of Computer Science Harvey Mudd College 301 Platt Boulevard ddjohnson@hmc.edu ABSTRACT Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that can transform graph-structured inputs into output sequences. In this work I introduce a set of graph-based transformations, which I combine to construct a versatile extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and mod- ify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a fam- ily of Turing machines. 1 INTRODUCTION Many different types of data can be formulated using a graph structure. One form of data that lends itself to a graphical representation is data involving relationships (edges) between entities (nodes). Abstract maps of places and paths between them also have a natural graph representation, where places are nodes and paths are edges. In addition, many data structures can be expressed in graphical form, including linked lists and binary trees. Substantial research has been done on producing output when given graph-structured input (Kashima et al., 2003; Shervashidze et al., 2011; Perozzi et al., 2014; Bruna et al., 2013; Duvenaud et al., 2015). Of particular relevance to this work are Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), or GNNs, which extends recursive neural networks by assigning states to each node in a graph based on the states of adjacent nodes. Recently Li et al. (2016) have modified GNNs to use gated state updates and to produce output sequences. The resulting networks, called GG-NNs and GGS-NNs, are successful at solving a variety of tasks with graph-structured input. This work further builds upon GG-NNs and GGS-NNs by allowing graph-structured intermediate representations, as well as graph-structured outputs. This is accomplished using a more flexible graph definition, along with a set of graph transformations which take a graph and other information as input and produce a modified version of the graph. Combining these transformations with a recurrent input model yields the Gated Graph Transformer Neural Network model (GGT-NN), which incrementally constructs a graph given natural language input, and can either produce a final graph representing its current state, or use the graph to produce a natural language output. Extending GG-NNs in this way opens up a wide variety of applications. Since many types of data can be naturally expressed as a graph, it is possible to train a GGT-NN model to manipulate a meaningful graphical internal state. In this paper I demonstrate the GGT-NN model on the bAbI task dataset, which contains a set of stories about the state of the world. By encoding this state as a graph, a GGT-NN can learn to update the world state based on the input sentences and answer questions based on its internal graph. I also demonstrate that this architecture can learn complex update rules by training it to model a simple 1D cellular automaton and arbitrary 4-state Turing machines. This requires the network to learn how to transform its internal state based on the rules of each task. 1 Under review as a conference paper at ICLR 2017 1.1 GRU Gated Recurrent Units (GRU) are a type of recurrent network cell introduced by Cho et al. (2014). Each unit uses a reset gate r and an update gate z, and updates according to r(t) = σ ( Wrx (t) + Urh (t−1) + br ) z(t) = σ ( Wzx (t) + Uzh (t−1) + bz ) h̃(t) = φ ( Wx + U ( r(t) � h(t−1) ) + b ) h(t) = z� h(t−1) + (1− z)� h̃(t) where σ is the logistic sigmoid function, φ is an activation function (here tanh is used), x(t) is the input vector at timestep t, h(t) is the hidden output vector at timestep t, and W,U,Wr,Ur,Wz, Uz , b, br and bz are learned weights and biases. Note that � denotes elementwise multiplication. 1.2 GG-NN AND GGS-NN Gated Graph Neural Networks (GG-NN) are a form of graphical neural network models described by Li et al. (2016). In a GG-NN, a graph G = (V, E) consists of a set V of nodes v with unique values 1, . . . , |V| and a set E of directed edges e = (v, v′) ∈ V × V oriented from v to v′. Each node has an annotation xv ∈ RN and a hidden state hv ∈ RD. Additionally, each edge has a type ye ∈ {1, · · · ,M}. Initially, h(1)v is set to the annotation xv padded with zeros. Then nodes exchange information for some fixed number of timesteps T according to the propagation model h (1) v = [x>v ,0] > a (t) v = A>v:[h (t−1)> 1 · · ·h (t−1)> |V| ] > (1) z (t) v = σ(Wza (t) v + Uh (t−1) v ) r (t) v = σ(Wra (t) v + Urh (t−1) v ) h̃ (t) v = tanh(Wa (t) v + U(r (t) v � h(t−1)v )) h(t)v = (1− z(t)v )� h(t−1)v + z(t)v � h̃(t)v . Here a(t)v represents the information received by each node from its neighbors in the graph, and the matrix A ∈ RD|V|×2D|V| has a specific structure that determines how nodes communicate. The first half of A, denoted A(out) ∈ RD|V|×D|V|, corresponds to outgoing edges, whereas the second half A(in) ∈ RD|V|×D|V| corresponds to incoming edges. Each edge type y corresponds to specific forward and backward propagation matrices Py,P′y ∈ RD×D which determine how to propagate information across an edge of that type in each direction. The D × D-sized submatrix of A(out) in position i, j contains Py if an edge of type y connects nodes ni to nj , or 0 if no such edge connects in that direction. Similarly, theD×D-sized submatrix of the matrix A(in) in position i, j contains P′y if an edge of type y connects nodes nj to ni, or 0 if no such edge connects in that direction. Av: ∈ RD×2D|V| is the submatrix of A corresponding to node v. Thus, multiplication by Av: in 1 is equivalent to taking the following sum: a(t)v = ∑ v′∈V ( M∑ y=1 sedge(v, v ′, y)�Py + sedge(v′, v, y)�P′y ) h (t−1) v′ (2) where sedge(v, v′, y) is 1 if e = (v, v′) ∈ E and ye = y, and 0 otherwise. The output from a GG-NN is flexible depending on the task. For node selection tasks, a node score ov = g(h (T ) v ,xv) is given for each node, and then a softmax operation is applied. Graph-level outputs are obtained by combining an attention mechanism i and a node representation function j, both implemented as neural networks, to produce the output representation hG = tanh (∑ v∈V σ(i(h (T ) v ,xv))� tanh(j(h(T )v ,xv)) ) (3) Gated Graph Sequence Neural Networks (GGS-NN) are an extension of GG-NNs to sequential output o(1), . . . ,o(K). At each output step k, the annotation matrix X is given by X (k) = [x (k) 1 , . . . ,x (k) |V|] > ∈ R|V|×LV . A GG-NN Fo is trained to predict an output sequence o(k) from X (k), and another GG-NN FX is trained to predict X (k+1) from X (k). Prediction of the output at each step is performed as in a normal GG-NN, and prediction of X (k+1) from the set of all final hidden statesH(k,T ) (after T propagation steps of FX) occurs according to the equation x (k+1) v = σ ( j(h (k,T ) v ,x (k) v ) ) . 2 Under review as a conference paper at ICLR 2017 2 DIFFERENTIABLE GRAPH TRANSFORMATIONS In this section, I describe some modifications to the graph structure to make it fully differentiable, and then propose a set of transformations which can be applied to a graph structure in order to transform it. In particular, I redefine a graph G = (V, C) ∈ Γ as a set V of nodes v with unique values 1, . . . , |V|, and a connectivity matrix C ∈ R|V|×|V|×Y , where Y is the number of possible edge types. As before, each node has an annotation xv ∈ RN and a hidden state hv ∈ RD. However, there is an additional constraint that ∑N j=1 xv,j = 1. One can then interpret xv,j as the level of belief that v should have type j out of N possible node types. Each node also has a strength sv ∈ [0, 1]. This represents the level of belief that node v should exist, where sv = 1 means the node exists, and sv = 0 indicates that the node should not exist and thus should be ignored. Similarly, elements of C are constrained to the range [0, 1], and thus one can interpret Cv,v′,y as the level of belief that there should be a directed edge of type y from v to v′. (Note that it is possible for there to be edges of multiple types between the same two nodes v and v′, i.e. it is possible for Cv,v′,y = Cv,v′,y′ = 1 where y 6= y′.) 2.1 NODE ADDITION The node addition transformation Tadd : Γ × Rα → Γ takes as input a graph G and an input vector a ∈ Rα, and produces a graph G′ with additional nodes. The annotation and strength of each new node is determined by a function fadd : Rα × Rβ → R × RN × Rβ , where α is the length of the input vector, β is the length of the internal state vector, and as before N is the number of node types. The new nodes are then produced according to (s|VG |+i,x|VG |+i,hi) = fadd(a,hi−1), (4) starting with h0 initialized to some learned initial state, and recurrently computing sv and xv for each new node, up to some maximum number of nodes. Based on initial experiments, I found that implementing fadd as a GRU layer followed by 2 hidden tanh layers was effective, although other recurrent networks would likely be similarly effective. The node hidden states hv are initialized to zero. The recurrence should be computed as many times as the maximum number of nodes that might be produced. The recurrent function fadd can learn to output sv = 0 for some nodes to create fewer nodes, if necessary. 2.2 NODE STATE UPDATE The node state update transformation Th : Γ×Rα → Γ takes as input a graph G and an input vector a ∈ Rα, and produces a graph G′ with updated node states. This is accomplished by performing a GRU-style update for each node, where the input is a concatenation of a and that node’s annotation vector xv and the state is the node’s hidden state, according to rv = σ (Wr[a xv] + Urhv + br) , zv = σ (Wz[a xv] + Uzhv + bz) , h̃′v = tanh (W[a xv] + U (r� hv) + b) , h′v = zv � h′v + (1− zv)� h̃′v 2.2.1 DIRECT REFERENCE UPDATE For some tasks, performance can be improved by providing information to nodes of a particular type only. For instance, if the input is a sentence, and one word of that sentence directly refers to a node type (e.g., if nodes of type 1 represent Mary, and Mary appears in the sentence), it can be helpful to allow all nodes of type 1 to perform an update using this information. To accomplish this, Th can be modified to take node types into account. (This modification is denoted Th,direct.) Instead of a single vector a ∈ Rα, the direct-reference transformation takes in A ∈ RN×α, where An ∈ Rα is the input vector for nodes with type n. The update equations then become av = xvA rv = σ (Wr[av xv] + Urhv + br) , zv = σ (Wz[av xv] + Uzhv + bz) , h̃′v = tanh (W[av xv] + U (r� hv) + b) , h′v = zv � h′v + (1− zv)� h̃′v 3 Under review as a conference paper at ICLR 2017 2.3 EDGE UPDATE The edge update transformation TC : Γ×Rα → Γ takes a graph G and an input vector a ∈ Rα, and produces a graph G′ with updated edges. For each pair of nodes (v, v′), the update equations are cv,v′ = fset(a,xv,hv,xv′ ,hv′) rv,v′ = freset(a,xv,hv,xv′ ,hv′) C′v,v′ = (1− Cv,v′)� cv,v′ + Cv,v′ � (1− rv,v′). The functions fset, freset : Rα×2N×2D → [0, 1]Y are implemented as neural networks. (In my experiments, I used a simple 2-layer fully connected network.) cv,v′,y gives the level of belief in [0, 1] that an edge from v to v′ of type y should be created if it does not exist, and rv,v′,y gives the level of belief in [0, 1] that an edge from v to v′ of type y should be removed if it does. Setting both to zero results in no change for that edge, and setting both to 1 toggles the edge state. 2.4 PROPAGATION The propagation transformation Tprop : Γ → Γ takes a graph G = G(0) and runs a series of T propagation steps (as in GG-NN), returning the resulting graph G′ = G(T ). The GG-NN propagation step is extended to handle node and edge strengths, as well as to allow more processing to occur to the information transferred across edges. The full propagation equations for step t are a(t)v = ∑ v′∈V sv′ M∑ y=1 Cv,v′,y � f fwdy (xv′ ,h (t−1) v′ ) + Cv′,v,y � f bwd y (xv′ ,h (t−1) v′ ) (5) z(t)v = σ(Wz[a (t) v xv] + Uh (t−1) v + bz) (6) r(t)v = σ(Wr[a (t) v xv] + Urh (t−1) v + br) (7) h̃ (t) v = tanh(W[a (t) v xv] + U(r (t) v � h(t−1)v ) + bh) (8) h(t)v = (1− z(t)v )� h(t−1)v + z(t)v � h̃ (t) v . (9) Equation 5 has been adjusted in the most significant manner (relative to 2). In particular, sv′ restricts propagation so that nodes with low strength send less information to adjacent nodes, sedge has been replaced with C to allow edges with fractional strength, and the propagation matrices Py,P′y have been replaced with arbitrary functions f fwdy , f bwd y : RN × RD → Rα, where α is the length of the vector a. I used a fully connected layer to implement each function in my experiments. Equations 6, 7, and 8 have also been modified slightly to add a bias term. 2.5 AGGREGATION The aggregation transformation Trepr : Γ → Rα produces a graph-level representation vector from a graph. It functions very similarly to the output representation of a GG-NN, given in equation 3, but is modified slightly to take into account node strengths. As in GG-NN, both i and j are neural networks, and in practice a single fully conected layer appears to be adequate for both. hG = tanh (∑ v∈V svσ(i(h (T ) v ,xv))� tanh(j(h(T )v ,xv)) ) . 3 GATED GRAPH TRANSFORMER NEURAL NETWORK (GGT-NN) Combining a series of these transformations yields a Gated Graph Transformer Neural Network (GGT-NN). Depending on the configuration of the transformations, a GGT-NN can take textual or graph-structured input, and produce textual or graph-structured output. Here I describe one partic- ular GGT-NN configuration, designed to build and modify a graph based on a sequence of input sentences, and then produce an answer to a query. For each sentence k, each word is converted to a one-hot vector w(k)l , and the sequence of words (of length L) is passed through a GRU layer to produce a sequence of partial-sentence represen- tation vectors p(k)l . The full sentence representation vector i (k) is initialized to the last partial 4 Under review as a conference paper at ICLR 2017 Algorithm 1 Graph Transformation Pseudocode 1: G ← ∅ 2: for k from 1 to K do 3: G ← Th(G, i(k)) 4: if direct reference enabled then 5: G ← Th,direct(G,D(k)) 6: end if 7: if intermediate propagation enabled then 8: G ← Tprop(G) 9: end if 10: haddG ← Trepr(G) 11: G ← Tadd(G, [i(k) haddG ]) 12: G ← TC(G, i(k)) 13: end for 14: G ← T queryh (G, i query) 15: if direct reference enabled then 16: G ← T queryh,direct(G,D query) 17: end if 18: G ← T queryprop (G) 19: hanswerG ← T query repr (G) 20: return foutput(hanswerG ) representation vector p(k)L . Furthermore, a direct-reference input matrix D (k) is set to the sum of partial representation vectors corresponding to the words that directly reference a node type, i.e. D (k) n = ∑ l∈Rn p (k) l where Rn is the set of words in sentence k that directly refer to node type n. Next, a series of graph transformations are applied, as depicted in Algorithm 1. Depending on the task, direct reference updates and per-sentence propagation can be enabled or disabled. The output function foutput will depend on the specific type of answer desired. If the answer is a single word, foutput can be a multilayer perceptron followed by a softmax operation. If the answer is a sequence of words, foutput can use a recurrent network (such as a GRU) to produce a sequence of outputs. Note that transformations with different superscripts (Th and T queryh , for instance) refer to similar transformations with different learned weights. 3.1 SUPERVISION As with many supervised models, one can evaluate the loss based on the likelihood of producing an incorrect answer, and then minimize the loss by backpropagation. However, based on initial experiments, the model appeared to require additional supervision to extract meaningful graph- structured data. To provide this additional supervision, I found it beneficial to provide the correct graph at each timestep and train the network to produce that graph. This occurs in two stages, first when new nodes are proposed, and then when edges are adjusted. For the edge adjustment, the edge loss between a correct edge matrix C∗ and the computed edge matrix C is given by Ledge = ∑ C∗ � ln(C) + (1− C∗)� ln(1− C). The node adjustment is slightly more complex. Multiple nodes are added in each timestep, and those nodes are added in some order, but the order of the nodes is arbitrary. The order in which the nodes are created does not matter, only the existence of the nodes is important. Thus it should be possible for the network to determine the optimal ordering of the nodes. In fact, this is important because there is no guarantee that the nodes will be ordered consistently in the training data. Vinyals et al. (2016) demonstrate a simple method for training a network to output unordered sets: the network produces a sequence of outputs, and these outputs are compared with the closest order- ing of the training data, i.e., the ordering of the training data which would produce the smallest loss when compared with the network output. Vinyals et al. show that when using this method, the net- work arbitrarily chooses an ordering which may not be the optimal ordering for the task. However, in this case any ordering should be sufficient, and I found the arbitrary orderings selected in this way to work well in practice. In particular, letting s∗π(v) and x ∗ π(v) denote the correct strength and annotations of node v under ordering π, the loss becomes Lnode = max π |Vnew|∑ v=|Vold|+1 s∗π(v) ln(sv) + (1− s ∗ π(v)) ln(1− sv) + x ∗ π(v) ln(xv). At this point the correct values C∗, s∗π(v) and x ∗ π(v) are substituted into the graph for further process- ing. Note that only the edges and the new nodes are replaced by the supervision. The hidden states of all existing nodes are propagated without adjustment. 5 Under review as a conference paper at ICLR 2017 Direct reference No direct reference Task Accuracy No. ex. req. ≥ 95% Accuracy No. ex. req. ≥ 95% 1 - Single Supporting Fact 100% 100 99.3% 1000 2 - Two Supporting Facts 100% 250 94.3% FAIL 3 - Three Supporting Facts 98.7% 1000 88.0% FAIL 4 - Two Arg. Relations 98.8% 1000 97.8% 1000 5 - Three Arg. Relations 87.2% FAIL 80.2% FAIL 6 - Yes/No Questions 100% 100 92.3% FAIL 7 - Counting 100% 250 94.4% FAIL 8 - Lists/Sets 100% 250 96.7% 1000 9 - Simple Negation 100% 250 88.4% FAIL 10 - Indefinite Knowledge 96.6% 1000 71.4% FAIL 11 - Basic Coreference 100% 100 99.8% 1000 12 - Conjunction 99.9% 500 99.3% 1000 13 - Compound Coref. 100% 100 99.2% 1000 14 - Time Reasoning 97.8% 1000 44.9% 1000 15 - Basic Deduction 99.1% 500 100% 500 16 - Basic Induction 100% 100 100% 500 17 - Positional Reasoning 88.9% FAIL 51.3% FAIL 18 - Size Reasoning 97.9% 1000 89.4% FAIL 19 - Path Finding 100% 500 29.4% FAIL 20 - Agent’s Motivations 100% 250 99.0% 250 Table 1: Performance of GGT-NN on the bAbI tasks. “No. ex. req. ≥ 95%” refers to the number of training examples required before the network was able to reach 95% accuracy or better on the task. 4 EXPERIMENTS 4.1 BABI TASKS I evaluated the GGT-NN model on the bAbI tasks, a set of simple natural-language tasks, where each task is structured as a sequence of sentences followed by a query (Weston et al., 2016). The gener- ation procedure for the bAbI tasks includes a “Knowledge” object after each sentence, representing the current state of knowledge after that sentence. I exposed this knowledge object in graph format, and used this to train a GGT-NN in supervised mode. The knowledge object provides names for each node type, and direct reference was performed based on these names: if a word in the sentence matched a node type name, it was parsed as a direct reference to all nodes of that type. For details on this graphical format, see Appendix A. 4.1.1 RESULTS I trained two versions of the GGT-NN model for each task: one with and one without direct refer- ence. Tasks 3 and 5, which involve a complex temporal component, were trained with intermediate propagation, whereas all of the other tasks were not because the structure of the tasks made such complexity unnecessary. Most task models were configured to output a single word, but task 19 was configured using a GRU to output multiple words, and task 8 (the listing task) was configured to output a strength for each possible word to allow multiple words to be selected without having to consider ordering. Results are shown in Table 1. The GGT-NN model with direct reference performed very well on the majority of the tasks, reaching accuracies of at least 95% in all but two tasks, and reaching 100% accuracy in the majority of the tasks. Additionally, for many of the tasks, the model was able to reach 95% accuracy using 500 or fewer of the 1000 training examples. The two exceptions were task 5 (Three Arg. Relations) and task 17 (Positional Reasoning), for which the model was not able to attain a high accuracy. Task 5 involves sophisticated temporal reasoning, and thus requires a complex graphical structure to model accurately. Task 17 has a larger number of possible entities than the other tasks: each entity consists of a color (chosen from five options) and a shape (chosen from four shapes), for a total of 20 unique entities that must be represented separately. It is likely that these additional complexities caused the network performance to suffer. 6 Under review as a conference paper at ICLR 2017 Of particular interest is the performance of the GGT-NN model with direct reference on task 19, the pathfinding task. Previous models, such as the end-to-end memory networks described by Sukhbaatar et al. (2015), have struggled to learn this task. On the other hand, GGS-NN models were able to successfully learn the pathfinding task, but required the input to be preprocessed into graphical form even during testing (Li et al., 2016). The current results demonstrate that the pro- posed GGT-NN model is able to solve the pathfinding task when given textual input. In general, the GGT-NN model with direct reference performs better than the model without it (see Table 1). Although the model without direct reference reaches 95% accuracy on more than half of the tasks, it fails to reach 95% accuracy on multiple other tasks. Additionally, when compared to the direct-reference model, it requires more training examples in order to reach the accuracy threshold. This indicates that, although the model can be used without direct reference, adding direct reference greatly improves the training of the model. 4.2 RULE DISCOVERY To demonstrate the power of GGT-NN to model a wide variety of graph-based problems, I applied the GGT-NN to two additional tasks. In each task, a sequence of data structures were transformed into a graphical format, and the GGT-NN was tasked with predicting the data for the next timestep based on the current timestep. No additional information was provided as textual input; instead, the network was tasked with learning the rules governing the evolution of the graph structure over time. 4.2.1 CELLULAR AUTOMATON TASK The first task used was a 1-dimensional cellular automaton, specifically the binary cellular automa- ton known as Rule 30 (Wolfram, 2002). Rule 30 acts on an infinite set of cells, each with a binary state (either 0 or 1). At each timestep, each cell deterministically changes state based on its previous state and the states of its neighbors. In particular, the update rules are Current neighborhood 111 110 101 100 011 010 001 000 Next value 0 0 0 1 1 1 1 0 Cell states can be converted into graphical format by treating the cells as a linked list. Each of the cells is represented by a node with edges connecting it to the cell’s neighbors, and a value edge is used to indicate whether the cell is 0 or 1. This format is described in more detail in Appendix A. 4.2.2 TURING MACHINES The second task was simulating an arbitrary 2-symbol 4-state Turing machine. A Turing machine operates on an infinite tape of cells, each containing a symbol from a finite set of possible symbols. It has a head, which points at a particular cell and can read and write the symbol at that cell. It also has an internal state, from a finite set of states. At each timestep, based on the current state and the contents of the cell at the head, the machine writes a new symbol, changes the internal state, and can move the head left or right or leave it in place. The action of the machine depends on a finite set of rules, which specify the actions to take for each state-symbol combination. Note that the version of Turing machine used here has only 2 symbols, and requires that the initial contents of the tape be all 0 (the first symbol) except for finitely many 1s (the second symbol). When converting a Turing machine to graphical format, the tape of the machine is modeled as a linked list of cells. Additionally, each state of the machine is denoted by a state node, and edges between these nodes encode the transition rules. There is also a head node, which connects both to the current cell and to the current state of the machine. See Appendix A for more details. 4.2.3 ANALYSIS The GGT-NN model was trained on 1000 examples of the Rule 30 automaton with different ini- tial states, each of which simulated 7 timesteps of the automaton, and 20,000 examples of Turing machines with different rules and initial tape contents, each of which simulated 6 timesteps of the Turing machine. Performance was then evaluated on 1000 new examples generated with the same format. The models were evaluated by picking the most likely graph generated by the model, and 7 Under review as a conference paper at ICLR 2017 Original Task Generalization: 20 Generalization: 30 Automaton 100.0% 87.0% 69.5% Turing 99.9% 90.4% 80.4% Table 2: Accuracy of GGT-NN on the Rule 30 Automaton and Turing Machine tasks. 1000 iterations 2000 iterations 3000 iterations 7000 iterations Ground truth Figure 1: Visualization of network performance on the Rule 30 Automaton task. Top node (purple) represents zero, bottom node (blue) represents 1, and middle nodes (green, orange, and red) repre- sent individual cells. Blue edges indicate adjacent cells, and gold edges indicate the value of each cell. Three timesteps occur between each row. comparing it with the correct graph. The percent accuracy denotes the fraction of the examples for which these two graphs were identical at all timesteps. In addition to evaluating the performance on identical tasks, the generalization ability of the models was also assessed. The same trained models were evaluated on versions of the task with 20 and 30 timesteps of simulation. Results are shown in Table 2. The models successfully learned the assigned tasks, reaching high levels of accuracy for both tasks. Additionally, the models show the ability to generalize to large inputs, giving a perfect output in the majority of extended tasks. For visualization purposes, Figure 1 shows the model at various stages of training when evaluated starting with a single 1 cell. 5 CONCLUSION The results presented here show that GGT-NNs are able to successfully model a wide variety of tasks using graph-structured states and potentially could be useful in solving many other types of problems. The specific GGT-NN model described here can be used as-is for tasks consisting of a sequence of input sentences and graphs, optionally followed by a query. In addition, due to the modular nature of GGT-NNs, it is possible to reconfigure the order of the transformations to produce a model suitable for a different task. As one example, Appendix B describes a version of the model that uses the full sequence of sentence graphs while computing the answer to the query, instead of basing the answer on the final graph only. One downside of the current model is that the time and space required to train the model increase very quickly as the complexity of the task increases, which limits the model’s applicability. It would be very advantageous to develop optimizations that would allow the model to train faster and with smaller space requirements. There are exciting potential uses for the GGT-NN model. One particularly interesting application would be using GGT-NNs to extract graph-structured information from unstructured textual de- scriptions. More generally, the graph transformations provided here may allow machine learning to interoperate more flexibly with other data sources and processes with structured inputs and outputs. ACKNOWLEDGMENTS I would like to thank Harvey Mudd College for computing resources. I would also like to thank the developers of the Theano library, which I used to run my experiments. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575. 8 Under review as a conference paper at ICLR 2017 REFERENCES Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, pp. 2224–2232, 2015. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729–734. IEEE, 2005. Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs. In ICML, volume 3, pp. 321–328, 2003. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. ICLR, 2016. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre- sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701–710. ACM, 2014. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg- wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539– 2561, 2011. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440–2448, 2015. Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016. URL http://arxiv.org/abs/ 1605.02688. John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D Peterson, et al. XSEDE: accelerating scientific discovery. Computing in Science & Engineering, 16(5):62–74, 2014. Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. ICLR, 2016. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. ICLR, 2016. Stephen Wolfram. A new kind of science, volume 5. Wolfram media Champaign, 2002. 9 http://arxiv.org/abs/1605.02688 http://arxiv.org/abs/1605.02688 Under review as a conference paper at ICLR 2017 1. John grabbed the milk. 2. John travelled to the bedroom. 3. Sandra took the football. 4. John went to the garden. 5. John let go of the milk. 6. Sandra let go of the football. 7. John got the football. 8. John grabbed the milk. Where is the milk? actor_is_in_location Milk John Bedroom Football Sandra Garden gettable_is_in_location gettable_is_in_actor Figure 2: Diagram of one sample story from the bAbI dataset (Task 2), along with a graphical representation of the knowledge state after the italicized sentence. APPENDIX A GRAPH FORMAT DETAILS A.1 BABI TASKS The knowledge graph object used during generation of the bAbI tasks is structured as a dictionary relating entities to each other with specific relationship types. Entities are identified based on their names, and include people (John, Mary, Sandra), locations (bedroom, kitchen, garden), objects (football, apple, suitcase), animals (mouse, wolf, cat), and colors (white, yellow, green), depending on the particular task. Relationships between entities are also expressed as strings, and are directed: if John is holding the milk there is an “is in” relationship from “milk” to “John”; if Sandra is in the bedroom there is an “is in” relationship from “Sandra” to “bedroom”; if Lily is green there is a “has color” relationship from “Lily” to “green”, etc. The transformation from the knowledge object to a graph is straighforward: each entity used is assigned to a new node type, and relationships between entities are represented as edges between the correpsonding nodes. To avoid confusion from overloaded relationships (such as “is in” being used to represent an object being held by a person as well as a person being in a room), relation names are given a distinct edge type depending on the usage context. For instance, when a person is carrying an object, the generic “is in” relationship becomes an edge of type “gettable is in actor”. Some of the graph representations had to be modified in order to ensure that they contained all of the necessary information. For instance, task 3 requires the network to remember where items were in the past, but the knowledge object only contained references to their current locations. In these cases, a linked list structure was added to the knowledge object to allow the history information to be represented in the graph. In particular, each time an item changed locations, a new “record” node was added, with a “previous” edge to the previous history node and a “value” edge to the current location of the item. Each item then connected to the most recent history node using a “history-head” edge. This ensures that the history of each node is present in the graph. In a few of the tasks, specific entities had multi-word representations. While this works for normal input, it makes it difficult to do direct reference, since direct reference is checked on an individual word level. These tasks were modified slightly so that the entities are referred to with single words (e.g. “red square” instead of “red square”). An example of a graph produced from the bAbI tasks is given in Figure 2. A.2 CELLULAR AUTOMATON The cellular automaton task was mapped to graphical format as follows: Nodes have 5 types: zero, one, init-cell, left-cell, and right-cell. Edges have 2 types: value, and next-r. There is always exactly one “zero” node and one “one” node, and all of the cell nodes form a linked list, with a “value” edge 10 Under review as a conference paper at ICLR 2017 1. init 1 2. init 1 3. init 1 4. init 1 5. init 1 6. init 0 7. init 0 8. init 0 9. init 1 10. init 1 11. init 1 12. init 1 13. init 0 14. simulate 15. simulate 16. simulate 17. simulate 18. simulate 19. simulate 20. simulate Zero One New cells (left) New cells (right) Initial cells Value edges Neighbor edges Figure 3: Diagram of one example from the automaton task, along with a graphical representation of the automaton state after the fourth simulate command (italicized). 1. rule state 3 0 0 state 0 L 2. rule state 1 0 1 state 0 R 3. rule state 2 1 1 state 2 L 4. rule state 3 1 0 state 3 L 5. rule state 0 1 0 state 0 R 6. rule state 0 0 1 state 2 N 7. rule state 2 0 0 state 2 R 8. rule state 1 1 1 state 0 N 9. start state 1 10. input symbol 0 head 11. input symbol 0 12. input symbol 0 13. input symbol 1 14. run 15. run 16. run 17. run 18. run 19. run Current state States and rules Head Cells Zero One Current cell Figure 4: Diagram of an example from the Turing machine task, with a graphical representation of the machine state after the second run command (italicized). connecting to either zero or one, and a “next-r” edge pointing to the next cell to the right (or no edge for the rightmost cell). At the start of each training example, there are 13 timesteps with input of the form “init X” where X is 0 or 1. These timesteps indicate the first 13 initial cells. Afterward, there are 7 “simulate” inputs. At each of these timesteps, one new left-cell node is added on the left, one new right-cell node is added on the right, and then all cells update their value according to the Rule 30 update rules. An example of the graphical format for the cellular automaton task is given in Figure 3. A.3 TURING MACHINE For the Turing machine task, nodes were assigned to 8 types: state-A, state-B, state-C, state-D, head, cell, 0, and 1. Edges have 16 types: head-cell, next-left, head-state, value, and 12 types of the form rule-R-W-D, where R is the symbol read (0 or 1), W is the symbol written (0 or 1), and D is the direction to move afterward (Left, Right, or None). State nodes are connected with rule edges, which together specify the rules governing the Turing machine. Cell nodes are connected to adjacent cells with next-left edges, and to the symbol on the tape with value edges. Finally, the head node is connected to the current state with a head-state edge, and to the current cell of the head with a head-cell edge. At the start of each training example, each of the rules for the Turing machine are given, in the form “rule state-X R W state-Y D”. Next, the initial state is given in the format “start state-X”, and the initial contents of the tape (of length 4) are given sequentially in the format “input symbol-X”, with the position for the head to start marked by “input symbol-X head”. Finally, there are 6 “run” inputs, after each of which the head node updates its edges and the cell at the head updates its value according to the rules of the Turing machine. If the head leaves the left or right of the tape, a new node is introduced there. An example of the graphical format for the Turing machine task is given in Figure 4. 11 Under review as a conference paper at ICLR 2017 Algorithm 2 Sequence-Extended Pseudocode G0 ← ∅ . Initialize G to an empty graph for k from 1 to K do . Process each sentence Gk ← Th(Gk−1, i(k)) if direct reference enabled then Gk ← T directh (Gk,D(k)) end if if intermediate propagation enabled then Gk ← Tprop(Gk) end if haddGk ← Trepr(Gk) Gk ← Tadd(Gk, [i(k) haddGk ]) Gk ← TC(Gk, i(k)) end for hanswersummary ← 0 . Initialize hanswersummary to the zero vector for k from 1 to K do . Process the query for each graph Gk ← T queryh (Gk, i query) if direct reference enabled then Gk ← T query,directh (Gk,D query) end if Gk ← T queryprop (Gk) hanswerGk ← T query repr (Gk) hanswersummary ← fsummarize(hanswerGk ,h answer summary) end for return foutput(hanswersummary) APPENDIX B GRAPH SEQUENCE INPUT The model described in Section 3 conditions the output of the model on the final graph produced by the network. This is ideal when the graph represents all of the necessary knowledge for solving the task. However, it may also be desirable for each graph to represent a subset of knowledge corre- sponding to a particular time, and for the output to be based on the sequence of graphs produced. For instance, in the third bAbI task (which requires reasoning about the temporal sequence of events) each graph could represent the state of the word at that particular time, instead of representing the full sequence of events prior to that time. In Appendix A, section A.1, I describe a transformation to the tasks which allows all information to be contained in the graph. But this adds complexity to the graphical structure. If it were possible for the model to take into account the full sequence of graphs, instead of just the final one, we could maintain the simplicity of the graph transformation. To this end, I present an extension of the GGT-NN model that can produce output using the full graphical sequence. In the extended model, the graphical output of the network after each input sentence is saved for later use. Then, when processing the query, the same set of query transfor- mations are applied to every intermediate graph, producing a sequence of representation vectors hanswer1 , . . . ,h answer K . These are then combined into a final summary representation vector h answer summary using a recurrent network such as a GRU layer, from which the output can be produced. The modi- fied pseudocode for this is shown in Algorithm 2. I evaluated the extended model on bAbI tasks 3 and 5, the two tasks which asked questions about a sequence of events. (Note that although Task 14 also involves a sequence of events, it uses a set of discrete named time periods and so is not applicable to this modification.) The model was trained on each of these tasks, without the extra record and history nodes used to store the sequence, instead simply using the sequence of graphs to encode the relevant information. Due to the simpler graphs produced, intermediate propagation was also disabled. 12 Under review as a conference paper at ICLR 2017 Direct reference No direct reference Task Accuracy Accuracy 3 - Three Supporting Facts 90.3% 65.4% 5 - Three Arg. Relations 89.8% 74.2% Table 3: Performance of the sequence-extended GGT-NN on the two bAbI tasks with a temporal component. Results from training the model are shown in Table 3. The accuracy of the extended model appears to be slightly inferior to the original model in general, although the extended direct-reference model of task 5 performs slightly better than its original counterpart. One possible explanation for the inferiority of the extended model is that the increased amount of query processing made the model more likely to overfit on the training data. Even so, the extended model shows promise, and could be advantageous for modeling complex tasks for which preprocessing the graph would be impractical. 13 Introduction GRU GG-NN and GGS-NN Differentiable Graph Transformations Node Addition Node State Update Direct Reference Update Edge Update Propagation Aggregation Gated Graph Transformer Neural Network (GGT-NN) Supervision Experiments bAbI Tasks Results Rule Discovery Cellular Automaton Task Turing Machines Analysis Conclusion Graph Format Details bAbI Tasks Cellular Automaton Turing Machine Graph Sequence Input 