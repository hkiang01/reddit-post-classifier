Stochastic Variational Deep Kernel LearningNIPS 2016Paper: https://arxiv.org/abs/1611.00336Code: https://people.orie.cornell.edu/andre...Authors: Andrew Gordon Wilson*, Zhiting Hu*, Ruslan Salakhutdinov, Eric P. XingThis work can be used as a plug-in to stand-alone deep networks, with minor additional runtime overhead, in exchange for improved predictive performance, interpretability, and full predictive distributions.SV-DKL exploits algebraic structure in deep kernels formed from (e.g. convolutional) deep neural networks in conjunction with stochastic variational inference. The resulting approach is an extremely scalable Gaussian process which can handle non-Gaussian likelihoods and stochastic mini-batch training, with very flexible kernel functions.We apply the method to many different classification problems, showing improvements in performance, but also steps towards more interpretable deep learning.Our poster is onMon Dec 5th 06:00 -- 09:30 PM @ Area 5+6+7+8 #100Longer talk about new directions for Gaussian processes and kernel methods: https://slideshot.epfl.ch/play/k5FuJc..."Scalable Gaussian Processes for Scientific Discovery".