 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1602.05179 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1602 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Benjamin Scellier Yoshua Bengio Bookmark (what is this?) Computer Science > Learning Title: Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation Authors: Benjamin Scellier, Yoshua Bengio (Submitted on 16 Feb 2016 (v1), last revised 26 Sep 2016 (this version, v4)) Abstract: We introduce Equilibrium Propagation (e-prop), a learning algorithm for energy-based models. This algorithm involves only one kind of neural computation both for the first phase (when the prediction is made) and the second phase (after the target is revealed) of training. Contrary to backpropagation in feedforward networks, there is no need for special computation in the second phase of our learning algorithm. Equilibrium Propagation combines features of Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: the algorithm computes the exact gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of e-prop corresponds to only nudging the first-phase fixed point towards a configuration that has lower cost value. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The theory developed in this paper shows that the signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to the objective function. Thus, this work makes it more plausible that a mechanism similar to backpropagation could be implemented by brains. Subjects: Learning (cs.LG) Cite as: arXiv:1602.05179 [cs.LG]   (or arXiv:1602.05179v4 [cs.LG] for this version) Submission history From: Benjamin Scellier [view email] [v1] Tue, 16 Feb 2016 20:46:51 GMT (300kb,D) [v2] Wed, 24 Feb 2016 11:13:08 GMT (301kb,D) [v3] Tue, 20 Sep 2016 16:15:26 GMT (436kb,D) [v4] Mon, 26 Sep 2016 09:55:15 GMT (439kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 