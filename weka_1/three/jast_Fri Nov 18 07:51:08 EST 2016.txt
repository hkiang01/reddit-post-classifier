 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.05763 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.AI stat stat.ML References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Jane X. Wang Zeb Kurth-Nelson Dhruva Tirumala Hubert Soyer Joel Z. Leibo ... Bookmark (what is this?) Computer Science > Learning Title: Learning to reinforcement learn Authors: Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick (Submitted on 17 Nov 2016 (v1), last revised 24 Nov 2016 (this version, v2)) Abstract: In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience. Comments: 17 pages, 7 figures, 1 table Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:1611.05763 [cs.LG]   (or arXiv:1611.05763v2 [cs.LG] for this version) Submission history From: Jane Wang [view email] [v1] Thu, 17 Nov 2016 16:29:11 GMT (3617kb,D) [v2] Thu, 24 Nov 2016 15:35:02 GMT (3268kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 