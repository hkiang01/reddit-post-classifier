 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01186 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.NE < prev | next > new | recent | 1611 Change to browse by: cs cs.LG stat stat.ML References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Sihan Li Jiantao Jiao Yanjun Han Tsachy Weissman Bookmark (what is this?) Computer Science > Neural and Evolutionary Computing Title: Demystifying ResNet Authors: Sihan Li, Jiantao Jiao, Yanjun Han, Tsachy Weissman (Submitted on 3 Nov 2016) Abstract: We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process. Subjects: Neural and Evolutionary Computing (cs.NE); Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1611.01186 [cs.NE]   (or arXiv:1611.01186v1 [cs.NE] for this version) Submission history From: Sihan Li [view email] [v1] Thu, 3 Nov 2016 20:55:49 GMT (460kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 