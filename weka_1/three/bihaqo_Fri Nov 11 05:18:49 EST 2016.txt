 Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.03214 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Timur Garipov Dmitry Podoprikhin Alexander Novikov Dmitry P. Vetrov Bookmark (what is this?) Computer Science > Learning Title: Ultimate tensorization: compressing convolutional and FC layers alike Authors: Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, Dmitry Vetrov (Submitted on 10 Nov 2016) Abstract: Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset. Comments: NIPS 2016 workshop: Learning with Tensors: Why Now and How? Subjects: Learning (cs.LG) Cite as: arXiv:1611.03214 [cs.LG]   (or arXiv:1611.03214v1 [cs.LG] for this version) Submission history From: Alexander Novikov [view email] [v1] Thu, 10 Nov 2016 08:07:46 GMT (22kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. 