When training a sequence to sequence model or maybe a CTC based ASR system, you are either forced to use a batch size of one, or need to account for varying sequence lengths for each batch element in every single network element in your model.

Tensorflow's tf.nn.dynamic_rnn() function handles this, by just updating the state of an RNN, when the current time index is still valid for a batch entry. Otherwise, the old state is kept and it outputs zeros for all newer frames. A blog post about this is provided in [1] and was already discussed in /r/MachineLearning. The lasagne wrapper uses a mask_input for the lasagne.layers.RecurrentLayer [3].

Nevertheless, this does not solve the problem for other parts of a model. A model might as well contain CNN layers, where one dimension is dynamic. Even a regular RNN might have a final linear layer, which also needs to account for different sequence lengths.

An often used technique is to use masking in the loss function. Chainer for example does this, by introducing an ignore label [2].

The problem becomes more apparent, when you want to use normalization (i.e. batch normalization or normalization along the time axis). In case of a batch normalization, gradients incoming to an underlying linear layer may change its weight matrix along invalid paths.

Sorting your data in approximately equal lengths utterances or using bucketing is a good idea, but does not entirely solve the problem.

TLDR: Can different sequence lengths be handled at a framework level or does every network element need to be changed in order to be sequence-lengths-ready?

[1] https://danijar.com/variable-sequence-lengths-in-tensorflow/
[2] https://github.com/pfnet/chainer/blob/285e6558640425f61b6aa8c00564ccf37643babb/chainer/functions/loss/softmax_cross_entropy.py#L63
[3] http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html#lasagne.layers.CustomRecurrentLayer