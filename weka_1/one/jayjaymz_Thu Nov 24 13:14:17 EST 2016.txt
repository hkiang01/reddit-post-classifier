Hello, 

I am working on a character sequence prediction problem. I have created a (stateful as is defined in Keras) 3 layer RNN in Keras that takes sequences of equal length and tries to predict the next character. I'm training it on a 80million character dataset. I'm using mini-batches and my loss function is (categorical) cross entropy.

The definition of perplexity I'm refering to can be found [here](http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf).

What I can not understand is if and how you can calculate perplexity given a single batch's loss, since I'm trying in mini batches.

        loss = training_model.train_on_batch(x, y)

Is this cross entropy error I'm getting the same as in the definition of entropy? Especially seeing how x and y are of size(batch_size, seq_length, vocab_size) can I actually use this error to calculate perplexity?

Code for my implementation is here: https://github.com/AuthEceSoftEng/rnn2source/blob/master/src/char-rnn.py

Feel free to ask for clarifications, thanks!
