Several years ago I saw Jeff Hawkins present on the NuPIC platform for on-line machine learning.

Last week I was thinking about our company's Service Oriented Architecture and all our pipes and collectors for monitoring and stats and logs and so forth, and how unwieldy and difficult to wrangle it is, and it occurred to me that perhaps ML can assist here.

I reason that I could set up a simple NN that took as input the current stream of scalar data from a single monitor, and output [FAULT, NO-FAULT].

NO-FAULT should be the default state, and emits as long as the input follows predicted traffic.

FAULT emits when the input goes some percentage outside of the prediction.

Deploy independent ML for every input, or pipe several related inputs into one?

Regardless, the goal would be to eliminate the need for an expert human to interpret 'good' data vs 'problem' data and set discrete alerting thresholds. Eliminate the need to have deployment-specific configurations; all hosts get the same treatment and the company benefits from better alerting.

This was demonstrated on HTM for monitoring energy consumption of an office building.

Is HTM still the best model for training machines to do this, or have other tools/techniques emerged?
