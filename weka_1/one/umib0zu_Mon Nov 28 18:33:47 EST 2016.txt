I'm listening to Hinton's lectures in his NN4ML, and he mentioned something interesting that sort of opened up a question for me about the information theory of a neural network model. If we think at an abstract level of a communication channel, a markov chain model with N states has a information capacity of lg_2(N). He mentioned that in the early days of language models where we train a model to output the next letter given a previous letter, this was the best we could do with computer technology, but he mentioned RNNs have better "information". I'm just not sure what he means by information.

My first question, does he mean information capacity? This would make more sense since of course, the information capacity of a markov chain is just the maximum entropy of it's output, log_2(N). For an RNN, there's obviously more capacity, since the capacity is the mutual information of X (the inputs) and Y (the output).

Also, here's the second question, why is this *more* than a standard NN or regression model? I don't see how if we abstracted and treated the RNN as a black box that takes input and spits out an output, how there is more mutual information than an appropriately modeled regression model that takes the same number of inputs, maybe an offset model that takes some previous k letters? Both models take the same number of inputs, and output the same set of character strings, so how is one better than the other in terms of mutual information?