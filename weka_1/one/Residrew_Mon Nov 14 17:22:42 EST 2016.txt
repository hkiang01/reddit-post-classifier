*[Asked this](https://www.reddit.com/r/learnmachinelearning/comments/5cnrph/time_series_classification/) first in /r/learnmachinelearning with essentially no replies so I'm forwarding my questions here (if there's another sub more suitable please let me know).*

***

Hi, fairly new to ML. I've had trouble grasping/remembering concepts through just reading, so I've made an effort to jump onto a project with a problem I'm interested in.

I have data of raw positional tracking data (25 fps) from NBA games (10 players and the ball), and I'm trying to detect when an on ball screen occurs. [Found a paper](http://www.sloansportsconference.com/wp-content/uploads/2014/02/2014_SSAC_Recognizing-on-Ball-Screens.pdf) that did this, which I plan on loosely follow the steps in the paper. I've also found a good source to watch the games associated to the data I have for labelling. 

To start i've implemented the segmentation of the data into actions [*2 Data Segmentation*], and watched one game and labelled all on ball screens to verify the sensitivity of detecting screens.

Right now I'm stuck with a number of questions...

***

**Feature Extraction**

Feature extraction is one thing I'm general pretty clueless about. In the paper [3.2 Feature Extraction], they take all pairwise distances between the ball-handler, his defender, the screener, and the basket (6 distance-time series) and defined the moment that the ball handler and the screen are closest. From what it looks like the features extracted were the time where min occurs, and the mean and average speeds for before and after the defined moment.

Why are those features here chosen, and in what is the general procedure for extracting features?

After that they "discretize each continuous feature into five binary features based on quintiles". What does that mean? What is the benefit of doing it?  

I'm pretty interested in general on feature engineering and want to read more about feature extraction and feature selection. Does anyone have a solid resource(s) on the topic?

***

**Small Dataset**

As one person there's no way I can watch all the games and label them (~600 games). In the paper their training/test set was only 252/234 time series respectively. From my understanding having a smaller dataset means it's much harder to avoid overfitting. In what ways can that be achieved and how large should be the dataset be? 

***

Lastly what are the best methods / approaches for time series classification? The paper uses a SVM but from some initial research, Hidden Markov Models, Dynamic Time Warping, and LSTMs are what I see commonly mentioned.
