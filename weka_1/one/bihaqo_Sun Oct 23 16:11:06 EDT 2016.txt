Hi!

I’m a PhD student in machine learning. I'm going to make a Coursera course with my colleagues, that will be a part of ML- and statistics-based specialization. Our MOOC will last **4-5 weeks**. We are currently discussing the main topic of the course.

Our’s and our research group's main expertise is in Bayesian methods (we are bayesgroup.ru after all), convolutional neural networks, and RNNs.
We have several ideas and need to pick one:

* “Recurrent Neural Networks”. RNNs is a part of many deep learning courses, but lots of people have never coded/ran an LSTM. Our course will cover main aspects, tips and tricks of RNNs and will offer some hands-on tutorials on them along with a chance to solve real-world problems. 
* “How to read deep learning papers”. More and more folks are entering deep learning these days, but reading state-of-the-art papers still scares lots of people away. We plan to focus the course on reading assignments. The student will be asked to read and analyze papers, and maybe write a short essay. After that, we will analyze the very same papers in the lectures and share some tips on what to look at. We will discuss stuff like “why did they do this thing instead of this one?”, discuss the connections to other papers and reasonable things to try next. We will also look into the official reviews for these papers (for NIPS and ICLR papers, where the reviews are available). It would be tough to make such course work, though...
* “Bayesian methods for machine learning“. The general form of the EM-algorithm, MCMC methods, variational inference and maybe variational autoencoders, matrix calculus (like differentiating a function w.r.t. a matrix). Coding homeworks (like [using EM algorithm to recover a villain photo from many noisy observations](http://cmp.felk.cvut.cz/cmp/courses/recognition/Labs/em/index_en.html)).
* “Practical neural networks”. Different topic each week (CNNs, RNNs, NLP, etc.), and a Kaggle competition as a coding homework. In lectures, there would be practical tips like “try this kind of augmentation”. To see what I mean, look at this brilliant blog post: [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/). It focuses on a particular Kaggle competition and goes step by step from a simplest neural network to a top notch one. Each step is motivated like “we increased the model capacity and it indeed overfitted. Let's add dropout -- see, it helps”.

What topic do you guys think would be most beneficial for the community? Do you have any suggestions or additional ideas?

**UPD**

Thanks everyone for the feedback, I think we will stick with the "Practical Bayesian methods" as most of you suggested. The rough plan is to have 3 lectures about the basic techniques (like EM and MCMC) and after each lecture show how do they apply to VAE. So after the first lecture, we will introduce VAE idea, but will not tell anything about the training, and will build up from that. The last two lectures will be dedicated to neural-Bayesian models other than VAE.

BTW, if you liked any of the course ideas I mentioned above, go ahead and still it to make your own course! 