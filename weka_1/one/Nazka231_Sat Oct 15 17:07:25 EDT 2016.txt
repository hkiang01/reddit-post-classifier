 Skip to content the morning paper an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer Home About Subscribe Towards deep symbolic reinforcementÂ learning October 12, 2016 Towards deep symbolic reinforcement learning Garnelo et al, 2016 Every now and then I read a paper that makes a really strong connection with me, one where I canâ€™t stop thinking about the implications and I canâ€™t wait to share it with all of you. For me, this is one such paper. In the great see-saw of popularity for artificial intelligence techniques, symbolic reasoning and neural networks have taken turns, each having their dominant decade(s). The popular wisdom is that data-driven learning techniques (machine learning) won. Symbolic reasoning systems were just too hard and fragile to be successful at scale. But what if weâ€™re throwing the baby out with the bath water? What if instead of having to choose between the two approaches, we could combine them: a system that can learn representations, and then perform higher-order reasoning about those representations? Such combinations could potentially bring to bear the fullness of AI research over the last decades. I love this idea because: (a) it feels intuitively right (we as humans learn to recognise types of things, and then form probability-based rules about their behaviour for example); (b) it is very data efficient, to appropriate a phrase:Â  â€œa rule can be worth 1000(+) data points!â€ (c) it opens up the possibility to incorporate decades of research into modern learning systems, where you canâ€™t help but think there would be some quick wins. â€¦ we show that the resulting system â€“ though just a prototype â€“ learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural Deep Reinforcement Learning system on a stochastic variant of the game. In short, this feels to me like something that could represent a real breakthrough and a step-change in the power of learning systems. You should take that with a pinch of salt, because Iâ€™m just an interested outsider following along. I hope that Iâ€™m right though! Why combine symbolic reasoning and deep learning? Contemporary Deep Reinforcement Learning (DRL) systems achieve impressive results but still suffer from a number of drawbacks: They inherit from deep learning the need for very large training sets, so that they learn very slowly They are brittle in the sense that a trained network that performs well on one task often performs poorly on a new task, even if it is very similar to the original one. They do not use high-level processes such as planning, causal reasoning, or analogical reasoning to fully exploit the statistical regularities present in the training data. They are opaque â€“ it is typically difficult to extract a human-comprehensible chain of reasons for the action choice the system makes . In contrast, classical AI uses language-like propositional representations to encode knowledge. Â  Thanks to their compositional structure, such representations are amenable to endless extension and recombination, an essential feature for the acquisition and deployment of high-level abstract concepts, which are key to general intelligence. Moreover, knowledge expressed in propositional form can be exploited by multiple high-level reasoning processes and has general-purpose application across multiple tasks and domains. Features such as these, derived from the benefits of human language, motivated several decades of research in symbolic AI. But as an approach to general intelligence, classical symbolic AI has been disappointing. A major obstacle here is the symbol grounding problemâ€¦ The symbol grounding problem is this: where do the symbols come from? They are typically hand-crafted rather than grounded in data from the real world. This brings a number of problems: They cannot support support ongoing adaptation to a new environment They cannot capture the rich statistics of real-world perceptual data They create a barrier to full autonomy Machine learning has none of these problems! So what if we could take the good bits from machine learning, and combine them with the good bits from classical AI? Use machine learning to learn symbolic representations, and then use symbolic reasoning on top of those learned symbols for action selection (in the case of DRL). You end with a system architecture that looks like this: Four principles for building Deep Symbolic Reinforcement Learning (DSRL) systems Support conceptual abstraction by mapping high-dimensional raw input into a lower dimensional conceptual state space, and then using symbolic methods that operate at a higher level of abstraction.Â  â€œThis facilitates both data efficient learning and transfer learning as well as providing a foundation for other high-level cognitive processes such as planning, innovative problem solving, and communication with other agents (including humans). Enable compositional structure that supports combining and recombining elements in an open-ended way.Â  â€œTo handle uncertainty, we propose probabilistic first-order logic for the semantic underpinnings of the low-dimensional conceptual state space representation onto which the neural front end must map the systemâ€™s high-dimensional raw input. Build on top of common sense priors â€“ it is unrealistic to expect an end-to-end reinforcement learning system to succeed with no prior assumptions about the domain. For example: objects frequently move, and typically do so in continuous trajectories, there are stereotypical events such as beginning to move, stopping, coming into contact with other objects, and so on. Support causal reasoning through discovery of the causal structure of the domain and symbolic rules expressed in terms of both the domain and the common sense priors. To carry out analogical inference at a more abstract level, and thereby facilitate the transfer of expertise from one domain to another, the narrative structure of the ongoing situation needs to be mapped to the causal structure of a set of previously encountered situations. As well as maximising the benefit of past experience, this enables high-level causal reasoning processes to be deployed in action selection, such as planning, lookahead, and off-line exploration (imagination). A worked example Well it all sounds good, but does it actually work? The prototype system built by the authors learns to play four variations on a simple game. An agent moves around a square space populated by circles and crosses. It receives a positive reward for every cross it â€˜collectsâ€™, and a negative reward for every circle. Four variations are used: the first has only circles in a fixed grid, the second both circles and crosses in a fixed grid, the third only circles but in a random grid, and the fourth both circles and crosses in a random grid. The system has three stages: low-level symbol generation, representation building, and reinforcement learning. The first stage use a convolutional neural network (autoencoder) trained on 5000 randomly generated images on game objects scattered across the screen. The activations in the middle layer of the CNN are used directly for detection of objects in the scene. The salient areas in an image result in higher activation throughout the layers of the convolutional network. Given the geometric simplicity of the games, this is enough to extract the individual objects from any given frame. The objects identified this way are then assigned a symbolic type according to the geometric properties computed by the autoencoder. This is done by comparing the activation spectra of the salient pixels across features. At this point we know the type and position of the objects. The second stage learns to track objects across frames in order to learn from their dynamics. The system is supported in this task by a common sense prior, â€œobject persistence over timeâ€. This is broken down into three measures ultimately combined into a single value: how close an object in one frame is to an object in another frame (spatial proximity is an indicator it may be the same object); how likely it is that an object transformed from one type into another (by learning a transition probability matrix); and what change there is in the neighbourhoodÂ  of an object. The representation learned at the end of the second stage differs from that of the first stage in two key ways: It is extended to understand changes over time, and Positions of objects are represented by relative coordinates to other objects rather than using absolute coordinates. â€œThis approach is justified by the common sense prior that local relations between multiple objects are more relevant than the global properties of single objects.â€ WeÂ  now have a concise spatio-temporal representation of the game situation, one that captures not only what objects are in the scene along with their locations and types, but also what they are doing. In particular, it represents frame-to-frame interactions between objects, and the changes in type and relative position that result. This is the input to reinforcement learning, the third and final stage of the pipeline. Finally we enter the reinforcement learning stage, where the relative location representation of objects greatly reduces the state space, on the assumption that things that are far apart (both in the game and in the real world) tend to have little influence on each other. In order to implement this independence we train a separate Q function for each interaction between two object types. The main idea is to learn several Q functions for the different interactions and query those that are relevant for the current situation. Given the simplicity of the game and the reduced state space that results from the sparse symbolic representation we can approximate the optimal policy using tabular Q-learning. Evaluation results You can use precision and recall metrics to evaluate agent performance â€“ where precision is interpreted as the percentage of objects you collect that are positive (crosses), and recall is interpreted as the percentage of available positive objects that you actually do collect. The first results show that the agent improves with training to a precision of 70%. Itâ€™s interesting to compare the performance of DQN and the DSRL system. DQN does better in the the grid scenario, but when objects are position at random the DQN agent struggles to learn an effective policy within 1000 epochs. The DSRL system performs much better on this variant: To evaluate transfer learning, both DQN and DSRL were trained on the grid variation, and then tested on the random variant.Â  DQN does no better than chance, whereas the DSRL system is able to approach 70% precision. Â  We conjecture that DQN struggles with this game because it has to form a statistical picture of all possible object placements, which would require a much larger number of games. In contrast, thanks to the conceptual abstraction made possible by its symbolic front end, our system very quickly â€œgetsâ€ the game and forms a set of general rules that covers every possible initial configuration. This demonstration merely hints at the potential for a symbolic front end to promote data efficient learning, potential that we aim to exploit more fully in future work. Our proof-of-concept system also illustrates one aspect of the architectureâ€™s inherent capacity for transfer learningâ€¦ Moreover, the DSRL system can explain its actions: every action choice can be analysed in terms of the Q functions involved in the decision. These Q functions describe the types of objects involved in the interaction as well as their relations, so we can track back to the reasons that led to a certain decision. And this is just the beginningâ€¦ We could wire in a more sophisticated deep network capable of unsupervised learning to disentangle representations in the earlier stages. We can exploit many more of the achievements in classical AI, notably the incorporation of inductive logic programming, formal techniques for analogical reasoning, and building in a planning component that exploits the knowledge of the cause structure of the domain acquired during the learning process. Â  In domains with sparse reward, itâ€™s often possible for an agent to discover a sequence of actions leading to a reward state through off-line search rather than on-line exploration. Contemporary logic-based planning methods are capable of efficiently finding large plans in complex domains (eg:[37]), and it would be rash not to exploit the potential of these techniques. Finally, one day the symbolic components of the proposed architecture could well be one day using neurally-based implementations of symbolic reasoning functions. In the mean time, an architecture that combines deep neural networks with directly implemented symbolic reasoning seems like a promising research direction. Share this: Twitter LinkedIn Email Print Like this: Like Loading... Related from â†’ Uncategorized â† Progressive neural networks Go to statement consideredÂ harmful â†’ 15 Comments leave one â†’ Mats Jonsson permalink October 12, 2016 6:56 am This is one of those â€˜send thrills down your spineâ€™ papers. Thanks! Reply ya permalink October 15, 2016 11:05 pm Its a shame the paper does not mention or reference existing work in this area. Reply ctaank permalink October 16, 2016 1:23 am I think the approach youâ€™ve outlined is definitely the way to go. Reply Sean Aubin permalink October 16, 2016 5:29 am Disclaimer: I work in the lab that created Spaun. This seems kind of similar to Spaun [0]. You use Deep Learning to get the features you want and then you do symbolic like operations on those feature vectors to accomplish reinforcement learning. At least Spaun used neurons for the rule manipulation too. I also donâ€™t believe the transfer learning claim at all, because the tasks are way too similar. Still, itâ€™s a step in the right direction as far as Iâ€™m concerned [1]. That being said, good luck find a set of general prior rules and scaling them. [0] http://science.sciencemag.org/content/338/6111/1202 [1] https://medium.com/@seanaubin/why-does-ai-still-suck-and-can-it-suck-less-9db36be294dc#.akb82s8o9 Reply Kevin permalink October 17, 2016 3:51 pm Hi, Did you have a look at COQ program which is aimed to prove through computation Mathematical Theorem ? it seems sharing a lot of common points. Thank you Kind re Reply Tim Vergenz permalink November 22, 2016 2:31 am Amazingly exciting. I feel like this is starting to head towards the way the human brain works: blending association-based learning (i.e. artificially, neural networks; or biologically, the subconscious part of the human brainâ€“see â€œBlink: The Power of Thinking Without Thinkingâ€ [2007] for an interesting laymanâ€™s discussion of the latter) with sequential rational processing. Reading this gave me chills. Aside: Could you set the og:image meta tag for the page to be the first figure with the general architecture?ğŸ™‚ It does a better job inviting reader in IMO. Reply adriancolyer permalink* November 22, 2016 11:11 am Glad you enjoyed the paper, itâ€™s one of my recent favourites too. Iâ€™ve set the architecture diagram as the featured image in WordPress â€“ hopefully that will do the trickâ€¦. Regards, Adrian. Reply Tim Vergenz permalink November 22, 2016 3:40 pm Thanks! Seems like it worked.ğŸ™‚ This is so exciting. Weâ€™re finally starting to make AI approaches that combine logical thinking with association-based learningâ€“much like how the human brain works! For the interested, hereâ€™s a 3-minute excerpt from â€œBlinkâ€ (2007). Connecting the paper to this book was what made me so excited. â€” â€œImagine that I were to ask you to play a very simple gambling game. In front of you are four decks of cardsâ€”two of them red and the other two blue. Each card in those four decks either wins you a sum of money or costs you some money. â€¦ What you donâ€™t know at the beginning, however, is that the red decks are a minefield. The rewards are high, but when you lose on the red cards, you lose a lot. Actually, you can win only by taking cards from the blue decks, which offer a nice steady diet of $50 payouts and modest penalties. The question is how long will it take you to figure this out? A group of scientists at the University of Iowa did this experiment a few years ago, and what they found is that after weâ€™ve turned over about fifty cards, most of us start to develop a hunch about whatâ€™s going on. We donâ€™t know why we prefer the blue decks, but weâ€™re pretty sure at that point that they are a better bet. After turning over about eighty cards, most of us have figured out the game and can explain exactly why the first two decks are such a bad idea. â€¦ But the Iowa scientists [also] did something else. â€¦ They hooked each gambler up to a machine that measured the activity of the sweat glands below the skin in the palms of their hands. [These glands respond to stress.] â€¦ What [they] found is that gamblers started generating stress responses to the red decks by the tenth card, forty cards before they were able to say that they had a hunch about what was wrong with those two decks. More importantly, right around the time their palms started sweating, their behavior began to change as well. They started favoring the blue cards and taking fewer and fewer cards from the red decks. In other words, the gamblers figured the game out before they realized they had figured the game out: they began making the necessary adjustments long before they were consciously aware of what adjustments they were supposed to be making. â€¦ [I]n those moments, our brain uses two very different strategies to make sense of the situation. The first is the one weâ€™re most familiar with. Itâ€™s the conscious strategy. We think about what weâ€™ve learned, and eventually we come up with an answer. This strategy is logical and definitive. But it takes us eighty cards to get there. Itâ€™s slow, and it needs a lot of information. Thereâ€™s a second strategy, though. It operates a lot more quickly. It starts to kick in after ten cards, and itâ€™s really smart, because it picks up the problem with the red decks almost immediately. It has the drawback, however, that it operatesâ€”at least at firstâ€”entirely below the surface of consciousness. It sends its messages through weirdly indirect channels, such as the sweat glands in the palms of our hands. Itâ€™s a system in which our brain reaches conclusions without immediately telling us that itâ€™s reaching conclusions.â€ Tim Vergenz permalink November 22, 2016 3:41 pm Thanks! Seems to have worked.ğŸ™‚ For the interested, hereâ€™s a 3-minute excerpt from Blink (2007). The obvious parallel is most of why I got so excited about this article. â€” â€œImagine that I were to ask you to play a very simple gambling game. In front of you are four decks of cardsâ€”two of them red and the other two blue. Each card in those four decks either wins you a sum of money or costs you some money. â€¦ What you donâ€™t know at the beginning, however, is that the red decks are a minefield. The rewards are high, but when you lose on the red cards, you lose a lot. Actually, you can win only by taking cards from the blue decks, which offer a nice steady diet of $50 payouts and modest penalties. The question is how long will it take you to figure this out? A group of scientists at the University of Iowa did this experiment a few years ago, and what they found is that after weâ€™ve turned over about fifty cards, most of us start to develop a hunch about whatâ€™s going on. We donâ€™t know why we prefer the blue decks, but weâ€™re pretty sure at that point that they are a better bet. After turning over about eighty cards, most of us have figured out the game and can explain exactly why the first two decks are such a bad idea. â€¦ But the Iowa scientists [also] did something else. â€¦ They hooked each gambler up to a machine that measured the activity of the sweat glands below the skin in the palms of their hands. [These glands respond to stress.] â€¦ What [they] found is that gamblers started generating stress responses to the red decks by the tenth card, forty cards before they were able to say that they had a hunch about what was wrong with those two decks. More importantly, right around the time their palms started sweating, their behavior began to change as well. They started favoring the blue cards and taking fewer and fewer cards from the red decks. In other words, the gamblers figured the game out before they realized they had figured the game out: they began making the necessary adjustments long before they were consciously aware of what adjustments they were supposed to be making. â€¦ [I]n those moments, our brain uses two very different strategies to make sense of the situation. The first is the one weâ€™re most familiar with. Itâ€™s the conscious strategy. We think about what weâ€™ve learned, and eventually we come up with an answer. This strategy is logical and definitive. But it takes us eighty cards to get there. Itâ€™s slow, and it needs a lot of information. Thereâ€™s a second strategy, though. It operates a lot more quickly. It starts to kick in after ten cards, and itâ€™s really smart, because it picks up the problem with the red decks almost immediately. It has the drawback, however, that it operatesâ€”at least at firstâ€”entirely below the surface of consciousness. It sends its messages through weirdly indirect channels, such as the sweat glands in the palms of our hands. Itâ€™s a system in which our brain reaches conclusions without immediately telling us that itâ€™s reaching conclusions.â€ Reply Trackbacks Machine Learning Roundup 10/13/2016 | Big Analytics Bookmarks for October 14th through October 15th : Extenuating Circumstances Misc stuff: The verification gap, ML training and more â€“ The Foretellix Blog Distilled News | Data Analytics & R Artificial Intelligence and life in 2030 | the morning paper Building machines that learn and think like people | the morning paper Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. (Â LogÂ OutÂ /Â ChangeÂ ) You are commenting using your Twitter account. (Â LogÂ OutÂ /Â ChangeÂ ) You are commenting using your Facebook account. (Â LogÂ OutÂ /Â ChangeÂ ) You are commenting using your Google+ account. (Â LogÂ OutÂ /Â ChangeÂ ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Subscribe never miss an issue! The Morning Paper delivered straight to your inbox. Search Archives Archives Select Month December 2016 Â (2) November 2016 Â (22) October 2016 Â (21) September 2016 Â (20) July 2016 Â (16) June 2016 Â (22) May 2016 Â (22) April 2016 Â (12) March 2016 Â (23) February 2016 Â (21) January 2016 Â (23) December 2015 Â (10) November 2015 Â (21) October 2015 Â (22) September 2015 Â (22) August 2015 Â (22) June 2015 Â (21) May 2015 Â (21) April 2015 Â (14) March 2015 Â (23) February 2015 Â (23) January 2015 Â (21) December 2014 Â (15) November 2014 Â (20) October 2014 Â (20) May 2011 Â (3) Most read in the last few days Slicer: Auto-sharding for datacenter applications The amazing power of word vectors Morpheus: Towards automated SLOs for enterprise clusters Early detection of configuration errors to reduce failure damage Building machines that learn and think like people Firmament: Fast, centralized cluster scheduling at scale Twice the bits, twice the trouble: vulnerabilities induced by migrating to 64-bit platforms When CSI meets public wifi: Inferring your mobile phone password via wifi signals Towards deep symbolic reinforcement learning The Linux Scheduler: a Decade of Wasted Cores RSS RSS - Posts RSS - Comments Live on twitter My Tweets Blog at WordPress.com. Send to Email Address Your Name Your Email Address Cancel Post was not sent - check your email addresses! Email check failed, please try again Sorry, your blog cannot share posts by email. %d bloggers like this: 