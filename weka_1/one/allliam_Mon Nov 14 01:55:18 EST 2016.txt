I'm wondering what research has been done on when (and why) gradient descent (in all its forms) fails to learn a random neural network. I'm assuming the structure of the model being trained is the same as the random network we are attempting to learn.  I'm particularly interested in how large and deep a network needs to be before SGD fails to find a good solution. Also, there are a lot of hyper-parameters to [experiment over](https://arxiv.org/pdf/1206.5533v2.pdf), and I would be curious if there are any conclusions about the conditions of a network which would make learning more sensitive to a particular hyper-parameter.

Does it make sense to use random neural networks as a proxy for the problems we care about? If not, has anyone found regularities of the parameters (edge weights) of DNNs that solve real problems?