Hey there guys,

I have a question about the paper on Direct Feedback Alignment (well actually two questions), specifically this paper 
( https://arxiv.org/abs/1609.01596). 

1) For both FA and DFA we still take the analytic derivative of the layer but replace the W matrix transpose with a random matrix and for DFA replace the "chained gradient" from the previous layer with e, correct?

2) What exactly is e? Is it just y_hat-y or is there something else to it like the derivative of MSE?

Just to add a clarification to question 1, if it was a convolutional layer, we would take the derivative of the convolution operation instead of just B (which is the derivative of a linear operation), right?
Then, we would replace the all the weight matrix transposes in the analytic derivative with a fixed random matrix B, right?


Thanks in advance guys!