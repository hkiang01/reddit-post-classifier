In several applications using LSTMs/RNNs (such as char-rnn or Alex Graves handwriting generation), at training time we learn a recurrent network that predicts the next input as the output of the previous time-step. At prediction time, to do multi-step prediction we feed the last input and the output we obtain is considered to be the input at the next time-step. This process is continued until we get predictions for as many steps as we need.
I understand that since the LSTM is trained to do one-step prediction very accurately, this model still ends up with good predictions for multiple steps after the last input. But, as is bound to happen, the uncertainty/error in the predictions grows at each step and we could end up with very inaccurate predictions after 2-3 steps.
I am currently trying the model introduced by Graves for a sequence prediction and I observe that my one-step prediction is quite accurate but the error blows up after 2 steps.
I was wondering, is there a better way to do this, that has been explored in the past? Note that the problem is not to generate new text or handwriting, but conditioned on a part of the sequence, predict the next few time-steps.