
{"author":"jaromiru","created":"Wed Oct 12 11:40:19 EDT 2016","text":" ヤロミル about AI Skip to content Let\u2019s make a DQN: Theory September 27, 2016DQNヤロミル This article is part of series Let\u2019s make a DQN. 1. Theory 2. Implementation 3. Debugging 4. Full DQN 5. Double DQN and Prioritized experience replay Introduction In February 2015, a group of researches from Google DeepMind published a paper1 which marks a milestone in machine learning. They presented a novel, so called DQN network, which could achieve breathtaking results by playing a set of Atari games, receiving only a visual input. In these series of articles, we will progressively develop our knowledge to build a state-of-the-art agent, that will be able to learn to solve variety of tasks just by observing the environment. We will explain the needed theoretical background in less technical terms and then build a program to demonstrate how the theory works in practice. These articles assume some acquaintance with reinforcement learning and artificial neural networks. For those of you, who are not familiar with these terms, I recommend to have a look at a great book from professor Sutton, 19982 first. We will use Python to program our agent, Keras library to create an artificial neural network (ANN) and OpenAI Gym toolkit as the environment. Background The ultimate goal of reinforcement learning is to find a sequence of actions from some state, that lead to a reward. Undoubtedly, some action sequences are better than others. Some sequences might lead to a greater reward, some might get it faster. To develop this idea more, let\u2019s put it down more formally. The agent is in a state s and has to choose one action a, upon which it receives a reward r and come to a new state s\u2019. The way the agent chooses actions is called policy. Let\u2019s define a function Q(s, a) such that for given state s and action a it returns an estimate of a total reward we would achieve starting at this state, taking the action and then following some policy. There certainly exist policies that are optimal, meaning that they always select an action which is the best in the context. Let\u2019s call the Q function for these optimal policies Q*. If we knew the true Q* function, the solution would be straightforward. We would just apply a greedy policy to it. That means that in each state s, we would just choose an action a that maximizes the function Q* \u2013 . Knowing this, our problem reduces to find a good estimate of the Q* function and apply the greedy policy to it. Let\u2019s write a formula for this function in a symbolic way. It is a sum of rewards we achieve after each action, but we will discount every member with γ: γ is called a discount factor and when set it to , it makes sure that the sum in the formula is finite. Value of each member exponentially diminish as they are more and more in the future and become zero in the limit. The γ therefore controls how much the function Q in state s depends on the future and so it can be thought of as how much ahead the agent sees. Typically we set it to a value close, but lesser to one. The actions are chosen according to the greedy policy, maximizing the Q* function. When we look again at the formula, we see that we can write it in a recursive form: We just derived a so called Bellman equation (it\u2019s actually a variation for action values in deterministic environments). If we turn the formula into a generic assignment we have a key idea of Q-learning introduced by Watkins, 19893. It was proven to converge to the desired Q*, provided that there are finite number of states and each of the state-action pair is presented repeatedly4. This is great news. It means that we could use this assignment every time our agent experience a new transition and over time, it would converge to the Q* function. This approach is called online learning and we will discuss it in more detail later. However, in the problems we are trying to solve, the state usually consists of several real numbers and so our state space is infinite. These numbers could represent for example a position and velocity of the agent or, they could mean something as complex as RGB values of a current screen in an emulator. We can\u2019t obviously use any table to store infinite number of values. Instead, we will approximate the Q function with a neural network. This network will take a state as an input and produce an estimation of the Q function for each action. And if we use several layers, the name comes naturally \u2013 Deep Q-network (DQN). But the original proof about the convergence does not hold anymore. Actually, the authors of the original research acknowledged that using a neural network to represent the Q function is known to be unstable1. To face with this issue, they introduced several key ideas to stabilize the training, which are mainly responsible for the success we see. Let\u2019s present first of them. Experience replay During each simulation step, the agent perform an action a in state s, receives immediate reward r and come to a new state s\u2019. Note that this pattern repeats often and it goes as (s, a, r, s\u2019). The basic idea of online learning is that we use this sample to immediately learn from it. Because we are using a neural network, we can\u2019t simply use the assignment but instead, we can shift our estimation towards this target: We can do it by performing a gradient descend step with this sample. We intuitively see that by repeating this many times, we are introducing more and more truth into the system and could expect the system to converge. Unfortunately, it is often not the case and it will require some more effort. The problem with online learning is that the samples arrive in order they are experienced and as such are highly correlated. Because of this, our network will most likely overfit and fail to generalize properly. The second issue with online learning is that we are not using our experience effectively. Actually, we throw away each sample immediately after we use it. The key idea of experience replay5 is that we store these transitions in our memory and during each learning step, sample a random batch and perform a gradient descend on it. This way we solve both issues. Lastly, because our memory is finite, we can typically store only a limited number of samples. Because of this, after reaching the memory capacity we will simply discard the oldest sample. Exploration If we always followed the greedy policy, we might never find out that some actions are better then others. A simple technique to resolve this is called ε-greedy policy. This policy behaves greedily most of the time, but chooses a random action with probability ε. The interesting fact here is that using the previously introduced update formula still shifts the estimated Q function towards the optimal Q* function, even if we use ε-greedy policy to acquire new experience. This is caused by the max in the formula. Because we are not learning the Q function for the policy we are using, but Q* instead, the method is called to be off-policy. Implementation Is it enough to get us started? An earlier research6 shows us it is. In the next chapter, we will implement a simple Q-network that will solve a famous cart-pole problem. About the author: Jaromír Janisch has graduated in 2012 as a Software Engineer from University of West Bohemia. Until 2016 he worked in several software companies, mainly as a developer. He\u2019s been always passionate about progress in artificial intelligence and decided to join the field as a researcher. He is currently preparing to start his Ph.D. studies. In this blog he strives to extract main points from sometimes complicated scientific research, reformulate them in his own words and make it more accessible for people outside the field. Mnih et al. \u2013 Human-level control through deep reinforcement learning, Nature 518, 2015 ↩ ↩ Sutton R. and Barto A. \u2013 Reinforcement Learning: An Introduction, MIT Press, 1998 ↩ Watkins Ch. \u2013 Learning from delayed rewards, PhD. thesis, Cambridge University, 1989 ↩ Watkins Ch. and Dayan P. \u2013 Q-learning, Machine Learning 8, 1992 ↩ Lin L. \u2013 Reinforcement Learning for Robots Using Neural Networks, PhD. thesis, Carnegie Mellon University Pittsburgh, 1993 ↩ Mnih et al. \u2013 Playing Atari with Deep Reinforcement Learning, arXiv:1312.5602v1, 2013 ↩ Share this article: Share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Click to share on Twitter (Opens in new window) Post navigation Let\u2019s make a DQN: Implementation → Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (Address never made public) Name Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. ","flair":"four\tProject"}
{"author":"lukemtesta","created":"Mon Oct 24 16:09:17 EDT 2016","text":"I need help finding a dataset with handwritten unicode characters. Some exist covering latin numerals and the English alphabet but none include characters for punctuation and mathematical operators. \n\nImageNet and VOC doesn't cover these. The MNIST covers digits 0-9, likewise Char74K has numeral and lower\/upper case letters for the Latin and Hindu-Arabic systems. \nMore suited datasets like HWRT and Detexify are private. I've also considered curating a dataset with Flickr but could not scrape enough images per category. Does anyone have any advice\/know a public dataset that meet my criteria? Thanks in advance!","flair":"one\tDiscussion"}
{"author":"downtownslim","created":"Mon Oct 17 12:36:44 EDT 2016","text":" Home Moments Search query Search Twitter Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Language: English Bahasa Indonesia Bahasa Melayu Català Čeština Dansk Deutsch English UK Español Filipino Français Hrvatski Italiano Magyar Nederlands Norsk Polski Português Română Slovenčina Suomi Svenska Tiếng Việt Türkçe Ελληνικά Български език Русский Српски Українська мова עִבְרִית العربية فارسی मराठी हिन्दी বাংলা ગુજરાતી தமிழ் ಕನ್ನಡ ภาษาไทย 한국어 日本語 简体中文 繁體中文 Have an account? Log in Have an account? Remember me · Forgot password? New to Twitter? Sign up rsalakhu's profile Russ Salakhutdinov @rsalakhu Russ Salakhutdinov @rsalakhu Professor at Carnegie Mellon University, Director of AI Research at Apple cs.cmu.edu\/~rsalakhu\/ Joined January 2015 © 2016 Twitter About Help Terms Privacy Cookies Ads info Dismiss Close Previous Next Close Go to a person's profile Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Close Retweet this to your followers? Optional comment for Retweet   Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @   140 Retweet Tweet Close Are you sure you want to delete this Tweet? Cancel Delete Close Promote this Tweet Close Block Cancel Block Add a location to your Tweets When you tweet with a location, Twitter stores that location. You can switch location on\/off before each Tweet and always have the option to delete your location history. Learn more Turn location on Not now Close Profile summary Close Your lists Close Create a new list List name Description Under 100 characters, optional Privacy Public · Anyone can follow this list Private · Only you can access this list Save list Close Close Copy link to Tweet Here's the URL for this Tweet. Copy it to easily share with friends. Close Embed this Tweet Embed this Video Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? Include parent Tweet Include media Preview Close Log in to Twitter Remember me · Forgot password? Don't have an account? Sign up » Close Sign up for Twitter Not on Twitter? Sign up, tune into the things you care about, and get updates as they happen. Sign up Have an account? Log in » Close Two-way (sending and receiving) short codes: Country Code For customers of United States 40404 (any) Canada 21212 (any) United Kingdom 86444 Vodafone, Orange, 3, O2 Brazil 40404 Nextel, TIM Haiti 40404 Digicel, Voila Ireland 51210 Vodafone, O2 India 53000 Bharti Airtel, Videocon, Reliance Indonesia 89887 AXIS, 3, Telkomsel, Indosat, XL Axiata Italy 4880804 Wind 3424486444 Vodafone » See SMS short codes for other countries Close Confirmation Close   Close Close Buy Now Close Buy Now Hmm... Something went wrong. Please try again. Skip all Welcome home! This timeline is where you\u2019ll spend most of your time, getting instant updates about what matters to you. Tweets not working for you? Hover over the profile pic and click the Following button to unfollow any account. Say a lot with a little When you see a Tweet you love, tap the heart \u2014 it lets the person who wrote it know you shared the love. Spread the word The fastest way to share someone else\u2019s Tweet with your followers is with a Retweet. Tap the icon to send it instantly. Join the conversation Add your thoughts about any Tweet with a Reply. Find a topic you\u2019re passionate about, and jump right in. Learn the latest Get instant insight into what people are talking about now. Get more of what you love Follow more accounts to get instant updates about topics you care about. Find what's happening See the latest conversations about any topic instantly. Never miss a Moment Catch up instantly on the best stories happening as they unfold. Back Next Next Tweet from user Previous Tweet Next Tweet Follow Following Unfollow Blocked Unblock Pending Cancel Russ Salakhutdinov \u200F@rsalakhu Oct 17 Excited about joining Apple as a director of AI research in addition to my work at CMU. Apply to work with my team https:\/\/jobs.apple.com\/us\/search?pr=1#mixes&ss=52662972&t=0&so=&lo=0*USA&pN=0&openJobId=52662972 \u2026 Retweets 397 Likes 1,001 8:40 AM - 17 Oct 2016 from Pittsburgh, PA 0 replies 397 retweets 1,001 likes Reply Retweet 397 Retweeted 397 Like 1K Liked 1K More Copy link to Tweet Embed Tweet Oriol Vinyals \u200F@OriolVinyalsML Oct 17 @rsalakhu Congrats and best of luck opening Apple to publications : ) 0 replies 0 retweets 33 likes Reply Retweet Retweeted Like 33 Liked 33 More Copy link to Tweet Embed Tweet Ethan Caballero \u200F@ethancaballero Oct 17 @rsalakhu so is apple going to be less secretive now? 0 replies 0 retweets 18 likes Reply Retweet Retweeted Like 18 Liked 18 More Copy link to Tweet Embed Tweet Jason Yosinski \u200F@jasonyo Oct 17 @rsalakhu Exciting news! Does this happen to concur with any increased ability to publish from within Apple? 0 replies 0 retweets 4 likes Reply Retweet Retweeted Like 4 Liked 4 More Copy link to Tweet Embed Tweet Brian Roemmele \u200F@BrianRoemmele Oct 17 @rsalakhu Russ, Wow! Congratulations! This will be an amazing journey! Cant wait to see where you take us! 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet AI:Mechanic \u200F@AiMechanic Oct 17 Congratulations to @rsalakhu and @Apple! Nice move. We all hope the results of your research will remain open. 0 replies 1 retweet 2 likes Reply Retweet 1 Retweeted 1 Like 2 Liked 2 More Copy link to Tweet Embed Tweet Alexandra Carvalho \u200F@alextwittau Oct 18 @rsalakhu Congratulations on the new role I'm sure you'll have a great impact on #deeplearning #machineintelligence #ArtificialIntelligence 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Rahel Jhirad \u200F@RahelJhirad Oct 17 @rsalakhu congrats! happy to see the 'in addition' part ... remains ... 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Yasser Souri \u200F@yassersouri Oct 17 @rsalakhu Consider publishing more papers from apple. 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet daniel \u200F@dmarthal Oct 17 @rsalakhu Do you plan on hiring remote? 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Rob Allpress \u200F@Roballpress Oct 17 @rsalakhu well done. I hope you can make Siri understand follow up questions and know a top hit on a web search to give a hands free answer 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet Benjamin Lyon \u200F@BenAppleLyon Oct 17 @rsalakhu Great to have you joining the team! 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet Poyan \u200F@osxusr Oct 17 @rsalakhu big congrats man! Looking forward to see your work! 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Bryon Aragam \u200F@itsrainingdata Oct 17 Bryon Aragam Retweeted Russ Salakhutdinov Excited to see @rsalakhu join forces with @Apple. Congrats to Russ!https:\/\/twitter.com\/rsalakhu\/status\/788041946325479424 \u2026 Bryon Aragam added, Russ Salakhutdinov @rsalakhu Excited about joining Apple as a director of AI research in addition to my work at CMU. Apply to work with my team https:\/\/jobs.apple.com\/us\/search?pr=1#mixes&ss=52662972&t=0&so=&lo=0*USA&pN=0&openJobId=52662972 \u2026 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Alexandr Kalinin \u200F@alxndrkalinin Oct 17 @itsrainingdata great exodus of academic continues. is Russ gonna be adjunct at cmu? 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Bryon Aragam \u200F@itsrainingdata Oct 17 @alxndrkalinin I just saw the news this morning, so I'm not sure what the arrangement is 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Orie Lawn \u200F@BIRDSFALLSILENT Oct 19 @rsalakhu Will you be pushing for Apple to join Open AI? Ya know, to prevent destruction of humanity. @elonmusk 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Recite News \u200F@ReciteNews Oct 17 @rsalakhu your Tweet was quoted in an article by @CNEThttps:\/\/www.cnet.com\/news\/apple-has-hired-its-first-director-of-ai-research\/ \u2026 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet rajatmonga \u200F@rajatmonga Oct 17 @rsalakhu congratulations! 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Dustin Tran \u200F@dustinvtran Oct 17 congrats @rsalakhu! 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Back to top ↑ Loading seems to be taking a while. Twitter may be over capacity or experiencing a momentary hiccup. Try again or visit Twitter Status for more information. Promoted Tweet false © 2016 Twitter About Help Terms Privacy Cookies Ads info ","flair":"two\tNews"}
{"author":"rober9876543210","created":"Thu Oct 20 10:45:29 EDT 2016","text":" 5 algorithms to train a neural network By Alberto Quesada, Artelnics. The procedure used to carry out the learning process in a neural network is called the training algorithm. There are many different training algorithms, whith different characteristics and performance. Problem formulation The learning problem in neural networks is formulated in terms of the minimization of a loss function, f. This function is in general, composed of an error and a regularization terms. The error term evaluates how a neural network fits the data set. On the other hand, the regularization term is used to prevent overfitting, by controlling the effective complexity of the neural network. The loss function depends on the adaptative parameters (biases and synaptic weights) in the neural network. We can conveniently group them together into a single n-dimensional weight vector w. The picture below represents the loss function f(w). As we can see in the previous picture, the point w* is minima of the loss function. At any point A, we can calculate the first and second derivatives of the loss function. The first derivatives are gropued in the gradient vector, whose elements can be written ᐁif(w) = df\/dwi (i = 1,...,n) Similarly, the second derivatives of the loss function can be grouped in the Hessian matrix, Hi,jf(w) = d2f\/dwi·dwj (i,j = 1,...,n) The problem of minimizing continuous and differentiable functions of many variables has been widely studied. Many of the conventional approaches to this problem are directly applicable to that of training neural networks. One-dimensional optimization Although the loss function depends on many parameters, one-dimensional optimization methods are of great importance here. Indeed, they are are very often used in the training process of a neural network. Indeed, many training algorithms first compute a training direction d and then a traning rate η that minimizes the loss in that direction, f(η). The next picture illustrates this one-dimensional function. The points η1 and η2 define an interval that contains the minimum of f, η*. In this regard, one-dimensional optimization methods search for the minimum of a given one-dimensional function. Some of the algorithms which are widely used are the golden section method and the Brent's method. Both reduce the bracket of a minumum until the distance between the two outer points in the bracket is less than a defined tolerance. Multidimensional optimization The learning problem for neural networks is formulated as searching of a parameter vector w* at which the loss function f takes a minimum value. The necessary condition states that if the neural network is at a minimum of the loss function, then the gradient is the zero vector. The loss function is, in general, a non linear function of the parameters. As a consequence, it is not possible to find closed training algorithms for the minima. Instead, we consider a search through the parameter space consisting of a succession of steps. At each step, the loss will decrease by adjusting the neural network parameters. In this way, to train a neural network we start with some parameter vector (often chosen at random). Then, we generate a sequence of parameters, so that the loss function is reduced at each iteration of the algorithm. The change of loss between two steps is called the loss decrement. The training algorithm stops when a specified condition, or stopping criterion, is satisfied. Now, we are going to describe the most importat training algorithms for neural networks. 1. Gradient descent Gradient descent, also known as steepest descent, is the simplest training algorithm. It requires information from the gradient vector, and hence it is a first order method. Let denote f(wi) = fi and ᐁf(wi) = gi. The method begins at a point w0 and, until a stopping criterion is satisfied, moves from wi to wi+1 in the training direction di = -gi. Therefore, the gradient descent method iterates in the following way: wi+1 = wi - di·ηi,   i=0,1,... The parameter η is the training rate. This value can either set to a fixed value or found by one-dimensional optimization along the training direction at each step. An optimal value for the training rate obtained by line minimization at each successive step is generally preferable. However, there are still many software tools that only use a fixed value for the training rate. The next picture is an activity diagram of the training process with gradient descent. As we can see, the parameter vector is improved in two steps: First, the gradient descent training direction is computed. Second, a suitable training rate is found. The gradient descent training algorithm has the severe drawback of requiring many iterations for functions which have long, narrow valley structures. Indeed, the downhill gradient is the direction in which the loss function decreases most rapidly, but this does not necessarily produce the fastest convergence. The following picture illustrates this issue. Gradient descent is the recommended algorithm when we have very big neural networks, with many thousand parameters. The reason is that this method only stores the gradient vector (size n), and it does not store the Hessian matrix (size n2). 2. Newton's method The Newton's method is a second order algorithm because it makes use of the Hessian matrix. The objective of this method is to find better training directions by using the second derivatives of the loss function. Let denote f(wi) = fi, ᐁf(wi) = gi and Hf(wi) = Hi. Consider the quadratic approximation of f at w0 using the Taylor's series expansion f = f0 + g0 · (w - w0) + 0.5 · (w - w0)2 · H0 H0 is the Hessian matrix of f evaluated at the point w0. By setting g equal to 0 for the minimum of f(w), we obtain the next equation g = g0 + H0 · (w - w0) = 0 Therefore, starting from a parameter vector w0, Newton's method iterates as follows wi+1 = wi - Hi-1·gi,   i=0,1,... The vector Hi-1·gi is known as the Newton's step. Note that this change for the parameters may move towards a maximum rather than a minimum. This occurs if the Hessian matrix is not positive definite. Thus, the function evaluation is not guaranteed to be reduced at each iteration. In order to prevent such troubles, the Newton's method equation usually modified as: wi+1 = wi - (Hi-1·gi)·ηi,   i=0,1,... The training rate, η, can either set to a fixed value or found by line minimization. The vector d = Hi-1·gi is now called the Newton's training direction. The state diagram for the training process with the Newton's method is depicted in the next figure. Here improvement of the parameters is performed by obtaining first the Newton's training direction and then a suitable training rate. The picture below illustrates the performance of this method. As we can see, the Newton's method requires less steps than gradient descent to find the minimum value of the loss function. However, the Newton's method has the difficulty that the exact evaluation of the Hessian and its inverse are quite expensive in computational terms. 3. Conjugate gradient The conjugate gradient method can be regarded as something intermediate between gradient descent and Newton's method. It is motivated by the desire to accelerate the typically slow convergence associated with gradient descent. This method also avoids the information requirements associated with the evaluation, storage, and inversion of the Hessian matrix, as required by the Newton's method. In the conjugate gradient training algorithm, the search is performed along conjugate directions which produces generally faster convergence than gradient descent directions. These training directions are conjugated with respect to the Hessian matrix. Let denote d the training direction vector. Then, starting with an initial parameter vector w0 and an initial training direction vector d0 = -g0, the conjugate gradient method constructs a sequence of training directions as: di+1 = gi+1 + di·γi,   i=0,1,... Here γ is called the conjugate parameter, and there are different ways to calculate it. Two of the most used are due to Fletcher and Reeves and to Polak and Ribiere. For all conjugate gradient algorithms, the training direction is periodically reset to the negative of the gradient. The parameters are then improved according to the next expression. The training rate, η, is usually found by line minimization. wi+1 = wi + di·ηi,   i=0,1,... The picture below depicts an activity diagram for the training process with the conjugate gradient. Here improvement of the parameters is done by first computing the conjugate gradient training direction and then suitable training rate in that direction. This method have proved to be more effective than gradient descent in training neural networks. Since it does not require the Hessian matrix, conjugate gradient is also recommended when we have very big neural networks. 4. Quasi-Newton method Application of the Newton's method is computationally expensive, since it requires many operations to evaluate the Hessian matrix and compute its inverse. Alternative approaches, known as quasi-Newton or variable metrix methods, are developed to solve that drawback. These methods, instead of calculating the Hessian directly and then evaluating its inverse, build up and approximation to the inverse Hessian at each iteration of the algorithm. This approximation is computed using only information on the first derivatives of the loss function. The Hessian matrix is composed of the second partial derivatives of the loss function. The main idea behind the quasi-Newton method is to approximate the inverse Hessian by another matrix G, using only the first partial derivatives of the loss function. Then, the quasi-Newton formula can be expressed as: wi+1 = wi - (Gi·gi)·ηi,   i=0,1,... The training rate η can either set to a fixed value or found by line minimization. The inverse Hessian approximation G has different flavours. Two of the most used are the Davidon\u2013Fletcher\u2013Powell formula (DFP) and the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno formula (BFGS). The activity diagram of the quasi-Newton training process is illustrated bellow. Improvement of the parameters is performed by first obtaining the quasi-Newton training direction and then finding a satisfactory training rate. This is the default method to use in most cases: It is faster than gradient descent and conjugate gradient, and the exact Hessian does not need to be computed and inverted. 5. Levenberg-Marquardt algorithm The Levenberg-Marquardt algorithm, also known as the damped least-squares method, has been designed to work specifically with loss functions which take the form of a sum of squared errors. It works without computing the exact Hessian matrix. Instead, it works with the gradient vector and the Jacobian matrix. Consider a loss function which can be expressed as a sum of squared errors of the form f = ∑ ei2,   i=0,...,m Here m is the number of instances in the data set. We can define the Jacobian matrix of the loss function as that containing the derivatives of the errors with respect to the parameters, Ji,jf(w) = dei\/dwj (i = 1,...,m & j = 1,...,n) Where m is the number of instances in the data set and n is the number of parameters in the neural network. Note that the size of the Jacobian matrix is m·n. The gradient vector of the loss function can be computed as: ᐁf = 2 JT·e Here e is the vector of all error terms. Finally, we can approximate the Hessian matrix with the following expression. Hf ≈ 2 JT·J + λI Where λ is a damping factor that ensures the positiveness of the Hessian and I is the identity matrix. The next expression defines the parameters improvement process with the Levenberg-Marquardt algorithm wi+1 = wi - (JiT·Ji+λiI)-1·(2 JiT·ei),   i=0,1,... When the damping parameter λ is zero, this is just Newton's method, using the approximate Hessian matrix. On the other hand, when λ is large, this becomes gradient descent with a small training rate. The parameter λ is initialized to be large so that first updates are small steps in the gradient descent direction. If any iteration happens to result in a failure, then λ is increased by some factor. Otherwise, as the loss decreases, λ is decreased, so that the Levenberg-Marquardt algorithm approaches the Newton method. This process typically accelerates the convergence to the minimum. The picture below represents a state diagram for the training process of a neural network with the Levenberg-Marquardt algorithm. The first step is to calculate the loss, the gradient and the Hessian approximation. Then the damping parameter is adjusted so as to reduce the loss at each iteration. As we have seen the Levenberg-Marquardt algorithm is a method tailored for functions of the type sum-of-squared-error. That makes it to be very fast when training neural networks measured on that kind of errors. However, this algorithm has some drawbacks. The first one is that it cannnot be applied to functions such as the root mean squared error or the cross entropy error. Also, it is not compatible with regularization terms. Finally, for very big data sets and neural networks, the Jacobian matrix becomes huge, and therefore it requires a lot of memory. Therefore, the Levenberg-Marquardt algorithm is not recommended when we have big data sets and\/or neural networks. Memory and speed comparison The next graph depicts the computational speed and the memory requirements of the training algorithms discussed in this post. As we can see, the slowest training algorithm is usually gradient descent, but it is the one requiring less memory. On the contrary, the fastest one might be the Levenberg-Marquardt algorithm, but usually requires a lot of memory. A good compromise might be the quasi-Newton method. To conclude, if our neural networks has many thousands of parameters we can use gradient descent or conjugate gradient, in order to save memory. If we have many neural networks to train with just a few thousands of instances and a few hundreds of parameters, the best choice might be the Levenberg-Marquardt algorithm. In the rest of situations, the quasi-Newton method will work well. Share Home Blog Do you need help? Contact us. | Legal notice © 2016, Artificial Intelligence Techniques, Ltd. ","flair":"three\tResearch"}
{"author":"singham","created":"Tue Nov 15 00:53:20 EST 2016","text":" Google Research Blog The latest news from Research at Google Enhance! RAISR Sharp Images with Machine Learning Monday, November 14, 2016 Posted by Peyman Milanfar, Research Scientist Everyday the web is used to share and store millions of pictures, enabling one to explore the world, research new topics of interest, or even share a vacation with friends and family. However, many of these images are either limited by the resolution of the device used to take the picture, or purposely degraded in order to accommodate the constraints of cell phones, tablets, or the networks to which they are connected. With the ubiquity of high-resolution displays for home and mobile devices, the demand for high-quality versions of low-resolution images, quickly viewable and shareable from a wide variety of devices, has never been greater. With \u201CRAISR: Rapid and Accurate Image Super-Resolution\u201D, we introduce a technique that incorporates machine learning in order to produce high-quality versions of low-resolution images. RAISR produces results that are comparable to or better than the currently available super-resolution methods, and does so roughly 10 to 100 times faster, allowing it to be run on a typical mobile device in real-time. Furthermore, our technique is able to avoid recreating the aliasing artifacts that may exist in the lower resolution image. Upsampling, the process of producing an image of larger size with significantly more pixels and higher image quality from a low quality image, has been around for quite a while. Well-known approaches to upsampling are linear methods which fill in new pixel values using simple, and fixed, combinations of the nearby existing pixel values. These methods are fast because they are fixed linear filters (a constant convolution kernel applied uniformly across the image). But what makes these upsampling methods fast, also makes them ineffective in bringing out vivid details in the higher resolution results. As you can see in the example below, the upsampled image looks blurry \u2013 one would hesitate to call it enhanced. Left: Low-res original, Right: simple (bicubic) upsampled version (2x). Image Credit: Masa Ushioda\/Seapics\/Solent News With RAISR, we instead use machine learning and train on pairs of images, one low quality, one high, to find filters that, when applied to selectively to each pixel of the low-res image, will recreate details that are of comparable quality to the original. RAISR can be trained in two ways. The first is the \"direct\" method, where filters are learned directly from low and high-resolution image pairs. The other method involves first applying a computationally cheap upsampler to the low resolution image (as in the figure above) and then learning the filters from the upsampled and high resolution image pairs. While the direct method is computationally faster, the 2nd method allows for non-integer scale factors and better leveraging of hardware-based upsampling. For either method, RAISR filters are trained according to edge features found in small patches of images, - brightness\/color gradients, flat\/textured regions, etc. - characterized by direction (the angle of an edge), strength (sharp edges have a greater strength) and coherence (a measure of how directional the edge is). Below is a set of RAISR filters, learned from a database of 10,000 high and low resolution image pairs (where the low-res images were first upsampled). The training process takes about an hour. Collection of learned 11x11 filters for 3x super-resolution. Filters can be learned for a range of super-resolution factors, including fractional ones. Note that as the angle of the edge changes, we see the angle of the filter rotate as well. Similarly, as the strength increases, the sharpness of the filters increases, and the anisotropy of the filter increases with rising coherence. From left to right, we see that the learned filters correspond selectively to the direction of the underlying edge that is being reconstructed. For example, the filter in the middle of the bottom row is most appropriate for a strong horizontal edge (gradient angle of 90 degrees) with a high degree of coherence (a straight, rather than a curved, edge). If this same horizontal edge is low-contrast, then a different filter is selected such one in the top row. In practice, at run-time RAISR selects and applies the most relevant filter from the list of learned filters to each pixel neighborhood in the low-resolution image. When these filters are applied to the lower quality image, they recreate details that are of comparable quality to the original high resolution, and offer a significant improvement to linear, bicubic, or Lanczos interpolation methods. Top: RAISR algorithm at run-time, applied to a cheap upscaler\u2019s output. Bottom: Low-res original (left), bicubic upsampler 2x (middle), RAISR output (right) Some examples of RAISR in action can be seen below: Top: Original, Bottom: RAISR super-resolved 2x. Original image from Andrzej Dragan Left: Original, Right: RAISR super-resolved 3x. Image courtesy of Marc Levoy One of the more complex aspects of super-resolution is getting rid of aliasing artifacts such as Moire patterns and jaggies that arise when high frequency content is rendered in lower resolution (as is the case when images are purposefully degraded). Depending on the shape of the underlying features, these artifacts can be varied and hard to undo. Example of aliasing artifacts seen on the lower right (Image source) Linear methods simply can not recover the underlying structure, but RAISR can. Below is an example where the aliased spatial frequencies are apparent under the numbers 3 and 5 in the low-resolution original on the left, while the RAISR image on the right recovered the original structure. Another important advantage of the filter learning approach used by RAISR is that we can specialize it to remove noise, or compression artifacts unique to individual compression algorithms (such as JPEG) as part of the training process. By providing it with examples of such artifacts, RAISR can learn to undo other effects besides resolution enhancement, having them \u201Cbaked\u201D inside the resulting filters. Left: Low res original, with strong aliasing. Right: RAISR output, removing aliasing. Super-resolution technology, using one or many frames, has come a long way. Today, the use of machine learning, in tandem with decades of advances in imaging technology, has enabled progress in image processing that yields many potential benefits. For example, in addition to improving digital \u201Cpinch to zoom\u201D on your phone, one could capture, save, or transmit images at lower resolution and super-resolve on demand without any visible degradation in quality, all while utilizing less of mobile data and storage plans. To learn more about the details of our research and a comparison to other current architectures, check out our paper, which will appear soon in the IEEE Transactions on Computational Imaging. Google Labels: Computational Imaging , Machine Learning , Machine Perception    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"three\tResearch"}
{"author":"julian88888888","created":"Wed Oct 12 00:10:53 EDT 2016","text":" By Tomasz Rewak LinkedIn See also: trepl.xyz Github: repo Your browser does not support the HTML5 canvas tag. Restart Restart to applly changes End race alpha version ","flair":"four\tProject"}
{"author":"max_likelihood","created":"Thu Nov 03 09:59:08 EDT 2016","text":"Hi,\n\ni honestly don't get it. Don't get me wrong, I am a huge fan of open-source, but it's 2016 and researchers are still publishing their deep learning implementations in proprietary MATLAB. Thus, their work is unaccessible to most of us. What are your best-practices of getting these implementations to work (especially caffe-based matlab code)? Compiling the code to C? Thanks a lot!","flair":"one\tDiscussion"}
{"author":"Xamius","created":"Thu Oct 06 12:03:09 EDT 2016","text":"My data:\nI am using random forest to essentially predict which price each person should get to increase revenue uplift. \n\nSo basically a\/b\/n pricing. 25% get 1 of 4 prices. \n\nI then run 4 models to predict how much a customer would spend on each price (IE I separate the data by the price the customer gets, so model is run on 4 separate datasets).\n\nI then use the 4 models on the validation\/test data to see how much the new customers would spend for each price. I then take the max of those 4 predicted prices and use that as the predicted price we should give that customer. \n\nI then compare the predicted price point with the actual price the customer was given and calculate the mean revenue for those where predicted=actual. I compare the average revenue vs. average revenue for all customers in the dataset.\n\nProblem:\nThe model gets some good accuracy on the training data but does not score well uplift wise on the validation or test data. The conversion % is really low, like 5%. So I usually take a sample of the 0s to match the 1s and then run the model. I have also tried cross validation using the randomforest package in R.\n\nI am looking for opinions on other modeling techniques I should try, and if there is something else I should try sampling wise. \n\nI have tried random forest both in the regression method I described and also the classification method, where I measure the probablity of each person buying the product for the 4 price points, and then taking the price with the max probability as the predicted price point.\n\nI have also tried logistic and linear regression, as well as researching svm and gradiant boosting models.  ","flair":"null\tnull"}
{"author":"throwaway775849","created":"Tue Nov 22 17:13:49 EST 2016","text":"Current RNN Decoders for sequence to sequence tasks model conditional probability as a function of the input and the previous timestep's hidden state. Are there alternative architecture more intuitively appropriate? \n\nWe see interesting architectures for the encoder side, such as recursive networks like TreeLSTM and explicit hierarchical models, but why not invert a tree for use on the decoder side?\n\nMy feeling is that the syntax of a prediction evolves secondarily following the semantic intent, and so decoding L-&gt;R is unnatural \/ suboptimal. When people speak, it's obvious we don't immediately know exactly what to say. It's likely there is a hierarchical process of decoding and organizing verbal phrases. \n\nFull syntactic structure seems to be realized only after semantic intent. The intent is realized to what is likely a topical anchor word or subject around which the thought is structured. These topics or phrases can be seen as latent variables. Once these phrases are realized, it is a much easier process for the relationship, description, or information conveyed by the sentence to be arranged or generated with appropriate syntax, utilizing those pieces.\n\nDecoding L-&gt;R breaks this assumption, and makes the task more difficult than if prediction was sequence agnostic. For example, to verbalize the sentence \"At that time, the meal tasted good.\", a more probable pattern in which the 'latent variables' are mentally realized may be (in order of realization, with \"|\" indicating mental timestep):\n\nthe meal **|** good **|** tasted **|** At that time, **|** .\n\n\nThe default decoding direction is to predict the probability distribution after the previous token, but couldn't a 'copy mechanism' be modified to act as a Left\/Right switch that allows the model to predict a distribution, but choose whether it belongs before all previous tokens OR after everything predicted?\n\nA more reasonable model with the same motivation would be to add more layers to the decoder. The point is to allow the first layer to predict probability distributions with a relaxed constraint on conditional probability of the next token, and the second layer would potentially capture information about the correct ordering of the previous layer's hidden states. Layers beyond the first would require more connections than just the previous RNN's hidden state, perhaps a connection to every cell in the previous layer. The flow of state would be forward in time across the first layer, and once the first layer completes, the first cell of the 2nd layer begins, forward across the entire 2nd layer, with each cell having a connection to every cell in the previous layer (or receiving their concatenated states). This could be modeled as a single chain RNN with peephole connections I believe. Has anyone tried this?\n\n**tldr;** Sequence prediction models' decoding in a left-&gt;right manner is suboptimal. Have there been experiments with multilayer or other decoder architectures using latent variables or hierarchical structure?","flair":"one\tDiscussion"}
{"author":"orange_robot338","created":"Wed Oct 12 18:01:23 EDT 2016","text":"I need something that I can reproduce paper results and experiment myself with different architectures and tuning, extracting embeddings and that sort of thing.\n\nI'm most familiar with Keras but I'm ok with not using it if isn't suitable for this. I know of a couple of frameworks such as TF, TF-Learn, TF-Slim, Theano so I'd like suggestions on which ones I should use. \n\nThank you very much.\n","flair":"three\tResearch"}
{"author":"magnusg7","created":"Mon Nov 14 04:35:01 EST 2016","text":" Quiver Interactive deep convolutional networks features visualization. View on GitHub Build your model in keras model = Model(...) Launch the visualization dashboard with 1 line of code quiver_engine.server.launch(model, input_folder='.\/imgs') Explore layer activations on all the different images in your input folder. ","flair":"four\tProject"}
{"author":"downtownslim","created":"Fri Oct 14 20:08:40 EDT 2016","text":"Some of the latest Intel processors support the AVX-512 family of vector instructions. These instructions operate on blocks of 512 bits (or 64 bytes). The benefit of such wide instructions is that even without increasing the processor clock speed, systems can still process a lot more data. Most code today operators over 64-bit words (8 bytes). In theory, keeping everything else constant, you could go 8 times faster by using AVX-512 instructions instead. Of course, not all code can make use of vector instructions\u2026 but that\u2019s not relevant. What matters is whether your \u201Chot code\u201D (where the processor spends much of its time) can benefit from them. In many systems, the hot code is made of tight loops that need to run billions of times. Just the kind of code that can benefit from vectorization! The hottest trend in software right now is \u201Cdeep learning\u201D. It can be used to classify pictures, recognize speech or play the game of Go. Some say that the quickest \u201Cget rich quick\u201D scheme right now is to launch a deep-learning venture, and get bought by one of the big players (Facebook, Google, Apple, Microsoft, Amazon). It is made easier by the fact that companies like Google have open sourced their code such as Tensorflow. Sadly for Intel, it has been mostly left out of the game. Nvidia graphics processors are the standard off-the-shelf approach to running deep-learning code. That\u2019s not to say that Intel lacks good technology. But for the kind of brute-force algebra that\u2019s required by deep learning, Nvidia graphics processors are simply a better fit. However, Intel is apparently preparing a counter-attack, of sort. In September of this year, they have discreetly revealed that their future processors will support dedicated deep-learning instructions. Intel\u2019s AVX-512 family of instructions is decomposed in sub-families. There will be two new sub-families for deep-learning: AVX512_4VNNIW and AVX512_4FMAPS.","flair":"two\tNews"}
{"author":"mostafa-samir","created":"Sat Nov 12 10:14:42 EST 2016","text":"This is a TensorFlow implementation of DeepMind's Differentiable Neural Computer (DNC) architecture introduced in their recent Nature paper: This implementation doesn't include all the tasks that was described in the paper, but it's focused on exploring and re-producing the general task-independent key characteristics of the architecture. However, the implementation was designed with extensibility in mind, so it's fairly simple to adapt it to further tasks. All experiments and tests ran on a machine with: This experiment is designed to demonstrate the various functionalities of the external memory access mechanisms such as in-order retrieval and allocation\/deallocation. A similar approach to that of the paper was followed by training a 2-layer feedforward model with only 10 memory locations on a copy task in which a series of 4 random binary sequences each of which is of size 6 (24 piece of information) was presented as input. Details about the training can be found here. The model was able to learn to copy the input successfully, and it indeed learned to use the mentioned memory mechanisms. The following figure (which resembles Extended Data Figure 1 in the paper) illustrates that. You can re-generate similar figures in the visualization notebook The figure differs a little from the one in the paper when it comes to the activation degrees of the gates. This could be due to the small size of the model and the relatively small training time. However, this doesn't affect the operation of the model. This experiment was designed to check: To approach that, a 2-layer feedforward model with 15 memory locations was trained on a copy problem in which a single sequence of random binary vectors of lengths between 1 and 10 was presented as input. Details of the training process can be found here. The model was then tested on pairs of increasing sequence lengths and increasing memory sizes with re-training on any of these pairs of parameters, and the fraction of correctly copied sequences out of a batch of 100 was recorded. The model was indeed able to generalize and use the available memory locations effectively without retraining. This is depicted in the following figure which resembles Extended Data Figure 2 from the paper. Similar figures can be re-generated in the visualization notebook If you're interested in using the implementation for new tasks, you should first start by reading the structure and basic usage guide to get comfortable with how the project is structured and how it can be extended to new tasks. If you intend to work with the source code of the implementation itself, you should begin with looking at the data flow diagrams to get a high-level overview of how the data moves from the input to the output across the modules of the implementation. This would ease you into reading the source code, which is okay-documented. You might also find the implementation notes helpful to clarify how some of the math is implemented.","flair":"four\tProject"}
{"author":"Kiuhnm","created":"Mon Oct 03 07:23:07 EDT 2016","text":"I've recently come across an interesting blog called [Off the convex path](http:\/\/www.offconvex.org\/) which I'm sure many of you already know.\n\nThis led me to reconsider more theoretical courses such as [Algorithmic Aspects of Machine Learning](https:\/\/ocw.mit.edu\/courses\/mathematics\/18-409-algorithmic-aspects-of-machine-learning-spring-2015\/) whose [videos](https:\/\/www.youtube.com\/playlist?list=PLB3sDpSRdrOvI1hYXNsa6Lety7K8FhPpx) are available on youtube.\n\nThe same professor also teaches a course called [Advanced Algorithms](http:\/\/people.csail.mit.edu\/moitra\/854.html) which, I think, is (indirectly) relevant to ML. As before, [videos](https:\/\/www.youtube.com\/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c) are available.\n\nNote that if you don't like\/need theory or you don't have at least a moderate math background, these courses are not for you.\n\nI'd like to see links to more theoretic stuff as well on this subreddit. I'm serious. New NN architectures are all well and good but training those beasts requires too much trial and error. We need more theoreticians, in my opinion. Even if you don't agree, I hope you like the courses :)","flair":"null\tnull"}
{"author":"tadellos","created":"Sat Oct 01 13:04:42 EDT 2016","text":" Alternating Back-Propagation for Generator Network Tian Han* , Yang Lu* , Song-Chun Zhu , and Ying Nian Wu * Equal contributions. University of California, Los Angeles (UCLA), USA Introduction This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the latent factors to the observed vector is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates between the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data. Paper The paper can be downloaded here. Code and Data The code and data can be downloaded here. For more details, please refer to the instruction. Qualitative Experiments Experiment 1: Modeling Texture Patterns Figure 1. Generating texture patterns. For each category, the first image displays one of the training images, and the second displays one of the generated images. Please click on the category names or images for more details. Bark Beehive Brick Coffee Grass Ivy Pave Water Rose Experiment 2: Modeling Sounds Figure 2. Generating sound patterns. For each category, the first sound track is the training data, and the second one is the generated sound. Please click on the category names for more details. Chirp chicken_coop cougar_snarl_growl_extend forest_extend hard_sheep_extend jazz_extend magkas_extend running_stream_extend Experiment 3: Modeling Object Patterns Figure 3. Generating object patterns. For each category, the first image is the training image, and the second image is generated images. Please click on the category names or images for more details. Face1000 Lion_tiger Experiment 4: Modeling Dynamic Texture Patterns Figure 4. Generating dynamic texture patterns. For each category, the first animation is the training sequence, and the middle animation is generated by linear dynamic system. The right animation is generated by our method. Flag Water Fall Water Stone Quantitative Experiments Experiment 5: Learning from incomplete data Figure 5. Learning from incomplete data. The 10 columns belong to experiments salt and pepper occlusion 50%, 70%, 90%, 90%, 90%, 90%, 90%, single region occlusion 20x20, 30x30, 30x30 respectively. Row 1: original images, not observed in learning. Row 2: training images. Row 3: recovered images during learning. Please click on the image for more details. Learning from incomplete data Experiment 6: Learning from indirect data Figure 6. Learning from indirect data. The first row displays the original 64 × 64 × 3 images, which are projected onto 1,000 white noise images. The second row displays the re-covered images during learning. Please click on the image for more details. Learning from indirect data Experiment 7: Comparison between our method and PCA Figure 7. Comparison between our method and PCA. Row 1: original testing images. Row 2: reconstructions by PCA eigenvectors learned from training images. Row 3: Recon- struction by the generator learned from training images. d = 20 for both methods. Please click on the image for more details. Comparison between our method and PCA Acknowledgement The code in our work is based on the Matlab code of MatConvNet of Vedaldi & Lenc (2015). We thank the authors for sharing their code with the community. We thank Yifei (Jerry) Xu for his help with the experiments. We thank Jianwen Xie for helpful discussions. The work is supported by NSF DMS 1310391, DARPA SIMPLEX N66001-15-C-4035 and ONR MURI N00014-16-1-2007. Top ","flair":"null\tnull"}
{"author":"stanislavb","created":"Sat Oct 01 08:13:47 EDT 2016","text":"Machine learning is like highschool sex. Everyone says they do it, nobody really does, and no one knows what it actually is. -- @Mikettownsend. Neural Redis is a Redis loadable module that implements feed forward neural networks as a native data type for Redis. The project goal is to provide Redis users with an extremely simple to use machine learning experience. Normally machine learning is operated by collecting data, training some system, and finally executing the resulting program in order to solve actual problems. In Neural Redis all this phases are compressed into a single API: the data collection and training all happen inside the Redis server. Neural networks can be executed while there is an ongoing training, and can be re-trained multiple times as new data from the outside is collected (for instance user events). The project starts from the observation that, while complex problems like computer vision need slow to train and complex neural networks setups, many regression and classification problems that are able to enhance the user experience in many applications, are approachable by feed forward fully connected small networks, that are very fast to train, very generic, and robust against non optimal parameters configurations. The goal is to help developers, especially of mobile and web applications, to have a simple access to machine learning, in order to answer questions like: Of course you can do more, since neural networks are pretty flexible. You can even have fun with computer visions datasets like MINST, however keep in mind that the neural networks implemented in Neural Redis are not optimized for complex computer visions tasks like convolutional networks (it will score 2.3%, very far from the state of art!), nor Neural Redis implements the wonders of recurrent neural networks. However you'll be surpirsed by the number of tasks in which a simple neural network that can be trained in minutes, will be able to discover linear ad non linear correlations. To run this extension you need Redis , grab it from Github, it is the default branch. Then compile the extension, and load it starting Redis with: Alternatively add the following in your file: WARNING: this is alpha code. It is likely to contain bugs and may easily crash the Redis server. Also note that currently only RDB persistence is implemented in the module, while AOF rewrite is not implemented at all. Use at your own risk. If you are not still scared enough, please consider that I wrote the more than 1000 lines of C code composing this extension, and this README file, in roughly two days. Note that this implementation may be hugely improved. For instance currently only the sigmoid activaction function and the root mean square loss functions are supported: while for the problems this module is willing to address this limited neural network implementation is showing to be quite flexible, it is possible to do much better depending on the problem at hand. In order to understand how the API works, here is an hello world example where we'll teach our neural network to do... additions :-) To create a new neural network we use the following command: The command creates a neural network, configured for regression tasks (as opposed to classification: well'll explain what this means in the course of this tutorial). Note that the command replied with \"13\". It means that the network has a total of 13 tunable parameters, considering all the weights that go from units or biases to other units. Larger networks will have a lot more parameters. The neural network has 2 inputs, 3 hidden layers, and a single output. Regression means that given certain inputs and desired outputs, we want the neural network to be able to understand the function that given the inputs computes the outputs, and compute this function when new inputs are presented to it. The option means that it is up to Redis to normalize the data it receives, so there is no need to provide data in the -\/+ 1 range. The options and means that we want an internal memory for the dataset of 50 and 10 items respectively for the training dataset, and the testing dataset. The learning happens using the training dataset, while the testing dataset is used in order to detect if the network is able to generalize, that is, is really able to understand how to approximate a given function. At the same time, the testing dataset is useful to avoid to train the network too much, a problem known as overfitting. Overfitting means that the network becomes too much specific, at the point to be only capable of replying correctly to the inputs and outputs it was presented with. Now it is time to provide the network with some data, so that it can learn the function we want to approximate: We are saying: given the inputs 1 and 2, the output is 3. The reply to the command is the number of data items stored in the neural network memory, respectively in the training and testing data sets. We continue like that with other examples: At this point we need to train the neural network, so that it can learn: The command starts a training thread. the option means that we want the training to stop before overfitting starts to happen. Using the command you can see if the network is still training. However in this specific case, the network will take a few milliseconds to train, so we can immediately try if it actually learned how to add two numbers: Well, more or less it works. Let's look at some internal info now: As you can see we have 6 dataset items and 2 test items. We configured the network at creation time to have space for 50 and 10 items. As you add items with the network will put items evenly on both datasets, proportionally to their respective size. Finally when the datasets are full, old random entries are replaced with new ones. We can also see that the network was trained with 1344 steps for 0 seconds (just a few milliseconds). Each step is the training performed with a single data item, so the same 6 items were presented to the network for 244 cycles in total. If we try to use our network with values outside the range it learned with, we'll see it failing: This happens because the automatic normalization will consider the maximum values seen in the training dataset. So if you plan to use auto normalization, make sure to show the network samples with different values, including inputs at the maximum of the data you'll want to use the network with in the future. Regression approximates a function having certain inputs and outputs in the training data set. Classification instead is the task of, given a set of inputs representing something, to label it with one of a fixed set of labels. For example the inputs may be features of Greek jars, and the classification output could be one of the following three jar types: As a programmer you may think that, the output class, is just a single output number. However neural networks don't work well this way, for example classifying type 0 with an output between 0 and 0.33, type 1 with an output between 0.33 and 0.66, and finally type 2 with an output between 0.66 and 1, will not work well at all. The way to go instead is to use three distinct outputs, where we set two always to 0, and a single one to 1, corresponding to the type the output represents, so: When you create a neural network with the command, and use as second argument instead of , Neural Redis will do the above transformation for you, so when you train your network with you'll just use, as output, as single number: 0, 1 or 2. Of course, you need to create the network with three outputs like that: Our network is currently untrained, but it can already be run, even if the replies it will provide are totally random: As you can see, the network voted for type 0, since the first output is greater than the others. There is a Neural Redis command that saves you the work of finding the greatest output client side in order to interpret the result as a number between 0 and 2. It is identical to but just outputs directly the class ID, and is called : However note that ofter is useful for classification problems. For example a blogging platform may want to train a neural network to predict the template that will appeal more to the user, based on the registration data we just obtained, that include the country, sex, age and category of the blog. While the prediction of the network will be the output with the highest value, if we want to present different templates, it makes sense to present, in the listing, as the second one the one with the second highest output value and so forth. Before diving into a practical classification example, there is a last thing to say. Networks of type CLASSIFIER are also trained in a different way: instead of giving as output a list of zeros and ones you directly provide to the class ID as a number, so in the example of the jars, we don't need to write to specify as output of the provided data sample the third class, but we should just write: The \"2\" will be translated into \"0 0 1\" automatically, as \"1\" would be translated to \"0 1 0\" and so forth. Kaggle.com is hosting a machine learning competition. One of the datasets they use, is the list of the Titanic passengers, their ticket class, fair, number of relatives, age, sex, and other information, and... If they survived or not during the Titanic incident. You can find both the code and a CSV with a reduced dataset of 891 entries in the directory of this Github repository. In this example we are going to try to predict, given a few input variables, if a specific person is going to survive or not, so this is a classification task, where we label persons with two different labels: survived or died. This problem is pretty similar, even if a bit more scaring, than the problem of labeling users or predicting their response in some web application according to their behavior and the other data we collected in the past (hint: machine learning is all about collecting data...). In the CSV there are a number of information about each passenger, but here in order to make the example simpler we'll use just the following fields: If there is a correlation between this input variables and the ability to survive, our neural network should find it. Note that while we have six inputs, we'll need a total network with 9 total inputs, since sex and ticket class, are actually input classes, so like we did in the output, we'll need to do in the input. Each input will signal if the passenger is in one of the possible classes. This are our nine inputs: We have a bit less than 900 passengers (I'm using a reduced dataset here), however we want to take about 200 for verification at application side, without sending them to Redis at all. The neural network will also use part of the dataset for verification, since here I'm planning to use the automatic training stop feature, in order to detect overfitting. Such a network can be created with: Also note that we are using a neural network with a single hidden layer (the layers between inputs and outputs are called hidden, in case you are new to neural networks). The hidden layer has 15 units. This is still a pretty small network, but we expect that for the amount of data and the kind of correlations that there could be in this data, this could be enough. It's possible to test with different parameters, and I plan to implement a command so that it will be possible to change this things on the fly. Also note that since we defined a testing dataset maximum size to be half the one of the training dataset (1000 vs 500), will automatically put one third of the entires in the testing dataset. If you check the Ruby program that implements this example inside the source distribution, you'll see how data is fed directly as it is to the network, since we asked for auto normalization: The function is able to both send data or evaluate the error rate. After we load 601 entries from the dataset, before any training, the output of will look like this: As expected, we have 401 training items and 200 testing dataset. Note that for networks declared as classifiers, we have an additional field in the info output, which is . Once we train the network this field will be populated with the percentage (from 0% to 100%) of items in the testing dataset which were misclassified by the neural network. It's time to train our network: If we check the output after the training, we'll discover a few interesting things (only quoting the relevant part of the output): The network was trained for 0.29 seconds. At the end of the training, that was stopped for overfitting, the error rate in the testing dataset was 19%. You can also specify to train for a given amonut of seconds or cycles. For now we just use the feature since it is simpler. However we'll dig into more details in the next section. We can now show the output of the Ruby program after its full execution: Does not look too bad, considering how simple is our model and the fact we trained with just 401 data points. Modeling just on the percentage of people that survived VS the ones that died, we could miss-predict more than 100 passengers. We can also play with a few variables interactively in order to check what are the inputs that make a difference according to our trained neural network. Let's start asking the probable outcome for a woman, 30 years old, first class, without siblings and parents: The network is positive she survived, with 90% of probabilities. What if she is a lot older than 30 years old, let's say 70? This lowers her probability to 88.7%. And if she traveled in third class with a very cheap ticket? This time is 50% and 50%... Throw your coin. The gist of this example is that, many problems you face as a developer in order to optimize your application and do better choices in the interaction with your users, are Titanic problems, but not in their size, just in the fact that a simple model can \"solve\" them. One thing that makes neural networks hard to use in an interactive way like the one they are proposed in this Redis module, is for sure overfitting. If you train too much, the neural network ends to be like that one student that can exactly tell you all the words in the lesson, but if you ask a more generic question about the argument she or he just wonders and can't reply. So the command option attempts to detect overfitting to stop the training before it's too late. How is this performed? Well the current solution is pretty trivial: as the training happens, we check the current error of the neural network between the training dataset and the testing dataset. When overfitting kicks in, usually what we see is that the network error rate starts to be lower and lower in the training dataset, but instead of also reducing in the testing dataset it inverts the tendency and starts to grow. To detect this turning point is not simple for two reasons: So while kinda does what it advertises (but I'll work on improving it in the future, and there are neural network experts that know much better than me and can submit a kind Pull Request :-), there are also means to manually train the network, and see how the error changes with training. For instance, this is the error rate in the Titanic dataset after the automatic stop: We can use the and options in order to train for a specific amount of time (note that these options are also applicable when is specified). Normally is set to 10000, which are milliseconds, so to 10 seconds of total training before killing the training thread. Let's train our network for 30 seconds, without auto stop. As a side note, while one or more trainings are in progress, we can list them: After the training stops, let's show info again: You can see that our network overtrained: the error rate of the training dataset is now lower: 0.06. But actually the performances in data it never saw before, that is the testing dataset, is greater at 0.20! And indeed, it classifies in a wrong way 21% of entries instead of 18.50%. However it's not always like that, so to test things manually is a good idea when working at machine learning experiments, especially with this module that is experimental. An interesting example is the program inside the directory: it will load the dataset into Redis, which is a very popular dataset with three variants of Iris flowers with their sepal and petal features. If you run the program, the percentage of entries classified in a wrong way will be 4%, however if you train the network a few more cycles with: You'll see that often the error will drop to 2%. When using , there is an additional option that can be specified (it has no effects alone), that is: . When backtracking is enabled, while the network is trained, every time there is some hint that the network may start to overfit, the current version of the network is saved. At the end of the training, if the saved network is better (has a smaller error) compared to the current one, it is used instead of the final version of the trained network. This avoids certain pathological runs when is used but overfitting is not detected. However, it adds running time since we need to clone the NN from time to time during the training. For example using with the Iris dataset (see the file inside the examples directory) it never overtrains, while without about 2% of the runs may overtrain. The Titanic example is surely more interesting, however it is possible that most relations between inputs and outputs are linear, so we'll now try a non linear classification task, just for the sake of showing the capabilities of a small neural network. In the examples directory of this source distribution there is an example called , we'll use it as a reference. We'll just setup a classification problem where the neural network will be asked to classify two inputs, which are from our point of view two coordinates in a 2D space, into three different classes: 0, 1 and 2. While the neural network does not know this, we'll generate the data so that different classes actually map to three different circles in the 2D space: the circles also contain intersections. The function that generates the dataset is the following: With going from 0 to 2*PI, is just a circle, so the above functions are just circles, plus the calls in order to introduce noise. Basically if I trace the above three classes of points in a graphical way with load81, I obtain the following image: The program , it will generate the same set of points and will feed them into the neural network configured to accept 2 inputs and output one of three possible classes. After about 2 seconds of training, we try to visualize what the neural network has learned (also part of the command) in this way: for each point in an grid, we ask the network to classify the point. This is the ASCII-artist result: As you can see, while the problem had no linear solution, the neural network was able to split the 2D space into areas, with the holes where there is the intersection between the circles areas, and thiner surfaces where the circles actually cross each other (in the intersection between the two circumferences there are points of two different classes). This example was not practical perhaps but shows well the power of the neural network in non linear tasks. Neural Redis is not the right tool for advanced NLP tasks, and for sentiment analysis, which is a very hard problem, there are RNNs and other, more complex tools, that can provide state of art results. However exactly for the same reason, SA is a very good example to show how to model problems, and that even the simplest of the intuitions can allow Neural Redis to handle problems in a decent way (even if far from the top specialized systems) after a training of 5 minutes or so. This case study is based on the source code inside the examples directory called . It uses a very popular dataset used for sentiment analysis benchmarking, composed of 2000 movies reviews, 1000 which are positive, and 1000 negative. The reviews are like the following: Normally we should try to do the best we can do in order to pre-process the data, but we are lazy dogs, so we don't do anything at all. However we still need to map our inputs and outputs to meaningful parameters. For the outputs, it's trivial, is a categorization task: negative or positive. But how do we map words to inputs? Normally you assign different words to different IDs, and then use such IDs as indexes. This creates two problems in our case: So I did the following. Let's say our network is composed of 3000 inputs, 100 hidden units, and the 2 outputs for the classification. We split the initial inputs into two sides: 1500 of inputs just take the single words. The other 1500 inputs, we use for combinations of two words. What I did was to just use hashing to map the words in the text to the input units: This is a bit crazy, I'm curious to know if it's something that people tried in the past, since different words and different combinations of words will hash to the same, so we'll get a bit less precise results, however it is unlikely that words highly polarized in the opposite direction (positive VS negative) will hash to the same bucket, if we use enough inputs. So each single word and combination of words is a \"vote\" in the input unit. As we scan the sentences to give the votes, we also sum all the single votes we gave, so that we finally normalize to make sure all our inputs summed will give \"1\". This way the sentiment analysis does not depend by the length of the sentence. While this approach is very simple, it works and produces a NN in a matter of seconds that can score 80% in the 2000 movies dataset. I just spent a couple of hours on it, probably it's possible to do much better with a more advanced scheme. However the gist of this use case is: be creative when trying to map your data to the neural network. If you run you'll see the network quickly converging and at the end, you'll be able to type sentences that the NN will classify as positive or negative: Of course you'll find a number of sentences that the net will classify in the wrong way... However the longer sentence you type and more similar to an actual movie review, the more likely it can predict it correctly. In the above tutorial not all the options of all the commands may be covered, so here there is a small reference with all the commands supported by this extension and associated options. Create a new neural network if the target key is empty, or returns an error. Add a data sample into the training or testing dataset (if specified as last argument) or evenly into one or the other, according to their respective sizes, if no target is specified. For neural networks of type CLASSIFIER the output must be just one, in the range from 0 to . It's up to the network to translate the class ID into a set of zeros and ones. The command returns the number of data samples inside the training and testing dataset. If the target datasets are already full, a random entry is evicted and substituted with the new data. Run the network stored at key, returning an array of outputs. Like but can be used only with NNs of type CLASSIFIER. Instead of outputting the raw neural network outputs, the command returns the output class directly, which is, the index of the output with the greatest value. Train a network in a background thread. When the training finishes automatically updates the weights of the trained networks with the new ones and updates the training statistics. The command works with a copy of the network, so it is possible to use the network while it is undergoing a training. If no AUTOSTOP is specified, trains the network till the maximum number of cycles or milliseconds are reached. If no maximum number of cycles is specified there are no cycles limits. If no milliseconds are specified, the limit is set to 10000 milliseconds (10 seconds). If AUTOSTOP is specified, the training will still stop when the maximum umber of cycles or milliseconds is specified, but will also try to stop the training if overfitting is detected. Check the previous sections for a description of the (still naive) algorithm the implementation uses in order to stop. If BACKTRACK is specified, and AUTOSTOP is also specified, while the network is trained, the trainer thread saves a copy of the neural network every time it has a better score compared to the previously saved one and there are hints suggesting that overfitting may happen soon. This network is used later if it is found to have a smaller error. Show many internal information about the neural network. Just try it :-) Set the neural network weights to random ones (that is, the network will completely unlearn what it learned so far), and reset training statistics. However the datasets are not touched at all. This is useful when you want to retrain a network from scratch. The main aim of Neural Redis, which is currently just a 48h personal hackatlon, is to show the potential that there is in an accessible API that provides a simple to use machine learning tool, that can be used and trained interactively. However the neural network implementation can be surely improved in different ways, so if you are an expert in this field, feel free to submit changes or ideas. One thing that I want to retain is the simplicity of the outer layer: the API. However the techniques used in the internals can be more complex in order to improve the results. There is to note that, given the API exported, the implementation of the neural network should be, more than state of art in solving a specific problem, more designed in order to work well enough in a large set of conditions. While the current fully connected network has its limits, it together with BPROP learning shows to be quite resistant to misuses. So an improved version should be able to retain, and extend this quality. The simplest way to guarantee this is to have a set of benchmarks of different types using open datasets, and to score different implementations against it.","flair":"null\tnull"}
{"author":"gwern","created":"Fri Sep 30 15:15:13 EDT 2016","text":"Automatically identifying that an image is not suitable\/safe for work (NSFW), including offensive and adult images, is an important problem which researchers have been trying to tackle for decades. Since images and user-generated content dominate the Internet today, filtering NSFW images becomes an essential component of Web and mobile applications. With the evolution of computer vision, improved training data, and deep learning algorithms, computers are now able to automatically classify NSFW image content with greater precision. Defining NSFW material is subjective and the task of identifying these images is non-trivial. Moreover, what may be objectionable in one context can be suitable in another. For this reason, the model we describe below focuses only on one type of NSFW content: pornographic images. The identification of NSFW sketches, cartoons, text, images of graphic violence, or other types of unsuitable content is not addressed with this model. To the best of our knowledge, there is no open source model or algorithm for identifying NSFW images. In the spirit of collaboration and with the hope of advancing this endeavor, we are releasing our deep learning model that will allow developers to experiment with a classifier for NSFW detection, and provide feedback to us on ways to improve the classifier. Our general purpose Caffe deep neural network model (Github code) takes an image as input and outputs a probability (i.e a score between 0-1) which can be used to detect and filter NSFW images. Developers can use this score to filter images below a certain suitable threshold based on a ROC curve for specific use-cases, or use this signal to rank images in search results. In recent years, CNNs have become very successful in image classification problems [1] [5] [6]. Since 2012, new CNN architectures have continuously improved the accuracy of the standard ImageNet classification challenge. Some of the major breakthroughs include AlexNet (2012) [6], GoogLeNet [5], VGG (2013) [2] and Residual Networks (2015) [1]. These networks have different tradeoffs in terms of runtime, memory requirements, and accuracy. The main indicators for runtime and memory requirements are: Ideally we want a network with minimum flops and minimum parameters, which would achieve maximum accuracy. We train the models using a dataset of positive (i.e. NSFW) images and negative (i.e. SFW \u2013 suitable\/safe for work) images. We are not releasing the training images or other details due to the nature of the data, but instead we open source the output model which can be used for classification by a developer. We use the Caffe deep learning library and CaffeOnSpark; the latter is a powerful open source framework for distributed learning that brings Caffe deep learning to Hadoop and Spark clusters for training models (Big shout out to Yahoo\u2019s CaffeOnSpark team!). While training, the images were resized to 256x256 pixels, horizontally flipped for data augmentation, and randomly cropped to 224x224 pixels, and were then fed to the network. For training residual networks, we used scale augmentation as described in the ResNet paper [1], to avoid overfitting. We evaluated various architectures to experiment with tradeoffs of runtime vs accuracy. Tradeoffs of different architectures: accuracy vs number of flops vs number of params in network. The deep models were first pre-trained on the ImageNet 1000 class dataset. For each network, we replace the last layer (FC1000) with a 2-node fully-connected layer. Then we fine-tune the weights on the NSFW dataset. Note that we keep the learning rate multiplier for the last FC layer 5 times the multiplier of other layers, which are being fine-tuned. We also tune the hyper parameters (step size, base learning rate) to optimize the performance. We observe that the performance of the models on NSFW classification tasks is related to the performance of the pre-trained model on ImageNet classification tasks, so if we have a better pretrained model, it helps in fine-tuned classification tasks. The graph below shows the relative performance on our held-out NSFW evaluation set. Please note that the false positive rate (FPR) at a fixed false negative rate (FNR) shown in the graph is specific to our evaluation dataset, and is shown here for illustrative purposes. To use the models for NSFW filtering, we suggest that you plot the ROC curve using your dataset and pick a suitable threshold. Comparison of performance of models on Imagenet and their counterparts fine-tuned on NSFW dataset. We are releasing the thin ResNet 50 model, since it provides good tradeoff in terms of accuracy, and the model is lightweight in terms of runtime (takes < 0.5 sec on CPU) and memory (~23 MB). Please refer our git repository for instructions and usage of our model. We encourage developers to try the model for their NSFW filtering use cases. For any questions or feedback about performance of model, we encourage creating a issue and we will respond ASAP. Results can be improved by fine-tuning the model for your dataset or use case. If you achieve improved performance or you have trained a NSFW model with different architecture, we encourage contributing to the model or sharing the link on our description page. Disclaimer: The definition of NSFW is subjective and contextual. This model is a general purpose reference model, which can be used for the preliminary filtering of pornographic images. We do not provide guarantees of accuracy of output, rather we make this available for developers to explore and enhance as an open source project. We would like to thank Sachin Farfade, Amar Ramesh Kamat, Armin Kappeler, and Shraddha Advani for their contributions in this work. [1] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \u201CDeep residual learning for image recognition\u201D arXiv preprint arXiv:1512.03385 (2015). [2] Simonyan, Karen, and Andrew Zisserman. \u201CVery deep convolutional networks for large-scale image recognition.\u201D; arXiv preprint arXiv:1409.1556(2014). [3] Iandola, Forrest N., Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. \u201CSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 1MB model size.\u201D; arXiv preprint arXiv:1602.07360 (2016). [4] He, Kaiming, and Jian Sun. \u201CConvolutional neural networks at constrained time cost.\u201D In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360. 2015. [5] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. \u201CGoing deeper with convolutions\u201D In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9. 2015. [6] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \u201CImagenet classification with deep convolutional neural networks\u201D In Advances in neural information processing systems, pp. 1097-1105. 2012.","flair":"null\tnull"}
{"author":"Yeebster","created":"Thu Sep 29 23:44:02 EDT 2016","text":"I'm working on incorporating some world knowledge into an NLP task, and I'd like to use data from MIT's [Open Mind Common Sense project](http:\/\/media.mit.edu\/research\/groups\/5994\/open-mind-common-sense). \n\nI found a lot of materials online referencing [csc.media.mit.edu](http:\/\/csc.media.mit.edu) and [openmind.media.mit.edu](http:\/\/openmind.media.mit.edu), however both sites are down.\nThere seemed to be a bunch of subprojects within OMCS according to its [github page](https:\/\/github.com\/commonsense\/omcs), but the only one that currently seems active is [ConceptNet](http:\/\/conceptnet5.media.mit.edu\/). \n\nAccording to this [site](http:\/\/ttt.media.mit.edu\/research\/openmind.html), OMCS collected knowledge like \"every person is younger than their mother\" and \"you can push something with a straight stick\", but I wasn't able to find such knowledge by searching on ConceptNet (ConceptNet's relations don't seem to  be flexible enough to describe this kind of knowledge).\n\nI'm quite curious about what could be on [openmind.media.mit.edu](http:\/\/openmind.media.mit.edu) and [csc.media.mit.edu](http:\/\/csc.media.mit.edu). If they contain a superset of the knowledge in ConceptNet, I'd love to use them. ","flair":"null\tnull"}
{"author":"pilooch","created":"Wed Nov 16 07:03:59 EST 2016","text":"Subreddit, who would be game for a beer at NIPS this december ? \n\nI was thinking a bar either 7ish or 10ish pm. Depending on interest, I have a few contacts in the city who could help talking to a bar owner and reserve some space in advance.\n\nI've put a doodle at http:\/\/doodle.com\/poll\/3qi9i4ncgqnnpv33#table to select one evening ?\n\nGood ideas 'often' go with good IPAs ^^ , bad ideas too...\n\nEDIT: this seems to converge on either Tuesday 12\/6 or Wednesday 12\/7 starting at 10pm. I'll wait for another 10 days then update this post with final day + venue. Any suggestion welcome.\n","flair":"three\tResearch"}
{"author":"testingTestingIBS","created":"Tue Oct 11 23:51:05 EDT 2016","text":"I think convets are essentially like a bag of words classifier but the vocabulary is defined by the network and not pre-specified like in HoG.\n\nWhat I think would be a more powerful recognition system is to use bag of words with something like SIFT.  First you see how many feature vectors match and then you see if the constellation of matching points is some kind of rigid deformation like affine.\n\nWhat do people think about this?","flair":"null\tnull"}
{"author":"SkiddyX","created":"Wed Nov 02 22:56:50 EDT 2016","text":"I was very sceptical that they would be useful or have any effect, however, the subreddit quality has seemed to increased (less blog spam, annoying videos, more research paper discussions). What are other's thoughts on this?","flair":"one\tDiscussion"}
{"author":"mttd","created":"Tue Oct 04 08:32:49 EDT 2016","text":" Skip to content the morning paper an interesting\/influential\/important paper from the world of CS every weekday morning, as selected by Adrian Colyer Home About Subscribe Cyclades: Conflict-free asynchronous machine learning October 4, 2016 CYCLADES: Conflict-free asynchronous machine learning Pan et al. NIPS 2016 \u201CConflict-free,\u201D the magic words that mean we can process things concurrently or in parallel at full speed, with no need for coordination. Today\u2019s paper introduces Cyclades, a system for speeding up machine learning on a single NUMA node. In the evaluation, the authors used NUMA nodes with 18 CPUs and 1TB memory. Extending Cyclades to work across NUMA nodes, or even in a distributed setting using a parameter server is reserved for future investigation. The overall hierarchy of ML speed and scale probably looks something like this: Clusters of GPUs [w. CPU support] \u2013 Speed and Scale Multi-GPUs on a single node [w. CPU support] for SPEED, Clusters of CPUs for SCALE Multi-core CPUs on a single node Single core (thread) systems Remember not to underestimate what can be achieved on a single thread. Because of their accessibility and ease of programming, systems that work on a single box can have great utility. Cyclades efficiently utilises multiple cores through a graph partitioning step that allocates connected-component subgraphs to cores, eliminating the need for cross-core coordination. One of the compelling things that is unique about Cyclades is that unlike other systems we\u2019ve seen (Asynchronous Complex Analytics, Petuum) that use asynchronous processing to speed up ML, Cyclades maintains serial equivalence. That is, it guarantees to give the same results as a serial implementation. Without serial equivalence, other systems rely on analyses that the deviation from serial is not significant (stays within some bound). Since it returns exactly the same output as a serial implementation, any algorithm parallelized by our framework inherits the correctness proof of the serial counterpart without modifications. Additionally, if a particular heuristic serial algorithm is popular, but does not have a rigorous analysis, such as backpropagation on neural networks, Cyclades still guarantees that its execution will return a serially equivalent output. Cyclades works for a large family of algorithms based on stochastic updates. This includes logistic regression, least squares, support vector machines, word embeddings, stochastic gradient descent, matrix completion and factorization, and more. At the core of Cyclades is a clever partitioning strategy. Consider a graph with two types of vertices (bipartite), update functions and model variables. We create an edge from an update function to a variable if that function reads or writes it. From this we can create a conflict graph (Cyclades never actually fully materializes such a graph, but it\u2019s a useful conception). The conflict graph contains only update function vertices, and links two vertices with an edge if they share at least one variable in the update graph. The key now is to partition the conflict graph into connected components, and allocate those connected components to cores. This relies on a result established in a 2016 Electronic Journal of Combinatorics paper, \u201CThe phase transition in site percolation on pseudo-random graphs,\u201D by Krivelevich. No, I\u2019m not brave enough to try and cover a Journal of Combinatorics paper on this blog! The result is as follows: Let G be a graph on n vertices, with maximum vertex degree Δ. Let us sample each vertex independently with probability p = (1 \u2013 &eps;)\/&Delta and define as G\u2019 the induced subgraph on the sampled vertices. Then, the largest connected component of G\u2019 has size at most (4\/&esp;2) log n, with high probability. This is pithily summarized by the authors as \u2018frugal sampling shatters conflicts.\u2019 It tells us that by careful sampling of the vertices in the conflict graph, we can induce a subgraph with at least O(log n \/ &eps;2) components to distribute across cores. (There\u2019s something of importance in the \u2018induced subgraph\u2019 phrase that I feel I don\u2019t fully understand. Because what really matters surely is the size of the largest connected component in G (not G\u2019)? (We do the actual computation on G, right?). My interpretation is that by sampling we can create a subgraph G\u2019 from which we can infer connected components that with high probability match the connected components of G (and have certain maximum size)\u2026 But then why would the number of components depend on our sampling frequency?? The results indicate that the process clearly works, so we can press on, but if anyone can put me straight here I\u2019d appreciate it!). Moreover, since there are no conflicts between different conflict-groups, the processing of updates per any single group will never interact with the variables corresponding to the updates of another conflict group. Cyclades samples in batches of size B = (1-&eps;)n\/Δ, and identifies conflict groups for each batch using a connected components algorithm. The appendix shows that this can be done efficiently in time O(num_edges . log2n \/ P) where P is the number of processor cores available. We can put all the pieces together for the complete Cyclades algorithm: The inner loop is parallelized and can be performed asynchronously with no need for memory locks as all the cores access non-overlapping subsets of the input x. This gives good cache coherence, and as each core potentially access the same coordinates multiple times, good cache locality. Observe that Cyclades bypasses the need to establish convergence guarantees for the parallel algorithm. And we have the main theorem: Does it work in practice? The authors implemented Cyclades in C++ and tested it against HogWild! on equivalent number of cores, and also against a single threaded implementation . Cyclades is initially slower but converges faster: It achieves a relative speed-up of up to 5x over HogWild! when both systems are given the same number of cores. When we decrease the step-size after each epoch, Cyclades shows even stronger relative performance: It would be interesting to see where else the Krivelevich connected component result can be applied, it seems that many graph computations could potentially benefit. Share this: Twitter LinkedIn Email Print Like this: Like Loading... Related from → Uncategorized ← The load, capacity, and availability of quorum systems Why does deep and cheap learning work so well? → 5 Comments leave one → mikhailfranco permalink October 5, 2016 12:24 pm \u2018Induced subgraph\u2019 only includes edges that exist between vertices in the sample, not edges that go outside the sampled vertex set. For frugal sampling, there will probably be many small connected components within each connected component of the original graph. Reply Jonathan Dursi permalink October 5, 2016 4:47 pm \u201CMy interpretation is that by sampling we can create a subgraph G\u2019 from which we can infer connected components that with high probability match the connected components of G\u2026\u201D I don\u2019t think that\u2019s what\u2019s going on here \u2013 for instance, in the example in Figure 2 (that shows the \u201Cshattering\u201D), there\u2019s only one connected component. So not only are the connected components of the subsample-induced G\u2019 a poor approximation to the connected components of G, there\u2019d be no way to perform the calculation if it had to be performed without conflicts. (The same is true of any of the subgraphs). I think what\u2019s going on here is that the serial case would perform the (conflicting) updates in some arbitrary order, based just on the order the loop hits the items, and any of those \u201Cserializable\u201D orders would be perfectly legitimate \u2013 you just want to avoid race conditions where one task is updating a value while another is writing it. So take the SGD update in appendix A, and the gradient of f(x). You don\u2019t want to update x_k using a gradient that has some updated values and some old values. There\u2019s a number of orderings of k which would give a good answer, but even more which wouldn\u2019t. There are deterministic algorithms for decomposing the problem into non-overlapping subproblems, performing those in parallel, and then doing any remaining steps; those work but are quite expensive (and aren\u2019t in general always going to give you an optimal decomposition anyway). This seems to me to be a very cute and simple way (with a surprisingly deep reason why it works well) to stochastically generate and distribute such (over-)decompositions efficiently which performs well in theory and in practice for an interestingly broad range of problems. Because of its simplicity and its effectiveness, I expect that it will see a significant amount of adoption\u2026 Reply Trackbacks Machine Learning Roundup (October 4, 2016) | The Big Analytics Blog Asynchronous methods for deep reinforcement learning | the morning paper Machine Learning Roundup 10\/11\/2016 | Big Analytics Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Subscribe never miss an issue! The Morning Paper delivered straight to your inbox. Search Archives Archives Select Month November 2016  (19) October 2016  (21) September 2016  (20) July 2016  (16) June 2016  (22) May 2016  (22) April 2016  (12) March 2016  (23) February 2016  (21) January 2016  (23) December 2015  (10) November 2015  (21) October 2015  (22) September 2015  (22) August 2015  (22) June 2015  (21) May 2015  (21) April 2015  (14) March 2015  (23) February 2015  (23) January 2015  (21) December 2014  (15) November 2014  (20) October 2014  (20) May 2011  (3) Most read in the last few days About Building machines that learn and think like people The amazing power of word vectors When CSI meets public wifi: Inferring your mobile phone password via wifi signals Artificial Intelligence and life in 2030 Smart Reply: Automated response suggestion for email Twice the bits, twice the trouble: vulnerabilities induced by migrating to 64-bit platforms Towards deep symbolic reinforcement learning The Linux Scheduler: a Decade of Wasted Cores Playing FPS games with deep reinforcement learning RSS RSS - Posts RSS - Comments Live on twitter My Tweets Blog at WordPress.com. Send to Email Address Your Name Your Email Address Cancel Post was not sent - check your email addresses! Email check failed, please try again Sorry, your blog cannot share posts by email. %d bloggers like this: ","flair":"null\tnull"}
{"author":"NicolasGuacamole","created":"Tue Sep 27 09:59:27 EDT 2016","text":"Ben Recht spoke about optimization a few days ago at the Simons Institute. His talk was a highly entertaining tour de force through about a semester of convex optimization. You should go watch it. It\u2019s easy to spend a semester of convex optimization on various guises of gradient descent alone. Simply pick one of the following variants and work through the specifics of the analysis: conjugate, accelerated, projected, conditional, mirrored, stochastic, coordinate, online. This is to name a few. You may also choose various pairs of attributes such as \u201Caccelerated coordinate\u201D descent. Many triples are also valid such as \u201Conline stochastic mirror\u201D descent. An expert unlike me would know exactly which triples are admissible. You get extra credit when you use \u201Csubgradient\u201D instead of \u201Cgradient\u201D. This is really only the beginning of optimization and it might already seem confusing. Thankfully, Ben kept things simple. There are indeed simple common patterns underlying many (if not all) variants of gradient descent. Ben did a fantastic job focusing on the basic template without getting bogged down in the details. He also made a high-level point that I strongly agree with. Much research in optimization focuses on convergence rates. That is, how many update steps do we need to minimize the function up to an epsilon error? Often fairly subtle differences in convergence rates are what motivates one particular variant of gradient descent over another. But there are properties of the algorithm that can affect the running time more powerfully than the exact convergence rate. A prime example is robustness. basic gradient descent is robust to noise in several important ways. Accelerated gradient descent is much more brittle. Showing that it is even polynomial time (and under what assumptions) is a rather non-trivial exercise depending on the machine model. I\u2019ve been saying for a while now that small improvements in running time don\u2019t trump major losses in robustness. The situation in optimization is an important place where the trade-off between robustness and efficiency deserves attention. Generally speaking, the question \u201Cwhich algorithm is better\u201D is rarely answered by looking at a single proxy such as \u201Cconvergence rate\u201D. With that said, let me discuss gradient descent first. Then I will try to motivate why it makes sense to expect an accelerated method and how one might have discovered it. My exposition is not particularly close to Ben\u2019s lecture. In particular, mistakes are mine. So, you should still go and watch that lecture. If you already know gradient descent, you can skip\/skim the first section. The goal is to minimize a convex function \\(f\\colon\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) without any constraints. We'll assume that \\(f\\) is twice differentiable and strongly convex. This means that we can squeeze a parabola between the tangent plane at \\(x\\) given by the gradient and the function itself. Formally, for some \\(\\ell>0\\) and all \\(x,z\\in\\mathbb{R}^n:\\) At the same time, we don\u2019t want the function to be \u201Ctoo convex\u201D. So, we\u2019ll require the condition, often called smoothness: This is a Lipschitz condition on the gradient map in disguise as it is equivalent to: Let's be a bit more concrete and consider from here on the important example of a convex function \\(f(x) = \\frac 12 x^T A x - b^T x,\\) where \\(A\\) is an \\(n\\times n\\) positive definite matrix and \\(b\\) is a vector. We have \\(\\nabla f(x) = Ax - b.\\) It's an exercise to check that the above conditions boil down to the spectral condition: \\(\\ell I \\preceq A \\preceq LI.\\) Clearly this problem has a unique minimizer given by \\(x^*=A^{-1}b.\\) In other words, if we can minimize this function, we'll know how to solve linear systems. Now, all that gradient descent does is to compute the sequence of points for some choice of the step parameter \\(t_k.\\) Our hope is that for some positive \\(\\alpha < 1\\), If this happens in every step, gradient descent converges exponentially fast towards the optimum. This is soberly called linear convergence in optimization. Since the function is smooth, this also guarantees convergence in objective value. Choosing the right step size \\(t\\) is an important task. If we choose it to small, our progress will be unnecessarily slow. If we choose it too large, we will overshoot. A calculation shows that if we put \\(t= 2\/(\\ell + L)\\) we get \\(\\alpha = (L-\\ell)\/(L+\\ell).\\) Remember that \\(\\kappa = L\/\\ell\\) is condition number of the matrix. More generally, you could define the condition number of \\(f\\) in this way. We have shown that So the potential function (or Lyapunov function) drops by a factor of roughly \\(1-1\/\\kappa\\) in every step. This is the convergence rate of gradient descent. What Nesterov showed in 1983 is that we can improve the convergence rate of gradient descent without using anything more than gradient information at various points of the domain. This is usually when people say something confusing about physics. It's probably helpful to others, but physics metaphors are not my thing. Let me try a different approach. Let's think about why what we were doing above wasn't optimal. Consider the simple example \\(f(x)= \\frac12 x^T A x- b^T x.\\) Recall, the function is minimized at \\(A^{-1}b\\) and the gradient satisfies \\(\\nabla f(x) = Ax-b.\\) Let's start gradient descent at \\(x_0=tb.\\) We can then check that where \\(A'=tA\\) and \\(b'=tb.\\) Why does this converge to \\(A^{-1}b\\)? The reason is that what gradient descent is computing is a degree \\(k\\) polynomial approximation of the inverse function. To see this, recall that for all scalars \\(|x|<1,\\) Since the eigenvalues of \\(A'\\) lie within \\((0,1),\\) this scalar function extends to the matrix case. Moreover, the approximation error when truncating the series at degree \\(k\\) is \\(O( (1-x)^k)).\\) In the matrix case this translates to error \\(O(\\| (I-A')^k \\|) = O( (1-\\ell\/L)^k).\\) This is exactly the convergence rate of gradient descent that we determined earlier. Why did we go through this exercise? The reason is that now we see that to improve on gradient descent it suffices to find a better low-degree approximation to the scalar function \\(1\/x.\\) What we'll be able to show is that we can save a square root in the degree while achieving the same error! Anybody familiar with polynomial approximation should have one guess when hearing \"quadratic savings in the degree\": Let's be clear. Our goal is to find a degree \\(k\\) polynomial \\(q_k(A)\\) which minimizes the residual Put differently we are looking for a polynomial of the form \\(p_k(z)=1-zq(z).\\) What we want is that the polynomial is as small as possible on the location of the eigenvalues of \\(A\\) which lie in the interval \\([\\ell,L].\\) At the same time, the polynomial must satisfy \\(p_k(0)=1.\\) This is exactly the property that Chebyshev polynomials achieve with the least possible degree! Quantitatively, we have the following lemma that I learned from Rocco Servedio. As Rocco said in that context: Lemma. There is a polynomial \\(p_k\\) of degree \\(O(\\sqrt{(L\/\\ell)\\log(1\/\\epsilon)})\\) such that \\(p_k(0)=1\\) and \\(p_k(x)\\le\\epsilon\\) for all \\(x\\in[\\ell,L].\\) The lemma implies that we get a quadratic savings in degree. Since we can build \\(p_k\\) from gradient information alone, we now know how to improve the convergence rate of gradient descent. It gets better. The Chebyshev polynomials satisfy a simple recursive definition that defines the \\(k\\)-th degree polynomial in terms of the previous two polynomials. This means that accelerated gradient descent only needs the previous two gradients with suitable coefficients: Figuring out the best possible coefficients \\(\\alpha_k,\\beta_k\\) leads to the above convergence rate. What's amazing is that this trick works for any convex function satisfying our assumptions and not just the special case we dealt with here!  In fact, this is what Nesterov showed. I should say that the interpretation in terms of polynomial approximations is lost (as far as I know).The polynomial approximation method I described was known much earlier in the context of eigenvalue computations. This is another fascinating connection I'll describe in the next section. Let me add that it can be shown that this convergence rate is optimal for any first-order (gradient only method) by taking \\(A\\) to be the Laplacian of a path of length \\(n\\). This is true even in our special case. It's optimal though in a weak sense: There is a function and a starting point such that the method needs this many steps. I would be interesting to understand how robust this lower bound is. Our discussion above was essentially about eigenvalue location. What does polynomial approximation have to do with eigenvalues? Recall, that the most basic way of computing the top eigenvalue of a matrix is the power method. The power method corresponds to a very basic polynomial, namely \\(p_k(x) = x^k.\\) This polynomial has the effect that it maps \\(1\\) to \\(1\\) and moves every number \\(|x|<1\\) closer to \\(0\\) at the rate \\(|x|^k.\\) Hence, if the top eigenvalue is \\(\\lambda\\) and the second eigenvalue is \\((1-\\epsilon)\\lambda,\\) then we need about \\(k\\approx 1\/\\epsilon\\) iterations to  approximately find \\(\\lambda.\\) Using exactly the same Chebyshev idea, we can improve this to \\(k=O(\\sqrt{1\/\\epsilon})\\) iterations! This method is often called Lanczos method. So, we have the precise correspondence: I find this quite amazing. In a future post I will return to the power method in greater detail in the context of noise-tolerant eigenvalue computation. I\u2019m embarrassed to admit that the first time I saw gradient descent in full generality was in grad school. I had seen the Perceptron algorithm in my last year as an undergraduate. At the time, I was unaware that like so many algorithms it is just a special case of gradient descent. Looking at the typical undergraduate curriculum, it seems like we spend a whole lot of time iterating through dozens of combinatorial algorithms for various problems. So much so that we often don\u2019t get around to teaching something as fundamental as gradient descent. It wouldn\u2019t take more than two lectures to teach the contents of this blog post (or one lecture if you\u2019re Ben Recht). Knowing gradient descent seems quite powerful. It\u2019s not only simple and elegant. It\u2019s also the algorithmic paradigm behind many algorithms in machine learning, optimization and numerical computation. Teaching it to undergraduates seems like a must. I just now realize that I haven\u2019t been an undergraduate in a while. Time flies. So perhaps this is already happening. To stay on top of future posts, subscribe to the  RSS feed or follow me on Twitter.","flair":"null\tnull"}
{"author":"vanboxel","created":"Tue Sep 27 18:00:18 EDT 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose Self-driving Car Comma.ai model Dan Van Boxel SubscribeSubscribedUnsubscribe2,0832K Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 795 views 14 Like this video? Sign in to make your opinion count. Sign in 15 0 Don't like this video? Sign in to make your opinion count. Sign in 1 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Streamed live on Sep 27, 2016 More modeling with the comma.ai self-driving car data set. And I managed to crash my computer half-way through; sorry about the short episode. Category Science & Technology License Standard YouTube License Show more Show less Loading... Advertisement Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Transform your car into a self-driving car for $1,000 - Duration: 3:39. The Verge 196,222 views 3:39 CNTK Revisited: Now with Python - Duration: 1:00:52. Dan Van Boxel 163 views New 1:00:52 Self-driving car Model Review - Duration: 54:29. Dan Van Boxel 270 views 54:29 KPMGVoice: Comm.ai | Chapter 5 of The Great Rewrite - Duration: 2:56. Forbes 1,118 views 2:56 Self-driving car with basic RNN - Duration: 59:28. Dan Van Boxel 288 views 59:28 George \"Geohot\" Hotz Presents the Comma One at Disrupt SF - Duration: 19:59. TechCrunch 41,147 views 19:59 Self-driving car Parameter Rescaling - Duration: 1:00:09. Dan Van Boxel 363 views 1:00:09 DanDoesData TensorFlow Anniversary! - Duration: 36:52. Dan Van Boxel 218 views 36:52 Self-driving Car Visualization and Gas Model - Duration: 1:01:38. Dan Van Boxel 1,398 views 1:01:38 Self-driving Car Comma.ai dataset - Duration: 1:00:25. Dan Van Boxel 705 views 1:00:25 How to Make Cars Smarter: A Step Towards Self-Driving Cars - Duration: 58:28. Pivotal 104 views 58:28 George Hotz on Comma.AI and the state of self-driving cars - Duration: 23:18. Machine Learning at Berkeley 9,518 views 23:18 The Self-Driving Car Company Coming For Tesla & Google - Duration: 4:25. Forbes 35,629 views 4:25 Meet the 26-Year-Old Hacker Who Built a Self-Driving Car... in His Garage - Duration: 6:53. Bloomberg 1,060,539 views 6:53 Self-driving car DeepDrive data inspection - Duration: 1:01:36. Dan Van Boxel 847 views 1:01:36 RI Seminar: John Leonard : Mapping, Localization, and Self-Driving Vehicles - Duration: 1:06:19. cmurobotics 9,420 views 1:06:19 Hacker George Hotz discusses his first official product: The Comma One - Duration: 5:28. CCTV America 6,176 views 5:28 Self-driving Car Autonomous Steering - Duration: 55:03. Dan Van Boxel 386 views 55:03 é\u20AC±åˆŠã\u201A¢ã\u201A¹ãƒ¢ãƒŽã\u20ACŒè\u2021ªå\u2039\u2022é�\u2039è»¢ã\u20AC� NVIDIA Japanãƒ»GPUã�¨è\u2021ªå\u2039\u2022é�\u2039è»¢ã�®é\u2013¢ä¿\u201Aï¼ˆé� è\u2014¤è«­ãƒ»æ¦Žæœ¬éº\u2014ç¾Žï¼\u2030 [ãƒ¢ãƒ¼ãƒ\u2039ãƒ³ã\u201A°CROSS] - Duration: 6:09. CUT CROSS 474 views 6:09 Deep Reinforcement Learning for Driving Policy - Duration: 34:55. Mobileye 2,157 views 34:55 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"null\tnull"}
{"author":"liviu-","created":"Sun Oct 16 10:50:33 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.02242 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.NE < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Neural and Evolutionary Computing Title: Temporal Ensembling for Semi-Supervised Learning Authors: Samuli Laine, Timo Aila (Submitted on 7 Oct 2016 (v1), last revised 7 Nov 2016 (this version, v2)) Abstract: In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally demonstrate good tolerance to incorrect labels. Comments: This version was submitted to ICLR 2017. The two methods are now closer to each other and use similar parameters. Also added more experimental results and a test with corrupted labels. Code released Subjects: Neural and Evolutionary Computing (cs.NE); Learning (cs.LG) Report number: NVR-2016-005 Cite as: arXiv:1610.02242 [cs.NE]   (or arXiv:1610.02242v2 [cs.NE] for this version) Submission history From: Samuli Laine [view email] [v1] Fri, 7 Oct 2016 12:15:42 GMT (21kb,D) [v2] Mon, 7 Nov 2016 13:27:40 GMT (99kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"triplefloat","created":"Fri Sep 30 03:05:14 EDT 2016","text":"Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural network models to such structured datasets. In the last couple of years, a number of papers re-visited this problem of generalizing neural networks to work on arbitrarily structured graphs (Bruna et al., ICLR 2014; Henaff et al., 2015; Duvenaud et al., NIPS 2015; Li et al., ICLR 2016; Defferrard et al., NIPS 2016; Kipf & Welling, 2016), some of them now achieving very promising results in domains that have previously been dominated by, e.g., kernel-based methods, graph-based regularization techniques and others. In this post, I will give a brief overview of recent developments in this field and point out strengths and drawbacks of various approaches. The discussion here will mainly focus on two recent papers: and a review\/discussion post by Ferenc Huszar: How powerful are Graph Convolutions? that discusses some limitations of these kinds of models. I wrote a short comment on Ferenc's review here (at the very end of this post). If you're already familiar with GCNs and related methods, you might want to jump directly to Embedding the karate club network. Generalizing well-established neural models like RNNs or CNNs to work on arbitrarily structured graphs is a challenging problem. Some recent papers introduce problem-specific specialized architectures (e.g. Duvenaud et al., NIPS 2015; Li et al., ICLR 2016; Jain et al., CVPR 2016), others make use of graph convolutions known from spectral graph theory (Bruna et al., ICLR 2014; Henaff et al., 2015) to define parameterized filters that are used in a multi-layer neural network model, akin to \"classical\" CNNs that we know and love. More recent work focuses on bridging the gap between fast heuristics and the slow, but somewhat more principled, spectral approach. Defferrard et al. (NIPS 2016) approximate smooth filters in the spectral domain using Chebyshev polynomials with free parameters that are learned in a neural network-like model. They achieve convincing results on regular domains (like MNIST), closely approaching those of a simple 2D CNN model. In Kipf & Welling (2016), we take a somewhat similar approach and start from the framework of spectral graph convolutions, yet introduce simplifications (we will get to those later in the post) that in many cases allow both for significantly faster training times and higher predictive accuracy, reaching state-of-the-art classification results on a number of benchmark graph datasets. Currently, most graph neural network models have a somewhat universal architecture in common. I will refer to these models as Graph Convolutional Networks (GCNs); convolutional, because filter parameters are typically shared over all locations in the graph (or a subset thereof as in Duvenaud et al., NIPS 2015). For these models, the goal is then to learn a function of signals\/features on a graph \\(\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})\\) which takes as input: and produces a node-level output \\(Z\\) (an \\(N\\times F\\) feature matrix, where \\(F\\) is the number of output features per node). Graph-level outputs can be modeled by introducing some form of pooling operation (see, e.g. Duvenaud et al., NIPS 2015). Every neural network layer can then be written as a non-linear function \\[ H^{(l+1)} = f(H^{(l)}, A) \\, ,\\] with \\(H^{(0)} = X\\) and \\(H^{(L)} = Z\\) (or \\(z\\) for graph-level outputs), \\(L\\) being the number of layers. The specific models then differ only in how \\(f(\\cdot, \\cdot)\\) is chosen and parameterized. As an example, let's consider the following very simple form of a layer-wise propagation rule: where \\(W^{(l)}\\) is a weight matrix for the \\(l\\)-th neural network layer and \\(\\sigma(\\cdot)\\) is a non-linear activation function like the \\(\\text{ReLU}\\). Despite its simplicity this model is already quite powerful (we'll come to that in a moment). But first, let us address two limitations of this simple model: multiplication with \\(A\\) means that, for every node, we sum up all the feature vectors of all neighboring nodes but not the node itself (unless there are self-loops in the graph). We can \"fix\" this by enforcing self-loops in the graph: we simply add the identity matrix to \\(A\\). The second major limitation is that \\(A\\) is typically not normalized and therefore the multiplication with \\(A\\) will completely change the scale of the feature vectors (we can understand that by looking at the eigenvalues of \\(A\\)). Normalizing \\(A\\) such that all rows sum to one, i.e. \\(D^{-1}A\\), where \\(D\\) is the diagonal node degree matrix, gets rid of this problem. Multiplying with \\(D^{-1}A\\) now corresponds to taking the average of neighboring node features. In practice, dynamics get more interesting when we use a symmetric normalization, i.e. \\(D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}\\) (as this no longer amounts to mere averaging of neighboring nodes). Combining these two tricks, we essentially arrive at the propagation rule introduced in Kipf & Welling (2016): with \\(\\hat{A} = A + I\\), where \\(I\\) is the identity matrix and \\(\\hat{D}\\) is the diagonal node degree matrix of \\(\\hat{A}\\). In the next section, we will take a closer look at how this type of model operates on a very simple example graph: Zachary's karate club network (make sure to check out the Wikipedia article!). Let's take a look at how our simple GCN model (see previous section or Kipf & Welling, 2016) works on a well-known graph dataset: Zachary's karate club network (see Figure above). We take a 3-layer GCN with randomly initialized weights. Now, even before training the weights, we simply insert the adjacency matrix of the graph and \\(X = I\\) (i.e. the identity matrix, as we don't have any node features) into the model. The 3-layer GCN now performs three propagation steps during the forward pass and effectively convolves the 3rd-order neighborhood of every node (all nodes up to 3 \"hops\" away). Remarkably, the model produces an embedding of these nodes that closely resembles the community-structure of the graph (see Figure below). Remember that we have initialized the weights completely at random and have not yet performed any training updates (so far)! This might seem somewhat surprising. A recent paper on a model called DeepWalk (Perozzi et al., KDD 2014) showed that they can learn a very similar embedding in a complicated unsupervised training procedure. How is it possible to get such an embedding more or less \"for free\" using our simple untrained GCN model? We can shed some light on this by interpreting the GCN model as a generalized, differentiable version of the well-known Weisfeiler-Lehman algorithm on graphs. The (1-dimensional) Weisfeiler-Lehman algorithm works as follows: Repeat for \\(k\\) steps or until convergence. In practice, the Weisfeiler-Lehman algorithm assigns a unique set of features for most graphs. This means that every node is assigned a feature that uniquely describes its role in the graph. Exceptions are highly regular graphs like grids, chains, etc. For most irregular graphs, this feature assignment can be used as a check for graph isomorphism (i.e. whether two graphs are identical, up to a permutation of the nodes). Going back to our Graph Convolutional layer-wise propagation rule (now in vector form): where \\(j\\) indexes the neighboring nodes of \\(v_i\\). \\(c_{ij}\\) is a normalization constant for the edge \\((v_i,v_j)\\) which originates from using the symmetrically normalized adjacency matrix \\(D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}\\) in our GCN model. We now see that this propagation rule can be interpreted as a differentiable and parameterized (with \\(W^{(l)}\\)) variant of the hash function used in the original Weisfeiler-Lehman algorithm. If we now choose an appropriate non-linearity and initialize the random weight matrix such that it is orthogonal (or e.g. using the initialization from Glorot & Bengio, AISTATS 2010), this update rule becomes stable in practice (also thanks to the normalization with \\(c_{ij}\\)). And we make the remarkable observation that we get meaningful smooth embeddings where we can interpret distance as (dis-)similarity of local graph structures! Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. We can use the semi-supervised learning algorithm for GCNs introduced in Kipf & Welling (2016). We simply label one node per class\/community (highlighted nodes in the video below) and start training for a couple of iterations: Note that the model directly produces a 2-dimensional latent space which we can immediately visualize. We observe that the 3-layer GCN model manages to linearly separate the communities, given only one labeled example per class. This is a somewhat remarkable result, given that the model received no feature description of the nodes. At the same time, initial node features could be provided, which is exactly what we do in the experiments described in our paper (Kipf & Welling, 2016) to achieve state-of-the-art classification results on a number of graph datasets. Research on this topic is just getting started. The past several months have seen exciting developments, but we have probably only scratched the surface of these types of models so far. It remains to be seen how neural networks on graphs can be further taylored to specific types of problems, like, e.g., learning on directed or relational graphs, and how one can use learned graph embeddings for further tasks down the line, etc. This list is by no means exhaustive and I expect further interesting applications and extensions to pop up in the near future. Let me know in the comments below if you have some exciting ideas or questions to share! Max Welling, Taco Cohen, Chris Louizos and Karen Ullrich (for many discussions and feedback both on the paper and this blog post). Also I'd like to thank Ferenc Huszar for highlighting some drawbacks of these kinds of models. This blog post constitutes by no means an exhaustive review of the field of neural networks on graphs. I have left out a number of both recent and older papers to make this post more readable and to give it a coherent story line. The papers that I mentioned here will nonetheless serve as a good start if you want to dive deeper into this topic and get a complete overview of what is around and what has been tried so far. If you want to use some of this in your own work, you can cite our paper on Graph Convolutional Networks: We have released the code for Graph Convolutional Networks on GitHub: https:\/\/github.com\/tkipf\/gcn. You can follow me on Twitter for future updates.","flair":"null\tnull"}
{"author":"Buck-Nasty","created":"Sun Nov 20 13:50:39 EST 2016","text":"Neural networks are taking the world of computing by storm. Researchers have used them to create machines that are learning a huge range of skills that had previously been the unique preserve of humans\u2014object recognition, face recognition, natural language processing, machine translation. All these skills, and more, are now becoming routine for machines. So there is great interest in creating more capable neural networks that can push the boundaries of artificial intelligence even further. The focus of this work is in creating circuits that operate more like neurons, so-called neuromorphic chips. But how to make these circuits significantly faster? Today, we get an answer of sorts thanks to the work of Alexander Tait and pals at Princeton University in New Jersey. These guys have built an integrated silicon photonic neuromorphic chip and show that it computes at ultrafast speeds. Optical computing has long been the great dream of computer science. Photons have significantly more bandwidth than electrons and so can process more data more quickly. But the advantages of optical data processing systems have never outweighed the additional cost of making them, and so they have never been widely adopted. That has started to change in some areas of computing, such as analog signal processing, which requires the kind of ultrafast data processing that only photonic chips can provide. Now neural networks are opening up a new opportunity for photonics. \u201CPhotonic neural networks leveraging silicon photonic platforms could access new regimes of ultrafast information processing for radio, control, and scientific computing,\u201D say Tait and co. At the heart of the challenge is to produce an optical device in which each node has the same response characteristics as a neuron. The nodes take the form of tiny circular waveguides carved into a silicon substrate in which light can circulate. When released this light then modulates the output of a laser working at threshold, a regime in which small changes in the incoming light have a dramatic impact on the laser\u2019s output. Crucially, each node in the system works with a specific wavelength of light\u2014a technique known as wave division multiplexing. The light from all the nodes can be summed by total power detection before being fed into the laser. And the laser output is fed back into the nodes to create a feedback circuit with a non-linear character. An important question is just how closely this non-linearity mimics neural behavior. Tait and co measure the output and show that it is mathematically equivalent to a device known as a continuous-time recurrent neural network. \u201CThis result suggests that programming tools for CTRNNs could be applied to larger silicon photonic neural networks,\u201D they say. That\u2019s an important result because it means the device that Tait and co have made can immediately exploit the vast range of programming nous that has been gathered for these kinds of neural networks. They go on to demonstrate how this can be done using a network consisting of 49 photonic nodes. They use this photonic neural network to solve the mathematical problem of emulating a certain kind of differential equation and compare it to an ordinary central processing unit. The results show just how fast photonic neural nets can be. \u201CThe effective hardware acceleration factor of the photonic neural network is estimated to be 1,960 × in this task,\u201D say Tait and co. That\u2019s a speed up of three orders of magnitude. That opens the doors to an entirely new industry that could bring optical computing into the mainstream. \u201CSilicon photonic neural networks could represent first forays into a broader class of silicon photonic systems for scalable information processing,\u201D say Taif and co. And others are working in this area too. Earlier this year, Yichen Shen at MIT and a few pals proposed the architecture behind a fully optical neural network and demonstrated elements of it using a programmable nanophotonic processor. Of course much depends on how well the first generation of electronic neuromorphic chips perform. Photonic neural nets will have to offer significant advantages to be widely adopted and will therefore require much more detailed characterization. Clearly, there are interesting times ahead for photonics. This story was updated on November 22 to include additional work done by researchers at MIT.","flair":"two\tNews"}
{"author":"KeponeFactory","created":"Sun Oct 02 16:55:22 EDT 2016","text":" Quantum-Chemical Insights from Deep Tensor Neural Networks Kristof T. Schütt1, Farhad Arbabzadah1, Stefan Chmiela1, Klaus R. Müller1,2,∗ and Alexandre Tkatchenko3,4\u2020 1Machine Learning Group, Technische Universität Berlin, Marchstr. 23, 10587 Berlin, Germany 2Department of Brain and Cognitive Engineering, Korea University, Anam-dong, Seongbuk-gu, Seoul 136-713, Republic of Korea 3Fritz-Haber-Institut der Max-Planck-Gesellschaft, Faradayweg 4-6, D-14195, Berlin, Germany 4Physics and Materials Science Research Unit, University of Luxembourg, L-1511 Luxembourg Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning spur similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal\/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability \u2013 a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems. Chemistry permeates all aspects of our life, from the de- velopment of new drugs to the food that we consume and materials we use on a daily basis. Chemists rely on empir- ical observations based on creative and painstaking experi- mentation that leads to eventual discoveries of molecules and materials with desired properties and mechanisms to synthe- size them. Many discoveries in chemistry can be guided by searching large databases of experimental or computational molecular structures and properties by using concepts based on chemical similarity. Because the structure and properties of molecules are determined by the laws of quantum mechan- ics, ultimately chemical discovery must be based on funda- mental quantum principles. Indeed, electronic structure cal- culations and intelligent data analysis (machine learning, ML) have recently been combined aiming towards the goal of ac- celerated discovery of chemicals with desired properties [1\u2013 8]. However, so far the majority of these pioneering efforts have focused on the construction of reduced models trained on large datasets of density-functional theory calculations. In this work, we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical properties of molecular systems beyond those trivially contained in the training dataset. Obviously, computational models are not predictive if they lack accu- racy. In addition to being interpretable, size extensive and efficient, our deep tensor neural network (DTNN) approach is uniformly accurate (1 kcal\/mol) throughout compositional and configurational chemical space. On the more fundamental side, the mathematical construction of the DTNN model pro- vides statistically rigorous partitioning of extensive molecular properties into atomic contributions \u2013 a long-standing chal- lenge for quantum-mechanical calculations of molecules. MOLECULAR DEEP TENSOR NEURAL NETWORKS It is common to use a carefully chosen representation of the problem at hand as a basis for machine learning [9\u201311]. For example, molecules can be represented as Coulomb ma- trices [7, 12, 13], scattering transforms [14], bags of bonds (BoB) [15], smooth overlap of atomic positions (SOAP) [16, 17], or generalized symmetry functions [18, 19]. Kernel- based learning of molecular properties transforms these repre- sentations non-linearly by virtue of kernel functions. In con- trast, deep neural networks [20] are able to infer the underly- ing regularities and learn an efficient representation in a layer- wise fashion [21]. Molecular properties are governed by the laws of quantum mechanics, which yield the remarkable flexibility of chemical systems, but also impose constraints on the behavior of bond- ing in molecules. The approach presented here utilizes the many-body Hamiltonian concept for the construction of the DTNN architecture (see Fig. 1), embracing the principles of quantum chemistry, while maintaining the full flexibility of a complex data-driven learning machine. DTNN receives molecular structures through a vector of nuclear charges Z and a matrix of atomic distances D en- suring rotational and translational invariance by construction (Fig. 1A). The distances are expanded in a Gaussian basis, yielding a feature vector d̂i j, which accounts for the different nature of interactions at various distance regimes. The total energy EM for the molecule M composed of N atoms is written as a sum over N atomic energy contributions Ei, thus satisfying permutational invariance with respect to atom indexing. Each atom i is represented by a coefficient vector c ∈ RB, where B is the number of basis functions, or features. Motivated by quantum-chemical atomic basis set ex- pansions, we assign an atom type-specific descriptor vector cZi to these coefficients c (0) i . Subsequently, this atomic ex- pansion is repeatedly refined by pairwise interactions with the ar X iv :1 60 9. 08 25 9v 4 [ ph ys ic s. ch em -p h] 7 N ov 2 01 6 2 10 15 20 25 # atoms 0.0 0.5 1.0 1.5 2.0 2.5 3.0 m ea n ab s. e rr or [k ca l m ol − 1 ] 0 2500 5000 # add. calcs. ≤ 15 atoms 1.4 1.6 1.8 2.0 2.2 m ea n ab s. e rr or −10 0 100 200 t im e step 0 −10 −20 −30 −40 to ta l e n e rg y [k ca l m o l− 1 ] FIG. 1: Prediction and explanation of molecular energies with a deep tensor neural network (DTNN). (A) Molecules are encoded as input for the neural network by a vector of nuclear charges and an inter-atomic distance matrix. This description is complete and invariant to rotation and translation. (B) Illustration of the network architecture. Each atom type corresponds to a vector of coefficients c(0)i which is repeatedly refined by interactions vi j. The interactions depend on the current representation c(t)j as well as the distance di j to an atom j. After T iterations, an energy contribution Ei is predicted for the final coefficient vector c(T )i . The molecular energy E is the sum over these atomic contributions. (C) Mean absolute errors of predictions for the GDB-9 dataset of 129,000 molecules as a function of the number of atoms. The employed neural network uses two interaction passes (T = 2) and 50000 reference calculation during training. The inset shows the error of an equivalent network trained on 5000 GDB-9 molecules with 20 or more atoms, as small molecules with 15 or less atoms are added to the training set. (D) Extract from the calculated (black) and predicted (orange) molecular dynamics trajectory of toluene. The curve on the right shows the agreement of the predicted and calculated energy distributions. (E) Energy contribution Eprobe (or local chemical potential ΩH(r), see text) of a hydrogen test charge on a ∑ i \u2016r − ri\u2016−2 isosurface for various molecules from the GDB-9 dataset for a DTNN model with T = 2. 3 surrounding atoms c(t+1)i = c (t) i + ∑ j,i vi j, (1) where the interaction term vi j reflects the influence of atom j at a distance Di j on atom i. Note that this refinement step is seamlessly integrated into the architecture of the molecu- lar DTNN, and is therefore adapted throughout the learning process. Considering a molecule as a graph, T refinements of the coefficient vectors are comprised of all walks of length T through the molecule ending at the corresponding atom [22\u2013 24]. From the point of view of many-body interatomic inter- actions, subsequent refinement steps t correlate atomic neigh- borhoods with increasing complexity. While the initial atomic representation only considers iso- lated atoms, the interaction terms characterize how the basis functions of two atoms overlap with each other at a certain distance. Each refinement step aims to reduce these over- laps, thereby embedding the atoms of the molecule into their chemical environment. Following this procedure, the DTNN implicitly learns an atom-centered basis that is unique and ef- ficient with respect to the property to be predicted. Non-linear coupling between the atomic vector features and the interatomic distances is achieved by a tensor layer [25, 26], such that the coefficient k of the refinement is given by vi jk = tanh ( c(t)j Vkd̂i j + (W cc(t)j )k + (W dd̂i j)k + bk ) , (2) where bk is the bias of feature k and Wc and Wd are the weights of atom representation and distance, respectively. The slice Vk of the parameter tensor V ∈ RB×B×G combines the inputs multiplicatively. Since V incorporates many parameters, using this kind of layer is both computationally expensive as well as prone to overfitting. Therefore, we employ a low-rank tensor factorization, as described in [27], such that vi j = tanh [ W f c ( (Wc f c j + b f1 ) ◦ (Wd f d̂ij + b f2 ) )] , (3) where \u2019◦\u2019 represents element-wise multiplication while Wc f , b f1 , Wd f , b f2 and W f c are the weight matrices and correspond- ing biases of atom representations, distances and resulting fac- tors, respectively. As the dimensionality of Wc f c j and Wd f d̂ij corresponds to the number of factors, choosing only a few drastically decreases the number of parameters, thus solving both issues of the tensor layer at once. Arriving at the final embedding after a given number of in- teraction refinements, two fully-connected layers predict an energy contribution from each atomic coefficient vector, such that their sum corresponds to the total molecular energy EM . Therefore, the DTNN architecture scales with the number of atoms in a molecule, fully capturing the extensive nature of the energy. All weights, biases as well as the atom type- specific descriptors were initialized randomly and trained us- ing stochastic gradient descent [28]. LEARNING MOLECULAR ENERGIES To demonstrate the versatility of the proposed DTNN, we train models with up to three interaction passes T = 3 for both compositional and configurational degrees of freedom in molecular systems. The DTNN accuracy saturates at T = 3, and leads to a strong correlation between atoms in molecules, as can be visualized by the complexity of the potential learned by the network (see Fig. 1E). For training, we employ chem- ically diverse datasets of equilibrium molecular structures, as well as molecular dynamics (MD) trajectories for small molecules [28]. We employ two subsets of the GDB-13 database [29, 30] referred to as GDB-7, including more than 7,000 molecules with up to 7 heavy (C, N, O, F) atoms, and GDB-9, consisting of 129,000 molecules with up to 9 heavy atoms [31]. In both cases, the learning task is to predict the molecular total energy calculated with density-functional the- ory (DFT). All GDB molecules are stable and synthetically accessible according to organic chemistry rules [30]. Molec- ular features such as functional groups or signatures include single, double and triple bonds; (hetero-) cycles, carboxy, cyanide, amide, amine, alcohol, epoxy, sulfide, ether, ester, chloride, aliphatic and aromatic groups. For each of the many possible stoichiometries, many constitutional isomers are con- sidered, each being represented only by a low-energy confor- mational isomer. As Table S2 demonstrates, DTNN achieves a mean absolute error (MAEs) of 1.0 kcal\/mol on both GDB datasets, training on 5.8k GDB-7 (80%) and 25k (20%) GDB-9 reference cal- culations, respectively [24]. Fig. 1C shows the performance on GDB-9 depending on the size of the molecule. We ob- serve that larger molecules have lower errors because of their abundance in the training data. The per-atom DTNN energy prediction and the fact that chemical interactions have a finite distance range means that the DTNN model will yield a con- stant error per atom upon increasing molecular size. To assess the effective range of chemical interactions we have imposed a distance cutoff to interatomic interactions of 3Å, yielding only a 0.1 kcal\/mol increase in the error. However, this dis- tance cutoff restricts only the direct interactions considered in the refinement steps. With multiple refinements, the effective cutoff increases by a factor of T due to indirect interactions over multiple atoms. Given large enough molecules, so that a reasonable distance cutoff can be chosen, scaling to larger molecules will require only to have well-represented local en- vironments. Along the same vein, we trained the network on a restricted subset of 5k molecules with more than 20 atoms. By adding smaller molecules to the training set, we are able to re- duce the test error from 2.1 kcal\/mol to less than 1.5 kcal\/mol (see inset in Fig. 1C). This result demonstrates that our model is able to transfer knowledge learned from small molecules to larger molecules with diverse functional groups. While only encompassing conformations of a single molecule, reproducing MD simulation trajectories poses a radically different challenge to predicting energies of purely 4 equilibrium structures. We learned potential energies for MD trajectories of benzene, toluene, malonaldehyde and salicylic acid, carried out at a rather high temperature of 500 K to achieve exhaustive exploration of the potential-energy sur- face of such small molecules. The neural network yields mean absolute errors of 0.05 kcal\/mol, 0.18 kcal\/mol, 0.17 kcal\/mol and 0.39 kcal\/mol for these molecules, respectively (see Table S2). Fig. 1D shows the excellent agreement be- tween the DFT and DTNN MD trajectory of toluene as well as the corresponding energy distributions. The DTNN errors are much smaller than the energy of thermal fluctuations at room temperature (∼0.6 kcal\/mol), meaning that DTNN potential- energy surfaces can be utilized to calculate accurate molecu- lar thermodynamic properties by virtue of Monte Carlo simu- lations. The ability of DTNN to accurately describe equilibrium structures within the GDB-9 database and MD trajectories of selected molecules of chemical relevance demonstrates the feasibility of developing a universal machine learning archi- tecture that can capture compositional as well as configura- tional degrees of freedom in the vast chemical space (see Ap- plications section for further analysis). While the employed architecture of the DTNN is universal, the learned coeffi- cients are different for GDB-9 and MD trajectories of single molecules. −110 −80 −50 −150 −115 −80 −140 −100 −60 −145 −105 −65 FIG. 2: Chemical potentials ΩMA (r) for A = {C,N,O,H} atoms for benzene, toluene, salicylic acid, and malondehyde. The isosurface was generated for ∑ i \u2016r − ri\u2016−2 = 3.8 Å−2 (the index i is used to sum over all atoms of the corresponding molecule). -859.9 -858.3 -857.8 -857.4 -857.4 -857.3 -856.9 -856.8 -856.8 -856.6 -845.1 -843.8 -842.1 -841.9 -841.9 -841.7 -841.7 -841.4 -841.2 -841.1 FIG. 3: Classification of molecular carbon ring stability. Shown are 20 molecules (10 most stable and 10 least stable) with respect to the energy of the carbon ring predicted by the DTNN model. FIG. 4: Isomer energies with chemical formula C7O2H10. DTNN trained on the GDB-9 database is able to acurately discriminate be- tween 6095 different isomers of C7O2H10, which exhibit a non-trivial spectrum of relative energies. APPLICATIONS Quantum-chemical insights Beyond predicting accurate energies, the true power of DTNN lies in its ability to provide novel quantum-chemical insights. In the context of DTNN, we define a local chem- ical potential ΩMA (r) as an energy of a certain atom type A, located at a position r in the molecule M. While the DTNN models the interatomic interactions, we only allow the atoms of the molecule act on the probe atom, while the probe does not influence the molecule [28]. The spatial and chemical sen- sitivity provided by our DTNN approach is shown in Fig. 1E for a variety of fundamental molecular building blocks. In this case, we employed hydrogen as a test charge, while the results 5 for ΩMC,N,O(r) are shown in Fig. 2. Despite being trained only on total energies of molecules, the DTNN approach clearly grasps fundamental chemical concepts such as bond satura- tion and different degrees of aromaticity. For example, the DTNN model predicts the C6O3H6 molecule to be \u201Cmore aro- matic\u201D than benzene or toluene (see Fig. 1E). Remarkably, it turns out that C6O3H6 does have higher ring stability than both benzene and toluene and DTNN predicts it to be the molecule with the most stable aromatic carbon ring among all molecules in the GDB-9 database (see Fig. 3). Further chemical effects learned by the DTNN model are shown in Fig. 2 that demonstrates the differences in the chemical poten- tial distribution of H, C, N, and O atoms in benzene, toluene, salicylic acid, and malonaldehyde. For example, the chemical potentials of different atoms over an aromatic ring are qualita- tively different for H, C, N, and O atoms \u2013 an evident fact for a trained chemist. However, the subtle chemical differences described by DTNN are accompanied by chemically accurate predictions \u2013 a challenging task for humans. Because DTNN provides atomic energies by construction, it allows us to classify molecules by the stability of differ- ent building blocks, for example aromatic rings or methyl groups. An example of such classification is shown in Fig. 3, where we plot the molecules with most stable and least sta- ble carbon aromatic rings in GDB-9. The distribution of atomic energies is shown in Fig. S4, while Fig. S5 lists the full stability ranking. The DTNN classification leads to in- teresting stability trends, notwithstanding the intrinsic non- uniqueness of atomic energy partitioning. However, unlike atomic projections employed in electronic-structure calcula- tions, the DTNN approach has a firm foundation in statis- tical learning theory. In quantum-chemical calculations, ev- ery molecule would correspond to a different partitioning de- pending on its self-consistent electron density. In contrast, the DTNN approach learns the partitioning on a large molecular dataset, generating a transferable and global \u201Cdressed atom\u201D representation of molecules in chemical space. Recalling that DTNN exhibits errors below 1 kcal\/mol, the classification shown in Fig. 3 can provide useful guidance for the chemi- cal discovery of molecules with desired properties. Analytical gradients of the DTNN model with respect to chemical com- position or ΩMA (r) could also aid in the exploration of chemical compound space [32]. Energy predictions for isomers: Towards mapping chemical space The quantitative accuracy achieved by DTNN and its size extensivity paves the way to the calculation of configurational and conformational energy differences \u2013 a long-standing chal- lenge for machine learning approaches [7, 12, 13, 33]. The reliability of DTNN for isomer energy predictions is demon- strated by the energy distribution in Fig. 4 for molecular iso- mers with C7O2H10 chemical formula (a total of 6095 isomers in the GDB-9 dataset). Training a common network for compositional and con- figurational degrees of freedoms requires a more complex model. Furthermore, it comes with technical challenges such as sampling and multiscale issues since the MD trajectories form clusters of small variation within the chemical com- pound space. As a proof of principle, we trained the DTNN to predict various MD trajectories of the C7O2H10 isomers. To this end, we calculated short MD trajectories of 5000 steps each for 113 randomly picked isomers as well as consistent total energies for all equilbrium structures. The training set is composed of the isomers in equilibrium as well as 50% of each MD trajectory. The remaining MD calculations are used for validation and testing. Despite the vastly increased com- plexity, our DTNN model achieves a mean absolute error of 1.7 kcal\/mol, providing a proof-of-principle demonstration of describing complex chemical spaces. DISCUSSION DTNNs provide an efficient way to represent chemical en- vironments allowing for chemically accurate predictions. To this end, an implicit, atom-centered basis is learned from ref- erence ab initio calculations. Employing this representation, atoms can be embedded in their chemical environment within a few refinement steps. Furthermore, DTNNs have the ad- vantage that the embedding is built recursively from pairwise distances. Therefore, all necessary invariances (translation, rotation, permutation) are guaranteed to be exploited by the model. In previous approaches, potential-energy surfaces were constructed by fitting many-body expansions with neural net- works [34\u201336]. However, these methods require a separate NN for each non-equivalent many-body term in the expansion. Since DTNN learns a common basis in which the atom in- teract, higher-order interactions can obtained more efficiently without separate treament. Approaches like SOAP [16, 17] or manually crafted atom- centered symmetry functions [18, 19, 37] are, like DTNN, based on representing chemical environments. All these ap- proaches have in common that size-extensivity regarding the number of atoms is achieved by predicting atomic energy con- tributions using a non-linear regression method (e.g., neural networks or kernel ridge regression). However, the previous approaches have a fixed set of basis functions describing the atomic environments. In contrast, DTNNs are able to adapt to the problem at hand in a data-driven fashion. Beyond the obvious advantage of not having to manually select symme- try functions and carefully tune hyper-parameters of the rep- resentation, this property of the DTNN makes it possible to gain insights by analyzing the learned representation. Obviously, more work is required to extend this predictive power for larger molecules, where the DTNN model will have to be combined with a reliable model for long-range inter- atomic (van der Waals) interactions. The intrinsic interpola- tion smoothness achieved by the DTNN model can also be 6 used to identify molecules with peculiar electronic structure. Fig. S6 shows a list of molecules with the largest DTNN errors compared to reference DFT calculations. It is noteworthy that most molecules in this figure are characterized by unconven- tional bonding and the electronic structure of these molecules has potential multi-reference character. The large prediction errors could stem from these molecules being not sufficiently represented by the training data. On the other hand, DTNN predictions might turn out to be closer to the correct answer due to its smooth interpolation in chemical space. Higher- level quantum-chemical calculations would be required to in- vestigate this interesting hypothesis in the future. OUTLOOK We have proposed and developed a deep tensor neural net- work that enables understanding of quantum-chemical many- body systems beyond properties contained in the training dataset. The DTNN model is scalable with molecular size, ef- ficient, and achieves uniform accuracy of 1 kcal\/mol through- out compositional and configuration space for molecules of intermediate size. The DTNN model leads to novel insights into chemical systems, a fact that we illustrated on the exam- ple of relative aromatic ring stability, local molecular chemi- cal potentials, relative isomer energies, and the identification of molecules with peculiar electronic structure. Many avenues remain for improving the DTNN model on multiple fronts. Among these we mention the extension of the model to increasingly larger molecules, predicting alltomic forces and frequencies, and non-extensive electronic and opti- cal properties. We propose the DTNN model as a versatile framework for understanding complex quantum-mechanical systems based on high-throughput electronic structure calcu- lations. ∗ Electronic address: klaus-robert.mueller@tu-berlin.de \u2020 Electronic address: alexandre.tkatchenko@uni.lu [1] B. Kang and G. Ceder, Nature 458, 190 (2009). [2] J. K. Nørskov, T. Bligaard, J. Rossmeisl, and C. H. Christensen, Nature Chem. 1, 37 (2009). [3] J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Sánchez-Carrera, A. Gold-Parker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik, J. Phys. Chem. Lett. 2, 2241 (2011). [4] E. O. Pyzer-Knapp, C. Suh, R. Gómez-Bombarelli, J. Aguilera- Iparraguirre, and A. Aspuru-Guzik, Annu. Rev. Mater. Res. 45, 195 (2015). [5] S. Curtarolo, G. L. Hart, M. B. Nardelli, N. Mingo, S. Sanvito, and O. Levy, Nature Mater. 12, 191 (2013). [6] J. C. Snyder, M. Rupp, K. Hansen, K.-R. Müller, and K. Burke, Phys. Rev. Lett. 108, 253002 (2012). [7] M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. Von Lilien- feld, Phys. Rev. Lett. 108, 058301 (2012). [8] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilien- feld, J. Chem. Theory Comput. 11, 2087 (2015). [9] C. M. Bishop, Pattern Recognition and Machine Learning (Springer, 2006). [10] L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl, and M. Scheffler, Phys. Rev. Lett. 114, 105503 (2015). [11] K. Schütt, H. Glawe, F. Brockherde, A. Sanna, K. Müller, and E. Gross, Phys. Rev. B 89, 205118 (2014). [12] G. Montavon, M. Rupp, V. Gobre, A. Vazquez-Mayagoitia, K. Hansen, A. Tkatchenko, K.-R. Müller, and O. A. von Lilien- feld, New J. Phys. 15, 095003 (2013). [13] K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp, M. Scheffler, O. A. Von Lilienfeld, A. Tkatchenko, and K.-R. Muller, J. Chem. Theory Comput. 9, 3404 (2013). [14] M. Hirn, N. Poilvert, and S. Mallat, arXiv preprint arXiv:1502.02077 (2015). [15] K. Hansen, F. Biegler, R. Ramakrishnan, W. Pronobis, O. A. von Lilienfeld, K.-R. Müller, and A. Tkatchenko, J. Phys. Chem. Lett. 6, 2326 (2015). [16] A. P. Bartók, R. Kondor, and G. Csányi, Phys. Rev. B 87, 184115 (2013). [17] A. P. Bartók, M. C. Payne, R. Kondor, and G. Csányi, Phys. Rev. Lett. 104, 136403 (2010). [18] J. Behler, J. Chem. Phys. 134, 074106 (2011). [19] J. Behler, Phys. Chem. Chem. Phys. 13, 17930 (2011). [20] Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436 (2015). [21] G. Montavon, M. L. Braun, and K.-R. Müller, Journ. Mach. Learn. Res. 12, 2563 (2011). [22] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, IEEE Trans. Neural Netw. 20, 61 (2009). [23] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, in NIPS, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (2015) pp. 2224\u20132232. [24] \u201CSupplementary text.\u201D . [25] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, in EMNLP, Vol. 1631 (2013) p. 1642. [26] I. Sutskever, J. Martens, and G. E. Hinton, in ICML (2011) pp. 1017\u20131024. [27] G. W. Taylor and G. E. Hinton, in ICML (2009) pp. 1025\u20131032. [28] \u201CMaterials and methods are available as supplementary materi- als.\u201D . [29] L. C. Blum and J.-L. Reymond, J. Am. Chem. Soc. 131, 8732 (2009). [30] J.-L. Reymond, Acc. Chem. Res. 48, 722 (2015). [31] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilien- feld, Sci. Data 1, 140022 (2014). [32] O. A. von Lilienfeld, Int. J. Quantum Chem. 113, 1676 (2013). [33] S. De, A. P. Bartók, G. Csányi, and M. Ceriotti, arXiv preprint arXiv:1601.04077 (2015). [34] M. Malshe, R. Narulkar, L. Raff, M. Hagan, S. Bukkapatnam, P. Agrawal, and R. Komanduri, J. Chem. Phys. 130, 184102 (2009). [35] S. Manzhos and T. Carrington Jr, J. Chem. Phys. 125, 084109 (2006). [36] S. Manzhos and T. Carrington Jr, J. Chem. Phys. 129, 224104 (2008). [37] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401 (2007). mailto:klaus-robert.mueller@tu-berlin.de mailto:alexandre.tkatchenko@uni.lu 7 Supplementary Materials Materials and Methods Data We employ two subsets of the GDB database [1], referred to in this paper as GDB-7 and GDB-9. GDB-7 contains 7211 molecules with up to 7 heavy atoms out of the elements C, N, O, S and Cl, saturated with hydrogen [2]. Similarly, GDB-9 includes 133,885 molecules with up to 9 heavy atoms out of C, O, N, F [3]. Both data sets include calculations of atomiza- tion energies employing density functional theory [4] with the PBE0 [5] and B3LYP [6\u201310] exchange-correlation potential, respectively. The molecular dynamics trajectories are calculated at a temperature of 500 K and resolution of 0.5fs using density functional theory with the PBE exchange-correlation potential [11]. The data sets for benzene, toluene, malonaldehyde and salicylic acid consist of 627k, 442k, 993k and 320k time steps, respectively. In the presented experiments, we predict the potential energy of the MD geometries. The deep tensor neural network model The molecular energies of the various data sets are predicted using a deep tensor neural network. The core idea is to represent atoms in the molecule as vectors depending on their type and to subsequently refine the representation by embedding the atoms in their neighborhood. This is done in a sequence of interaction passes where the atom representations influence each other in a pair-wise fashion. While each of these refinements depends only on the pair-wise atomic distances, multiple passes enable the architecture to also take angular information into account. Due to this decomposition of atomic interactions, an efficient representation of embedded atoms is learned following quantum chemical principles. In the following, we describe the deep tensor neural network step-by-step, including hyper-parameters used in our experi- ments. 1. Assign initial atomic descriptors We assign an initial coefficient vector to each atom i of the molecule according to its nuclear charge Z: c(0)i = cZi ∈ R B, (4) where B is the number of basis functions. All presented models use atomic descriptors with 30 coefficients. We initialize each coefficient randomly following cZ ∼ N(0, 1\/ √ B). 2. Gaussian feature expansion of the inter-atomic distances The inter-atomic distances di j are spread across many dimensions by a uniform grid of Gaussians d̂i j = [ exp ( − (di j − (µmin + k∆µ))2 2σ2 )] 0≤k≤µmax\/∆µ , (5) with ∆µ being the gap between two Gaussians of width σ. In our experiments, we set both to 0.2 Å. The center of the first Gaussian µmin was set to −1, while µmax was chosen depending on the range of distances in the data (10 Å for GDB-7 and benzene, 15 Å for toluene, malonaldehyde and salicylic acid and 20 Å for GDB-9). 3. Perform T interaction passes Each coefficient vector c(t)i , corresponding to atom i after t passes, is corrected by the interactions with the other atoms of the molecule: c(t+1)i = c (t) i + ∑ j,i vi j. (6) Here, we model the interaction v as follows: vi j = tanh ( W f c((Wc f c j + b f1 ) ◦ (Wd f d̂ij + b f2 )) ) , (7) where the circle (◦) represents the element-wise matrix product. The factor representation in the presented models employs 60 neurons. 8 4. Predict energy contributions Finally, we predict the energy contributions Ei from each atom i. Employing two fully-connected layers, for each atom a scaled energy contribution Êi is predicted: oi = tanh(Wout1 c(T )i + b out1 ) (8) Êi = Wout2 oi + bout2 (9) In our experiments, the hidden layer oi possesses 15 neurons. To obtain the final contributions, Êi is shifted to the mean Eµ and scaled by the standard deviation Estd of the energy per atom estimated on the training set. Ei = (Êi + Eµ)Estd (10) This procedure ensures a good starting point for the training. 5. Obtain the molecular energy E = ∑ i Ei The bias parameters as well as Wout2 are initially set to zero. All other weight matrices are initialized drawing from a uniform distribution according to [12]. The deep tensor neural networks have been trained for 3000 epochs minimizing the squared error, using stochastic gradient descent with 0.9 momentum and a constant learning rate [13]. The final results are taken from the models with the best validation error in early stopping. Computational cost of training and prediction All DTNN models were trained and executed on an NVIDIA Tesla K40 GPU. The computational cost of the employed models depends on the number of reference calculations, the number of interaction passes as well as the number of atoms per molecule. The training times for all models and data sets are shown in Table I, ranging from 6 hours for 5.768 reference calculations of GDB-7 with one interaction pass, to 162 hours for 100.000 reference calculations of the GDB-9 data set with 3 interaction passes. On the other hand, the prediction is instantaneous: all models predict examples from the employed data sets in less than 1 ms. Fig. 12 shows the scaling of the prediction time with the number of atoms and interaction layers. Even for a molecule with 100 atoms, a DTNN with 3 interaction layers requires less than 5 ms for a prediction. The prediction as well as the training steps scale linearly with the number of interaction passes and quadratically with the number of atoms, since the pairwise atomic distances are required for the interactions. For large molecules it is reasonable to introduce a distance cutoff (future work). In that case, the DTNN will also scale linearly with the number of atoms. Computing the local potentials of the DTNN Given a trained neural network as described in the previous section, one can extract the coefficients vectors c(t)i for each atom i and each interaction pass t for a molecule of interest. From each final representation c(T )i , the energy contribution Ei of the corresponding atom to the molecular energy can be obtained. Instead, we let the molecule act on a probe atom, described by its charge z and the pairwise distances d1, . . . , dn to the atoms of the molecule: c(t+1)probe = c (t) probe + n∑ j=1 v j, (11) with v j = tanh ( W f c((Wc f c j + b f1 ) ◦ (Wd f d̂ j + b f2 )) ) . While this is equivalent to how the coefficient vectors of the molecule are corrected, here, the molecule does not get to be influenced by the probe. Now, the energy of the probe atom is predicted as usual from the final representation c(T )probe. Interpreting this as a local potential Ω M A (r) generated by the molecule, we can use the neural network to visualize the learned interactions as illustrated in Fig. 5. The presented energy surfaces show the potential for different probe atoms plotted on an isosurface of ∑n i=1 d −2 i . We used Mayavi [14] for the visualization of the surfaces. 9 Computing an alchemical path with the DTNN The alchemical paths in Fig. 11 were generated by gradually moving the atoms as well as interpolating between the initial coefficient vectors for changes of atom types. Given two nuclear charges A, B, the coefficient vector for any charge Zi = αiA + (1 − α)B with 0 ≤ α ≤ 1 is given by cZi = αicA + (1 − αi)cB. (12) Similarly, in order to add or remove atoms, we introduce fading factors β1, . . . , βn ∈ [0, 1] for each atom. This way, influences on other atoms c(t+1)i = c (t) i + ∑ j,i β jv(c(t)j ,Di j) (13) as well as energy contributions to the molecular energy E = ∑n i=1 βiEi can be faded out. 10 Supplementary Text Discussion of the results Table II shows the mean absolute (MAE) and root mean squared errors (RMSE) as well as standard errors over five randomly drawn training sets. For GDB-9 and the MD data sets, 1k reference calculations were used as validation set for early stopping, while the remaining data was used for testing. In case of GDB-7, validation and test set contained 10% of the reference calculations each. With respect to the mean absolute error, chemical accuracy can be achieved on all employed data sets using models with two or three interactions passes. Figs. 6 and 7 show the dependence of the performance on the number of training examples for the benzene MD data set and GDB-9, respectively. In both learning curves (A), an increase from 1.000 to 10.000 training examples reduces the error drastically while another increase to 100.000 examples yields comparatively small improvement. The error distributions (B) show that models with two and three interaction passes trained on at least 25.000 GDB-9 references calculations predict 95% of the unknown molecules with an error of 3.0 kcal\/mol or lower. Correspondingly, the same models trained on 25.000 or more MD reference calculations of benzene predict 95% of the unknown benzene configurations with a maximum error lower than 1.3 kcal\/mol. Beyond a certain number of reference calculations, the models with one interaction pass perform significantly worse in all the- ses respects. Thus, multiple interaction passes indeed enrich the learned feature representation as demonstrated by the increased predictability of previously unseen molecules. Relations to other deep neural networks Deep learning has lead to major advances in computer vision, language processing, speech recognition and other applica- tions [15]. In our model, we embed the atom type in a vector space RB. This idea is inspired by word embeddings (word2vec) employed in natural language processing [16]. In order to model inter-atomic effects, we need to represent the influence of an atom represented by c j at the distance di j. To account for multiple regimes of atomic distances as well as different dimen- sionality of the two inputs, we apply the Gaussian feature mapping described above. Similar approaches have been applied to the entries of the Coulomb matrix for the prediction of molecular properties before [2]. A natural way to connect distance and atom representation is a tensor layer as used in text generation [17], reasoning [18] or sentiment analysis [19]. For an efficient computation as well as regularization, we employ a factorized tensor layer, corresponding to a low-rank approximation of the tensor product [20]. Convolutional neural networks have been applied to images, speech and text with great success due to their ability to capture local structure [21\u201326]. In a convolution layer, local filters are applied to local environments, e.g., image patches, extracting features relevant to the classification task. Similarly, local correlations of atoms may be exploited in a chemistry setting. The atom interaction in our model can indeed be regarded as a non-linear generalization of a convolution. In contrast to images however, atoms of molecules are not arranged on a grid. Therefore, the convolution kernels need to be continuous. We define a function Ct : R3 → RB yielding cti = Ct(ri) at the atom positions. Now, we can rewrite the interactions as Ct+1(ri)k = Ct(ri)k + ∑ j,i h( f (r j)kg(\u2016r j − ri\u2016)k), (14) with f (r j) = Wc f Ct(r j) + b f1 , (15) g(Di j) = Wd f d̂ij + b f2 , (16) h(x) = tanh(W f cx). (17) For h being the identity, the sum is equivalent to a discrete convolution. 11 Figures FIG. 5: Illustration of how the surface plots are obtained from a trained network as shown in Fig. 1. The deep network can be interpreted as representing a local potential ΩMA (r) created by the atoms of the molecule. Putting a probe atom A with nuclear charge z at a position r described by the distances to the atoms of the molecule d1, . . . , dn yields an energy Eprobe. 12 103 104 105 0 1 2 3 4 5 6 7 8 9 10000 25000 50000 100000 0 1 2 3 4 5 6 FIG. 6: Chemical compound space. Errors depending on the size of the training set for models with T = 1, 2, 3 interaction passes trained on GDB-9. (A) Mean absolute error of neural networks depending on the number of training examples. Error bars correspond to standard errors over five repetitions. For more than 5k examples, the error bars vanish due to standard errors below 0.05 kcal\/mol. (B) Error distribution for models trained on 10k, 25k, 50k and 100k training examples. The box spans between the 25% and 75% quantiles, while the whiskers mark the 5% and 95% quantiles. 13 103 104 105 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 10000 25000 50000 10000010000 25000 50000 100000 0.0 0.1 0.2 0.3 0.4 FIG. 7: Molecular dynamics. Errors depending on the size of the training set for models with T = 1, 2, 3 interaction passes trained on the Benzene data set. (A) Mean absolute error of neural networks depending on the number of training examples. Error bars correspond to standard errors over five repetitions. For more than 10k examples, the error bars vanish due to standard errors below 0.01 kcal\/mol. (B) Error distribution for models trained on 10k, 25k, 50k and 100k training examples. The box spans between the 25% and 75% quantiles, while the whiskers mark the 5% and 95% quantiles. 14 90 80 70 60 0% 10% 20% 30% hydrogen 140 130 120 110 0% 10% 20% 30% carbon 140 120 100 80 0% 10% 20% 30% nitrogen 150 120 90 60 0% 10% 20% 30% oxygen FIG. 8: Distribution of atomic energy contributions Ei in the GDB-9 data set. The energy contributions were predicted using the GDB-9 model with two interaction passes trained on 50k reference calculations. 15 -859.9 -858.3 -857.8 -857.4 -857.4 -857.3 -856.9 -856.8 -856.8 -856.6 -856.6 -856.3 -856.2 -856.2 -856.2 -856.0 -856.0 -855.8 -855.7 -855.7 -855.7 -855.6 -855.5 -855.4 -855.4 -855.4 -855.4 -855.3 -855.3 -855.3 -855.3 -855.2 -855.0 -854.9 -854.9 -854.9 -854.8 -854.8 -854.8 -854.7 -854.7 -854.7 -854.6 -854.6 -854.6 -854.5 -854.5 -854.5 -854.5 -854.4 -854.3 -854.3 -854.3 -854.1 -854.1 -854.1 -854.1 -854.0 -854.0 -854.0 -854.0 -854.0 -854.0 -853.9 -853.9 -853.9 -853.8 -853.8 -853.8 -853.8 -853.8 -853.7 -853.7 -853.6 -853.6 -853.5 -853.5 -853.5 -853.5 -853.5 -853.5 -853.4 -853.4 -853.4 -853.3 -853.3 -853.3 -853.2 -853.2 -853.1 -853.1 -853.0 -853.0 -852.9 -852.9 -852.9 -852.8 -852.8 -852.8 -852.8 -852.8 -852.7 -852.6 -852.6 -852.6 -852.6 -852.5 -852.5 -852.5 -852.5 -852.4 -852.4 -852.4 -852.4 -852.4 -852.4 -852.3 -852.3 -852.3 -852.2 -852.1 -852.1 -852.1 -852.0 -852.0 -852.0 -852.0 -852.0 -851.9 -851.9 -851.9 -851.9 -851.9 -851.8 -851.8 -851.8 -851.8 -851.7 -851.7 -851.7 -851.6 -851.6 -851.5 -851.5 -851.5 -851.5 -851.5 -851.5 -851.4 -851.4 -851.3 -851.3 -851.3 -851.3 -851.3 -851.2 -851.2 -851.2 -851.2 -851.2 -851.1 -851.0 -851.0 -851.0 -851.0 -850.9 -850.9 -850.8 -850.8 -850.8 -850.8 -850.7 -850.7 -850.7 -850.7 -850.7 -850.7 -850.7 -850.6 -850.6 -850.6 -850.6 -850.6 -850.6 -850.5 -850.5 -850.5 -850.5 -850.4 -850.4 -850.3 -850.3 -850.3 -850.3 -850.3 -850.2 -850.2 -850.2 -850.2 -850.2 -850.1 -850.1 -850.1 -850.1 -850.0 -849.9 -849.9 -849.8 -849.8 -849.8 -849.8 -849.7 -849.7 -849.6 -849.6 -849.6 -849.5 -849.3 -849.3 -849.2 -849.1 -849.1 -849.0 -849.0 -848.9 -848.9 -848.8 -848.8 -848.7 -848.6 -848.6 -848.6 -848.6 -848.5 -848.5 -848.5 -848.3 -848.3 -848.1 -848.1 -848.1 -848.0 -848.0 -848.0 -848.0 -847.9 -847.9 -847.9 -847.8 -847.7 -847.6 -847.6 -847.5 -847.5 -847.5 -847.5 -847.4 -847.4 -847.1 -847.0 -846.9 -846.7 -846.7 -846.7 -846.7 -846.6 -846.5 -846.5 -846.4 -846.4 -846.4 -846.4 -846.2 -846.0 -846.0 -845.9 -845.8 -845.8 -845.6 -845.5 -845.1 -843.8 -842.1 -841.9 -841.9 -841.7 -841.7 -841.4 -841.2 -841.1 FIG. 9: List of 6-membered carbon rings ordered by the sum of energy contributions of the ring atoms. The energy contributions were predicted using the GDB-9 model with three interaction passes trained on 50k reference calculations. Energy contributions are given in kcal\/mol. 16 FIG. 10: Top-10 largest prediction errors on the GDB-9 model with two interaction passes trained on 50k reference calculations. 17 FIG. 11: An alchemical path of the DTNN trained on 50k GDB-9 reference calculations with T = 2. The DTNN model is able to smoothly create, remove and move atoms as well as continuously change their element-specific characteristics. A path leading from benzene to s-triazine was computed by only changing, removing and changing types of atoms (blue). In the second path (orange), atoms were also moved to the new equilibrium positions. The black dots mark the energy of DFT reference calculations. 18 FIG. 12: Prediction time needed for a molecule depending on the number of atoms and number of interaction passes T of the employed DTNN. All predictions were computed on an NVIDIA Tesla K40 GPU. 19 Tables Data set # training examples T = 1 T = 2 T = 3 GDB-7 5768 6 7 8 GDB-9 25k 28 35 42 50k 55 71 82 100k 110 139 162 Benzene 25k 21 27 32 50k 44 53 61 100k 84 104 121 Toluene 25k 24 27 32 50k 45 55 64 100k 88 108 127 Malonaldehyde 25k 21 25 29 50k 41 52 59 100k 85 106 117 Salicylic acid 25k 22 31 32 50k 44 54 65 100k 91 109 125 TABLE I: Training duration for the presented neural networks with up to three interaction passes (T = 1, 2, 3) in hours. All models were trained with stochastic gradient descent with momentum for 3.000 epochs on an NVIDIA Tesla K40 GPU. 20 Data set # training examples T=1 T=2 T=3 MAE RMSE MAE RMSE MAE RMSE GDB-7 5768 1.28 ± 0.04 1.99 ± 0.14 1.04 ± 0.02 1.43 ± 0.02 1.04 ± 0.01 1.45 ± 0.01 GDB-9 25k 1.61 ± 0.02 2.31 ± 0.02 1.09 ± 0.01 1.62 ± 0.02 1.04 ± 0.02 1.53 ± 0.02 50k 1.49 ± 0.02 2.14 ± 0.03 0.96 ± 0.01 1.37 ± 0.03 0.94 ± 0.01 1.37 ± 0.01 100k 1.54 ± 0.03 2.17 ± 0.04 0.93 ± 0.02 1.33 ± 0.03 0.84 ± 0.02 1.21 ± 0.02 Benzene 25k 0.07 ± 0.00 0.10 ± 0.00 0.05 ± 0.00 0.06 ± 0.00 0.04 ± 0.00 0.06 ± 0.00 50k 0.06 ± 0.00 0.08 ± 0.00 0.04 ± 0.00 0.05 ± 0.00 0.04 ± 0.00 0.05 ± 0.00 100k 0.07 ± 0.00 0.10 ± 0.00 0.05 ± 0.00 0.06 ± 0.00 0.05 ± 0.00 0.06 ± 0.00 Toluene 25k 0.48 ± 0.01 0.63 ± 0.01 0.20 ± 0.00 0.28 ± 0.00 0.23 ± 0.00 0.31 ± 0.01 50k 0.44 ± 0.00 0.59 ± 0.01 0.18 ± 0.00 0.24 ± 0.00 0.18 ± 0.00 0.24 ± 0.00 100k 0.42 ± 0.01 0.56 ± 0.01 0.16 ± 0.00 0.21 ± 0.00 0.17 ± 0.00 0.22 ± 0.00 Malonaldehyde 25k 0.54 ± 0.00 0.74 ± 0.00 0.23 ± 0.00 0.34 ± 0.00 0.23 ± 0.00 0.33 ± 0.00 50k 0.49 ± 0.01 0.68 ± 0.01 0.20 ± 0.00 0.28 ± 0.00 0.19 ± 0.00 0.27 ± 0.00 100k 0.51 ± 0.01 0.70 ± 0.01 0.18 ± 0.00 0.25 ± 0.00 0.17 ± 0.00 0.24 ± 0.00 Salicylic acid 25k 0.80 ± 0.02 1.05 ± 0.03 0.54 ± 0.02 0.72 ± 0.03 0.79 ± 0.02 1.03 ± 0.03 50k 0.73 ± 0.01 0.94 ± 0.01 0.41 ± 0.00 0.54 ± 0.00 0.50 ± 0.01 0.65 ± 0.01 100k 0.67 ± 0.01 0.88 ± 0.01 0.39 ± 0.01 0.51 ± 0.01 0.42 ± 0.01 0.54 ± 0.01 TABLE II: Errors of neural networks with up to 3 interaction passes for various data sets and numbers of reference calculations used in training in kcal mol−1. Mean absolute errors (MAE), root mean squared errors (RMSE) as well as respective standard errors of the mean are printed. Additionally, the maximum error over all folds is given. Best results are printed in bold. 21 ∗ Electronic address: klaus-robert.mueller@tu-berlin.de \u2020 Electronic address: alexandre.tkatchenko@uni.lu [1] L. C. Blum and J.-L. Reymond, J. Am. Chem. Soc. 131, 8732 (2009). [2] G. Montavon, M. Rupp, V. Gobre, A. Vazquez-Mayagoitia, K. Hansen, A. Tkatchenko, K.-R. Müller, and O. A. von Lilienfeld, New J. Phys. 15, 095003 (2013). [3] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, Sci. Data 1, 140022 (2014). [4] P. Hohenberg and W. Kohn, Phys. Rev. 136, B864 (1964). [5] J. P. Perdew, M. Ernzerhof, and K. Burke, J. Chem. Phys. 105, 9982 (1996). [6] A. D. Becke, Phys. Rev. A 38, 3098 (1988). [7] C. Lee, W. Yang, and R. G. Parr, Phys. Rev. B 37, 785 (1988). [8] S. H. Vosko, L. Wilk, and M. Nusair, Can. J. Phys. 58, 1200 (1980). [9] P. Stephens, F. Devlin, C. Chabalowski, and M. J. Frisch, J. Phys. Chem. 98, 11623 (1994). [10] A. Becke, J. Chem. Phys 98, 5648 (1993). [11] J. P. Perdew, K. Burke, and M. Ernzerhof, Phys. Rev. Lett. 77, 3865 (1996). [12] X. Glorot and Y. Bengio, in AISTATS (2010) pp. 249\u2013256. [13] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, in Neural networks: Tricks of the trade (Springer, 2012) pp. 9\u201348. [14] P. Ramachandran and G. Varoquaux, Comput. Sci. Eng. 13, 40 (2011). [15] Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436 (2015). [16] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, in NIPS (2013) pp. 3111\u20133119. [17] I. Sutskever, J. Martens, and G. E. Hinton, in ICML (2011) pp. 1017\u20131024. [18] R. Socher, D. Chen, C. D. Manning, and A. Ng, in NIPS (2013) pp. 926\u2013934. [19] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, in EMNLP, Vol. 1631 (2013) p. 1642. [20] G. W. Taylor and G. E. Hinton, in ICML (2009) pp. 1025\u20131032. [21] D. Ciresan, U. Meier, and J. Schmidhuber, in CVPR (2012) pp. 3642\u20133649. [22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, in NIPS, edited by F. Pereira, C. Burges, L. Bottou, and K. Weinberger (2012) pp. 1097\u20131105. [23] Y. LeCun and Y. Bengio, The handbook of brain theory and neural networks 3361, 1995 (1995). [24] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., IEEE Signal Process. Mag. 29, 82 (2012). [25] T. N. Sainath, B. Kingsbury, G. Saon, H. Soltau, A.-r. Mohamed, G. Dahl, and B. Ramabhadran, Neural Networks 64, 39 (2015). [26] R. Collobert and J. Weston, in ICML (2008) pp. 160\u2013167. mailto:klaus-robert.mueller@tu-berlin.de mailto:alexandre.tkatchenko@uni.lu Molecular Deep Tensor Neural Networks Learning molecular energies Applications Quantum-chemical insights Energy predictions for isomers: Towards mapping chemical space Discussion Outlook Acknowledgments References Materials and Methods Data The deep tensor neural network model Computational cost of training and prediction Computing the local potentials of the DTNN Computing an alchemical path with the DTNN Supplementary Text Discussion of the results Relations to other deep neural networks Figures Tables References ","flair":"null\tnull"}
{"author":"hardmaru","created":"Mon Oct 10 14:40:33 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.02306 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm Authors: Vina Ayumi, L.M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy (Submitted on 7 Oct 2016) Abstract: Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\\%). On the CIFAR10 dataset, currently, state of the art is 96.53\\% using fractional pooling, while this proposed method achieves 99.14\\%. Comments: Accepted to be published at IEEE ICACSIS 2016. arXiv admin note: text overlap with arXiv:1610.01925 Subjects: Computer Vision and Pattern Recognition (cs.CV) MSC classes: 68Txx ACM classes: I.2.10 Cite as: arXiv:1610.02306 [cs.CV]   (or arXiv:1610.02306v1 [cs.CV] for this version) Submission history From: Mohamad Ivan Fanany [view email] [v1] Fri, 7 Oct 2016 14:39:50 GMT (424kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Guanoco","created":"Thu Nov 17 02:20:39 EST 2016","text":"Hey ML,\n\nI started coming here around 4 months ago (on and off) and I was wondering who are you?\n\nAt first I thought there were only ML enthusiasts here until I got replies from actual faculty. Also talk about people meeting and NIPS and such...\n\nSo the question is... Who are you or better yet what is your motive for being here?\n\nI can say I work in research in the private sector, but my work as very little to do with ML. I am mostly here to see trends and also try to understand new algorithms which might be relevant in the near future.","flair":"one\tDiscussion"}
{"author":"FusionGaming","created":"Sun Oct 09 16:04:52 EDT 2016","text":" Perceptual Losses for Real-Time Style Transfer and Super-Resolution Justin Johnson, Alexandre Alahi, and Li Fei-Fei Department of Computer Science, Stanford University {jcjohns, alahi, feifeili}@cs.stanford.edu Abstract. We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks us- ing a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features ex- tracted from pretrained networks. We combine the benefits of both ap- proaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Com- pared to the optimization-based method, our network gives similar quali- tative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results. Keywords: Style transfer, super-resolution, deep learning 1 Introduction Many classic problems can be framed as image transformation tasks, where a system receives some input image and transforms it into an output image. Exam- ples from image processing include denoising, super-resolution, and colorization, where the input is a degraded image (noisy, low-resolution, or grayscale) and the output is a high-quality color image. Examples from computer vision include se- mantic segmentation and depth estimation, where the input is a color image and the output image encodes semantic or geometric information about the scene. One approach for solving image transformation tasks is to train a feed- forward convolutional neural network in a supervised manner, using a per-pixel loss function to measure the difference between output and ground-truth images. This approach has been used for example by Dong et al. for super-resolution [1], by Cheng et al. for colorization [2, 3], by Long et al. for segmentation [4], and by Eigen et al. for depth and surface normal prediction [5, 6]. Such approaches are efficient at test-time, requiring only a forward pass through the trained network. However, the per-pixel losses used by these methods do not capture perceptual differences between output and ground-truth images. For example, consider two 2 Johnson, Alahi, and Fei-Fei Style Content Gatys et al. [11] Ours S ty le T ra n sf er S u p er - R es o lu ti o n Ground Truth Bicubic SRCNN [13] Perceptual loss Fig. 1. Example results for style transfer (top) and ×4 super-resolution (bottom). For style transfer, we achieve similar results as Gatys et al. [11] but are three orders of magnitude faster. For super-resolution our method trained with a perceptual loss is able to better reconstruct fine details compared to methods trained with per-pixel loss. identical images offset from each other by one pixel; despite their perceptual similarity they would be very different as measured by per-pixel losses. In parallel, recent work has shown that high-quality images can be generated using perceptual loss functions based not on differences between pixels but in- stead on differences between high-level image feature representations extracted from pretrained convolutional neural networks. Images are generated by mini- mizing a loss function. This strategy has been applied to feature inversion [7] by Mahendran et al., to feature visualization by Simonyan et al. [8] and Yosinski et al. [9], and to texture synthesis and style transfer by Gatys et al. [10\u201312]. These approaches produce high-quality images, but are slow since inference requires solving an optimization problem. In this paper we combine the benefits of these two approaches. We train feed- forward transformation networks for image transformation tasks, but rather than using per-pixel loss functions depending only on low-level pixel information, we train our networks using perceptual loss functions that depend on high-level features from a pretrained loss network. During training, perceptual losses mea- sure image similarities more robustly than per-pixel losses, and at test-time the transformation networks run in real-time. We experiment on two tasks: style transfer and single-image super-resolution. Both are inherently ill-posed; for style transfer there is no single correct output, and for super-resolution there are many high-resolution images that could have generated the same low-resolution input. Success in either task requires semantic reasoning about the input image. For style transfer the output must be semanti- cally similar to the input despite drastic changes in color and texture; for super- resolution fine details must be inferred from visually ambiguous low-resolution inputs. In principle a high-capacity neural network trained for either task could implicitly learn to reason about the relevant semantics; however, in practice we Perceptual Losses for Real-Time Style Transfer and Super-Resolution 3 need not learn from scratch: the use of perceptual loss functions allows the trans- fer of semantic knowledge from the loss network to the transformation network. For style transfer our feed-forward networks are trained to solve the opti- mization problem from [11]; our results are similar to [11] both qualitatively and as measured by objective function value, but are three orders of magnitude faster to generate. For super-resolution we show that replacing the per-pixel loss with a perceptual loss gives visually pleasing results for ×4 and ×8 super-resolution. 2 Related Work Feed-forward image transformation. In recent years, a wide variety of image transformation tasks have been trained with per-pixel loss functions. Semantic segmentation methods [4, 6, 14\u201317] produce dense scene labels by running networks in a fully-convolutional manner over input images, training with a per-pixel classification loss. Recent methods for depth [6, 5, 18] and sur- face normal estimation [6, 19] are similar, transforming color input images into geometrically meaningful output images using a feed-forward convolutional net- work trained with per-pixel regression [5, 6] or classification [19] losses. Some methods move beyond per-pixel losses by penalizing image gradients [6], fram- ing CRF inference as a recurrent layer trained jointly with the rest of the network [17], or using a CRF loss layer [18] to enforce local consistency in the output. The architecture of our transformation networks are inspired by [4] and [16], which use in-network downsampling to reduce the spatial extent of feature maps followed by in-network upsampling to produce the final output image. Perceptual optimization. A number of recent papers have used optimiza- tion to generate images where the objective is perceptual, depending on high- level features extracted from a convolutional network. Images can be generated to maximize class prediction scores [8, 9] or individual features [9] in order to understand the functions encoded in trained networks. Similar optimization tech- niques can also be used to generate high-confidence fooling images [20, 21]. Mahendran and Vedaldi [7] invert features from convolutional networks by minimizing a feature reconstruction loss in order to understand the image in- formation retained by different network layers; similar methods had previously been used to invert local binary descriptors [22, 23] and HOG features [24]. The work of Dosovitskiy and Brox [25] is particularly relevant to ours, as they train a feed-forward neural network to invert convolutional features, quickly approximating a solution to the optimization problem posed by [7]. However, their feed-forward network is trained with a per-pixel reconstruction loss, while our networks directly optimize the feature reconstruction loss of [7]. Style Transfer. Gatys et al. [11] perform artistic style transfer, combin- ing the content of one image with the style of another by jointly minimizing the feature reconstruction loss of [7] and a style reconstruction loss also based on features extracted from a pretrained convolutional network; a similar method had previously been used for texture synthesis [10]. Their method produces high- quality results, but is computationally expensive since each step of the optimiza- 4 Johnson, Alahi, and Fei-Fei Input Image Image Transform Net Style Target Content Target Loss Network (VGG-16) Fig. 2. System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process. tion problem requires a forward and backward pass through the pretrained net- work. To overcome this computational burden, we train a feed-forward network to quickly approximate solutions to their optimization problem. Concurrent with our work, [26, 27] also propose feed-forward approaches for fast style transfer. Image super-resolution. Image super-resolution is a classic problem for which a variety of techniques have been developed. Yang et al. [28] provide an ex- haustive evaluation of the prevailing techniques prior to the widespread adoption of convolutional neural networks. They group super-resolution techniques into prediction-based methods (bilinear, bicubic, Lanczos, [29]), edge-based meth- ods [30, 31], statistical methods [32\u201334], patch-based methods [30, 35\u201341], and sparse dictionary methods [42, 43]. Recently [1] achieved excellent performance on single-image super-resolution using a three-layer convolutional neural network with a per-pixel Euclidean loss. Other recent methods include [44\u201346]. 3 Method As shown in Figure 2, our system consists of two components: an image trans- formation network fW and a loss network φ that is used to define several loss functions `1, . . . , `k. The image transformation network is a deep residual convo- lutional neural network parameterized by weights W ; it transforms input images x into output images ŷ via the mapping ŷ = fW (x). Each loss function computes a scalar value `i(ŷ, yi) measuring the difference between the output image ŷ and a target image yi. The image transformation network is trained using stochastic gradient descent to minimize a weighted combination of loss functions: W ∗ = arg min W Ex,{yi} [∑ i=1 λi`i(fW (x), yi) ] (1) Perceptual Losses for Real-Time Style Transfer and Super-Resolution 5 To address the shortcomings of per-pixel losses and allow our loss functions to better measure perceptual and semantic differences between images, we draw inspiration from recent work that generates images via optimization [7\u201311]. The key insight of these methods is that convolutional neural networks pretrained for image classification have already learned to encode the perceptual and semantic information we would like to measure in our loss functions. We therefore make use of a network φ pretrained for image classification as a fixed loss network in order to define our loss functions. Our deep convolutional transformation network is thus trained using loss functions that are also deep convolutional networks. We use the loss network φ to define a feature reconstruction loss `φfeat and style reconstruction loss `φstyle that measure differences in content and style be- tween images. For each input image x we have a content target yc and a style target ys. For style transfer the content target yc is the input image x and the output image ŷ should combine the content of x = yc with the style of ys; we train one network per style target. For super-resolution the input x is a low-resolution input, the content target yc is the ground-truth high-resolution image, and style reconstruction loss is not used; we train one network per super-resolution factor. 3.1 Image Transformation Networks Our image transformation networks roughly follow the architectural guidelines set forth by [47]. We eschew pooling layers, instead using strided and fractionally strided convolutions for in-network downsampling and upsampling. Our network body comprises five residual blocks [48] using the architecture of [49]. All non- residual convolutional layers are followed by batch normalization [50] and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that the output has pixels in the range [0, 255]. The first and last layers use 9×9 kernels; all other convolutional layers use 3×3 kernels. The exact architectures of our networks can be found in the supplementary material1. Inputs and Outputs. For style transfer the input and output are color images of shape 3×256×256. For super-resolution with upsampling factor f , the output is a high-resolution patch of shape 3×288×288 and the input is a low-resolution patch of shape 3× 288\/f × 288\/f . Since the image transformation networks are fully-convolutional, at test-time they can be applied to images of any resolution. Downsampling and Upsampling. For super-resolution with an upsampling factor of f , we use several residual blocks followed by log2 f convolutional layers with stride 1\/2. This is different from [1] who use bicubic interpolation to up- sample the low-resolution input before passing it to the network. Rather than relying on a fixed upsampling function, fractionally-strided convolution allows the upsampling function to be learned jointly with the rest of the network. For style transfer our networks use two stride-2 convolutions to downsample the input followed by several residual blocks and then two convolutional layers with stride 1\/2 to upsample. Although the input and output have the same size, there are several benefits to networks that downsample and then upsample. 1 Available at the first author\u2019s website. 6 Johnson, Alahi, and Fei-Fei y relu2_2 relu3_3 relu4_3 relu5_1 relu5_3 Fig. 3. Similar to [7], we use optimization to find an image ŷ that minimizes the feature reconstruction loss `φ,jfeat(ŷ, y) for several layers j from the pretrained VGG-16 loss network φ. As we reconstruct from higher layers, image content and overall spatial structure are preserved, but color, texture, and exact shape are not. The first is computational. With a naive implementation, a 3×3 convolution with C filters on an input of size C ×H ×W requires 9HWC2 multiply-adds, which is the same cost as a 3 × 3 convolution with DC filters on an input of shape DC × H\/D ×W\/D. After downsampling, we can therefore use a larger network for the same computational cost. The second benefit has to do with effective receptive field sizes. High-quality style transfer requires changing large parts of the image in a coherent way; therefore it is advantageous for each pixel in the output to have a large effective receptive field in the input. Without downsampling, each additional 3×3 convo- lution increases the effective receptive field size by 2. After downsampling by a factor of D, each 3× 3 convolution instead increases effective receptive field size by 2D, giving larger effective receptive fields with the same number of layers. Residual Connections. He et al. [48] use residual connections to train very deep networks for image classification. They argue that residual connections make the identity function easier to learn; this is an appealing property for image transformation networks, since in most cases the output image should share structure with the input image. The body of our network thus consists of several residual blocks, each of which contains two 3 × 3 convolutional layers. We use the residual block design of [49], shown in the supplementary material. 3.2 Perceptual Loss Functions We define two perceptual loss functions that measure high-level perceptual and semantic differences between images. They make use of a loss network φ pre- trained for image classification, meaning that these perceptual loss functions are themselves deep convolutional neural networks. In all our experiments, the loss network φ is the 16-layer VGG network [51] pretrained on ImageNet [52]. Feature Reconstruction Loss. Rather than encouraging the pixels of the output image ŷ = fW (x) to exactly match the pixels of the target image y, we instead encourage them to have similar feature representations as computed by the loss network φ. Let φj(x) be the activations of the jth layer of the network φ when processing the image x; if j is a convolutional layer then φj(x) will be Perceptual Losses for Real-Time Style Transfer and Super-Resolution 7 y relu1_2 relu2_2 relu3_3 relu4_3 Fig. 4. Similar to [11], we use optimization to find an image ŷ that minimizes the style reconstruction loss `φ,jstyle(ŷ, y) for several layers j from the pretrained VGG-16 loss network φ. The images ŷ preserve stylistic features but not spatial structure. a feature map of shape Cj × Hj × Wj . The feature reconstruction loss is the (squared, normalized) Euclidean distance between feature representations: `φ,jfeat(ŷ, y) = 1 CjHjWj \u2016φj(ŷ)− φj(y)\u201622 (2) As demonstrated in [7] and reproduced in Figure 3, finding an image ŷ that minimizes the feature reconstruction loss for early layers tends to produce images that are visually indistinguishable from y. As we reconstruct from higher layers, image content and overall spatial structure are preserved but color, texture, and exact shape are not. Using a feature reconstruction loss for training our image transformation networks encourages the output image ŷ to be perceptually similar to the target image y, but does not force them to match exactly. Style Reconstruction Loss. The feature reconstruction loss penalizes the out- put image ŷ when it deviates in content from the target y. We also wish to penalize differences in style: colors, textures, common patterns, etc. To achieve this effect, Gatys et al. [10, 11] propose the following style reconstruction loss. As above, let φj(x) be the activations at the jth layer of the network φ for the input x, which is a feature map of shape Cj × Hj ×Wj . Define the Gram matrix Gφj (x) to be the Cj × Cj matrix whose elements are given by Gφj (x)c,c\u2032 = 1 CjHjWj Hj∑ h=1 Wj∑ w=1 φj(x)h,w,cφj(x)h,w,c\u2032 . (3) If we interpret φj(x) as giving Cj-dimensional features for each point on a Hj ×Wj grid, then Gφj (x) is proportional to the uncentered covariance of the Cj-dimensional features, treating each grid location as an independent sample. It thus captures information about which features tend to activate together. The Gram matrix can be computed efficiently by reshaping φj(x) into a matrix ψ of shape Cj ×HjWj ; then Gφj (x) = ψψT \/CjHjWj . 8 Johnson, Alahi, and Fei-Fei The style reconstruction loss is then the squared Frobenius norm of the dif- ference between the Gram matrices of the output and target images: `φ,jstyle(ŷ, y) = \u2016G φ j (ŷ)−G φ j (y)\u2016 2 F . (4) The style reconstruction loss is well-defined even when ŷ and y have different sizes, since their Gram matrices will both have the same shape. As demonstrated in [11] and reproduced in Figure 5, generating an image ŷ that minimizes the style reconstruction loss preserves stylistic features from the target image, but does not preserve its spatial structure. Reconstructing from higher layers transfers larger-scale structure from the target image. To perform style reconstruction from a set of layers J rather than a single layer j, we define `φ,Jstyle(ŷ, y) to be the sum of losses for each layer j ∈ J . 3.3 Simple Loss Functions In addition to the perceptual losses defined above, we also define two simple loss functions that depend only on low-level pixel information. Pixel Loss. The pixel loss is the (normalized) Euclidean distance between the output image ŷ and the target y. If both have shape C ×H ×W , then the pixel loss is defined as `pixel(ŷ, y) = \u2016ŷ − y\u201622\/CHW . This can only be used when when we have a ground-truth target y that the network is expected to match. Total Variation Regularization. To encourage spatial smoothness in the output image ŷ, we follow prior work on feature inversion [7, 22] and super- resolution [53, 54] and make use of total variation regularizer `TV (ŷ). 4 Experiments We perform experiments on two image transformation tasks: style transfer and single-image super-resolution. Prior work on style transfer has used optimization to generate images; our feed-forward networks give similar qualitative results but are up to three orders of magnitude faster. Prior work on single-image super- resolution with convolutional neural networks has used a per-pixel loss; we show encouraging qualitative results by using a perceptual loss instead. 4.1 Style Transfer The goal of style transfer is to generate an image ŷ that combines the content of a target content image yc with the the style of a target style image ys. We train one image transformation network per style target for several hand-picked style targets and compare our results with the baseline approach of Gatys et al. [11]. Baseline. As a baseline, we reimplement the method of Gatys et al. [11]. Given style and content targets ys and yc and layers j and J at which to perform feature and style reconstruction, an image ŷ is generated by solving the problem ŷ = arg min y λc` φ,j feat(y, yc) + λs` φ,J style(y, ys) + λTV `TV (y) (5) Perceptual Losses for Real-Time Style Transfer and Super-Resolution 9 Fig. 5. Our style transfer networks and [11] minimize the same objective. We compare their objective values on 50 images; dashed lines and error bars show standard devia- tions. Our networks are trained on 256× 256 images but generalize to larger images. where λc, λs, and λTV are scalars, y is initialized with white noise, and opti- mization is performed using L-BFGS. Unconstrained optimization of Equation 5 often results in images with pixels outside the range [0, 255]. For a more fair comparison with our method whose output is constrained to this range, for the baseline we minimize Equation 5 using projected L-BFGS by clipping the image y to the range [0, 255] at each iteration. Optimization usually converges to satis- factory results within 500 iterations. This method is slow because each iteration requires a forward and backward pass through the VGG-16 loss network φ. Training Details. We train style transfer networks on the MS-COCO dataset [55]. We resize each of the 80k training images to 256 × 256 and train with a batch size of 4 for 40k iterations, giving roughly two epochs over the training data. We use Adam [56] with learning rate 1× 10−3. The output images are regular- ized with total variation regularization with a strength of between 1× 10−6 and 1×10−4, chosen via cross-validation per style target. We do not use weight decay or dropout, as the model does not overfit within two epochs. For all style transfer experiments we compute feature reconstruction loss at layer relu3_3 and style reconstruction loss at layers relu1_2, relu2_2, relu3_3, and relu4_3 of the VGG-16 loss network φ. Our implementation uses Torch [57] and cuDNN [58]; training takes roughly 4 hours on a single GTX Titan X GPU. Qualitative Results. In Figure 6 we show qualitative examples comparing our results with the baseline for a variety of style and content images. In all cases the hyperparameters λc, λs, and λTV are exactly the same between the two methods; all content images come from the MS-COCO 2014 validation set. Although our models are trained with 256× 256 images, they can be applied in a fully-convolutional manner to images of any size at test-time. In Figure 7 we show examples of style transfer using our models on 512× 512 images. Overall our results are qualitatively similar to the baseline, but in some cases our method produces images with more repetitive patterns. For example in the Starry Night images in Figure 6, our method produces repetitive (but not identical) yellow splotches; the effect can become more obvious at higher resolutions, as seen in Figure 7. 10 Johnson, Alahi, and Fei-Fei Style The Starry Night, Vincent van Gogh, 1889 Style The Muse, Pablo Picasso, 1935 Style Composition VII, Wassily Kandinsky, 1913 Style The Great Wave off Kanagawa, Hokusai, 1829-1832 Style Sketch Style The Simpsons Content [11] Ours Content [11] Ours Fig. 6. Example results of style transfer using our image transformation networks. Our results are qualitatively similar to Gatys et al. [11] but are much faster to generate (see Table 1). All generated images are 256× 256 pixels. Perceptual Losses for Real-Time Style Transfer and Super-Resolution 11 Fig. 7. Example results for style transfer on 512 × 512 images by applying models trained on 256× 256 images. The style images are the same as Figure 6. Quantitative Results. The baseline and our method both minimize Equa- tion 5. The baseline performs explicit optimization over the output image, while our method is trained to find a solution for any content image yc in a single forward pass. We may therefore quantitatively compare the two methods by measuring the degree to which they successfully minimize Equation 5. We run our method and the baseline on 50 images from the MS-COCO validation set, using The Muse as a style image. For the baseline we record the value of the objective function at each iteration of optimization, and for our method we record the value of Equation 5 for each image; we also compute the value of Equation 5 when y is equal to the content image yc. Results are shown in Figure 5. The content image yc achieves a very high loss, and our method achieves a loss comparable to 50 to 100 iterations of explicit optimization. Although our networks are trained to minimize Equation 5 for 256 × 256 images, they also succeed at minimizing the objective when applied to larger images. We repeat the same quantitative evaluation for 50 images at 512× 512 and 1024 × 1024; results are shown in Figure 5. Even at higher resolutions our model achieves a loss comparable to 50 to 100 iterations of the baseline method. Speed. Table 1 compares the runtime of our method and the baseline for several image sizes; for the baseline we report times for varying numbers of optimization iterations. Across all image sizes, our method takes about half the time of a single iteration of the baseline. Compared to 500 iterations of the baseline method, our method is three orders of magnitude faster. Our method processes 512 × 512 images at 20 FPS, making it feasible to run in real-time or on video. 4.2 Single-Image Super-Resolution In single-image super-resolution, the task is to generate a high-resolution out- put image from a low-resolution input. This is an inherently ill-posed prob- lem, since for each low-resolution image there exist multiple high-resolution im- ages that could have generated it. The ambiguity becomes more extreme as the 12 Johnson, Alahi, and Fei-Fei Gatys et al. [11] Speedup Image Size 100 300 500 Ours 100 300 500 256× 256 3.17 9.52s 15.86s 0.015s 212x 636x 1060x 512× 512 10.97 32.91s 54.85s 0.05s 205x 615x 1026x 1024× 1024 42.89 128.66s 214.44s 0.21s 208x 625x 1042x Table 1. Speed (in seconds) for our style transfer networks vs the baseline for various iterations and resolutions. We achieve similar qualitative results (Figure 6) in less time than a single optimization step of the baseline. All benchmarks use a Titan X GPU. super-resolution factor grows; for large factors (×4, ×8), fine details of the high- resolution image may have little or no evidence in its low-resolution version. To overcome this problem, we train super-resolution networks not with the per-pixel loss typically used [1] but instead with a feature reconstruction loss (see Section 3) to allow transfer of semantic knowledge from the pretrained loss network to the super-resolution network. We focus on ×4 and ×8 super- resolution since larger factors require more semantic reasoning about the input. The traditional metrics used to evaluate super-resolution are PSNR and SSIM [59], both of which have been found to correlate poorly with human as- sessment of visual quality [60\u201362]. PSNR and SSIM rely on low-level differences between pixels, and PSNR operates under the assumption of additive Gaussian noise. In addition, PSNR is equivalent to the per-pixel loss `pixel, so as measured by PSNR a model trained to minimize per-pixel loss should always outperform a model trained to minimize feature reconstruction loss. We therefore emphasize that the goal of these experiments is not to achieve state-of-the-art PSNR or SSIM results, but instead to showcase the qualitative difference between models trained with per-pixel and feature reconstruction losses. Model Details. We train models to perform ×4 and ×8 super-resolution by minimizing feature reconstruction loss at layer relu2_2 from the VGG-16 loss network φ. We train with 288×288 patches from 10k images from the MS-COCO training set, and prepare low-resolution inputs by blurring with a Gaussian kernel of width σ = 1.0 and downsampling with bicubic interpolation. We train with a batch size of 4 for 200k iterations using Adam [56] with a learning rate of 1×10−3 without weight decay or dropout. As a post-processing step, we perform histogram matching between our network output and the low-resolution input. Baselines. As a baseline model we use SRCNN [1] for its state-of-the-art per- formance. SRCNN is a three-layer convolutional network trained to minimize per-pixel loss on 33× 33 patches from the ILSVRC 2013 detection dataset. SR- CNN is not trained for ×8 super-resolution, so we can only evaluate it on ×4. SRCNN is trained for more than 109 iterations, which is not computation- ally feasible for our models. To account for differences between SRCNN and our model in data, training, and architecture, we train image transformation net- works for ×4 and ×8 super-resolution using `pixel; these networks use identical data, architecture, and training as the networks trained to minimize `feat. Perceptual Losses for Real-Time Style Transfer and Super-Resolution 13 Ground Truth This image Set5 mean Bicubic 31.78 \/ 0.8577 28.43 \/ 0.8114 Ours (`pixel) 31.47 \/ 0.8573 28.40 \/ 0.8205 SRCNN [13] 32.99 \/ 0.8784 30.48 \/ 0.8628 Ours (`feat) 29.24 \/ 0.7841 27.09 \/ 0.7680 Ground Truth This Image Set14 mean BSD100 mean Bicubic 21.69 \/ 0.5840 25.99 \/ 0.7301 25.96 \/ 0.682 Ours (`pixel) 21.66 \/ 0.5881 25.75 \/ 0.6994 25.91 \/ 0.6680 SRCNN [13] 22.53 \/ 0.6524 27.49 \/ 0.7503 26.90 \/ 0.7101 Ours (`feat) 21.04 \/ 0.6116 24.99 \/ 0.6731 24.95 \/ 63.17 Fig. 8. Results for ×4 super-resolution on images from Set5 (top) and Set14 (bottom). We report PSNR \/ SSIM for each example and the mean for each dataset. More results (including FSIM [63] and VIF [64] metrics) are shown in the supplementary material. Evaluation. We evaluate all models on the standard Set5 [65], Set14 [66], and BSD100 [46] datasets. We report PSNR and SSIM [59], computing both only on the Y channel after converting to the YCbCr colorspace, following [1, 44]. Results. We show results for ×4 super-resolution in Figure 8. Compared to the other methods, our model trained for feature reconstruction does a very good job at reconstructing sharp edges and fine details, such as the eyelashes in the first image and the individual elements of the hat in the second image. In addition to the automated metrics shown in Figure 8, we also ran a user study on Amazon Mechanical Turk to evaluate our ×4 results on the BSD100 dataset. In each trial workers were shown a nearest-neighbor upsampling of an image and results from two methods, and were asked to pick the result they preferred. All trials were randomized and five workers evaluated each image pair. Between SRCNN and `feat, a majority of workers preferred `feat on 96% of images. More details of this study can be found in the supplementary material. Results for ×8 super-resolution are shown in Figure 9. Again we see that our `feat model does a good job at edges and fine details compared to other models, such as the horse\u2019s legs and hooves. The `feat model does not sharpen edges 14 Johnson, Alahi, and Fei-Fei Ground Truth This image Set5 mean Set14 mean BSD100 mean Bicubic 22.75 \/ 0.5946 23.80 \/ 0.6455 22.37 \/ 0.5518 22.11 \/ 0.5322 Ours (`pixel) 23.42 \/ 0.6168 24.77 \/ 0.6864 23.02 \/ 0.5787 22.54 \/ 0.5526 Ours (`feat) 21.90 \/ 0.6083 23.26 \/ 0.7058 21.64 \/ 0.5837 21.35 \/ 0.5474 Fig. 9. Results for ×8 super-resolution results on an image from the BSD100 dataset. We report PSNR \/ SSIM for the example image and the mean for each dataset. More results (including FSIM [63] and VIF [64]) are shown in the supplementary material. indiscriminately; compared to the `pixel model, the `feat model sharpens the boundary edges of the horse and rider but the background trees remain diffuse, suggesting that the `feat model may be more aware of image semantics. Many of the results from our `feat models have grid-like artifacts at the pixel level which harm their PSNR and SSIM compared to baseline methods. Similar artifacts are visible in Figure 3 upon magnification, suggesting that they are a result of the feature reconstruction loss and not the architecture of the image transformation network. Figure 3 shows more pronounced distortions as images are reconstructed from higher-level features, motivating the use of the relu2_2 features used for training our `feat super-resolution models. Since our `pixel and our `feat models share the same architecture, data, and training procedure, all differences between them are due to the difference between the `pixel and `feat losses. The `pixel loss gives fewer visual artifacts and higher PSNR values but the `feat loss does a better job at reconstructing fine details, leading to pleasing visual results. 5 Conclusion In this paper we have combined the benefits of feed-forward image transfor- mation tasks and optimization-based methods for image generation by training feed-forward transformation networks with perceptual loss functions. We have applied this method to style transfer where we achieve comparable performance and drastically improved speed compared to existing methods, and to single- image super-resolution where training with a perceptual loss allows the model to better reconstruct fine details and edges. In future work we hope to explore the use of perceptual loss functions for other image transformation tasks. Acknowledgments Our work is supported by an ONR MURI grant, Yahoo! Labs, and a hardware donation from NVIDIA. Perceptual Losses for Real-Time Style Transfer and Super-Resolution 15 References 1. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo- lutional networks. IEEE TPAMI (2016) 2. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: ICCV. (2015) 3. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. ECCV (2016) 4. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR. (2015) 5. Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using a multi-scale deep network. In: NIPS. (2014) 6. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In: ICCV. (2015) 7. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert- ing them. In: CVPR. (2015) 8. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. In: ICLR Workshop. (2014) 9. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural networks through deep visualization. In: ICML Deep Learning Workshop. (2015) 10. Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis using convolutional neural networks. In: NIPS. (2015) 11. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576 (2015) 12. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks. In: CVPR. (2016) 13. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for image super-resolution. In: ECCV. (2014) 14. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene labeling. IEEE TPAMI 35(8) (2013) 1915\u20131929 15. Pinheiro, P.H., Collobert, R.: Recurrent convolutional neural networks for scene labeling. In: ICML. (2014) 16. Noh, H., Hong, S., Han, B.: Learning deconvolution network for semantic segmen- tation. In: ICCV. (2015) 17. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., Torr, P.H.: Conditional random fields as recurrent neural networks. In: ICCV. (2015) 18. Liu, F., Shen, C., Lin, G.: Deep convolutional neural fields for depth estimation from a single image. In: CVPR. (2015) 19. Wang, X., Fouhey, D., Gupta, A.: Designing deep networks for surface normal estimation. In: CVPR. (2015) 20. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of neural networks. In: ICLR. (2014) 21. Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In: CVPR. (2015) 22. d\u2019Angelo, E., Alahi, A., Vandergheynst, P.: Beyond bits: Reconstructing images from local binary descriptors. In: ICPR. (2012) 23. d\u2019Angelo, E., Jacques, L., Alahi, A., Vandergheynst, P.: From bits to images: Inversion of local binary descriptors. IEEE transactions on pattern analysis and machine intelligence 36(5) (2014) 874\u2013887 16 Johnson, Alahi, and Fei-Fei 24. Vondrick, C., Khosla, A., Malisiewicz, T., Torralba, A.: Hoggles: Visualizing object detection features. In: ICCV. (2013) 25. Dosovitskiy, A., Brox, T.: Inverting visual representations with convolutional net- works. In: CVPR. (2016) 26. Ulyanov, D., Lebadev, V., Vedaldi, A., Lempitsky, V.: Texture networks: Feed- forward synthesis of textures and stylized images. In: ICML. (2016) 27. Li, C., Wand, M.: Precomputed real-time texture synthesis with markovian gen- erative adversarial networks. In: ECCV. (2016) 28. Yang, C.Y., Ma, C., Yang, M.H.: Single-image super-resolution: a benchmark. In: ECCV. (2014) 29. Irani, M., Peleg, S.: Improving resolution by image registration. CVGIP: Graphical models and image processing 53(3) (1991) 231\u2013239 30. Freedman, G., Fattal, R.: Image and video upscaling from local self-examples. ACM Transactions on Graphics (TOG) 30(2) (2011) 12 31. Sun, J., Sun, J., Xu, Z., Shum, H.Y.: Image super-resolution using gradient profile prior. In: CVPR. (2008) 32. Shan, Q., Li, Z., Jia, J., Tang, C.K.: Fast image\/video upsampling. In: ACM Transactions on Graphics (TOG). Volume 27., ACM (2008) 153 33. Kim, K.I., Kwon, Y.: Single-image super-resolution using sparse regression and natural image prior. IEEE TPAMI 32(6) (2010) 1127\u20131133 34. Xiong, Z., Sun, X., Wu, F.: Robust web image\/video super-resolution. Image Processing, IEEE Transactions on 19(8) (2010) 2017\u20132028 35. Freeman, W.T., Jones, T.R., Pasztor, E.C.: Example-based super-resolution. Com- puter Graphics and Applications, IEEE 22(2) (2002) 56\u201365 36. Chang, H., Yeung, D.Y., Xiong, Y.: Super-resolution through neighbor embedding. In: CVPR. (2004) 37. Glasner, D., Bagon, S., Irani, M.: Super-resolution from a single image. In: ICCV. (2009) 38. Yang, J., Lin, Z., Cohen, S.: Fast image super-resolution based on in-place example regression. In: CVPR. (2013) 39. Sun, J., Zheng, N.N., Tao, H., Shum, H.Y.: Image hallucination with primal sketch priors. In: CVPR. (2003) 40. Ni, K.S., Nguyen, T.Q.: Image superresolution using support vector regression. Image Processing, IEEE Transactions on 16(6) (2007) 1596\u20131610 41. He, L., Qi, H., Zaretzki, R.: Beta process joint dictionary learning for coupled feature spaces with application to single image super-resolution. In: CVPR. (2013) 42. Yang, J., Wright, J., Huang, T., Ma, Y.: Image super-resolution as sparse repre- sentation of raw image patches. In: CVPR. (2008) 43. Yang, J., Wright, J., Huang, T.S., Ma, Y.: Image super-resolution via sparse representation. Image Processing, IEEE Transactions on 19(11) (2010) 2861\u20132873 44. Timofte, R., De Smet, V., Van Gool, L.: A+: Adjusted anchored neighborhood regression for fast super-resolution. In: ACCV. (2014) 45. Schulter, S., Leistner, C., Bischof, H.: Fast and accurate image upscaling with super-resolution forests. In: CVPR. (2015) 46. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed self-exemplars. In: CVPR. (2015) 47. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convolutional generative adversarial networks. In: ICLR. (2016) 48. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016) Perceptual Losses for Real-Time Style Transfer and Super-Resolution 17 49. Gross, S., Wilber, M.: Training and investigating residual nets. http:\/\/torch.ch\/blog\/2016\/02\/04\/resnets.html (2016) 50. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICML. (2015) 51. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: ICLR. (2015) 52. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115(3) (2015) 211\u2013252 53. Aly, H.A., Dubois, E.: Image up-sampling using total-variation regularization with a new observation model. Image Processing, IEEE Transactions on 14(10) (2005) 1647\u20131659 54. Zhang, H., Yang, J., Zhang, Y., Huang, T.S.: Non-local kernel regression for image and video restoration. In: ECCV. (2010) 55. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV. (2014) 56. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR. (2015) 57. Collobert, R., Kavukcuoglu, K., Farabet, C.: Torch7: A matlab-like environment for machine learning. In: NIPS BigLearn Workshop. (2011) 58. Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., Shelhamer, E.: cuDNN: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014) 59. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. Image Processing, IEEE Transactions on 13(4) (2004) 600\u2013612 60. Hanhart, P., Korshunov, P., Ebrahimi, T.: Benchmarking of quality metrics on ultra-high definition video sequences. In: Digital Signal Processing (DSP), 2013 18th International Conference on, IEEE (2013) 1\u20138 61. Huynh-Thu, Q., Ghanbari, M.: Scope of validity of psnr in image\/video quality assessment. Electronics letters 44(13) (2008) 800\u2013801 62. Kundu, D., Evans, B.L.: Full-reference visual quality assessment for synthetic images: A subjective study. Proc. IEEE Int. Conf. on Image Processing (2015) 63. Zhang, L., Zhang, L., Mou, X., Zhang, D.: Fsim: a feature similarity index for image quality assessment. IEEE transactions on Image Processing 20(8) (2011) 2378\u20132386 64. Sheikh, H.R., Bovik, A.C.: Image information and visual quality. IEEE Transac- tions on Image Processing 15(2) (2006) 430\u2013444 65. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity single-image super-resolution based on nonnegative neighbor embedding. (2012) 66. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse- representations. In: Curves and Surfaces. Springer (2010) 711\u2013730 ","flair":"three\tResearch"}
{"author":"cryptobionic","created":"Tue Nov 08 22:55:59 EST 2016","text":"The current model was trained on screenshots taken from Pokemon Silver, Crystal, and Diamond, then tested on Pokemon Blue Version. Sample results below. -c --checkpoint_dir <str> [path to save the model] You can use this with some sample training images provided in . Run to start training on the small amount of sample images. This will create a directory called in the folder. If you get an error about CUDA running out of memory, reduce the batch size. The files in the folder are as follows: The training attempts to obtain the resized color image when given the resized gray image. I've included a trained model in the directory that you can run your own images on. You can either run the model on one image or a folder of images. For one image, run and pass it the model and the image as parameters. To run it on multiple images, run and pass it the model and the folder to the images. will save your images in the folder, where as will save them in the current directory. Examples: There are scripts included to help create your own dataset, which is desirable because the amount of data needed to obtain good results is a good amount. The results below were trained on about 50,000 images. The easiest method to obtain images is to extract them from Youtube walkthrough videos of different games. Given that you have a folder with videos use to extract images from each video. Just pass it the folder containing images. Depending on if the video had a border around the game, you may need to use to crop out the border. There are comments in the script you can uncomment to view the image before it crops all of them to be sure the cropping is correct. Finally, use to create resized and gray images for training.","flair":"four\tProject"}
{"author":"Jeibros","created":"Sat Oct 01 16:37:27 EDT 2016","text":"Hello,\n\nIs there any way by which ML can help calculating fast the appropriate order of carrying out an industrial operations? I'm thinking particularly on manufacturing industry: let's imagine that a customer wants a particular geometry to be designed in steel, and could an algorithm calculate the correct order? Or is this industry already sufficiently optimized? Thanks","flair":"null\tnull"}
{"author":"matt_hammond","created":"Fri Oct 21 07:51:27 EDT 2016","text":"Could someone more experienced in this matter help me out.\n\nI have a problem similar to face recognition but instead of faces I have images of dolphin fins. Turns out dolphins have unique fins. Now I want to build a neural network that will map an image of a dolphin fin to an N-dimensional vector representing that dolphin uniquely. I think this is called an embedding (correct me if I'm wrong). \n\nGoogle's FaceNet used a similar approach to map faces to a 128-dimensional vector. I'm wondering how would I go about doing this. Could I use a pretrained network (let's say FaceNet) and use transfer learning to recognize dolphin fins? If not, how big of a dataset would I need to make my own embedding for recognizing dolphins?\n\nPlease understand I don't have a lot of experience with neural networks. I just want to learn.","flair":"three\tResearch"}
{"author":"improbabble","created":"Wed Sep 28 11:12:57 EDT 2016","text":" Challenge 2016 Held in conjunction with ILSVRC at ECCV 2016 Results Contents: Summary: There are totally 92 valid submissions from 27 teams. Hikvision won the 1st place with 0.0901 top-5 error, MW won the 2nd place with 0.1019 top-5 error, and Trimps-Soushen won the 3rd place with 0.1030 top-5 error. Congratulations to all the teams. See below for the leaderboard and the team information. Rule: Each teams can only use the provided data in Places2 Challenge 2016 to train their networks. Standard pre-trained CNN models trained on Imagenet-1.2million and previous Places are allowed to use. Each teams can submit at most 5 prediction results. Ranks are based on the top-5 classification error of each submission. Scene classification with provided training data Team information Legend: Yellow background - winner in this task according to this metric & authors are willing to reveal the method White background - authors are willing to reveal the method Grey background - authors chose not to reveal the method Scene classification with provided training data Team name Entry description Top-5 classification error Hikvision Model D 0.0901 Hikvision Model E 0.0908 Hikvision Model C 0.0939 Hikvision Model B 0.0948 MW Model ensemble 2 0.1019 MW Model ensemble 3 0.1019 MW Model ensemble 1 0.1023 Hikvision Model A 0.1026 Trimps-Soushen With extra data. 0.103 Trimps-Soushen Ensemble 2 0.1042 SIAT_MMLAB 10 models fusion 0.1043 SIAT_MMLAB 7 models fusion 0.1044 SIAT_MMLAB fusion with softmax 0.1044 SIAT_MMLAB learning weights with cnn 0.1044 SIAT_MMLAB 6 models fusion 0.1049 Trimps-Soushen Ensemble 4 0.1049 Trimps-Soushen Ensemble 3 0.105 MW Single model B 0.1073 MW Single model A 0.1076 NTU-SC Product of 5 ensembles (top-5) 0.1085 NTU-SC Product of 3 ensembles (top-5) 0.1086 NTU-SC Sum of 3 ensembles (top-5) 0.1086 NTU-SC Sum of 5 ensembles (top-3) 0.1086 NTU-SC Single ensemble of 5 models (top-5) 0.1088 NQSCENE Four models 0.1093 NQSCENE Three models 0.1101 Samsung Research America: General Purpose Acceleration Group Simple Ensemble, 3 Inception v3 models w\/various hyper param changes, 32 multi-crop (60.11 top-1, 88.98 top-5 on val) 0.1113 fusionf Fusion with average strategy (12 models) 0.1115 fusionf Fusion with scoring strategy (14 models) 0.1117 fusionf Fusion with average strategy (13 models) 0.1118 YoutuLab weighted average1 at scale level using greedy search 0.1125 YoutuLab weighted average at model level using greedy search 0.1127 YoutuLab weighted average2 at scale level using greedy search 0.1129 fusionf Fusion with scoring strategy (13 models) 0.113 fusionf Fusion with scoring strategy (12 models) 0.1132 YoutuLab simple average using models in entry 3 0.1139 Samsung Research America: General Purpose Acceleration Group Model A0, weakly scaled, multi-crop. (59.61 top-1, 88.64 top-5 on val) 0.1142 SamExynos 3 model 0.1143 Samsung Research America: General Purpose Acceleration Group Ensemble B, 3 Inception v3 models w\/various hyper param changes + Inception v4 res2, 128 multi-crop 0.1152 YoutuLab average on base models 0.1162 NQSCENE Model B 0.117 Samsung Research America: General Purpose Acceleration Group Model A2, weakly scaled, single-crop & mirror. (58.84 top-1, 88.09 top-5 on val) 0.1188 NQSCENE Model A 0.1192 Samsung Research America: General Purpose Acceleration Group Model A1, weakly scaled, single-crop. (58.65 top-1, 88.07 top-5 on val) 0.1193 Trimps-Soushen Ensemble 1 0.1196 Rangers ensemble model 1 0.1208 SamExynos single model 0.121 Rangers ensemble model 2 0.1212 Everphoto ensemble by learned weights - 1 0.1213 Everphoto ensemble by product strategy 0.1218 Everphoto ensemble by learned weights - 2 0.1218 Everphoto ensemble by average strategy 0.1223 MIPAL_SNU Ensemble of two ResNet-50 with balanced sampling 0.1232 KPST_VB Model II 0.1233 KPST_VB Ensemble of Model I and II 0.1235 Rangers single model result of 69 0.124 Everphoto ensemble by product strategy (without specialist models) 0.1242 KPST_VB Model II with adjustment 0.125 KPST_VB Model I 0.1251 Rangers single model result of 66 0.1253 KPST_VB Ensemble of Model I and II with adjustment 0.1253 SJTU-ReadSense Ensemble 5 models with learnt weights 0.1272 SJTU-ReadSense Ensemble 5 models with weighted validation accuracies 0.1273 iMCB A combination of CNN models based on researched influential factors 0.1277 SJTU-ReadSense Ensemble 6 models with learnt weights 0.1278 SJTU-ReadSense Ensemble 4 models with learnt weights 0.1287 iMCB A combination of CNN models with a strategy w.r.t.validation accuracy 0.1299 Choong Based on VGG16, features are extracted from multiple layers. ROI proposal network is not applied. Every neuron from each feature layer is center of ROI candidate. 0.131 SIIT_KAIST 101-depth single model (val.error 12.90%) 0.131 DPAI Vison An ensemble model 0.1355 isia_ICT spectral clustering on confusion matrix 0.1355 isia_ICT fusion of 4 models with average strategy 0.1357 NUIST inception+shortcut CNN 0.137 isia_ICT MP_multiCNN_multiscale 0.1372 NUIST inception+shortcut CNN 0.1381 Viz Insight Multiple Deep Metaclassifiers 0.1386 iMCB FeatureFusion_2L 0.1396 iMCB FeatureFusion_3L 0.1404 DPAI Vison Single Model 0.1425 isia_ICT 2 models with size of 288 0.1433 Faceall-BUPT A single model with 150crops 0.1471 Baseline: ResNet152 0.1493 Baseline: VGG16 0.1499 iMCB A Single Model 0.1506 SJTU-ReadSense A single model (based on Inception-BN) trained on the Places365-Challenge dataset 0.1511 Baseline: GoogLeNet 0.1599 OceanVision A result obtained by VGG-16 0.1635 Baseline:AlexNet 0.1725 OceanVision A result obtained by alexnet 0.1867 OceanVision A result obtained by googlenet 0.1867 ABTEST GoogLeNet Model trained on LSUN dataset and fined tuned on Places2 0.3245 Vladimir Iglovikov VGG16 trained on 128x128 0.3552 Vladimir Iglovikov VGG19 trained on 128x128 0.3593 Vladimir Iglovikov average of VGG16 and VGG19 trained on 128x128 0.3712 Vladimir Iglovikov Resnet 50 trained on 128x128 0.4577 scnu407 VGG16+4D lstm 0.8831 Team information Team name Team members Abstract Hikvision Qiaoyong Zhong*, Chao Li, Yingying Zhang(#), Haiming Sun*, Shicai Yang*, Di Xie, Shiliang Pu (* indicates equal contribution) Hikvision Research Institute (#)ShanghaiTech University, work is done at HRI [DET] Our work on object detection is based on Faster R-CNN. We design and validate the following improvements: * Better network. We find that the identity-mapping variant of ResNet-101 is superior for object detection over the original version. * Better RPN proposals. A novel cascade RPN is proposed to refine proposals' scores and location. A constrained neg\/pos anchor ratio further increases proposal recall dramatically. * Pretraining matters. We find that a pretrained global context branch increases mAP by over 3 points. Pretraining on the 1000-class LOC dataset further increases mAP by ~0.5 point. * Training strategies. To attack the imbalance problem, we design a balanced sampling strategy over different classes. With balanced sampling, the provided negative training data can be safely added for training. Other training strategies, like multi-scale training and online hard example mining are also applied. * Testing strategies. During inference, multi-scale testing, horizontal flipping and weighted box voting are applied. The final mAP is 65.1 (single model) and 67 (ensemble of 6 models) on val2. [CLS-LOC] A combination of 3 Inception networks and 3 residual networks is used to make the class prediction. For localization, the same Faster R-CNN configuration described above for DET is applied. The top5 classification error rate is 3.46%, and localization error is 8.8% on the validation set. [Scene] For the scene classification task, by drawing support from our newly-built M40-equipped GPU clusters, we have trained more than 20 models with various architectures, such as VGG, Inception, ResNet and different variants of them in the past two months. Fine-tuning very deep residual networks from pre-trained ImageNet models, like ResNet 101\/152\/200, seemed not to be as good enough as what we expected. Inception-style networks could get better performance in considerably less training time according to our experiments. Based on this observation, deep Inception-style networks, and not-so-deep residuals networks have been used. Besides, we have made several improvements for training and testing. First, a new data augmentation technique is proposed to better utilize the information of original images. Second, a new learning rate setting is adopted. Third, label shuffling and label smoothing is used to tackle the class imbalance problem. Fourth, some small tricks are used to improve the performance in test phase. Finally we achieved a very good top 5 error rate, which is below 9% on the validation set. [Scene Parsing] We utilize a fully convolutional network transferred from VGG-16 net, with a module, called mixed context network, and a refinement module appended to the end of the net. The mixed context network is constructed by a stack of dilated convolutions and skip connections. The refinement module generates predictions by making use of output of the mixed context network and feature maps from early layers of FCN. The predictions are then fed into a sub-network, which is designed to simulate message-passing process. Compared with baseline, our first major improvement is that, we construct the mixed context network, and find that it provides better features for dealing with stuff, big objects and small objects all at once. The second improvement is that, we propose a memory-efficient sub-network to simulate message-passing process. The proposed system can be trained end-to-end. On validation set, the mean iou of our system is 0.4099 (single model) and 0.4156 (ensemble of 3 models), and the pixel accuracy is 79.80% (single model) and 80.01% (ensemble of 3 models). References [1] Ren, Shaoqing, et al. \"Faster R-CNN: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015. [2] Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. \"Training region-based object detectors with online hard example mining.\" arXiv preprint arXiv:1604.03540 (2016). [3] He, Kaiming, et al. \"Deep residual learning for image recognition.\" arXiv preprint arXiv:1512.03385 (2015). [4] He, Kaiming, et al. \"Identity mappings in deep residual networks.\" arXiv preprint arXiv:1603.05027 (2016). [5] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015). [6] Szegedy, Christian, et al. \"Rethinking the inception architecture for computer vision.\" arXiv preprint arXiv:1512.00567 (2015). [7] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016). [8] F. Yu and V. Koltun, \"Multi-scale context aggregation by dilated convolutions,\" in ICLR, 2016. [9] J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in CVPR, 2015. [10] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr, \"Conditional random fields as recurrent neural networks,\" in ICCV, 2015. [11] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. Yuille, \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\", arXiv:1606.00915, 2016. [12] P. O. Pinheiro, T. Lin, R. Collobert, P. Dollar, \"Learning to Refine Object Segments\", arXiv:1603.08695, 2016. MW Gang Sun (Institute of Software, Chinese Academy of Sciences) Jie Hu (Peking University) We leverage the theory named CNA [1] (capacity and necessity analysis) to guide the design of CNNs. We add more layers on the larger feature map (e.g., 56x56) to increase the capacity, and remove some layers on the smaller feature map (e.g., 14x14) to avoid ineffective architectures. We have verified the effectiveness on the models in [2], ResNet-like models [3], and Inception-ResNet-like models [4]. In addition, we also apply cropped patches from original images as training samples by selecting random area and aspect ratio. To increase the ability of generalization, we prune the model weights periodically. Moreover, we utilize balanced sampling strategy [2] and label smooth regularization [5] during training, to alleviate the bias from the non-uniform sample distribution among categories and partial incorrect training labels. We use the provided data (Places365) for training models, do not use any additional data, and train all models from scratch. The algorithm and architecture details will be described in our arXiv paper (available online shortly). [1] Xudong Cao. A practical theory for designing very deep convolutional neural networks, 2014. (unpublished) [2] Li Shen, Zhouchen Lin, Qingming Huang. Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks. In ECCV 2016. [3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. In CVPR 2016. [4] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. ArXiv:1602.07261,2016. [5] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. ArXiv:1512.00567,2016. Trimps-Soushen Jie Shao, Xiaoteng Zhang, Zhengyan Ding, Yixin Zhao, Yanjun Chen, Jianying Zhou, Wenfei Wang, Lin Mei, Chuanping Hu The Third Research Institute of the Ministry of Public Security, P.R. China. Object detection (DET) We use several pre-trained models, including ResNet, Inception, Inception-Resnet etc. By taking the predict boxes from our best model as region proposals, we average the softmax scores and the box regression outputs across all models. Other improvements include annotations refine, boxes voting and features maxout. Object classification\/localization (CLS-LOC) Based on image classification models like Inception, Inception-Resnet, ResNet and Wide Residual Network (WRN), we predict the class labels of the image. Then we refer to the framework of \"Faster R-CNN\" to predict bounding boxes based on the labels. Results from multiple models are fused in different ways, using the model accuracy as weights. Scene classification (Scene) We adopt different kinds of CNN models such as ResNet, Inception and WRN. To improve the performance of features from multiple scales and models, we implement a cascade softmax classifier after the extraction stage. Object detection from video (VID) Same methods as DET task were applied to each frame. Optical flow guided motion prediction helped to reduce the false negative detections. [1] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. NIPS 2015 [2] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alem. [3] Zagoruyko S, Komodakis N. Wide Residual Networks[J]. arXiv preprint arXiv:1605.07146, 2016. SIAT_MMLAB Sheng Guo, Linjie Xing, Shenzhen Institutes of Advanced Technology, CAS. Limin Wang, Computer Vision Lab, ETH Zurich. Yuanjun Xiong, Chinese University of Hong Kong. Jiaming Liu and Yu Qiao, Shenzhen Institutes of Advanced Technology, CAS. We propose a modular framework for large-scale scene recognition, called as multi-resolution CNN (MR-CNN) [1]. This framework addresses the characterization difficulty of scene concepts, which may be based on multi-level visual information, including local objects, spatial layout, and global context. Specifically, in this challenge submission, we utilizes four resolutions (224, 299, 336, 448) as the input sizes of MR-CNN architectures. For coarse resolution (224, 299), we exploit the existing powerful Inception architectures (Inception v2 [2], Inception v4 [3], and Inception-ResNet [3]), while for fine resolution (336, 448), we propose our new inception architectures by making original inception network deeper and wider. Our final submission is the prediction result of MR-CNNs by fusing the outputs of CNNs of different resolutions. In addition, we propose several principled techniques to reduce the over-fitting risk of MR-CNNs, including class balancing and hard sample mining. These simple yet effective training techniques enable us to further improve the generalization performance of MR-CNNs on the validation dataset. Meanwhile, we use an efficient parallel version of Caffe toolbox [4] to allow for the fast training of our proposed deeper and wider Inception networks. [1] L. Wang, S. Guo, W. Huang, Y. Xiong, and Y. Qiao, Knowledge guided disambiguation for large-scale scene classification with Multi-Resolution CNNs, in arXiv, 2016. [2] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in ICML, 2015. [3] C. Szegedy, S. Ioffe, and V. Vanhouche, Inception-v4, Inception-ResNet and the impact of residual connections on learning, in arXiv, 2016. [4] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, Temporal segment networks: towards good practices for deep action recognition, in ECCV, 2016. NTU-SC Jason Kuen, Xingxing Wang, Bing Shuai, Xiangfei Kong, Jianxiong Yin, Gang Wang*, Alex C Kot Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore. All of our scene classification models are built upon pre-activation ResNets [1]. For scene classification using the provided RGB images, we train from scratch a ResNet-200, as well as a relatively shallow Wide-ResNet [2]. In addition to RGB images, we make use of class activation maps [3] and (scene) semantic segmentation masks [4] as complementary cues, obtained from models pre-trained for ILSVRC image classification [5] and scene parsing [6] tasks respectively. Our final submissions consist of ensembles of multiple models. References [1] He, K., Zhang, X., Ren, S., & Sun, J. \u201CIdentity Mappings in Deep Residual Networks\u201D. ECCV 2016. [2] Zagoruyko, S., & Komodakis, N. \u201CWide Residual Networks\u201D. BMVC 2016. [3] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. \u201CLearning Deep Features for Discriminative Localization\u201D. CVPR 2016. [4] Shuai, B., Zuo, Z., Wang, G., & Wang, B. \"Dag-Recurrent Neural Networks for Scene Labeling\". CVPR 2016. [5] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... & Berg, A. C. \u201CImagenet large scale visual recognition challenge\u201D. International Journal of Computer Vision, 115(3), 211-252. [6] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., & Torralba, A. \u201CSemantic Understanding of Scenes through the ADE20K Dataset\u201D. arXiv preprint arXiv:1608.05442. NQSCENE Chen Yunpeng ( NUS ) Jin Xiaojie ( NUS ) Zhang Rui ( CAS ) Li Yu ( CAS ) Yan Shuicheng ( Qihoo\/NUS ) Technique Details for the Scene Classification: For the scene classification task, we propose the following methods to address the data imbalance issues (aka the long tail distribution issue) which benefit and boost the final performance: 1) Category-wise Data Augmentation: We implied a category wise data augmentation strategy, which associates each category with adaptive augmentation level. The augmentation level is updated iteratively during the training. 2) Multi-task Learning: We proposed a multipath learning architecture to jointly learn feature representations from the Imagnet-1000 dataset and Places-365 dataset. Vanilla ResNet-200 [1] is adopted with following elementary tricks: scale and aspect ratio augmentation, over-sampling, multi-scale (x224,x256,x288,x320) dense testing. In total, we have trained four models and fused them by averaging their scores. It costs about three days for training each model using MXNet [2] on a cluster with forty NVIDIA M40 (12GB). ------------------------------ [1] He, Kaiming, et al. \"Identity mappings in deep residual networks.\" arXiv preprint arXiv:1603.05027 (2016). [2] Chen, Tianqi, et al. \"Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems.\" arXiv preprint arXiv:1512.01274(2015). Samsung Research America: General Purpose Acceleration Group Dr. S. Eliuk (Samsung), C. Upright (Samsung), Dr. H. Vardhan (Samsung), T. Gale (Intern, North Eastern), S. Walsh (Intern University of Alberta). The General Purpose Acceleration Group is focused on accelerating training via HPC & distributed computing. We present Distributed Training Done Right (DTDR) where standard open-source models are trained in an effective manner via a multitude of techniques involving strong \/ weak scaling and strict distributed training modes. Several different models are used from standard Inception v3, to Inception v4 res2, and ensembles of such techniques. The training environment is unique as we can explore extremely deep models given the model-parallel nature of our partitioning of data. fusionf Nina Narodytska (Samsung Research America) Shiva Kasiviswanathan (Samsung Research America) Hamid Maei (Samsung Research America) We used several modifications of modern CNNs, including VGG[1], GoogleNet[2,4], and ResNet[3]. We used several fusion strategies, including a standard averaging and scoring scheme. We also used different subsets of models in different submissions. Training was performed on low-resolution dataset. We used balanced loading to take into account different numbers of images in each class. [1] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. [2] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich. Going Deeper with Convolutions. [3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun Deep Residual Learning for Image Recognition [4]Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning YoutuLab Xiaowei Guo, YoutuLab Ruixin Zhang, YoutuLab Yushi Yao, YoutuLab Pai Peng, YoutuLab Ke Li, YoutuLab We build a scene recognition system using deep CNN models. These CNN models are inspired by original resnet[1] and inception[2] network architectures. We train these models on challenge dataset and apply balanced sampling strategy[3] to adapt unbalanced challenge dataset. Moreover, DSD[4] process is applied to further improve model performance. In this competition, we submit five entries. The first and second are combinations of single scale results using weighted arithmetic average which weights is searched by greedy strategy. The third is a combination of single model results using same strategy with the first entry. The fourth and fifth are combinations using simple average strategy of single model results. [1] K. He, X. Zhang, S. Ren, J. Sun. Identity Mappings in Deep Residual Networks. In ECCV 2016. abs\/1603.05027 [2] C. Szegedy, S. Ioffe, V. Vanhoucke, A. Alemi. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. In ICLR 2016. abs\/1602.07261 [3] L. Shen, Z. Lin, Q. Huang. Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks. abs\/1512.05830 [4] S. Han, J. Pool, S. Narang, H. Mao, S. Tang, E. Elsen, B. Catanzaro, J. Tran, W. J. Dally. DSD: Regularizing Deep Neural Networks with Dense-Sparse-Dense Training Flow. abs\/1607.04381 SamExynos Qian Zhang(Beijing Samsung Telecom R&D Center) Peng Liu(Beijing Samsung Telecom R&D Center) Jinbin Lin(Beijing Samsung Telecom R&D Center) Junjun Xiong(Beijing Samsung Telecom R&D Center) Object localization: The submission is based on [1] and [2], but we modified the model, and the newtwork is 205 layers. Due to the limit of time and GPUs, we have just trained three CNN model for classification. The top-5 accuracy on the validation set with dense crops(scale:224,256,288,320,352,384,448,480) is 96.44% for the best single model. And the top-5 accuracy on the validation set with dense crops is 96.88% for three model ensemble. places365 classification: The submission is based on [3] and [4], we add 5 layers to resnet 50, and modified the network. Due to the limit of time and GPUs, we have just trained three CNN model for the scene classification task. The top-5 accuracy on the validation set with 72 crops is 87.79% for the best single model. And the top-5 accuracy on the validation set with multiple crops is 88.70% for three model ensemble. [1]Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun,Identity Mappings in Deep Residual Networks. ECCV 2016. [2]Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\". arXiv preprint arXiv:1602.07261 (2016) [3]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna. \"Rethinking the Inception Architecture for Computer Vision\". arXiv preprint arXiv:1512.00567 (2015) Rangers Y. Q. Gao, W. H. Luo, X. J. Deng, H. Wang, W. D. Chen, --- Everphoto Yitong Wang, Zhonggan Ding, Zhengping Wei, Linfu Wen Everphoto Our method is based on DCNN approaches. We use 5 models with different input scales and different network structures as basic models. They are derived from GoogleNet, VGGNet and ResNet. We also utilize the idea of dark knowledge [1] to train several specialist models, and use these specialist models to reassign probability scores and refine the basic outputs. Our final results are based on the ensemble of refined outputs. [1] Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network[J]. arXiv preprint arXiv:1503.02531, 2015. MIPAL_SNU Sungheon Park and Nojun Kwak (Graduate School of Convergence Science and Technology, Seoul National University) We trained two ResNet-50 [1] networks. One network used 7x7 mean pooling, and the other used multiple mean poolings with various sizes and positions. We also used balanced sampling strategy which is similar to [2] to deal with the imbalanced training set. [1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" CVPR, 2016. [2] Shen, Li, et al. \"Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks.\" arXiv, 2015. KPST_VB Nguyen Hong Hanh Seungjae Lee Junhyeok Lee In this work, we used pre-trained ResNet200(ImageNet)[1] and retrained the network on Place 365 Challenge data (256 by 256). We also estimated scene probability using the output of pretrained ResNet200 and scene vs. object (ImageNet 1000 class) distribution on training data. For classification, we used ensemble of two networks with multiple crops and adjusted on scene probability. [1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. *Our work is performed by deep learning analysis tool(Deep SDK by KPST). SJTU-ReadSense Qinchuan Zhang, Shanghai Jiao Tong University Junxuan Chen, Shanghai Jiao Tong University Thomas Tong, ReadSense Leon Ding, ReadSense Hongtao Lu, Shanghai Jiao Tong University We train two CNN models from the scratch. Model A based on Inception-BN [1] with one auxiliary classifier is trained on the Places365-Challenge dataset [2], which achieved 15.03% top-5 error on validation dataset. Model B based on ResNet [3] with depth of 50 layers is trained on the Places365-Standard dataset and finetuned for 2 epochs on the Places365-Challenge dataset due to the limit of time, which achieved 16.3% top-5 error on validation dataset. We also fuse features extracted from 3 baseline models [2] on the Places365-Challenge dataset and trained two fully connected layers with a softmax classifier. Moreover, we adopt the \"class-aware\" sampling strategy proposed by [4] for models trained on Places365-Challenge dataset to tackle the non-uniform distribution of images over 365 categories. We implement model A using Caffe [5] and conduct all other experiments using MXNet [6] to deploy larger batch size on a GPU. We train all models with a 224x224 crop randomly sampled from an 256x256 image or its horizontal flip, with the per-pixel mean subtracted. We apply 12-crops [7] for evaluation on validation and test datasets. We ensemble multiple models with weights (learnt on validation dataset or top-5 validation accuracies), and achieve 12.79% (4 models), 12.69% (5 models), 12.57% (6 models) top-5 error on validation dataset. [1] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [2] Places: An Image Database for Deep Scene Understanding. B. Zhou, A. Khosla, A. Lapedriza, A. Torralba and A. Oliva. Arxiv, 2016. [3] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016. [4] L. Shen, Z. Lin , Q. Huang. Relay backpropagation for effective learning of deep convolutional neural networks. arXiv:1512.05830, 2015. [5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [6] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C.n Zhang, and Z. Zhang. MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. In NIPS, 2015. [7] C. Szegedy,W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015. iMCB *Yucheng Xing, *Yufeng Zhang, Zhiqin Chen, Weichen Xue, Haohua Zhao, Liqing Zhang @Shanghai Jiao Tong University (SJTU) (* indicates equal contribution) In this competition, we submit five entries. The first model is a single model, which achieved 15.24% top-5 error on validation dataset. It is a Inception-V3[1] model that is modified and trained based on both the challenge and standard datasets[2]. When being tested, images are resized to 337*337 and then a 12-crops skill is used to get the 299*299 inputs to the model, which contributes to the improvement of performance. The second model is a fusion-feature model(FeatureFusion_2L), which achieved 13.74% top-5 error on validation dataset. It is a two layers fusion-feature network, whose input is the combination of fully-connected layer's features extracted from several well performed CNNs(i.e. pretrained models[3], such as Resnet, VGG, Googlenet).As a result, it turns out to be efficient in reducing the error rate. The third model is also a fusion-feature network(FeatureFusion_3L),which achieved 13.95% top-5 error on validation dataset. Comparing with the second model, it is a three layers fusion-feature network which contains two fully-connected layers. The fourth is the combination of CNN models with a strategy w.r.t.validation accuracy, which achieved 13% top-5 error on validation dataset. It combines the probabilities provided by the softmax layer from three CNNs, in which the influential factor of each CNN is determined by the validation accuracy. The fifth is the combination of CNN models based on researched influential factors, which achieved 12.65% top-5 error on validation dataset. There are six CNNs taken into consideration, while four models(Inception-V2, Inception-V3, FeatureFusion_2L and FeatureFusion_3L) of them are trained by us and the other two are pretrained. The influential factors of these models are optimized according to plenty of researches. [1] Szegedy, Christian, et al. \"Rethinking the Inception Architecture for Computer Vision.\" arXiv preprint arXiv:1512.00567 (2015). [2]B. Zhou, A. Khosla, A. Lapedriza, A. Torralba and A. Oliva. \"Places: An Image Database for Deep Scene Understanding.\" Arxiv, 2016. [3] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba and A. Oliva. \"Learning Deep Features for Scene Recognition using Places Database.\"Advances in Neural Information Processing Systems 27 (NIPS), 2014. Choong Choong Hwan Choi (KAIST) Abstract Ensemble of Deep learning model based on VGG16 & ResNet Based on VGG16, features are extracted from multiple layers. ROI proposal network is not applied. Every neuron from each feature layer is center of ROI candidate. Reference : [1] Liu, Wei, et. al. \"SSD: Single Shot Multibox Detector\" [2] K. Simonyan, A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" [3] Kaiming He, et. al., \"Deep Residual Learning for Image Recognition\" SIIT_KAIST Sihyeon Seong (KAIST) Byungju Kim (KAIST) Junmo Kim (KAIST) We used ResNet[1] (101 layers \/ 4GPUs) as our baseline model. From the model pre-trained with ImageNet classification dataset(provided by [2]), We re-tuned the model with Places365 dataset (256-resized small dataset). Then, we further fine-tuned the model based on the following ideas: i) Analyzing correlations between labels : We calculated correlations between each pair of predictions p(i), p(j) where i, j are classes. Then, highly correlated label pairs are extracted by thresholding the correlation coefficients. ii) Additional semantic label generation : Using the correlation table from i), we further generated super\/subclass labels by clustering them. Additionally, we generated 170 binary labels for separations of confusing classes, which maximize margins between highly correlated label pairs. iii) Boosting-like multi-loss terms : A large number of loss terms are combined for classifying the labels generated in ii). [1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" arXiv preprint arXiv:1512.03385 (2015). [2] https:\/\/github.com\/facebook\/fb.resnet.torch DPAI Vison Object detection: Chris Li, Savion Zhao, Bin Liu, Yuhang He, Lu Yang, Cena Liu Scene classification: Lu Yang, Yuhang He, Cena Liu, Bin Liu, Bo Yu Scene parsing: Bin Liu, Lu Yang, Yuhang He, Cena Liu, Bo Yu, Chris Li, Xiongwei Xia Object detection from video: Bin Liu, Cena Liu, Savion Zhao, Yuhang He, Chris Li Object detection:Our methods is based on faster-rcnn and extra classifier. (1) data processing: data equalization by deleting lots of examples in threee dominating classes (person, dog, and bird); adding extra data for classes with training data less than 1000; (2) COCO pre-train; (3) Iterative bounding box regression + multi-scale (trian\/test) + random flip images (train \/ test) (4) Multimodel ensemble: resnet-101 and inception-v3 (5) Extra classifier with 200 classes which helps to promote recall and refine the detection scores of ultimate boxes. [1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[J]. arXiv preprint arXiv:1512.03385, 2015. [2] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks[C]\/\/Advances in neural information processing systems. 2015: 91-99. Scene classification: We trained the model on Caffe[1]. An ensemble of Inception-V3[2] and Inception-V4[3]. We totally integrated four models. Top1 error on validation is 0.431 and top5 error is 0.129. The single model is modified on Inception-V3[2], the top1 error on validation is 0.434, top5 error is 0.133. [1] Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093. 2014. [2]C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna. Rethinking the inception architecture for computer vision. arXiv preprint arXiv:1512.00567, 2015. [3] C.Szegedy,S.Ioffe,V.Vanhoucke. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. arXiv preprint arXiv:1602.07261, 2016. Scene parsing: We trained 3 models on modified deeplab[1] (inception-v3, resnet-101, resnet-152) and only used the ADEChallengeData2016[2] data. Multi-scale \\ image crop \\ image fliping \\ contrast transformation are used for data augmentation and decseCRF is used as post-processing to refine object boundaries. On validation with combining 3 models, witch achieved 0.3966 mIoU and 0.7924 pixel-accuracy. [1] L. Chen, G. Papandreou, I. K.; Murphy, K.; and Yuille, A. L. 2016. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. In arXiv preprint arXiv:1606.00915. [2] B. Zhou, H. Zhao, X. P. S. F. A. B., and Torralba, A. 2016. Semantic understanding of scenes through the ade20k dataset. In arXiv preprint arXiv:1608.05442. Object detection from video: Our methods is based on faster-rcnn and extra classifier. We train Faster-RCNN based on RES-101 with the provided training data. We also train extra classifier with 30 classes which helps to promote recall and refine the detection scores of ultimate boxes. isia_ICT Xinhan Song, Institute of Computing Technology Chengpeng Chen, Institute of Computing Technology Shuqiang jiang, Institute of Computing Technology For convenience, we use the 4 provided models as our basic models, which are used for the following fine-tuning or networks adaptation. Besides, considering the non-uniform and the tremendous image number of the Challenge Dataset, we only use the Standard Dataset for all the following steps. First, we fuse these models with average strategy as the baseline. And then, we add a SPP layer to VGG16 and ResNet152 perspectively to enable the models to be feed with images with larger scale. After fine-tuning the models, we also fuse them with average strategy, and we only submit the result of the size 288. we also perform spectral clustering on the confusion matrix extracted from validation data to get 20 clusters, which means that 365 classes are separated into 20 clusters mainly dependent on their co-relationship. To classify the classes in the same cluster more precisely, we train an extra classifier within each cluster, which is implemented by fine-tuning the networks with all the layers fixed except for fc8 layer and combining them into a network at last. D. Yoo, S. Park, J. Lee and I. Kweon. \u201CMulti-scale pyramid pooling for deep convolutional representation\u201D. In CVPR Workshop 2015 NUIST Jing Yang, Hui Shuai, Zhengbo Yu, Rongrong Fan, Qiang Ma, Qingshan Liu, Jiankang Deng 1.inception v2 [1] is used in the VID task, which is almost real time with GPU. 2.cascaded region regression is used to detect and track different instances. 3.context inference between instances within each video 4.online detector and tracker update to improve recall [1]Szegedy, Christian, et al. \"Going deeper with convolutions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [2]Ren, Shaoqing, et al. \"Faster R-CNN: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015. [3]Dai, Jifeng, et al. \"R-FCN: Object Detection via Region-based Fully Convolutional Networks.\" arXiv preprint arXiv:1605.06409 (2016). Viz Insight Biplab Ch Das, Samsung R&D Institute Bangalore Shreyash Pandey, Samsung R&D Institute Bangalore Ensembling approaches have been known to outperform individual classifiers on standard classification tasks [No free Lunch Theorem :)] In our approach we trained state of the art classifiers including variations of: 1.ResNet 2.VGGNet 3.AlexNet 4.SqueezeNet 5.GoogleNet Each of these classifiers were trained on different views of the provided places 2 challenge data. Multiple Deep Metaclassifiers were trained on the confidence of the labels predicted by above classifiers successfully accomplishing a non linear ensemble, where the weights of the neural network are in a way to maximize the accuracy of scene recognition. To impose further consistency between objects and scenes, a state of art classifier trained on imagenet was adapted to places via a zero shot learning approach. We did not use any external data for training the classifiers. However we balanced the data to make the classifiers get unbiased results. So some of the data remained unused. Faceall-BUPT Xuankun HUANG, BUPT, CHINA Jiangqi ZHANG, BUPT, CHINA Zhiqun HE, BUPT, CHINA Junfei ZHUANG, BUPT, CHINA Zesang HUANG, BUPT, CHINA Yongqiang Yao, BUPT, CHINA Kun HU, BUPT, CHINA Fengye XIONG, BUPT, CHINA Hongliang BAI, Beijing Faceall co., LTD Wenjian FENG, Beijing Faceall co., LTD Yuan DONG, BUPT, CHINA # Classification\/Localization We trained the ResNet-101, ResNet-152 and Inception-v3 for object classification. Multi-view testing and models ensemble is utilized to generate the final classification results. For localization task, we trained a Region Proposal Network to generate proposals of each image, and we fine-tuned two models with object-level annotations of 1,000 classes. Moreover, a background class is added into the network. Then test images are segmented into 300 regions by RPN and these regions are classified by the fine-tuned model into one of 1,001 classes. And the final bounding box is generated by merging the bounding rectangle of three regions. # Object detection We utilize faster-rcnn with the publicly available resnet-101. Other than the baseline, we adopt multi-scale roi to obtain features containing richer context information. For testing, we use 3 scales and merge these results using the simple strategy introduced last year. No validation data is used for training, and flipped images are used in only a third of the training epochs. # Object detection from video We use Faster R-CNN with Resnet-101 to do this as in the object detection task. One fifth of the images are tested with 2 scales. No tracking techniques are used because of some mishaps. # Scene classification We trained a single Inception-v3 network with multi-scale and tested with multi-view of 150 crops. On validation the top-5 error is about 14.56%. # Scene parsing We trained 6 models with net structure inspired by fcn8s and dilatedNet with 3 scales(256,384,512). Then we test with flipped images using pre-trained fcn8s and dilatedNet. The pixel-wise accuracy is 76.94％ and mean of the class-wise IoU is 0.3552. OceanVision Zhibin Yu Ocean University of China Chao Wang Ocean University of China ZiQiang Zheng Ocean University of China Haiyong Zheng Ocean University of China Our homepage: http:\/\/vision.ouc.edu.cn\/~zhenghaiyong\/ We are interesting in scene classification and we aim to build a net for this problem. ABTEST Ankan Bansal We have used a 22 layer GoogleNet [1] model to classify scenes. The model was trained on the LSUN [2] dataset and then finetuned on the Places dataset fot 365 categories. We did not use any intelligent data selection techniques. The network is simply trained using all the available data without considering the data distribution for different classes. Before training on LSUN, this network was trained using the Places205 dataset. The model was trained till it saturated at around 85% (Top-1) accuracy on the validation dataset of the LSUN challenge. Then the model was fine-tuned on the 365 categories in the Places2 challenge. We did not use the trained models provided by the organisers to initialise our network. References: [1] Szegedy, Christian, et al. \"Going deeper with convolutions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [2] Yu, Fisher, et al. \"Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.\" arXiv preprint arXiv:1506.03365 (2015). Vladimir Iglovikov Vladimir Iglovikov Hardware: Nvidia Titan X Software: Keras with Theano backend Time spent: 5 days All models trained on 128x128 resized from \"small 256x256\" dataset. [1] Modiffied VGG16 => validation top5 error => 0.36 [2] Modified VGG19 => validation top5 error => 0.36 [3] Modified Resnet 50 => validation top5 error => 0.46 [4] Average of [1] and [2] => validation top5 error 0.35 Main changes: Relu => Elu Optimizer => Adam Batch Normalization added to VGG16 and VGG19 scnu407 Li Shiqi South China Normal University Zheng Weiping South China Normal University Wu Jinhui South China Normal University We believe that the spatial relationships between objects in the image is a kind of time-series data. Therefore, we first use VGG16 to extract the features of the image, then add 4 LSTM layer in the back, four LSTM layer representing the four directions of the scanning feature map. [top] ","flair":"null\tnull"}
{"author":"zergling103","created":"Thu Nov 17 23:17:25 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1603.05631 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1603 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Xiaolong Wang Abhinav Gupta Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Generative Image Modeling using Style and Structure Adversarial Networks Authors: Xiaolong Wang, Abhinav Gupta (Submitted on 17 Mar 2016 (v1), last revised 26 Jul 2016 (this version, v2)) Abstract: Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1603.05631 [cs.CV]   (or arXiv:1603.05631v2 [cs.CV] for this version) Submission history From: Xiaolong Wang [view email] [v1] Thu, 17 Mar 2016 19:33:20 GMT (2102kb,D) [v2] Tue, 26 Jul 2016 03:54:23 GMT (2103kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"acornalert","created":"Tue Nov 08 15:19:44 EST 2016","text":"This is a real-time map of tweets, classified with Dracula.js, updated every minute. Dracula.js tries to determine if the addition of a :D or D: emoticon substantially changes the mood of the tweet. Dracula is an LSTM-based sequence tagger which operates on character embeddings. Find out more about how it works » or try a live demo »","flair":"four\tProject"}
{"author":"MrAyThree","created":"Sun Nov 27 11:17:32 EST 2016","text":"Hi All, \n\nI am fairly new to Machine Learning, and I was hoping I could get some tips for improvement here. I currently have an AI set up that learns how to play the classic game of snake. \n\nMy problem is that the snake gets caught up moving in a particular direction initially, and ignores another direction. This means that it is able to find the food in it's path using 3 directions (normally: left, right, and down) and not 4, meaning that any food that spawns above the snake never gets found. \n\nI would say the snake performs well in 1 out of 20 random starts. Which is very frustrating. With a smaller NN that does not take into acount the position of the snake's body and tail, it performs much better, but then is limited by its body size later in the game. \n\nAny suggestions for improvement, would be greatly appreciated. \n\nhttps:\/\/github.com\/mraythree\/SnakeGame","flair":"four\tProject"}
{"author":"jacobgil","created":"Mon Oct 24 14:01:31 EDT 2016","text":" My blog on Computer Vision and Machine learning A Computer Vision and Machine learning blog Jacob Gildenblat I'm a Computer Vision and Machine learning developer. View my complete profile Pages Github Linkedin Twitter Monday, October 24, 2016 Visualizations for regressing wheel steering angles in self driving cars with Keras Here is all the code and a trained model Eri Rubin also participated in this project. This post is about understanding how a self driving deep learning network decides to steer the wheel. NVIDIA published a very interesting paper, that describes how a deep learning network can be trained to steer a wheel, given a 200x66 RGB image from the front of a car. This repository shared a Tensorflow implementation of the network described in the paper, and (thankfully!) a dataset of image \/ steering angles collected from a human driving a car. The dataset is quite small, and there are much larger datasets available like in the udacity challenge. However it is great for quickly experimenting with these kind of networks, and visualizing when the network is overfitting is also interesting. I ported the code to Keras, trained a (very over-fitting) network based on the NVIDIA paper, and made visualizations. I think that if eventually this kind of a network will find use in a real world self driving car, being able to debug it and understand its output will be crucial. Otherwise the first time the network decides to make a very wrong turn, critics will say that this is just a black box we don\u2019t understand, and it should be replaced! First attempt : Treating the network as a black box - occlusion maps The first thing we will try, won\u2019t require any knowledge about the network, and in fact we won\u2019t peak inside the network, just look at the output. We\u201Dl create an occlusion map for a given image, where we take many windows in the image, mask them out, run the network, and see how the regressed angle changed. If the angle changed a lot - that window contains information that was important for the network decision. We then can assign each window a score based on how the angle changed! We need to take many windows, with different sizes - since we don\u2019t know in advance the sizes of important features in the image. Now we can make nice effects like filtering the occlusion map, and displaying the focused area on top of a blurred image: Some problems with this - Its expensive to create the visualization since we need many sliding windows, and it is possible that just masking out the windows created artificial features like sharp angles that were used by the network. Also - this tells us which areas were important for the network, but it doesn\u2019t give us any insight on why. Can we do better? Second attempt - peaking at the conv layers output features with hypercolumns So we want to understand what kind of features the network saw in the image, and how it used them for its final decision. Lets use a heuristic - take the outputs of the convolutional layers, resize them to the input image size, and aggregate them. The collection of these outputs are called hypercolumns, and here is a good blog post about getting them with Keras. One way of aggregating them is by just multiplying them - so pixels that had high activation in all layers will get a high score. We will take the average output image from each layer, normalize it, and multiply these values from wanted layers. In the NVIDIA model, the output from the last convolutional layer is a 18x1 image. If we peak only at that layer, we basically get a importance map for columns of the image: Anyway, this is quite naive and completely ignores the fully connected layers, and the fact that in certain situations some outputs are much more important than other outputs, but its a heuristic. Here is a video with visualuzations done by both occlusion mapping and hypercolumns: Third attempt - Getting there - class activation maps using gradients (The above image shows pixels that contribute to steering right) Class activation maps are a technique to visualize the importance of image pixels to the final output of the network. Basically you take the output of the last convolutional layer, you take a spatial average of that (global average pooling), and you feed that into a softmax for classification. Now you can look at the softmax weights used to give a category score - large weights mean important features - and multiply them by the corresponding conv outputs. Relative to the rest of the stuff we tried here - this technique is great. It gives us an insight of how exactly each pixel was used in the overall decision process. However this technique requires a specific network architecture - conv layers + GAP, so existing networks with fully connected layers, like the nvidia model, can\u2019t be used as is. We could just train a new model with conv layers + GAP (I actually did that), however we really want the fully connected layers here. They enable the network to reason spatially about the image - If it finds interesting features in the left part of the image - perhaps that road is blocked? This paper solves the issue, and generalizes class activation maps. To get the importance of images in the conv outputs, you use back propagation - you take the gradient of the target output with respect to the pixels in conv output images. Conv output images that are important for the final classification decision, will contain a lot of positive gradients. So to assign them an importance value - we can just take a spatial average of the gradients in each conv output image (global average pooling again). I wrote some Keras code to try this out for classification networks. So lets adapt this for the steering angle regression. We can\u2019t just always take gradient of the output, since now when the gradient is high, it isn\u2019t contributing to a certain category like in the classification case, but instead to a positive steering angle. And maybe the actual steering angle was negative. Lets look at the gradient of the regressed angle with respect to some pixel in some output image - If the gradient is very positive, that means that the pixel contributes to enlarging the steering angle - steering right. If the gradient is very negative, the pixel contributes to steering left. If the gradient is very small, the pixel contributes to not steering at all. We can divide the angles into ranges - if the actual output angle was large, we can peak at the image features that contributed to a positive steering angle, etc. If the angle is small, we will just take the inverse of the steering angle as our target - since then pixels that contribute to small angles will get large gradients. def grad_cam_loss(x, angle): if angle > 5.0 * scipy.pi \/ 180.0: return x elif angle < -5.0 * scipy.pi \/ 180.0: return -x else: return tf.inv(x) * np.sign(angle) Lets look at an example. For the same image, we could target pixels that contribute to steering right: And we could also target pixels that contribute to steering to the center: Posted by Jacob Gildenblat at 10:13 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest 1 comment: Limbani BhavikOctober 26, 2016 at 4:04 AM Self-Driving cars 2016 This topics says about self-driving cars in 2016.well first lets know the word self-driving cars tells us about. Self-driving cars means Autonomous cars. Now this means that there is no need of driver or to drive a car. Self driving car change world ReplyDelete Add comment Load more... Older Post Home Subscribe to: Post Comments (Atom) Blog Archive ▼  2016 (3) ▼  October (1) Visualizations for regressing wheel steering angle... ►  August (1) ►  March (1) ►  2015 (2) ►  April (2) ►  2014 (3) ►  December (1) ►  May (1) ►  April (1) Picture Window template. Powered by Blogger. ","flair":"four\tProject"}
{"author":"gpml","created":"Tue Nov 08 02:15:09 EST 2016","text":"PriceMinister is one of the leading marketplace of E-commerce in France, and is part of the Rakuten group, leader in Japanese market.  The goal of this challenge is to predict if a review of a user","flair":"two\tNews"}
{"author":"carlthome","created":"Mon Oct 24 18:22:26 EDT 2016","text":"The tensorflow-haskell package provides Haskell bindings to TensorFlow. This is not an official Google product. As an expedient we use docker for building. Once you have docker working, the following commands will compile and run the tests. There is also a demo application: The following instructions were verified with Mac OS X El Capitan. Note: you may need to upgrade your version of Clang if you get an error like the following: In that case you can just upgrade XCode and then run to get the new version of the compiler.","flair":"four\tProject"}
{"author":"lankks","created":"Mon Oct 10 20:03:46 EDT 2016","text":"I've been working on creating a binary text classifier for diagnosing language impaired children using the CHILDES corpus. I've been using the package polyssifier (https:\/\/github.com\/alvarouc\/polyssifier) to create and test the data and recently after adding a few extra features most of my classifiers have been predicting the correct result 100% of the time.\n\nSurely this can't be right? What can I do to check the validity of this result?\n\nhttp:\/\/imgur.com\/a\/reagU","flair":"four\tProject"}
{"author":"thesameoldstories","created":"Fri Nov 18 14:03:28 EST 2016","text":" Tryolabs Blog Tryolabs Subscribe Hire us Search Read time: 8 minutes The 10 main takeaways from MLconf SF Alan Fri, Nov 18, 2016 in Machine Learning Conference MLconf Tryolabs We recently sponsored and attended the MLconf in San Francisco. It was an awesome experience. Congratulations to all the awesome speakers, other sponsors and organizers! During the presentations, we gained a lot of insight from people who are using Machine Learning in the industry. In this post, we attempt to share some of what we learned. 1. It\u2019s (still) not all about Deep Learning It is true that Deep Learning (DL) has had real success in a variety of tasks like image and speech recognition, machine translation, games, and many others. Has DL displaced every other existing algorithm in the industry? Not so much. Classical Machine Learning (ML) is still very much used. In practice, algorithms like Gradient Boosting, Logistic Regression or SVM do work very well; often well enough that the industry is using them in plenty of cases. For example, Amazon uses Gradient Boosting to provide search results, and Quora openly discloses it uses several algorithms among which are Logistic Regression and Random Forests. In some verticals like healthcare, models cannot be treated as a black box since the human interpretability factor plays an important role. We thought this may hinder the use of Deep Learning for some applications, but Brian Lucena from Metis showed a set of techniques that allow us to get valuable insights even from black box models. 2. Choose the right problem to solve, with the right metric You want to do ML at your company, and after a brainstorming you find a gazillion places where it can be applied. Several talks highlighted the importance of choosing the right problem, not from the perspective of an awesome engineering challenge, but as the problem that provides the most business value to the company. It is often the case where even in big companies, how ML can bring the most value is not obvious. Eloquently presented by Elena Grewal (Data Science Manager at Airbnb) in her talk, the target metric of your ML algorithm should not be precision\/recall, but a business outcome. Evaluating the up front business impact of the ML solution can help you decide whether you are on the right track. 3. Fine tuning your models is 5% of a project Playing with your algorithms to get an extra 1% boost in accuracy is only a minuscule part of any Machine Learning project. Don\u2019t get deluded into thinking investing too much time into this is productive; there are probably other better uses of your time than playing with models and hyperparameters. You may invest into building a good unified data pipeline for gathering the data, do better data preprocessing, think about what to do with missing data. Also need to consider the tradeoff with the computing performance of the algorithms, since real world users may be sensitive to delays. You need to think about how to measure performance in production, evaluate its evolution and setup alerts, and also how to update models once in production without downtime. Machine Learning in a real world system is not that easy, y\u2019know? 4. Ensembles almost always work better An ensemble is a combination of models, so that we combine a series of k learned models, M1, M2,\u2026 , Mk, with the aim of creating an improved model M*, with better accuracy. For example, you may average the class probabilities output by the different classifiers or do a weighted sum. In Stanford\u2019s course on Convolutional Neural Networks (CS231n), Andrej Karpathy jokingly said that using an ensemble in the Deep Learning context always gives you an extra 2% boost in accuracy. Jokes aside, several talks highlighted that plenty of the models being used in the industry are as a matter of fact ensemble methods. We see this same trend in the top performing entries of many recent Kaggle competitions. Ensembles seem to be here to stay. 5. The trend towards personalization We are not talking just about targeting specific content for a user (such as ads), but the trend we are seeing in the industry is towards complete personalization: the world users see (be it custom feed, home page, etc.) is built specifically for them. This was exemplified in several talks, such as Stephanie deWet from Pinterest talking about the complete personalization of the homepage (mixture of recommendations from similar users and own tastes) and Guy Lebanon from Netflix talking about the personalization of image assets (promotional posters, screen captures, etc). 6. Manual curation of content is still used in practice If you use Gmail (or Inbox), you probably have already seen Google\u2019s Smart Reply feature. It sure is an impressive work, nowadays being used in more than 10% of all mobile replies. Anjuli Kannan (Research Engineer at Google) provided some insights into how this feature works. The replies are fully learned from the data, with no hand-crafted rules or features. It consists of a sequence to sequence model comprised of two Neural Networks that have been trained end to end and return a distribution of the probability over the words that make up the possible replies. One question we were asking ourselves was: how do they make sure the replies generated by such approach are of enough quality to display to the users? The networks may learn bad words, grammar mistakes, informal language, different tone than the user, etc. Just restricting the vocabulary in the replies is not sufficient (ie. \u201Cyour the best\u201D is bad grammar, but all the words are correct). The solution is to restrict to a fixed set of valid or high quality responses, derived automatically from data (using semi-supervised learning). It looks like we still have a long way before we can have an accurate and completely automated system. See the paper on Smart Reply. Javier standing in our booth, next to Google Research. 7. Avoid the curse of complexity When you have a big company in which multiple teams are trying to accomplish data science projects or solving non-trivial Machine Learning problems, it is very important to avoid overhead. Nikhil Garg (Engineering Manager at Quora) introduced the notion of curse of complexity, which occurs when different teams are using different pipelines, different ways to gather the data, and strongly coupled logic. This makes it different for new projects to take off, since there is not much reusability and they become too costly. Effort should be made to reuse tools and assets between the teams. The key is building a good Machine Learning platform (a collection of systems to optimize every pipeline), and its infrastructure. 8. Learn the best practices from established players In the last couple of years we have seen a shift from the corporate, secretive culture, to a more open and collaborative one. People no longer build their own work from scratch, but base their work on what others have built. This lets everyone move much quicker than before. During MLconf, people like Josh Wills (Head of Data Engineering at Slack) were not afraid to say they are standing on the shoulders of giants: leveraging what big players like Facebook, Netflix, Airbnb have done to learn the best practices, and use the best tools. Also, getting to work with people who have been part of these companies allows a culture shift which would be difficult to attain otherwise. Do the same; don\u2019t reinvent the wheel. And this takes us to\u2026 9. Everybody is using open source Data science has been democratized. Nowadays, there are a plethora of open source tools available to build upon. From data management to model building, some of these tools have been open sourced by big players (e.g. TensorFlow by Google, Airflow by Airbnb, etc.). This really empowers anyone to build powerful ML platforms. We learned that companies are not afraid to admit they are openly using and embracing open source. Be it TensorFlow, XGBoost, scikit-learn, and many others, it was clear from MLconf that even big corporations choose to use these tools instead of investing resources on building their own solutions. 10. Make sure you have support from the executives When you are not Google or Facebook, it\u2019s more difficult to invest into data, as there\u2019s more pressure to deliver results. Google already knows investing in this kind of projects works for them, but this is not necessarily true for other companies. There is a big overhead in starting Machine Learning projects. These projects usually take about 6 months to start delivering valuable results, so you must first convince the executives in the company that it is worth spending time and resources into the quest. This may include delivering a presentation that shows the business impact the proposal will have, and outlining the resources that need to be allocated. Conclusion All in all, the MLconf was an outstanding experience for us and we\u2019re very excited to take all these learnings into our projects. If you attended and feel that we missed some important point don\u2019t hesitate to share it with us in the comments below. I would like to thanks my colleagues Agustín Azzinnari and Javier Rey for collaborating with this post. Please enable JavaScript to view the Comments powered by Disqus. Comments powered by Disqus What to Read Next Machine Learning 101 Meetups Tryolabs is Sponsoring MLconf in San Francisco! Chatbots and automated online assistants Code Tips, Tricks, and Freebies. Delivered Monthly. Signup to our newsletter. Error Success! Sending... No spam, ever. We'll never share your email address and you can opt out at any time. Hire us ESTIMATED BUDGET 15k - 50k 50k - 75k 75k - 100k + 100k Subscribe to receive news and blog updates. SUBMIT Sending... Message successfully sent! There was an error :( ABOUT About us What we do Our Team OUR WORK Clients Products Brochure COMMUNITY Blog Open Source Careers CONTACT US Phone: (1) 415 429 3887 UY Phone: (598) 2716 8997 hello@tryolabs.com OFFICES US: 156 2nd Street, SF. UY: Rambla Gandhi 655\/701, MVD. SOCIAL Tryolabs © 2016. All rights reserved. ","flair":"two\tNews"}
{"author":"DanielWaterworth","created":"Fri Sep 30 04:24:25 EDT 2016","text":"I like to watch long-term technology and business trends and watch as they shape the products and services that I get to use and to write about. As I was preparing to write today\u2019s post, three such trends came to mind: As the industry pushes forward in accord with these trends, a couple of interesting challenges have surfaced over the past decade or so. Again, here\u2019s a quick list (yes, I do think in bullet points): The GPU (Graphics Processing Unit) was born of these trends, and addresses many of the challenges! Processors have reached the upper bound on clock rates, but Moore\u2019s Law gives designers more and more transistors to work with. Those transistors can be used to add more cache and more memory to a traditional architecture, but the von Neumann Bottleneck limits the value of doing so. On the other hand, we now have large markets for specialized hardware (gaming comes to mind as one of the early drivers for GPU consumption). Putting all of this together, the GPU scales out (more processors and parallel banks of memory) instead of up (faster processors and bottlenecked memory). Net-net: the GPU is an effective way to use lots of transistors to provide massive amounts of compute power! With all of this as background, I would like to tell you about the newest EC2 instance type, the P2. These instances were designed to chew through tough, large-scale machine learning, deep learning, computational fluid dynamics (CFD), seismic analysis, molecular modeling, genomics, and computational finance workloads. New P2 Instance Type This new instance type incorporates up to 8 NVIDIA Tesla K80 Accelerators, each running a pair of NVIDIA GK210 GPUs. Each GPU provides 12 GiB of memory (accessible via 240 GB\/second of memory bandwidth), and 2,496 parallel processing cores. They also include ECC memory protection, allowing them to fix single-bit errors and to detect double-bit errors. The combination of ECC memory protection and double precision floating point operations makes these instances a great fit for all of the workloads that I mentioned above. Here are the instance specs: All of the instances are powered by an AWS-Specific version of Intel\u2019s Broadwell processor, running at 2.7 GHz. The p2.16xlarge gives you control over C-states and P-states, and can turbo boost up to 3.0 GHz when running on 1 or 2 cores. The GPUs support CUDA 7.5 and above, OpenCL 1.2, and the GPU Compute APIs. The GPUs on the p2.8xlarge and the p2.16xlarge are connected via a common PCI fabric. This allows for low-latency, peer to peer GPU to GPU transfers. All of the instances make use of our new Enhanced Network Adapter (ENA \u2013 read Elastic Network Adapter \u2013 High Performance Network Interface for Amazon EC2 to learn more) and can, per the table above, support up to 20 Gbps of low-latency networking when used within a Placement Group. Having a powerful multi-vCPU processor and multiple, well-connected GPUs on a single instance, along with low-latency access to other instances with the same features creates a very impressive hierarchy for scale-out processing: P2 instances are VPC only, require the use of 64-bit, HVM-style, EBS-backed AMIs, and you can launch them today in the US East (Northern Virginia), US West (Oregon), and Europe (Ireland) regions as On-Demand Instances, Spot Instances, Reserved Instances, or Dedicated Hosts. Here\u2019s how I installed the NVIDIA drivers and the CUDA toolkit on my P2 instance, after first creating, formatting, attaching, and mounting (to ) an EBS volume that had enough room for the CUDA toolkit and the associated samples (10 GiB is more than enough): Note that and  are interactive programs; you need to accept the license agreements, choose some options, and enter some paths. Here\u2019s how I set up the CUDA toolkit and the samples when I ran : P2 and OpenCL in Action With everything set up, I took this Gist and compiled it on a p2.8xlarge instance: As you can see, I have a ridiculous amount of compute power available at my fingertips! New Deep Learning AMI As I said at the beginning, these instances are a great fit for machine learning, deep learning, computational fluid dynamics (CFD), seismic analysis, molecular modeling, genomics, and computational finance workloads. In order to help you to make great use of one or more P2 instances, we are launching a  Deep Learning AMI today. Deep learning has the potential to generate predictions (also known as scores or inferences) that are more reliable than those produced by less sophisticated machine learning, at the cost of a most complex and more computationally intensive training process. Fortunately, the newest generations of deep learning tools are able to distribute the training work across multiple GPUs on a single instance as well as across multiple instances each containing multiple GPUs. The new AMI contains the following frameworks, each installed, configured, and tested against the popular MNIST database: MXNet \u2013 This is a flexible, portable, and efficient library for deep learning. It supports declarative and imperative programming models across a wide variety of programming languages including C++, Python, R, Scala, Julia, Matlab, and JavaScript. Caffe \u2013 This deep learning framework was designed with  expression, speed, and modularity in mind. It was developed at the Berkeley Vision and Learning Center (BVLC) with assistance from many community contributors. Theano \u2013 This Python library allows you define, optimize, and evaluate mathematical expressions that involve multi-dimensional arrays. TensorFlow™ \u2013 This is an open source library for numerical calculation using data flow graphs (each node in the graph represents a mathematical operation; each edge represents multidimensional data communicated between them). Torch \u2013 This is a GPU-oriented scientific computing framework with support for machine learning algorithms, all accessible via LuaJIT. Consult the README file in to learn more about these frameworks. AMIs from NVIDIA You may also find the following AMIs to be of interest:","flair":"null\tnull"}
{"author":"alexbotev","created":"Thu Nov 17 23:13:45 EST 2016","text":"TLDR : Discussion do we need something like LLVM-IR in Machine learning. \n\nI personally think that we do, and the main reason is that all of the frameworks are doing more or less the same optimizations, which 95% does not depend on the hardware and can be done on the abstract graph (e.g. you can't for instance decide whether you use cuDNN or not without knowing the hardware). Rather than all of the doing their own, literally all users can benefit if this is a unified representation, and people optimizing it contribute together to the same project. For instance Theano's great, eve huge, collections for RNNs would directly help everyone else. MXNet static memory allocation is another example of something that any frontend\/framework would benefit from. \n\nChoice of language - I think for such, similar to Tf should be C++\/C. Why? This optimizations are more or less equivalent to a full blown compiler (sometimes maybe even harder). Traversing numerous times large graph - you care about speed. Also C++\/C can have external APIs to any other language, so it is easily portable. \n\nSo, I've started something like this here: https:\/\/github.com\/Metadiff\/graph-ir. It is fairly work in progress, however I hope that anyone who wishes to write their own framework with a backend can easily reuse it, especially cause it will give you autodiff for free. A few nice features, which I like, are automatic \"dynamic\" shape inference - e.g. even if you don't know a certain shape size, you can still reason if a == b. Also, I intend to add more meta data automatic propagation. One of the main reasons for this is to give you \"compile\" time guarantee that the graph is correct. This means no more cryptic messages from Theano and Tf because you did not provide something correctly, you get them exactly where you make them in the source code. Essentially guaranteeing no run-time errors except if you get overflow\/underflows (nans\/infs). Also, all frameworks have run time checks before each OP that inputs are correct. With this you ONLY need to check that the inputs satisfy their constraints and you are guaranteed that the whole function will execute correctly start to end. \n\nThere are however, some design aspects, which more or less I'm not sure what is a good choice. I was hoping if there are people interested in discussing design choices for this as any advice or suggestion might be very helpful. I hoped that I would have this working earlier, but in fact for a good design it takes me a lot more time to write it up. I also know now with so many frameworks, and given that I'm not backed by any company, if the community does not like this idea, it is doomed to fail. If that is the case than maybe the project will help enthusiasts interested in the inner workings of these frameworks to learn about these things. ","flair":"one\tDiscussion"}
{"author":"JohnBrady2016","created":"Thu Nov 17 03:05:00 EST 2016","text":" Under review as a conference paper at ICLR 2017 QUASI-RECURRENT NEURAL NETWORKS James Bradbury∗, Stephen Merity∗, Caiming Xiong & Richard Socher Salesforce Research Palo Alto, California {james.bradbury,smerity,cxiong,rsocher}@salesforce.com ABSTRACT Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep\u2019s computation on the previous timestep\u2019s out- put limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural se- quence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in par- allel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viabil- ity of QRNNs as a basic building block for a variety of sequence tasks. 1 INTRODUCTION Recurrent neural networks (RNNs), including gated variants such as the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) have become the standard model architecture for deep learning approaches to sequence modeling tasks. RNNs repeatedly apply a function with trainable parameters to a hidden state. Recurrent layers can also be stacked, increasing network depth, repre- sentational power and often accuracy. RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word- and character-level language modeling (Zaremba et al., 2014). RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016). Unfortunately standard RNNs, in- cluding LSTMs, are limited in their capability to handle tasks involving very long sequences, such as document classification or character-level machine translation, as the computation of features or states for different parts of the document cannot occur in parallel. Convolutional neural networks (CNNs) (Krizhevsky et al., 2012), though more popular on tasks in- volving image data, have also been applied to sequence encoding tasks (Zhang et al., 2015). Such models apply time-invariant filter functions in parallel to windows along the input sequence. CNNs possess several advantages over recurrent models, including increased parallelism and better scal- ing to long sequences such as those often seen with character-level language data. Convolutional models for sequence processing have been more successful when combined with RNN layers in a hybrid architecture (Lee et al., 2016), because traditional max- and average-pooling approaches to combining convolutional features across timesteps assume time invariance and hence cannot make full use of large-scale sequence order information. We present quasi-recurrent neural networks for neural sequence modeling. QRNNs address both drawbacks of standard models: like CNNs, QRNNs allow for parallel computation across both timestep and minibatch dimensions, enabling high throughput and good scaling to long sequences. Like RNNs, QRNNs allow the output to depend on the overall order of elements in the sequence. We describe QRNN variants tailored to several natural language tasks, including document-level sentiment classification, language modeling, and character-level machine translation. These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time. ∗Equal contribution 1 ar X iv :1 61 1. 01 57 6v 1 [ cs .N E ] 5 N ov 2 01 6 Under review as a conference paper at ICLR 2017 LSTM CNN LSTM\/Linear Linear LSTM\/Linear Linear fo-Pool Convolution fo-Pool Convolution Max-Pool Convolution Max-Pool Convolution QRNN Figure 1: Block diagrams showing the computation structure of the QRNN compared with typical LSTM and CNN architectures. Red signifies convolutions or matrix multiplications; a continuous block means that those computations can proceed in parallel. Blue signifies parameterless functions that operate in parallel along the channel\/feature dimension. LSTMs can be factored into (red) linear blocks and (blue) elementwise blocks, but computation at each timestep still depends on the results from the previous timestep. 2 MODEL Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions. Given an input sequence X ∈ RT×n of T n-dimensional vectors x1 . . .xT , the convolutional sub- component of a QRNN performs convolutions in the timestep dimension with a bank of m filters, producing a sequence Z ∈ RT×m of m-dimensional candidate vectors zt. In order to be useful for tasks that include prediction of the next token, the filters must not allow the computation for any given timestep to access information from future timesteps. That is, with filters of width k, each zt depends only on xt−k+1 through xt. This concept, known as a masked convolution (van den Oord et al., 2016), is implemented by padding the input to the left by the convolution\u2019s filter size minus one. We apply additional convolutions with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function. While the candidate vectors are passed through a tanh nonlinearity, the gates use an elementwise sigmoid. If the pooling function requires a forget gate ft and an output gate ot at each timestep, the full set of computations in the convolutional component is then: Z = tanh(Wz ∗X) F = σ(Wf ∗X) O = σ(Wo ∗X), (1) where Wz ,Wf , and Wo, each in Rk×n×m, are the convolutional filter banks and ∗ denotes a masked convolution along the timestep dimension. Note that if the filter width is 2, these equations reduce to the LSTM-like zt = tanh(W 1 zxt−1 +W 2 zxt) ft = σ(W 1 fxt−1 +W 2 fxt) ot = σ(W 1 oxt−1 +W 2 oxt). (2) Convolution filters of larger width effectively compute higher n-gram features at each timestep; thus larger widths are especially important for character-level tasks. Suitable functions for the pooling subcomponent can be constructed from the familiar elementwise gates of the traditional LSTM cell. We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which Balduzzi & Ghifary (2016) term \u201Cdynamic average pooling\u201D, uses only a forget gate: ht = ft � ht−1 + (1− ft)� zt, (3) 2 Under review as a conference paper at ICLR 2017 where � denotes elementwise multiplication. The function may also include an output gate: ct = ft � ct−1 + (1− ft)� zt ht = ot � ct. (4) Or the recurrence relation may include an independent input and forget gate: ct = ft � ct−1 + it � zt ht = ot � ct. (5) We term these three options f -pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize h or c to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time. A single QRNN layer thus performs an input-dependent pooling, followed by a gated linear combi- nation of convolutional features. As with convolutional neural networks, two or more QRNN layers should be stacked to create a model with the capacity to approximate more complex functions. 2.1 VARIANTS Motivated by several common natural language tasks, and the long history of work on related ar- chitectures, we introduce several extensions to the stacked QRNN described above. Notably, many extensions to both recurrent and convolutional models can be applied directly to the QRNN as it combines elements of both model types. Regularization An important extension to the stacked QRNN is a robust regularization scheme inspired by recent work in regularizing LSTMs. The need for an effective regularization method for LSTMs, and dropout\u2019s relative lack of efficacy when applied to recurrent connections, led to the development of recurrent dropout schemes, in- cluding variational inference\u2013based dropout (Gal & Ghahramani, 2016) and zoneout (Krueger et al., 2016). These schemes extend dropout to the recurrent setting by taking advantage of the repeating structure of recurrent networks, providing more powerful and less destructive regularization. Variational inference\u2013based dropout locks the dropout mask used for the recurrent connections across timesteps, so a single RNN pass uses a single stochastic subset of the recurrent weights. Zoneout stochastically chooses a new subset of channels to \u201Czone out\u201D at each timestep; for these channels the network copies states from one timestep to the next without modification. As QRNNs lack recurrent weights, the variational inference approach does not apply. Thus we extended zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels. Conveniently, this is equivalent to stochastically setting a subset of the QRNN\u2019s f gate channels to 1, or applying dropout on 1− f : F = 1− σ(dropout(1−Wf ∗X)) (6) Thus the pooling function itself need not be modified at all. We note that when using an off-the- shelf dropout layer in this context, it is important to remove automatic rescaling functionality from the implementation if it is present. In many experiments, we also apply ordinary dropout between layers, including between word embeddings and the first QRNN layer. Densely-Connected Layers We can also extend the QRNN architecture using techniques intro- duced for convolutional networks. For sequence classification tasks, we found it helpful to use skip-connections between every QRNN layer, a technique termed \u201Cdense convolution\u201D by Huang et al. (2016). Where traditional feed-forward or convolutional networks have connections only be- tween subsequent layers, a \u201CDenseNet\u201D with L layers has feed-forward or convolutional connections between every pair of layers, for a total ofL(L−1). This can improve gradient flow and convergence properties, especially in deeper networks, although it requires a parameter count that is quadratic in the number of layers. When applying this technique to the QRNN, we include connections between the input embeddings and every QRNN layer and between every pair of QRNN layers. This is equivalent to concatenating 3 Under review as a conference paper at ICLR 2017 fo-Pool Convolution fo-Pool Convolution fo-Pool Linear f-Pool Linear Attention Output gates Linear Figure 2: The QRNN encoder\u2013decoder architecture used for machine translation experiments. each QRNN layer\u2019s input to its output along the channel dimension before feeding the state into the next layer. The output of the last layer alone is then used as the overall encoding result. Encoder\u2013Decoder Models To demonstrate the generality of QRNNs, we extend the model architec- ture to sequence-to-sequence tasks, such as machine translation, by using a QRNN as encoder and a modified QRNN, enhanced with attention, as decoder. The motivation for modifying the decoder is that simply feeding the last encoder hidden state (the output of the encoder\u2019s pooling layer) into the decoder\u2019s recurrent pooling layer, analogously to conventional recurrent encoder\u2013decoder architec- tures, would not allow the encoder state to affect the gate or update values that are provided to the decoder\u2019s pooling layer. This would substantially limit the representational power of the decoder. Instead, the output of each decoder QRNN layer\u2019s convolution functions is supplemented at every timestep with the final encoder hidden state. This is accomplished by adding the result of the convo- lution for layer ` (e.g., W`z ∗X`, in RT×m) with broadcasting to a linearly projected copy of layer `\u2019s last encoder state (e.g., V`zh̃ ` T , in Rm): Z` = tanh(W`z ∗X` +V`zh̃`T ) F` = σ(W`f ∗X` +V`f h̃`T ) O` = σ(W`o ∗X` +V`oh̃`T ), (7) where the tilde denotes that h̃ is an encoder variable. Encoder\u2013decoder models which operate on long sequences are made significantly more powerful with the addition of soft attention (Bahdanau et al., 2015), which removes the need for the entire input representation to fit into a fixed-length encoding vector. In our experiments, we computed an attentional sum of the encoder\u2019s last layer\u2019s hidden states. We used the dot products of these encoder hidden states with the decoder\u2019s last layer\u2019s un-gated hidden states, applying a softmax along the encoder timesteps, to weight the encoder states into an attentional sum kt for each decoder timestep. This context, and the decoder state, are then fed into a linear layer followed by the output gate: αst = softmax all s (cLt · h̃Ls ) kt = ∑ s αsth̃ L s hLt = ot � (Wkkt +WccLt ), (8) where L is the last layer. While the first step of this attention procedure is quadratic in the sequence length, in practice it takes significantly less computation time than the model\u2019s linear and convolutional layers due to the simple and highly parallel dot-product scoring function. 4 Under review as a conference paper at ICLR 2017 Model Time \/ Epoch (s) Test Acc (%) BSVM-bi (Wang & Manning, 2012) − 91.2 2 layer sequential BoW CNN (Johnson & Zhang, 2014) − 92.3 Ensemble of RNNs and NB-SVM (Mesnil et al., 2014) − 92.6 2-layer LSTM (Longpre et al., 2016) − 87.6 Residual 2-layer bi-LSTM (Longpre et al., 2016) − 90.1 Our models Deeply connected 4-layer LSTM (cuDNN optimized) 480 90.9 Deeply connected 4-layer QRNN 150 91.4 D.C. 4-layer QRNN with k = 4 160 91.1 Table 1: Accuracy comparison on the IMDb binary sentiment classification task. All of our models use 256 units per layer; all layers other than the first layer, whose filter width may vary, use filter width k = 2. Train times are reported on a single NVIDIA K40 GPU. We exclude semi-supervised models that conduct additional training on the unlabeled portion of the dataset. 3 EXPERIMENTS We evaluate the performance of the QRNN on three different natural language tasks: document-level sentiment classification, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of equal hidden size on all three tasks while dra- matically improving computation speed. Experiments were implemented in Chainer (Tokui et al.). 3.1 SENTIMENT CLASSIFICATION We evaluate the QRNN architecture on a popular document-level sentiment classification bench- mark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)). Our best performance on a held-out development set was achieved using a four-layer densely- connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014). Dropout of 0.3 was applied between layers, and we used L2 regularization of 4 × 10−6. Optimization was performed on minibatches of 24 examples using RMSprop (Tieleman & Hinton, 2012) with learning rate of 0.001, α = 0.9, and � = 10−8. Small batch sizes and long sequence lengths provide an ideal situation for demonstrating the QRNN\u2019s performance advantages over traditional recurrent architectures. We observed a speedup of 3.2x on IMDb train time per epoch compared to the optimized LSTM implementation provided in NVIDIA\u2019s cuDNN library. For specific batch sizes and sequence lengths, a 16x speed gain is possible. Figure 4 provides extensive speed comparisons. In Figure 3, we visualize the hidden state vectors cLt of the final QRNN layer on part of an example from the IMDb dataset. Even without any post-processing, changes in the hidden state are visible and interpretable in regards to the input. This is a consequence of the elementwise nature of the recurrent pooling function, which delays direct interaction between different channels of the hidden state until the computation of the next QRNN layer. 3.2 LANGUAGE MODELING We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction. The experi- ment uses a standard preprocessed version of the Penn Treebank (PTB) by Mikolov et al. (2010). We implemented a gated QRNN model with medium hidden size: 2 layers with 640 units in each layer. Both QRNN layers use a convolutional filter width k of two timesteps. While the \u201Cmedium\u201D models used in other work (Zaremba et al., 2014; Gal & Ghahramani, 2016) consist of 650 units in 5 Under review as a conference paper at ICLR 2017 Figure 3: Visualization of the final QRNN layer\u2019s hidden state vectors cLt in the IMDb task, with timesteps along the vertical axis. Colors denote neuron activations. After an initial positive statement \u201CThis movie is simply gorgeous\u201D (off graph at timestep 9), timestep 117 triggers a reset of most hidden states due to the phrase \u201Cnot exactly a bad story\u201D (soon after \u201Cmain weakness is its story\u201D). Only at timestep 158, after \u201CI recommend this movie to everyone, even if you\u2019ve never played the game\u201D, do the hidden units recover. each layer, it was more computationally convenient to use a multiple of 32. As the Penn Treebank is a relatively small dataset, preventing overfitting is of considerable importance and a major focus of recent research. It is not obvious in advance which of the many RNN regularization schemes would perform well when applied to the QRNN. Our tests showed encouraging results from zoneout applied to the QRNN\u2019s recurrent pooling layer, implemented as described in Section 2.1. The experimental settings largely followed the \u201Cmedium\u201D setup of Zaremba et al. (2014). Optimiza- tion was performed by stochastic gradient descent (SGD) without momentum. The learning rate was set at 1 for six epochs, then decayed by 0.95 for each subsequent epoch, for a total of 72 epochs. We additionally used L2 regularization of 2 × 10−4 and rescaled gradients with norm above 10. Zoneout was applied by performing dropout with ratio 0.1 on the forget gates of the QRNN, without rescaling the output of the dropout function. Batches consist of 20 examples, each 105 timesteps. Comparing our results on the gated QRNN with zoneout to the results of LSTMs with both ordinary and variational dropout in Table 2, we see that the QRNN is highly competitive. The QRNN without zoneout strongly outperforms both our medium LSTM and the medium LSTM of Zaremba et al. (2014) which do not use recurrent dropout and is even competitive with variational LSTMs. This may be due to the limited computational capacity that the QRNN\u2019s pooling layer has relative to the LSTM\u2019s recurrent weights, providing structural regularization over the recurrence. Without zoneout, early stopping based upon validation loss was required as the QRNN would begin overfitting. By applying a small amount of zoneout (p = 0.1), no early stopping is required and the QRNN achieves competitive levels of perplexity to the variational LSTM of Gal & Ghahramani Model Parameters Validation Test LSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7 Variational LSTM (medium) (Gal & Ghahramani, 2016) 20M 81.9 79.7 LSTM with CharCNN embeddings (Kim et al., 2016) 19M − 78.9 Zoneout + Variational LSTM (medium) (Merity et al., 2016) 20M 84.4 80.6 Our models LSTM (medium) 20M 85.7 82.0 QRNN (medium) 18M 82.9 79.9 QRNN + zoneout (p = 0.1) (medium) 18M 82.1 78.3 Table 2: Single model perplexity on validation and test sets for the Penn Treebank language model- ing task. Lower is better. \u201CMedium\u201D refers to a two-layer network with 640 or 650 hidden units per layer. All QRNN models include dropout of 0.5 on embeddings and between layers. MC refers to Monte Carlo dropout averaging at test time. 6 Under review as a conference paper at ICLR 2017 LSTM LSTM (cuDNN) QRNN 0 100 200 300 400 500 T im e ( m s\/ b a tc h ) RNN Softmax Optimization Overhead Sequence length 32 64 128 256 512 B at ch si ze 8 5.5x 8.8x 11.0x 12.4x 16.9x 16 5.5x 6.7x 7.8x 8.3x 10.8x 32 4.2x 4.5x 4.9x 4.9x 6.4x 64 3.0x 3.0x 3.0x 3.0x 3.7x 128 2.1x 1.9x 2.0x 2.0x 2.4x 256 1.4x 1.4x 1.3x 1.3x 1.3x Figure 4: Left: Training speed for two-layer 640-unit PTB LM on a batch of 20 examples of 105 timesteps. \u201CRNN\u201D and \u201Csoftmax\u201D include the forward and backward times, while \u201Coptimization overhead\u201D includes gradient clipping, L2 regularization, and SGD computations. Right: Inference speed advantage of a 320-unit QRNN layer alone over an equal-sized cuDNN LSTM layer for data with the given batch size and sequence length. Training results are similar. (2016), which had variational inference based dropout of 0.2 applied recurrently. The best perform- ing variation also used Monte Carlo (MC) dropout averaging at test time of 1000 different masks, making it computationally expensive to run. When training on the PTB dataset with an NVIDIA K40 GPU, we found that the QRNN is sub- stantially faster than a standard LSTM, even when comparing against the optimized cuDNN LSTM. In Figure 4 we provide a breakdown of the time taken for Chainer\u2019s default LSTM, the cuDNN LSTM, and QRNN to perform a full forward and backward pass on a single batch during training of the RNN LM on PTB. For both LSTM implementations, running time was dominated by the RNN computations, even with the highly optimized cuDNN implementation. For the QRNN implementa- tion, however, the \u201CRNN\u201D layers are no longer the bottleneck. Indeed, there are diminishing returns from further optimization of the QRNN itself as the softmax and optimization overhead take equal or greater time. Note that the softmax, over a vocabulary size of only 10,000 words, is relatively small; for tasks with larger vocabularies, the softmax would likely dominate computation time. It is also important to note that the cuDNN library\u2019s RNN primitives do not natively support any form of recurrent dropout. That is, running an LSTM that uses a state-of-the-art regularization scheme at cuDNN-like speeds would likely require an entirely custom kernel. 3.3 CHARACTER-LEVEL NEURAL MACHINE TRANSLATION We evaluate the sequence-to-sequence QRNN architecture described in 2.1 on a challenging neu- ral machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points. Our best performance on a development set (TED.tst2013) was achieved using a four-layer encoder\u2013 decoder QRNN with 320 units per layer, no dropout or L2 regularization, and gradient rescaling to a maximum magnitude of 5. Inputs were supplied to the encoder reversed. The first encoder layer used convolutional filter width k = 6, while the other encoder layers used k = 2. Optimization was performed for 10 epochs on minibatches of 16 examples using Adam (Kingma & Ba, 2014) with α = 0.001, β1 = 0.9, β2 = 0.999, and � = 10−8. Decoding was performed using beam search with beam width 8 and length normalization α = 0.6. The modified log-probability ranking criterion is provided in the appendix. Results using this architecture were compared to an equal-sized four-layer encoder\u2013decoder LSTM with attention, applying dropout of 0.2. We again optimized using Adam; other hyperparameters were equal to their values for the QRNN and the same beam search procedure was applied. Table 3 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline. 7 Under review as a conference paper at ICLR 2017 Model Train Time BLEU (TED.tst2014) Word-level LSTM w\/attn (Ranzato et al., 2016) − 20.2 Word-level CNN w\/attn, input feeding (Wiseman & Rush, 2016) − 24.0 Our models Char-level 4-layer LSTM 4.2 hrs\/epoch 16.53 Char-level 4-layer QRNN with k = 6 1.0 hrs\/epoch 19.41 Table 3: Translation performance, measured by BLEU, and train speed in hours per epoch, for the IWSLT German-English spoken language translation task. All models were trained on in-domain data only, and use negative log-likelihood as the training criterion. Our models were trained for 10 epochs. The QRNN model uses k = 2 for all layers other than the first encoder layer. 4 RELATED WORK Exploring alternatives to traditional RNNs for sequence tasks is a major area of current research. Quasi-recurrent neural networks are related to several such recently described models, especially the strongly-typed recurrent neural networks (T-RNN) introduced by Balduzzi & Ghifary (2016). While the motivation and constraints described in that work are different, Balduzzi & Ghifary (2016)\u2019s concepts of \u201Clearnware\u201D and \u201Cfirmware\u201D parallel our discussion of convolution-like and pooling-like subcomponents. As the use of a fully connected layer for recurrent connections violates the con- straint of \u201Cstrong typing\u201D, all strongly-typed RNN architectures (including the T-RNN, T-GRU, and T-LSTM) are also quasi-recurrent. However, some QRNN models (including those with attention or skip-connections) are not \u201Cstrongly typed\u201D. In particular, a T-RNN differs from a QRNN as de- scribed in this paper with filter size 1 and f -pooling only in the absence of an activation function on z. Similarly, T-GRUs and T-LSTMs differ from QRNNs with filter size 2 and fo- or ifo-pooling respectively in that they lack tanh on z and use tanh rather than sigmoid on o. The QRNN is also related to work in hybrid convolutional\u2013recurrent models. Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification. Xiao & Cho (2016) also tackle text classification by applying convolutions at the character level, with a stride to reduce sequence length, then feeding these features into a bidirectional LSTM. A similar approach was taken by Lee et al. (2016) for character-level machine translation. Their model\u2019s encoder uses a convolutional layer followed by max-pooling to reduce sequence length, a four-layer highway network, and a bidirectional GRU. The parallelism of the convolutional, pooling, and highway layers allows training speed comparable to subword-level models without hard-coded text segmentation. The QRNN encoder\u2013decoder model shares the favorable parallelism and path-length properties ex- hibited by the ByteNet (Kalchbrenner et al., 2016), an architecture for character-level machine trans- lation based on residual convolutions over binary trees. Their model was constructed to achieve three desired properties: parallelism, linear-time computational complexity, and short paths between any pair of words in order to better propagate gradient signals. 5 CONCLUSION Intuitively, many aspects of the semantics of long sequences are context-invariant and can be com- puted in parallel (e.g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments show that the speed and accuracy advantages remain consistent across tasks and at both word and character levels. Extensions to both CNNs and RNNs are often directly applicable to the QRNN, while the model\u2019s hidden states are more interpretable than those of other recurrent architectures as its channels main- tain their independence across timesteps. We believe that QRNNs can serve as a building block for long-sequence tasks that were previously impractical with traditional RNNs. 8 Under review as a conference paper at ICLR 2017 REFERENCES Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016. James Bradbury and Richard Socher. MetaMind neural machine translation system for WMT 2016. In Proceedings of the First Conference on Machine Translation, Berlin, Germany. Association for Computational Linguistics, 2016. Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In NIPS, 2016. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, Nov 1997. ISSN 0899-7667. Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016. Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058, 2014. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. arXiv preprint arXiv:1508.06615, 2016. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo- lutional neural networks. In NIPS, 2012. David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regu- larizing RNNs by Randomly Preserving Hidden Activations. arXiv preprint arXiv:1606.01305, 2016. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In ICML, 2016. Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation without explicit segmentation. arXiv preprint arXiv:1610.03017, 2016. Shayne Longpre, Sabeek Pradhan, Caiming Xiong, and Richard Socher. A way out of the odyssey: Analyzing and combining recent insights for LSTMs. Submitted to ICLR, 2016. M. T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In EMNLP, 2015. Andrew L Maas, Andrew Y Ng, and Christopher Potts. Multi-dimensional sentiment analysis with learned representations. Technical report, 2011. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Grégoire Mesnil, Tomas Mikolov, Marc\u2019Aurelio Ranzato, and Yoshua Bengio. Ensemble of gen- erative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335, 2014. Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH, 2010. 9 Under review as a conference paper at ICLR 2017 Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Virtual adversarial training for semi-supervised text classification. arXiv preprint arXiv:1605.07725, 2016. Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for word representation. In EMNLP, 2014. Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train- ing with recurrent neural networks. In ICLR, 2016. Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2), 2012. Seiya Tokui, Kenta Oono, and Shohei Hido. Chainer: A next-generation open source framework for deep learning. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016. Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, 2012. Xin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang, and Xiaolong Wang. Predicting polarities of tweets by composing word embeddings with long short-term memory. In ACL, 2015. Sam Wiseman and Alexander M Rush. Sequence-to-sequence learning as beam-search optimization. arXiv preprint arXiv:1606.02960, 2016. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine trans- lation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Yijun Xiao and Kyunghyun Cho. Efficient character-level document classification by combining convolution and recurrent layers. arXiv preprint arXiv:1602.00367, 2016. Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. In ICML, 2016. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas- sification. In NIPS, 2015. Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Francis Lau. A C-LSTM neural network for text classification. arXiv preprint arXiv:1511.08630, 2015. 10 Under review as a conference paper at ICLR 2017 APPENDIX BEAM SEARCH RANKING CRITERION The modified log-probability ranking criterion we used in beam search for translation experiments is: log(Pcand) = T + α T . . . Ttrg + α Ttrg T∑ i=1 log(p(wi|w1 . . . wi−1)), (9) where α is a length normalization parameter (Wu et al., 2016), wi is the ith output character, and Ttrg is a \u201Ctarget length\u201D equal to the source sentence length plus five characters. This reduces at α = 0 to ordinary beam search with probabilities: log(Pcand) = T∑ i=1 log(p(wi|w1 . . . wi−1)), (10) and at α = 1 to beam search with probabilities normalized by length (up to the target length): log(Pcand) ∼ 1 T T∑ i=1 log(p(wi|w1 . . . wi−1)). (11) Conveniently, this ranking criterion can be computed at intermediate beam-search timesteps, obvi- ating the need to apply a separate reranking on complete hypotheses. 11 ","flair":"null\tnull"}
{"author":"ajmooch","created":"Wed Oct 26 12:35:44 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.07629 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: A Learned Representation For Artistic Style Authors: Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur (Submitted on 24 Oct 2016) Abstract: The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style. Comments: 9 pages. 15 pages of Appendix Subjects: Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG) Cite as: arXiv:1610.07629 [cs.CV]   (or arXiv:1610.07629v1 [cs.CV] for this version) Submission history From: Jonathon Shlens [view email] [v1] Mon, 24 Oct 2016 20:06:54 GMT (47073kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"nested_dreams","created":"Tue Nov 15 18:38:35 EST 2016","text":" Google Cloud Platform Blog Product updates, customer stories, and tips and tricks on Google Cloud Platform Announcing GPUs for Google Cloud Platform Tuesday, November 15, 2016 Posted by John Barrus, Product Manager, Google Cloud Platform CPU-based machines in the cloud are terrific for general purpose computing, but certain tasks such as rendering or large-scale simulations are much faster on specialized processors. Graphics Processing Units (GPUs) contain hundreds of times as many computational cores as CPUs and are great at accelerating risk analysis, studying molecular binding or optimizing the shape of a turbine blade. If your CPU-based instance feels like a Formula One race car but you\u2019re in need of a rocket, you\u2019re going to love our new cloud GPUs. Early in 2017, Google Cloud Platform will offer GPUs worldwide for Google Compute Engine and Google Cloud Machine Learning users. Complex medical analysis, financial calculations, seismic\/subsurface exploration, machine learning, video rendering, transcoding and scientific simulations are just some of the applications that can benefit from the highly parallel compute power of GPUs. GPUs in Google Cloud give you the freedom to focus on solving challenging computational problems while accessing GPU-equipped machines from anywhere. Whether you need GPUs for a few hours or several weeks, we\u2019ve got you covered. Google Cloud will offer AMD FirePro S9300 x2 that supports powerful, GPU-based remote workstations. We'll also offer NVIDIA® Tesla® P100 and K80 GPUs for deep learning, AI and HPC applications that require powerful computation and analysis. GPUs are offered in passthrough mode to provide bare metal performance. Up to 8 GPU dies can be attached per VM instance including custom machine types. Google Cloud GPUs give you the flexibility to mix and match infrastructure. You\u2019ll be able to attach up to 8 GPU dies to any non-shared-core machine, whether you\u2019re using an n1-highmem-8 instance with 3 TB of super-fast Local SSD or a custom 28 vCPU virtual machine with 180 GB of RAM. Like our VMs, GPUs will be priced per minute and GPU instances can be up and running within minutes from Google Cloud Console or from the gcloud command line. Whether you need one or dozens of instances, you only pay for what you use. During an early access program, customers have been running machine learning training, seismic analysis, simulations and visualization on GPU instances. Startup MapD gets excellent results with a GPU-accelerated database. \"These new instances of GPUs in the Google Cloud offer extraordinary performance advantages over comparable CPU-based systems and underscore the inflection point we are seeing in computing today. Using standard analytical queries on the 1.2 billion row NYC taxi dataset, we found that a single Google n1-highmem-32 instance with 8 attached K80 dies is on average 85 times faster than Impala running on a cluster of 6 nodes each with 32 vCPUs. Further, the innovative SSD storage configuration via NVME further reduced cold load times by a factor of five. This performance offers tremendous flexibility for enterprises interested in millisecond speed at over billions of rows.\" - Todd Mostak, MapD Founder and CEO The Foundry, a visual effects software provider for the entertainment industry has been experimenting with workstations in the cloud. \"At The Foundry, we're really excited about VFX in the cloud, and with the arrival of GPUs on Google Cloud Platform, we'll have access to the cutting edge of visualisation technology, available on-demand and charged by the minute. The potential ramifications for our industry are enormous..\" - Simon Pickles, Lead Engineer, Pipeline-in-the-Cloud Tell us about your GPU computing requirements and sign up to be notified about GPU-related announcements using this survey. Additional information is available on our webpage. Google Labels: Compute    Free Trial GCP Blogs Big Data & Machine Learning Kubernetes GCP Japan Blog Labels  Announcements 47 Big Data & Machine Learning 83 Compute 138 Containers 1 Containers & Kubernetes 34 CRE 6 Customers 86 Developer Tools & Insights 70 Events 29 Google Genomics 1 Infrastructure 23 Management Tools 33 Networking 14 Open Source 98 Partners 50 Pricing 19 Security & Identity 17 Solutions 16 Stackdriver 14 Storage & Databases 105 Weekly Roundups 16  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Aug Jul Jun May Apr Feed Subscribe by email Technical questions? Check us out on Stack Overflow. Subscribe to our monthly newsletter. Googleon Follow @googlecloud Follow Follow Company-wide Official Google Blog Enterprise Blog Student Blog Products Official Android Blog Chrome Blog Lat Long Blog Developers Ads Developer Blog Android Developers Blog Developers Blog Google Privacy Terms ","flair":"two\tNews"}
{"author":"neurodynamic","created":"Mon Oct 03 10:34:07 EDT 2016","text":"My goal with this post is to build intuition about natural gradients for optimizing over spaces of probability distributions (e.g. for variational inference). I\u2019ll examine a simple family of distributions (diagonal-covariance Gaussians) and try to see how natural gradients differ from standard gradients, and how they can make existing algorithms faster and more robust. For concrete motivation, here\u2019s a sneak preview of optimization improvement afforded by the natural gradient in a variational inference problem (higher values of the evidence lower bound, or ELBO, imply a better approximation) The punchline: using the natural gradient for variational inference often leads to quicker and more reliable optimization over the standard gradient. This improvement makes approximate Bayesian inference more practical for general use. To motivate the concept of natural gradients and statistical manifolds, consider variational inference (VI), a class of methods often used for approximate Bayesian inference. Bayesian inference typically starts with some model for data that describes a joint density, , over data and model parameters (and latent variables), . This joint density is proportional to the posterior, , which characterizes our beliefs about the values of parameters . Access to this distribution allows us to make Bayesian predictions, report posterior expectations, interpret posterior marginals, etc. For notational simplicity, we drop the and denote our (unnormalized) target as . The target is, in general, difficult to use. The VI approach is to approximate with some easy-to-use distribution, , parameterized by in some parameter space . VI typically proceeds by optimizing a divergence measure between and to find the that makes these distributions as similar as possible. For VI methods, this similarity measure is typically the Kullback\u2013Leibler (KL) divergence, . The exact KL is difficult to evaluate, and for practical purposes most VI algorithms introduce an objective (known as the evidence lower bound, or ELBO) that, when maximized, corresponds to minimizing the KL. For this post, the details of the ELBO are unimportant, so we will refer to this objective simply as and our optimization problem is To sum up, solving this optimization problem is approximate Bayesian inference \u2014 a difficult integral is replaced by an easier optimization problem. To solve this optimization problem, we\u2019ll consider gradient-based methods, such as stochastic gradient descent (sgd), adagrad, or adam. This is where the natural gradient enters \u2014 these gradient-based methods make small moves around the parameter space, where the definition of \u201Csmall\u201D is what characterizes the difference between the natural and the standard gradient. The next section focuses on understanding the natural gradient, and in the following section I\u2019ll derive (and interpret) the natural gradient for a particular family of variational distributions. We\u2019ll then see how it improves optimization in a Black-Box Stochastic VI (BB-SVI) problem for approximate Bayesian inference. First, let\u2019s state the obvious: each point corresponds to a full probability distribution. If we ignore this, and use general-purpose gradient methods (that treat like a Euclidean space), then we\u2019re giving up a ton of useful structure in the problem. To see what I mean, consider these two Gaussians, and is (their variances are equal, ). However, the distributions are quite different \u2014 their support barely overlaps; samples greater than unambiguously belong to (and vice versa). More importantly, if were our true target distribution, then would be a poor surrogate \u2014 here is huge. Now consider these two Gaussian distributions: The Euclidean distance between parameters is equivalent, . However, their support almost completely overlaps. If you saw a sample over this support, it would be difficult to distinguish which Gaussian it came from. If were our true target distribution, then would be a pretty good surrogate in terms of (not the best, but not bad). Notably, the Euclidean distance between parameters of and is the same in both cases, yet the statistical distance between the top two is enormous compared to the statistical distance between the bottom two. This is because naively comparing parameters ignores the structure of the . A statistical \u201Cdistance\u201D (e.g. symmetric- or total variation) incorporates this information by being a functional of the entire distribution When we optimize over a parameter space, we want small distances between a current value and some nearby value to reflect this structure. The local structure defined by our choice of statistical distance defines a statistical manifold, and in order to optimize over this manifold, we need to modify our standard gradients using an appropriate local scaling, otherwise known as a Riemannian metric . This local rescaling, along with the standard gradient, can be used to construct the natural gradient. A common Riemannian metric for statistical manifolds is the Fisher Information Matrix (\u201Cthe Fisher\u201D) Another, and perhaps more intuitive way to express the Fisher Information Matrix is the second derivative of the KL divergence (which is locally symmetric) This blog post by Roger Grosse goes into more detail on the Fisher Information Matrix, and how it pops up as the Riemannian Metric for statistical manifolds. For gradient-based optimization methods, we can replace standard gradients with the natural gradient which corresponds to the direction of steepest ascent along the statistical manifold. This blog post (by Nick Foti) does a great job describing the natural gradient, and why it corresponds to this direction. To better understand the natural gradient, let\u2019s take a closer look at the Fisher for a simple class of distributions: diagonal Gaussians. Perhaps the simplest (and most common) approximate posterior distribution is a diagonal Gaussian. Consider diagonal Gaussian distributions parameterized by , where is the mean vector and are the standard deviations (i.e. the square root of the diagonal of the covariance matrix). For convenience, we refer to means as and the log standard deviations as In this instance, the natural gradient is easy to derive and cheap to compute. To derive the Fisher, we first look at the gradient of the log density of a diagonal Gaussian (note that The Fisher information matrix is the outer product of this gradient. To obtain an analytical form for the Fisher, consider its first diagonal terms, corresponding to components of the mean vector, The off diagonal term corresponding to components and is easily seen to be zero (due to independence of dimensions) The off diagonal term corresponding to and is also 0 which makes use of the fact that the fourth central moment of a normal is known to be In summary, the Fisher for a diagonal Gaussian parameterized by Examining the Fisher for diagonal Gaussians allows us to see exactly how the natural gradient differs from the standard gradient. If a dimension has small variance, then preconditioning by the inverse Fisher makes the natural gradient smaller along the mean dimension, . Intuitively, this makes sense \u2014 we want our optimization routine to slow down when the component variance is small because small changes to the mean correspond to big changes in KL (see the two skinny Gaussians above), which can result in chatoic looking optimization traces. When the variance along a dimension is large, the inverse Fisher elongates the standard gradient along that dimension. Again, this makes sense \u2014 when the component variance is high we can move the mean a lot farther (in Euclidean distance) without moving that far in terms of KL (see the two wide Gaussians above). This means that natural gradient SGD moves a lot faster along this dimension when we\u2019re in a region of space that corresponds to higher variance. This automatic scaling is a benefit of incorporating local metric information in our optimization procedure. The added bonus is that the Fisher, in this case, is essentially free to compute. I wrote a simple autograd example (built on the existing BB-SVI example) to illustrate how the behavior of the natural gradient differs from the standard gradient. In this case, the posterior is a multivariate normal model with a non-conjugate prior. Optimizing the variational objective results in the following ELBO traces (higher implies a better approximation) For all step-sizes I tried, using momentum-less and the natural gradient found a better solution (or converged much more quickly) than using with the standard gradient. In most instances, makes quick progress early on (potentially accompanied by some instability), but + natural gradient catches up and converges more quickly. These results are even more pronounced in a higher dimensional The natural gradient is intuitive, easy and cheap to use, and often attains superior and quicker results than the standard gradient. And that\u2019s something we can all cheer about. (Thanks Matt Johnson for sanity checking this post and helping me understand most of these concepts!)","flair":"null\tnull"}
{"author":"perceptron01","created":"Fri Nov 18 05:50:59 EST 2016","text":"Sorry for another meta-thread, but after the [\"who are you\" thread](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5dek9l\/d_who_are_you\/?sort=new), I was wondering why are users not allowed to set flairs on this subreddit? Flairs such as \"MSc Computer Vision\" \/ \"Industry researcher\" \/ \"PhD Optimization\" \/ may add useful metadata to comments, give a hint about where a person's view is coming from, and generally help learning something about the person behind the comment.\n\nFor some examples: I know that \/r\/cscareerquestions and \/r\/gradschool uses them. [Here](https:\/\/www.reddit.com\/r\/GradSchool\/comments\/2nyuio\/grad_school_is_awesome\/)'s a thread from \/r\/gradschool\n\nObviously, the flairs are optional if enabled.","flair":"one\tDiscussion"}
{"author":"olBaa","created":"Tue Oct 25 15:54:32 EDT 2016","text":"Neural networks designed for sequence predictions have recently gained renewed interested by achieving state-of-the-art performance across areas such as speech recognition, machine translation or language modeling. However, these models are quite computationally demanding, which in turn can limit their application. In the area of language modeling, recent advances have been made leveraging massively large models that could only be trained on a large GPU cluster for weeks at a time. While impressive, these processing-intensive practices favor exploring on large computational infrastructures that are typically too expensive for academic environments and impractical in a production setting, limiting the speed of research, reproducibility, and usability of the results. Recognizing this computational bottleneck, Facebook AI Research (FAIR) designed a novel softmax function approximation tailored for GPUs to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax (more details in the paper), circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. This approach further reduces the computational cost by leveraging the specificities of modern architectures and matrix-matrix vector operations both at train and test time. This makes it particularly suited for GPUs whereas most previous approaches, such as hierachical softmax, NCE, and importance sampling, have been designed for standard CPUs. FAIR also developed and is open sourcing a library called torch-rnnlib that allows researchers to design new recurrent models and test these prototypes on GPUs with minimal effort. It also allows seamless access to fast baselines through the torch.cudnn bindings. Several standard recurrent networks such as RNN, LSTM and GRU are already implemented and we show below how one can design new recurrent networks with this library. These tools and techniques were then applied together against standard benchmarks, such as EuroParl and One Billion Word, which are complex training environments due to the vocabulary sizes used which prohibited us from fitting both a large model and full softmax on GPU. The results demonstrated that we can process 12,500 words\/sec on a single GPU, bringing a large gain in efficiency over standard approximations, decreasing the time from experiment to results, all while achieving an accuracy close to that of the full softmax. This will allow for engineers and researchers across academia and industry to train state-of-the-art models with limited resources in short periods of time. While they are many ways to define recurrent models, we follow this definition: Recurrent networks model sequences of variables over discrete time. It follows the Markov property, that is its future state only depends on its present state. The simplest recurrent model is Elman's recurrent model. At each time step , it produces an output given its current input variable and its past state. More precisely, Elman's recurrent model is defined by the following set of equations: where represents the (hidden) internal state of the network and is the sigmoid function. More sophisticated recurrent models have been proposed since then such as the LSTM, GRU or SCRNN. A good example of application is language modeling, which we now introduce and will be the focus of the remainder of this post. The goal of language modeling is to learn a probability distribution over a sequence of words from a given dictionary. The joint distribution is defined as a product of conditional distribution of words given their past. More precisely, the probability of a sequence of words is given as This problem is traditionally addressed with non-parametric models based on counting statistics (see Goodman, 2001, for details). More recently, parametric models based on recurrent neural networks have gained popularity for language modeling (for example, Jozefowicz et al., 2016, obtained state-of-the-art performance on the 1B word dataset). How to build a standard model with torch-rnnlib In torch-rnnlib, we give three different APIs for constructing a network with recurrent connections: You can also create your own model by defining a function that acts as the cell as well as an initialization function for the cell's state. There are pre-defined cells in such as LSTMs, RNNs, and GRUs. Here's how you can build an RNN from scratch: Since torch-rnnlib adheres to the nn module interface, simply call :cuda() on the model to pull it to GPU. The goal of rnnlib is to allow the user flexibility in creating new cells or using fast baselines. On this end, if you use either the first or second API for constructing recurrent networks in the above section you can easily use cudnn to greatly speed up your network. For the nn.{RNN, LSTM, GRU} interface, simply call the constructor with usecudnn = true: For the second API, simply replace rnnlib.makeRecurrent with rnnlib.makeCudnnRecurrent and change the cell function to a cell string that is present in the cudnn API. For example: This will generally result in at least a 2x speedup of the recurrent part of the model. Do note that this may not mean a 2x speedup of your whole model, especially if the majority of the computation does not take place in the recurrent portion. For example, if you have a softmax in your model that requires much more computation than the recurrent part, you may only see a 1.3x speedup. Figure 1. Comparison of running time as a function of size of the hidden layers between torch-rnnlib and other torch library. We show it with and without using Cudnn. We use an LSTM with 2 layers. When dealing with large output space (like in language modeling), the classifier can be the computational bottleneck of the model. Many solutions have been proposed in the past (hierarchical softmax, noise contrastive estimation, differentiated softmax) but they are usually designed for standard CPUs and rarely take full advantage of the specificity of GPUs. We develop a new approximation called the adaptive softmax: A softmax that adapt its computational budget to the distribution of the data. It balances between a good approximation and fast running time by making the most frequent classes faster to access but provides them with more resources. More precisely it learns a k-way hierarchical softmax that takes into account the architecture of the GPUs to assign computation efficiently. This computation allocation can be solved exactly using a simple dynamic programming algorithm. Several tricks have been used to further the computational burden of the classifier: we use shallow trees to avoid sequential computation and we fix a minimum number of classes per cluster to avoid wasting GPUs parallel computation power (more details in the paper). As shown in Table 1, the adaptive softmax almost match the performance of the full softmax while being significantly faster. It also outperforms other approximations on GPUs. Table 1. Performance on Text8. The lower ppl the better. Figure 2. Convergence of language model with different softmax approximations. It is built on top of an LSTM. In terms of technical details, beside the adatpive softmax, we use a relatively standard setting: For our small model we use a LSTM with 1 layer and 2048 units each and for the larger one 2 layers with 2048 neurons each. We train the model using Adagrad (citation needed) and a L2 regularization for the weights. We use a batch size of 128 and we set the back-propagation window size to 20. Concerning the Adaptative softmax, we use the optimal setting for the distribution of the train set of 1B words, that is 4 clusters with a dimension reduction of x4 over each cluster (see the paper for details). Table 2. Comparison on 1B word in perplexity (lower the better). Note that Jozefowicz et al., uses 32 GPUs for training. We only use 1 GPU. As shown in table 2, the small model achieve a perplexity of 43.9 within a couple of days. The larger model achieve a perplexity of 39.8 in 6 days. The current state-of-the-art performance is a perplexity of 30.0 (lower the better) and was achieved by Jozefowicz et al., 2016. They achieve this result using 32 GPUs over 3 weeks. They also report a perplexity of 44 achieved with a smaller model, using 18 GPU days to train. Our small model take 180 ms\/batch and achieve a perplexity of 50 after one epoch (that is ~14hours). Without Cudnn, the small model takes 230 ms\/batch which is only 30% slower.","flair":"three\tResearch"}
{"author":"antiprior","created":"Sun Nov 06 13:02:45 EST 2016","text":" Under review as a conference paper at ICLR 2017 DEEPCODER: LEARNING TO WRITE PROGRAMS Matej Balog∗ Department of Engineering University of Cambridge Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, Daniel Tarlow Microsoft Research ABSTRACT We develop a first line of attack for solving programming competition-style prob- lems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network\u2019s predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites. 1 INTRODUCTION A dream of artificial intelligence is to build systems that can write computer programs. Recently, there has been much interest in program-like neural network models (Graves et al., 2014; Weston et al., 2014; Kurach et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015; Neelakantan et al., 2016; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Zaremba et al., 2016; Graves et al., 2016), but none of these can write programs; that is, they do not generate human-readable source code. Only very recently, Riedel et al. (2016); Bunel et al. (2016); Gaunt et al. (2016) explored the use of gradient descent to induce source code from input-output examples via differentiable interpreters. However, Gaunt et al. (2016) show that differentiable interpreter-based program induction is inferior to discrete search-based techniques used by the programming languages community. We are then left with the question of how to make progress on program induction using machine learning techniques. In this work, we propose two main ideas: (1) learn to induce programs; that is, use a corpus of program induction problems to learn strategies that generalize across problems, and (2) integrate neural network architectures with search-based techniques rather than replace them. In more detail, we can contrast our approach to existing work on differentiable interpreters. In dif- ferentiable interpreters, the idea is to define a differentiable mapping from source code and inputs to outputs. After observing inputs and outputs, gradient descent can be used to search for a pro- gram that matches the input-output examples. This approach leverages gradient-based optimization, which has proven powerful for training neural networks, but each synthesis problem is still solved independently\u2014solving many synthesis problems does not help solve the next problem. We argue that machine learning can provide significant value towards solving Inductive Program Synthesis (IPS) by re-casting the problem as a big data problem. We show that training a neural network on a large number of generated IPS problems to predict cues from the problem description can help a search-based technique. In this work, we focus on predicting an order on the program space and show how to use it to guide search-based techniques that are common in the programming languages community. This approach has three desirable properties: first, we transform a difficult search problem into a supervised learning problem; second, we soften the effect of failures of the neural network by searching over program space rather than relying on a single prediction; and third, the neural network\u2019s predictions are used to guide existing program synthesis systems, allowing us to use and improve on the best solvers from the programming languages community. Empirically, we show orders-of-magnitude improvements over optimized standard search techniques and a Recurrent Neural Network-based approach to the problem. ∗Also affiliated with Max-Planck Institute for Intelligent Systems, Tübingen, Germany. Work done while author was an intern at Microsoft Research. 1 Under review as a conference paper at ICLR 2017 In summary, we define and instantiate a framework for using deep learning for program synthesis problems like ones appearing on programming competition websites. Our concrete contributions are: 1. defining a programming language that is expressive enough to include real-world program- ming problems while being high-level enough to be predictable from input-output examples; 2. models for mapping sets of input-output examples to program properties; and 3. experiments that show an order of magnitude speedup over standard program synthesis techniques, which makes this approach feasible for solving problems of similar difficulty as the simplest problems that appear on programming competition websites. 2 BACKGROUND ON INDUCTIVE PROGRAM SYNTHESIS We begin by providing background on Inductive Program Synthesis, including a brief overview of how it is typically formulated and solved in the programming languages community. The Inductive Program Synthesis (IPS) problem is the following: given input-output examples, produce a program that has behaviour consistent with the examples. Building an IPS system requires solving two problems. First, the search problem: to find consistent programs we need to search over a suitable set of possible programs. We need to define the set (i.e., the program space) and search procedure. Second, the ranking problem: if there are multiple programs consistent with the input-output examples, which one do we return? Both of these problems are dependent on the specifics of the problem formulation. Thus, the first important decision in formulating an approach to program synthesis is the choice of a Domain Specific Language. Domain Specific Languages (DSLs). DSLs are programming languages that are suitable for a specialized domain but are more restrictive than full-featured programming languages. For example, one might disallow loops or other control flow, and only allow string data types and a small number of primitive operations like concatenation. Most of program synthesis research focuses on synthesizing programs in DSLs, because full-featured languages like C++ enlarge the search space and complicate synthesis. Restricted DSLs can also enable more efficient special-purpose search algorithms. For example, if a DSL only allows concatenations of substrings of an input string, a dynamic program- ming algorithm can efficiently search over all possible programs (Polozov & Gulwani, 2015). The choice of DSL also affects the difficulty of the ranking problem. For example, in a DSL without if statements, the same algorithm is applied to all inputs, reducing the number of programs consistent with any set of input-output examples, and thus the ranking problem becomes easier. Of course, the restrictiveness of the chosen DSL also determines which problems the system can solve at all. Search Techniques. There are many techniques for searching for programs consistent with input- output examples. Perhaps the simplest approach is to define a grammar and then search over all derivations of the grammar, checking each one for consistency with the examples. This approach can be combined with pruning based on types and other logical reasoning (Feser et al., 2015). While simple, these approaches can be implemented efficiently, and they can be surprisingly effective. In restricted domains such as the concatenation example discussed above, special-purpose algorithms can be used. FlashMeta (Polozov & Gulwani, 2015) describes a framework for DSLs which allow decomposition of the search problem, e.g., where the production of an output string from an input string can be reduced to finding a program for producing the first part of the output and concatenating it with a program for producing the latter part of the output string. Another class of systems is based on Satisfiability Modulo Theories (SMT) solving. SMT combines SAT-style search with theories like arithmetic and inequalities, with the benefit that theory-dependent subproblems can be handled by special-purpose solvers. For example, a special-purpose solver can easily find integers x, y such that x < y and y < −100 hold, whereas an enumeration strategy may need to consider many values before satisfying the constraints. Many program synthesis engines based on SMT solvers exist, e.g., Sketch (Solar-Lezama, 2008) and Brahma (Gulwani et al., 2011). They convert the semantics of a DSL into a set of constraints between variables representing the program and the input-output values, and then call an SMT solver to find a satisfying setting of the program variables. This approach shines when special-purpose reasoning can be leveraged, but 2 Under review as a conference paper at ICLR 2017 complex DSLs can lead to very large constraint problems where constructing and manipulating the constraints can be a lot slower than an enumerative approach. Finally, stochastic local search can be employed to search over program space, and there is a long history of applying genetic algorithms to this problem. One of the most successful recent examples is the STOKE super-optimization system (Schkufza et al., 2016), which uses stochastic local search to find assembly programs that have the same semantics as an input program but execute faster. Ranking. While we focus on the search problem in this work, we briefly mention the ranking problem here. A popular choice for ranking is to choose the shortest program consistent with input- output examples (Gulwani, 2016). A more sophisticated approach is employed by FlashFill (Singh & Gulwani, 2015). It works in a manner similar to max-margin structured prediction, where known ground truth programs are given, and the learning task is to assign scores to programs such that the ground truth programs score higher than other programs that satisfy the input-output specification. 3 LEARNING INDUCTIVE PROGRAM SYNTHESIS (LIPS) In this section we outline the general approach that we follow in this work, which we call Learning Inductive Program Synthesis (LIPS). The details of our instantiation of LIPS appear in Sect. 4. The components of LIPS are (1) a DSL specification, (2) a data-generation procedure, (3) a machine learn- ing model that maps from input-output examples to program attributes, and (4) a search procedure that searches program space in an order guided by the model from (3). The framework is related to the formulation of Menon et al. (2013); the relationship and key differences are discussed in Sect. 6. (1) DSL and Attributes. The choice of DSL is important in LIPS, just as it is in any program synthesis system. It should be expressive enough to capture the problems that we wish to solve, but restricted as much as possible to limit the difficulty of the search. In LIPS we additionally specify an attribute function A that maps programs P of the DSL to finite attribute vectors a = A(P ). (Attribute vectors of different programs need not have equal length.) Attributes serve as the link between the machine learning and the search component of LIPS: the machine learning model predicts a distribution q(a | E), where E is the set of input-output examples, and the search procedure aims to search over programs P as ordered by q(A(P ) | E). Thus an attribute is useful if it is both predictable from input-output examples, and if conditioning on its value significantly reduces the effective size of the search space. Possible attributes are the (possibly position-dependent) presence or absence of high-level functions (e.g., does the program contain or end in a call to SORT). Other possible attributes include control flow templates (e.g., the number of loops and conditionals). In the extreme case, one may set A to the identity function, in which case the attribute is equivalent to the program; however, in our experiments we find that performance is improved by choosing a more abstract attribute function. (2) Data Generation. Step 2 is to generate a dataset D = ((P (n),a(n), E(n)))Nn=1 of programs P (n) in the chosen DSL, their corresponding attributes a(n), and accompanying input-output exam- ples E(n). There are many choices for data generation, ranging from enumerating valid programs in the DSL and pruning, to training a more sophisticated generative model of programs in the DSL. The key in the LIPS formulation is to ensure that it is feasible to generate a large dataset (ideally millions of programs). (3) Machine Learning Model. The machine learning problem is to learn a distribution of at- tributes given input-output examples, q(a | E). There is freedom to explore a large space of models, so long as the input component can encode E , and the output is a proper distribution over attributes (e.g., if attributes are a fixed-size binary vector, then a neural network with independent sigmoid outputs is appropriate; if attributes are variable size, then a recurrent neural network output could be used). Attributes are observed at training time, so training can use a maximum likelihood objective. (4) Search. The aim of the search component is to interface with an existing solver, using the predicted q(a | E) to guide the search. We describe specific approaches in the next section. 3 Under review as a conference paper at ICLR 2017 4 DEEPCODER Here we describe DeepCoder, our instantiation of LIPS including a choice of DSL, a data generation strategy, models for encoding input-output sets, and algorithms for searching over program space. 4.1 DOMAIN SPECIFIC LANGUAGE AND ATTRIBUTES We consider binary attributes indicating the presence or absence of high-level functions in the target program. To make this effective, the chosen DSL needs to contain constructs that are not so low-level that they all appear in the vast majority of programs, but at the same time should be common enough so that predicting their occurrence from input-output examples can be learned successfully. Following this observation, our DSL is loosely inspired by query languages such as SQL or LINQ, where high-level functions are used in sequence to manipulate data. A program in our DSL is a sequence of function calls, where the result of each call initializes a fresh variable that is either a singleton (integer) or an (integer) array. Functions can be applied to any of the inputs or previously computed (intermediate) variables. The output of the program is the return value of the last function call, i.e. the last variable. See Fig. 1 for an example program of length T = 4 in our DSL. a← [int] b← FILTER (<0) a c← MAP (*4) b d← SORT c e← REVERSE d An input-output example: Input: [-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11] Output: [-12, -20, -32, -36, -68] Figure 1: An example program in our DSL that takes a single integer array as its input. Overall, our DSL contains the first-order functions HEAD, LAST, TAKE, DROP, ACCESS, MINIMUM, MAXIMUM, REVERSE, SORT, SUM, and the higher-order functions MAP, FILTER, COUNT, ZIP- WITH, SCANL1. Higher-order functions require suitable lambda functions for their behaviour to be fully specified: for MAP our DSL provides lambdas (+1), (-1), (*2), (\/2), (*(-1)), (**2), (*3), (\/3), (*4), (\/4); for FILTER and COUNT there are predicates (>0), (<0), (%2==0), (%2==1) and for ZIPWITH and SCANL1 the DSL provides lambdas (+), (-), (*), MIN, MAX. A description of the semantics of all functions is provided in Appendix F. Note that while the language does not allow explicit control flow, many of its functions do perform branching and looping internally (e.g., SORT, COUNT, ...). Examples of more sophisticated programs expressible in our DSL, which were inspired by the simplest problems appearing on programming competition websites, are shown in Appendix A. 4.2 DATA GENERATION To generate a dataset, we enumerate programs in the DSL, heuristically pruning away those with eas- ily detectable issues such as a redundant variable whose value does not affect the program output, or, more generally, existence of a shorter equivalent program (equivalence can be overapproximated by identical behaviour on randomly or carefully chosen inputs). To generate valid inputs for a program, we enforce a constraint on the output value bounding integers to some predetermined range, and then propagate these constraints backward through the program to obtain a range of valid values for each input. If one of these ranges is empty, we discard the program. Otherwise, input-output pairs can be generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values. The binary attribute vectors are easily computed from the program source codes. 4.3 MACHINE LEARNING MODEL Observe how the input-output data in Fig. 1 is informative of the functions appearing in the program: the values in the output are all negative, divisible by 4, they are sorted in decreasing order, and they happen to be multiples of 4 of numbers appearing in the input. Our aim is to learn to recognize such patterns in the input-output examples, and to leverage them to predict the presence or absence of 4 Under review as a conference paper at ICLR 2017 individual functions. We employ neural networks to model and learn the mapping from input-output examples to attributes. We can think of these networks as consisting of two parts: 1. an encoder: a differentiable mapping from a set of N input-output examples generated by a single program to a latent real-valued vector, and 2. a decoder: a differentiable mapping from the latent vector representing a set of N input- output examples to predictions of the ground truth program\u2019s attributes. For the encoder we use a simple feed-forward architecture. First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value. Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20. (The range of integers is restricted to a finite range and each embedding is parametrized individually.) Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each. The third hidden layer thus provides an encoding of each individual input-output example. Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging. See Appendix C for more details. The advantage of this encoder lies in its simplicity, and we found it reasonably easy to train. A disadvantage is that it requires an upper bound L on the length of arrays appearing in the input and output. We confirmed that the chosen encoder architecture is sensible in that it performs empirically at least as well as an RNN encoder, a natural baseline, which may however be more difficult to train. DeepCoder learns to predict presence or absence of individual functions of the DSL. We shall see this can already be exploited by various search techniques to large computational gains. We use a decoder that pre-multiplies the encoding of input-output examples by a learned K×C matrix, where C = 34 is the number of functions in our DSL (higher-order functions and lambdas are predicted independently), and treats the resulting C numbers as log-unnormalized probabilities (logits) of each function appearing in the source code. Fig. 2 shows the predictions a trained neural network made from 5 input-output examples for the program shown in Fig. 1. (+ 1 ) (- 1 ) (* 2 ) (\/ 2 ) (* -1 ) (* *2 ) (* 3 ) (\/ 3 ) (* 4 ) (\/ 4 ) (> 0 ) (> 0 ) (% 2 = = 1 ) (% 2 = = 0 ) H E A D LA S T M A P FI LT E R S O R T R E V E R S E T A K E D R O P A C C E S S Z IP W IT H S C A N L1 + - * M IN M A X C O U N T M IN IM U M M A X IM U M S U M .0 .0 .1 .0 .0 .0 .0 .0 1.0 .0 .0 1.0 .0 .2 .0 .0 1.0 1.0 1.0 .7 .0 .1 .0 .4 .0 .0 .1 .0 .2 .1 .0 .0 .0 .0 Figure 2: Neural network predicts the probability of each function appearing in the source code. 4.4 SEARCH One of the central ideas of this work is to use a neural network to guide the search for a program consistent with a set of input-output examples instead of directly predicting the entire source code. This section briefly describes the search techniques and how they integrate the predicted attributes. Depth-first search (DFS). We use an optimized version of DFS to search over programs with a given maximum length T (see Appendix D for details). When the search procedure extends a partial program by a new function, it has to try the functions in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their predicted probabilities from the neural network. \u201CSort and add\u201D enumeration. A stronger way of utilizing the predicted probabilities of functions in an enumerative search procedure is to use a Sort and add scheme, which maintains a set of active functions and performs DFS with the active function set only. Whenever the search fails, the next most probable function (or several) are added to the active set and the search restarts with this larger active set. Note that this scheme has the deficiency of potentially re-exploring some parts of the search space several times, which could be avoided by a more sophisticated search procedure. Sketch. Sketch (Solar-Lezama, 2008) is a successful SMT-based program synthesis tool from the programming languages research community. While its main use case is to synthesize programs 5 Under review as a conference paper at ICLR 2017 by filling in \u201Choles\u201D in incomplete source code so as to match specified requirements, it is flexible enough for our use case as well. The function in each step and its arguments can be treated as the \u201Choles\u201D, and the requirement to be satisfied is consistency with the provided set of input-output examples. Sketch can utilize the neural network predictions in a Sort and add scheme as described above, as the possibilities for each function hole can be restricted to the current active set. 4.5 TRAINING LOSS FUNCTION We use the negative cross entropy loss to train the neural network described in Sect. 4.3, so that its predictions about each function can be interpreted as marginal probabilities. The LIPS framework dictates learning q(a | E), the joint distribution of all attributes a given the input-output examples, and it is not clear a priori how much DeepCoder loses by ignoring correlations between functions. However, under the simplifying assumption that the runtime of searching for a program of length T with C functions made available to a search routine is proportional to CT , the following result for Sort and add procedures shows that their runtime can be optimized using marginal probabilities. Lemma 1. For any fixed program length T , the expected total runtime of a Sort and add search scheme can be upper bounded by a quantity that is minimized by adding the functions in the order of decreasing true marginal probabilities. Proof. Predicting source code functions from input-output examples can be seen as a multi-label classification problem, where each set of input-output examples is associated with a set of relevant labels (functions appearing in the ground truth source code). Dembczynski et al. (2010) showed that in multi-label classification under a so-called Rank loss, it is Bayes optimal to rank the labels according to their marginal probabilities. If the runtime of search with C functions is proportional to CT , the total runtime of a Sort and add procedure can be monotonically transformed so that it is upper bounded by this Rank loss. See Appendix E for more details. 5 EXPERIMENTS In this section we report results from two categories of experiments. Our main experiments (Sect. 5.1) show that the LIPS framework can lead to significant performance gains in solving IPS by demon- strating such gains with DeepCoder. In Sect. 5.2 we illustrate the robustness of the method by demonstrating a strong kind of generalization ability across programs of different lengths. 5.1 DEEPCODER COMPARED TO BASELINES We trained a neural network as described in Sect. 4.3 to predict used functions from input-output examples and constructed a test set of P = 100 programs, ensured to be semantically disjoint from all programs on which the neural network was trained. For each test program we generated M = 5 input-output examples involving integers of magnitudes up to 256, passed the examples to the trained neural network, and fed the obtained predictions to the search procedures from Sect. 4.4. We also considered a RNN-based decoder generating programs using beam search (see Sect. 5.3 for details). To evaluate DeepCoder, we then recorded the time the search procedures needed to find a program consistent with the M input-output examples. As a baseline, we also ran all search procedures using a simple prior as function probabilities, computed from their global incidence in the program corpus. Table 1: Search speedups on programs of length T = 3 due to using neural network predictions. Time DFS Enumeration Sketch Beam to solve 20% 40% 60% 20% 40% 60% 20% 40% 20% Baseline 28ms 92ms 240ms 66ms 240ms 1000ms >103s >103s >103s DeepCoder 3ms 34ms 89ms 1.5ms 6ms 21ms 9s 13s 137s Speedup 8.1× 2.7× 2.7× 46× 42× 49× >116× >78× >7.3× 6 Under review as a conference paper at ICLR 2017 In the first, smaller-scale experiment (program search space size ∼ 2 × 106) we trained the neural network on programs of length T = 3, and the test programs were of the same length. Table 1 shows the times each search technique took to find a solution to given proportions of the test tasks. In the main experiment, we tackled a large-scale problem of searching for programs consistent with input-output examples generated from programs of length T = 5 (search space size on the order of 1010), supported by a neural network trained with programs of shorter length T = 4. The table in Fig. 3a shows significant speedups for both DFS and Sort and add enumeration, the two search techniques capable of solving the search problem in reasonable time frames. Note that Sort and add enumeration without the neural network (using prior probabilities of functions) exceeded the 104 second timeout in two cases, so the relative speedups shown are crude lower bounds. Time DFS Enumeration to solve 20% 40% 60% 20% 40% 60% Baseline 163s 2887s 6832s 8181s >104s >104s DeepCoder 24s 514s 2654s 9s 264s 4640s Speedup 6.8× 5.6× 2.6× 907× >37× >2× (a) 1 2 3 4 5 Length of test programs Ttest 100 101 102 103 S p e e d u p 1 2 3 4 Ttrain : none (b) Figure 3: Search speedups on programs of length T = 5 and influence of length of training programs. We hypothesize that the substantially larger performance gains on Sort and add schemes as compared to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence of functions anywhere in the program) and learning objective of the neural network are better matched to the Sort and add schemes. Indeed, a more appropriate attribute function for DFS would be one that is more informative of the functions appearing early in the program, since exploring an incorrect first function is costly with DFS. On the other hand, the discussion in Sect. 4.5 provides theoretical indication that ignoring the correlations between functions is not cataclysmic for Sort and add enumeration, since a Rank loss that upper bounds the Sort and add runtime can still be minimized. 5.2 GENERALIZATION ACROSS PROGRAM LENGTHS To investigate the encoder\u2019s generalization ability across programs of different lengths, we trained a network to predict used functions from input-output examples that were generated from programs of length Ttrain ∈ {1, . . . , 4}. We then used each of these networks to predict functions on 5 test sets containing input-output examples generated from programs of lengths Ttest ∈ {1, . . . , 5}, respectively. The test programs of a given length T were semantically disjoint from all training programs of the same length T and also from all training and test programs of shorter lengths T \u2032 < T . For each of the combinations of Ttrain and Ttest, Sort and add enumerative search was run both with and without using the neural network\u2019s predictions (in the latter case using prior probabilities) until it solved 20% of the test set tasks. Fig. 3b shows the relative speedup of the solver having access to predictions from the trained neural networks. These results indicate that the neural networks are able to generalize beyond programs of the same length that they were trained on. This is partly due to the search procedure on top of their predictions, which has the opportunity to correct for the presence of functions that the neural network failed to predict. Note that a sequence-to-sequence model trained on programs of a fixed length could not be expected to exhibit this kind of generalization ability. 5.3 ALTERNATIVE MODELS Encoder We evaluated replacing the feed-forward architecture encoder (Sect. 4.3) with an RNN, a natural baseline. Using a GRU-based RNN we were able to achieve results almost as good as using the feed-forward architecture, but found the RNN encoder more difficult to train. Decoder We also considered a purely neural network-based approach, where an RNN decoder is trained to predict the entire program token-by-token. We combined this with our feed-forward encoder by initializing the RNN using the pooled final layer of the encoder. We found it substantially 7 Under review as a conference paper at ICLR 2017 more difficult to train an RNN decoder as compared to the independent binary classifiers employed above. Beam search was used to explore likely programs predicted by the RNN, but it only lead to a solution comparable with the other techniques when searching for programs of lengths T ≤ 2, where the search space size is very small (on the order of 103). Note that using an RNN for both the encoder and decoder corresponds to a standard sequence-to-sequence model. However, we do do not rule out that a more sophisticated RNN decoder or training procedure could be possibly more successful. 6 RELATED WORK Machine Learning for Inductive Program Synthesis. There is relatively little work on using machine learning for programming by example. The most closely related work is that of Menon et al. (2013), in which a hand-coded set of features of input-output examples are used as \u201Cclues.\u201D When a clue appears in the input-output examples (e.g., the output is a permutation of the input), it reweights the probabilities of productions in a probabilistic context free grammar by a learned amount. This work shares the idea of learning to guide the search over program space conditional on input-output examples. One difference is in the domains. Menon et al. (2013) operate on short string manipulation programs, where it is arguably easier to hand-code features to recognize patterns in the input-output examples (e.g., if the outputs are always permutations or substrings of the input). Our work shows that there are strong cues in patterns in input-output examples in the domain of numbers and lists. However, the main difference is the scale. Menon et al. (2013) learns from a small (280 examples), manually-constructed dataset, which limits the capacity of the machine learning model that can be trained. Thus, it forces the machine learning component to be relatively simple. Indeed, Menon et al. (2013) use a log-linear model and rely on hand-constructed features. LIPS automatically generates training data, which yields datasets with millions of programs and enables high-capacity deep learning models to be brought to bear on the problem. Learning Representations of Program State. Piech et al. (2015) propose to learn joint embed- dings of program states and programs to automatically extend teacher feedback to many similar programs in the MOOC setting. This work is similar in that it considers embedding program states, but the domain is different, and it otherwise specifically focuses on syntactic differences between semantically equivalent programs to provide stylistic feedback. Li et al. (2016) use graph neural networks (GNNs) to predict logical descriptions from program states, focusing on data structure shapes instead of numerical and list data. Such GNNs may be a suitable architecture to encode states appearing when extending our DSL to handle more complex data structures. Learning to Infer. Very recently, Alemi et al. (2016) used neural sequence models in tandem with an automated theorem prover. Similar to our setup, a neural network component is trained to select premises that the theorem prover can use to prove a theorem. The main differences are in the domains, and that they train on an existing corpus of theorems. More broadly, if we view a DSL as defining a model and search as a form of inference algorithm, then there is a large body of work on using discriminatively-trained models to aid inference in generative models. Examples include Dayan et al. (1995); Kingma & Welling (2013); Shotton et al. (2013); Stuhlmüller et al. (2013); Heess et al. (2013); Jampani et al. (2015). 7 DISCUSSION AND FUTURE WORK We have presented a framework for improving IPS systems by using neural networks to translate cues in input-output examples to guidance over where to search in program space. Our empirical results show that for many programs, this technique improves the runtime of a wide range of IPS baselines by 1-3 orders. We have found several problems in real online programming challenges that can be solved with a program in our language, which validates the relevance of the class of problems that we have studied in this work. In sum, this suggests that we have made significant progress towards being able to solve programming competition problems, and the machine learning component plays an important role in making it tractable. There remain some limitations, however. First, the programs we can synthesize are only the simplest problems on programming competition websites and are simpler than most competition problems. Many problems require more complex algorithmic solutions like dynamic programming and search, 8 Under review as a conference paper at ICLR 2017 which are currently beyond our reach. Our chosen DSL currently cannot express solutions to many problems. To do so, it would need to be extended by adding more primitives and allow for more flexibility in program constructs (such as allowing loops). Second, we currently use five input-output examples with relatively large integer values (up to 256 in magnitude), which are probably more informative than typical (smaller) examples. While we remain optimistic about LIPS\u2019s applicability as the DSL becomes more complex and the input-output examples become less informative, it remains to be seen what the magnitude of these effects are as we move towards solving large subsets of programming competition problems. We foresee many extensions of DeepCoder. We are most interested in better data generation pro- cedures by using generative models of source code, and to incorporate natural language problem descriptions to lessen the information burden required from input-output examples. In sum, Deep- Coder represents a promising direction forward, and we are optimistic about the future prospects of using machine learning to synthesize programs. ACKNOWLEDGMENTS The authors would like to express their gratitude to Rishabh Singh for his valuable guidance on using the Sketch program synthesis system. REFERENCES Alex A. Alemi, François Chollet, Geoffrey Irving, Christian Szegedy, and Josef Urban. DeepMath - deep sequence models for premise selection. 2016. To appear. Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip H. S. Torr, and M. Pawan Kumar. Adaptive neural compilation. CoRR, abs\/1605.07969, 2016. URL http:\/\/arxiv.org\/abs\/1605. 07969. Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation, 7(5):889\u2013904, 1995. Krzysztof Dembczyński, Willem Waegeman, Weiwei Cheng, and Eyke Hüllermeier. On label de- pendence and loss minimization in multi-label classification. Machine Learning, 88(1):5\u201345, 2012. Krzysztof J. Dembczynski, Weiwei Cheng, and Eyke Hllermeier. Bayes optimal multilabel classifi- cation via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 279\u2013286, 2010. John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pp. 229\u2013239, 2015. Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. CoRR, abs\/1608.04428, 2016. URL http:\/\/arxiv.org\/abs\/1608.04428. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs\/1410.5401, 2014. URL http:\/\/arxiv.org\/abs\/1410.5401. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska- Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016. Sumit Gulwani. Programming by examples: Applications, algorithms, and ambiguity resolution. In Proceedings of the 8th International Joint Conference on Automated Reasoning (IJCAR), pp. 9\u201314, 2016. Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of loop-free programs. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pp. 62\u201373, 2011. 9 http:\/\/arxiv.org\/abs\/1605.07969 http:\/\/arxiv.org\/abs\/1605.07969 http:\/\/arxiv.org\/abs\/1608.04428 http:\/\/arxiv.org\/abs\/1410.5401 Under review as a conference paper at ICLR 2017 Nicolas Heess, Daniel Tarlow, and John Winn. Learning to pass expectation propagation messages. In Advances in Neural Information Processing Systems, pp. 3219\u20133227, 2013. Varun Jampani, Sebastian Nowozin, Matthew Loper, and Peter V Gehler. The informed sampler: A discriminative approach to bayesian inference in generative computer vision models. Computer Vision and Image Understanding, 136:32\u201344, 2015. Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recur- rent nets. In Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pp. 190\u2013198, 2015. Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In Proceedings of the 4th Interna- tional Conference on Learning Representations., 2016. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In Proceedings of the 4th International Conference on Learning Representations 2016, 2015. URL http:\/\/arxiv.org\/abs\/1511.06392. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In Proceedings of the 4th International Conference on Learning Representations (ICLR), 2016. URL http:\/\/arxiv.org\/abs\/1511.05493. Aditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W Lampson, and Adam Kalai. A machine learning framework for programming by example. In Proceedings of the International Conference on Machine Learning (ICML), 2013. Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer: Inducing latent pro- grams with gradient descent. In Proceedings of the 4th International Conference on Learning Representations 2016, 2016. Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, and Leonidas J. Guibas. Learning program embeddings to propagate feedback on student code. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp. 1093\u20131102, 2015. Oleksandr Polozov and Sumit Gulwani. Flashmeta: a framework for inductive program synthesis. In OOPSLA, pp. 107\u2013126, 2015. Scott E. Reed and Nando de Freitas. Neural programmer-interpreters. 2016. Sebastian Riedel, Matko Bosnjak, and Tim Rocktäschel. Programming with a differentiable forth interpreter. CoRR, abs\/1605.06640, 2016. URL http:\/\/arxiv.org\/abs\/1605.06640. Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic program optimization. Commununications of the ACM, 59(2):114\u2013122, 2016. Jamie Shotton, Toby Sharp, Alex Kipman, Andrew Fitzgibbon, Mark Finocchio, Andrew Blake, Mat Cook, and Richard Moore. Real-time human pose recognition in parts from single depth images. Communications of the ACM, 56(1):116\u2013124, 2013. Rishabh Singh and Sumit Gulwani. Predicting a correct program in programming by example. In Proceedings of the 27th Conference on Computer Aided Verification (CAV), pp. 398\u2013414, 2015. Armando Solar-Lezama. Program Synthesis By Sketching. PhD thesis, EECS Dept., UC Berkeley, 2008. Andreas Stuhlmüller, Jessica Taylor, and Noah D. Goodman. Learning stochastic inverses. 2013. URL http:\/\/stuhlmueller.org\/papers\/inverses-nips2013.pdf. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Informa- tion Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2440\u20132448, 2015. 10 http:\/\/arxiv.org\/abs\/1511.06392 http:\/\/arxiv.org\/abs\/1511.05493 http:\/\/arxiv.org\/abs\/1605.06640 http:\/\/stuhlmueller.org\/papers\/inverses-nips2013.pdf Under review as a conference paper at ICLR 2017 Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Proceedings of the 3rd International Conference on Learning Representations 2015, 2014. URL http:\/\/arxiv.org\/ abs\/1410.3916. Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple algorithms from examples. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, pp. 421\u2013429, 2016. 11 http:\/\/arxiv.org\/abs\/1410.3916 http:\/\/arxiv.org\/abs\/1410.3916 Under review as a conference paper at ICLR 2017 A EXAMPLE PROGRAMS This section shows example programs in our Domain Specific Language (DSL), together with input- output examples and short descriptions. These programs have been inspired by simple tasks appearing on real programming competition websites, and are meant to illustrate the expressive power of our DSL. Program 0: k← int b← [int] c← SORT b d← TAKE k c e← SUM d Input-output example: Input: 2, [3 5 4 7 5] Output: [7] Description: A new shop near you is selling n paintings. You have k < n friends and you would like to buy each of your friends a painting from the shop. Return the minimal amount of money you will need to spend. Program 1: w← [int] t← [int] c← MAP (*3) w d← ZIPWITH (+) c t e← MAXIMUM d Input-output example: Input: [6 2 4 7 9], [5 3 -6 1 0] Output: 27 Description: In soccer leagues, match winners are awarded 3 points, losers 1 point, and both teams get 1 point in the case of a tie. Com- pute the number of points awarded to the winner of a league given two arrays w, t of the same length, where w[i] (resp. t[i]) is the number of times team i won (resp. tied). Program 2: a← [int] b← [int] c← ZIPWITH (-) b a d← COUNT (>0) c Input-output example: Input: [6 2 4 7 9], [5 3 2 1 0] Output: 4 Description: Alice and Bob are comparing their results in a recent exam. Given their marks per ques- tion as two arrays a and b, count on how many questions Alice got more points than Bob. Program 3: h← [int] b← SCANL1 MIN h c← ZIPWITH (-) h b d← FILTER (>0) c e← SUM d Input-output example: Input: [8 5 7 2 5] Output: 5 Description: Perditia is very peculiar about her garden and wants that the trees standing in a row are all of non-increasing heights. Given the tree heights in centimeters in order of the row as an array h, compute how many centimeters she needs to trim the trees in total. Program 4: x← [int] y← [int] c← SORT x d← SORT y e← REVERSE d f← ZIPWITH (*) d e g← SUM f Input-output example: Input: [7 3 8 2 5], [2 8 9 1 3] Output: 79 Description: Xavier and Yasmine are laying sticks to form non-overlapping rectangles on the ground. They both have fixed sets of pairs of sticks of certain lengths (represented as arrays x and y of numbers). Xavier only lays sticks parallel to the x axis, and Yasmine lays sticks only parallel to y axis. Compute the area their rectangles will cover at least. Program 5: a← [int] b← REVERSE a c← ZIPWITH MIN a b Input-output example: Input: [3 7 5 2 8] Output: [3 2 5 2 3] Description: A sequence called Billy is looking into the mirror, wondering how much weight it could lose by replacing any of its elements by their mirror images. Given a description of Billy as an array b of length n, return an array c of minimal sum where each element c[i] is either b[i] or its mirror image b[n− i− 1]. 12 Under review as a conference paper at ICLR 2017 Program 6: t← [int] p← [int] c← MAP (-1) t d← MAP (-1) p e← ZIPWITH (+) c d f← MINIMUM e IO example: Input: [4 8 11 2], [2 3 4 1] Output: 1 Description: Umberto has a large collection of ties and match- ing pocket squares\u2014too large, his wife says\u2014and he needs to sell one pair. Given their values as arrays t and p, assuming that he sells the cheapest pair, and selling costs 2, how much will he lose from the sale? Program 7: s← [int] p← [int] c← SCANL1 (+) p d← ZIPWITH (*) s c e← SUM d IO example: Input: [4 7 2 3], [2 1 3 1] Output: 48 Description: Zack always promised his n friends to buy them candy, but never did. Now he won the lottery and counts how often and how much candy he promised to his friends, obtaining arrays p (num- ber of promises) and s (number of promised sweets). He announces that to repay them, he will buy s[1]+s[2]+...+s[n] pieces of candy for the first p[1] days, then s[2]+s[3]+...+s[n] for p[2], and so on, until he has fulfilled all promises. How much candy will he buy in total? Program 8: s← [int] b← REVERSE s c← ZIPWITH (-) b s d← FILTER (>0) c e← SUM d IO example: Input: [1 2 4 5 7] Output: 9 Description: Vivian loves rearranging things. Most of all, when she sees a row of heaps, she wants to make sure that each heap has more items than the one to its left. She is also obsessed with efficiency, so always moves the least possible number of items. Her dad really dislikes if she changes the size of heaps, so she only moves single items between them, making sure that the set of sizes of the heaps is the same as at the start; they are only in a different order. When you come in, you see heaps of sizes (of course, sizes strictly monotonically increasing) s[0], s[1], ... s[n]. What is the maximal number of items that Vivian could have moved? Fig. 4 shows the predictions made by a neural network trained on programs of length T = 4 that were ensured to be semantically disjoint from all 9 example programs shown in this section. For each task, the neural network was provided with 5 input-output examples. (+ 1 ) (- 1 ) (* 2 ) (\/ 2 ) (* -1 ) (* *2 ) (* 3 ) (\/ 3 ) (* 4 ) (\/ 4 ) (> 0 ) (< 0 ) (% 2 = = 1 ) (% 2 = = 0 ) H E A D LA S T M A P FI LT E R S O R T R E V E R S E T A K E D R O P A C C E S S Z IP W IT H S C A N L1 + - * M IN M A X C O U N T M IN IM U M M A X IM U M S U M 0: SORT b | TAKE a c | SUM d 1: MAP (*3) a | ZIPWITH + b c | MAXIMUM d 2: ZIPWITH - b a | COUNT (>0) c 3: SCANL1 MIN a | ZIPWITH - a b | FILTER (>0) c | SUM d 4: SORT a | SORT b | REVERSE d | ZIPWITH * d e | SUM f 5: REVERSE a | ZIPWITH MIN a b 6: MAP (-1) a | MAP (-1) b | ZIPWITH + c d | MINIMUM e 7: SCANL1 + b | ZIPWITH * a c | SUM d 8: REVERSE a | ZIPWITH - b a | FILTER (>0) c | SUM d .0 .2 .0 .1 .4 .0 .0 .2 .0 .1 .0 .2 .1 .0 .1 .0 .3 .4 .2 .1 .5 .2 .2 .6 .5 .2 .4 .0 .9 .1 .0 .1 .0 1.0 .1 .1 .1 .1 .0 .0 1.0 .0 .1 .0 .2 .1 .1 .1 .0 .3 1.0 .2 .1 .1 .0 .0 .1 1.0 .0 .6 .6 .0 .1 .1 .2 .0 .9 .0 .1 .2 .0 .1 .0 .0 .0 .1 .0 .1 .2 .2 .3 .3 .0 .0 .6 .0 .1 .1 .0 .0 .0 1.0 .3 .4 .5 .0 .5 .5 1.0 .0 .0 .0 .3 .1 .1 .1 .1 .0 .0 .0 .0 .0 .1 .0 .0 .0 .0 .0 .6 .2 .1 .1 .0 .0 .0 1.0 .3 .3 .3 .1 .2 .7 .0 .0 .1 1.0 .0 .0 .1 .4 .1 .4 .0 .0 .2 .0 .0 .2 .0 .2 .1 .2 .9 .2 .1 .0 .0 .0 .4 .6 .2 .2 .3 .3 .4 .1 .2 .4 .0 .4 .2 .2 .0 .2 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .0 .9 .0 .0 1.0 .0 .0 .0 1.0 .0 .2 .0 .0 1.0 .1 .0 .0 .0 .0 .1 .1 .0 .0 .0 .0 .0 .0 .0 .0 .0 .2 .2 .2 .7 .0 .3 .3 .1 .0 .0 .0 .0 1.0 .1 .9 .1 .0 .7 .2 .1 .8 .0 .0 .0 .0 .0 .0 .0 .1 .0 .0 .1 .0 .1 .1 .1 .1 .0 .1 .4 .1 .0 .0 .0 .0 .0 1.0 .8 .5 .4 1.0 .1 .0 .2 .0 .1 .7 .2 .1 .0 .1 .1 .0 .0 .1 .0 .1 .1 .1 .1 .0 .0 .0 .5 .5 .1 .0 .0 .0 .0 1.0 .4 .4 .5 .0 .3 .6 .0 .0 .1 1.0 Figure 4: Predictions of a neural network on the 9 example programs described in this section. Numbers in squares would ideally be close to 1 (function is present in the ground truth source code), whereas all other numbers should ideally be close to 0 (function is not needed). B EXPERIMENTAL RESULTS Results presented in Sect. 5.1 showcased the computational speedups obtained from the LIPS frame- work (using DeepCoder), as opposed to solving each program synthesis problem with only the 13 Under review as a conference paper at ICLR 2017 information about global incidence of functions in source code available. For completeness, here we show plots of raw computation times of each search procedure to solve a given number of problems. Fig. 5 shows the computation times of DFS, of Enumerative search with a Sort and add scheme, of the Sketch solver with a Sort and add scheme, and of Beam search, when searching for a program consistent with input-output examples generated from P = 100 different test programs of length T = 3. As discussed in Sect. 5.1, these test programs were ensured to be semantically disjoint from all programs used to train the neural networks, as well as from all programs of shorter length (as discussed in Sect. 4.2). 10-4 10-3 10-2 10-1 100 101 102 103 Solver computation time [s] 0 20 40 60 80 100 P ro g ra m s so lv e d DFS: using neural network DFS: using prior order Enumeration: Sort and add using neural network Enumeration: Sort and add in prior order Beam search Sketch: Sort and add using neural network Sketch: Sort and add in prior order Figure 5: Number of test problems solved versus computation time. Fig. 6 shows the computation times of DFS and Enumerative search with a Sort and add scheme when searching for programs consistent with input-output examples generated from P = 100 different test programs of length T = 5. The neural network was trained on programs of length T = 4. 10-4 10-3 10-2 10-1 100 101 102 103 104 Solver computation time [s] 0 20 40 60 80 100 P ro g ra m s so lv e d DFS: using neural network DFS: using prior order Enumeration: Sort and add using neural network Enumeration: Sort and add in prior order Figure 6: Number of test problems solved versus computation time. C THE NEURAL NETWORK As briefly described in Sect. 4.3, we used the following simple feed-forward architecture encoder: \u2022 For each input-output example in the set generated from a single ground truth program: \u2013 Pad arrays appearing in the inputs and in the output to a maximum length L = 20 with a special NULL value. \u2013 Represent the type (singleton integer or integer array) of each input and of the output using a one-hot-encoding vector. Embed each integer in the valid integer range (−256 to 255) using a learned embedding into E = 20 dimensional space. Also learn an embedding for the padding NULL value. \u2013 Concatenate the representations of the input types, the embeddings of integers in the inputs, the representation of the output type, and the embeddings of integers in the output into a single (fixed-length) vector. \u2013 Pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each. \u2022 Pool the last hidden layer encodings of each input-output example together by simple arith- metic averaging. Fig. 7 shows a schematic drawing of this encoder architecture, together with the decoder that performs independent binary classification for each function in the DSL, indicating whether or not it appears in the ground truth source code. 14 Under review as a conference paper at ICLR 2017 Inputs 1 Outputs 1 \u2026Program State State Embeddings Hiddens 1 Hiddens 2 Hiddens 3 Pooled Inputs 5 Outputs 5 \u2026 Final Activations Sigmoids Attribute Predictions Figure 7: Schematic representation of our feed-forward encoder, and the decoder. While DeepCoder learns to embed integers into a E = 20 dimensional space, we built the system up gradually, starting with a E = 2 dimensional space and only training on programs of length T = 1. Such a small scale setting allowed easier investigation of the workings of the neural network, and indeed Fig. 8 below shows a learned embedding of integers in R2. The figure demonstrates that the network has learnt the concepts of number magnitude, sign (positive or negative) and evenness, presumably due to FILTER (>0), FILTER (<0), FILTER (%2==0) and FILTER (%2==1) all being among the programs on which the network was trained. First embedding dimension φ1(n) S e co n d e m b e d d in g d im e n si o n φ 2 (n ) -256-255 -7 -6 -5 -4 -3 -2 -1 01 2 3 4 5 6 7 254 255 Null even positive numbers even negative number odd positive numbers odd negative numbers zero Null (padding value) Figure 8: A learned embedding of integers {−256,−255, . . . ,−1, 0, 1, . . . , 255} in R2. The color intensity corresponds to the magnitude of the embedded integer. 15 Under review as a conference paper at ICLR 2017 D DEPTH-FIRST SEARCH We use an optimized C++ implementation of depth-first search (DFS) to search over programs with a given maximum length T . In depth-first search, we start by choosing the first function (and its arguments) of a potential solution program, and then recursively consider all ways of filling in the rest of the program (up to length T ), before moving on to a next choice of first instruction (if a solution has not yet been found). A program is considered a solution if it is consistent with all M = 5 provided input-output examples. Note that this requires evaluating all candidate programs on the M inputs and checking the results for equality with the provided M respective outputs. Our implementation of DFS exploits the sequential structure of programs in our DSL by caching the results of evaluating all prefixes of the currently considered program on the example inputs, thus allowing efficient reuse of computation between candidate programs with common prefixes. This allows us to explore the search space at roughly the speed of ∼ 3× 106 programs per second. When the search procedure extends a partial program by a new function, it has to try the functions in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their predicted probabilities from the neural network. The probability of a function consisting of a higher- order function and a lambda is taken to be the minimum of the probabilities of the two constituent functions. E TRAINING LOSS FUNCTION In Sect. 4.5 we outlined a justification for using marginal probabilities of individual functions as a sensible intermediate representation to provide a solver employing a Sort and add scheme (we considered Enumerative search and the Sketch solver with this scheme). Here we provide a more detailed discussion. Predicting program components from input-output examples can be cast as a multilabel classification problem, where each instance (set of input-output examples) is associated with a set of relevant labels (functions appearing in the code that generated the examples). We denote the number of labels (functions) by C, and note that throughout this work C = 34. When the task is to predict a subset of labels y ∈ {0, 1}C , different loss functions can be employed to measure the prediction error of a classifier h(x) or ranking function f(x). Dembczynski et al. (2010) discuss the following three loss functions: \u2022 Hamming loss counts the number of labels that are predicted incorrectly by a classifier h: LH(y,h(x)) = C∑ c=1 1{yc 6=hc(x)} \u2022 Rank loss counts the number of label pairs violating the condition that relevant labels are ranked higher than irrelevant ones by a scoring function f : Lr(y, f(x)) = C∑ (i,j):yi=1,yj=0 1{fi<fj} \u2022 Subset Zero-One loss indicates whether all labels have been correctly predicted by h: Ls(y,h(x)) = 1{y 6=h(x)} Dembczynski et al. (2010) proved that Bayes optimal decisions under the Hamming and Rank loss functions, i.e., decisions minimizing the expected loss under these loss functions, can be computed from marginal probabilities pc(yc|x). This suggests that: \u2022 Multilabel classification under these two loss functions may not benefit from considering dependencies between the labels. 16 Under review as a conference paper at ICLR 2017 \u2022 \u201DInstead of minimizing the Rank loss directly, one can simply use any approach for single label prediction that properly estimates the marginal probabilities.\u201D (Dembczyński et al., 2012) Training the neural network with the negative cross entropy loss function as the training objective is precisely a method for properly estimating the marginal probabilities of labels (functions appearing in source code). It is thus a sensible step in preparation for making predictions under a Rank loss. It remains to discuss the relationship between the Rank loss and the actual quantity we care about, which is the total runtime of a Sort and add search procedure. Recall the simplifying assumption that the runtime of searching for a program of length T with C functions made available to the search is proportional to CT , and consider a Sort and add search for a program of length T , where the size of the active set is increased by 1 whenever the search fails. Starting with an active set of size 1, the total time until a solution is found can be upper bounded by 1T + 2T + · · ·+ CTA ≤ CT+1A ≤ CC T A where CA is the size of the active set when the search finally succeeds (i.e., when the active set finally contains all necessary functions for a solution to exist). Hence the total runtime of a Sort and add search can be upper bounded by a quantity that is proportional to CTA . Now fix a valid program solution P that requires CP functions, and let yP ∈ {0, 1}C be the indicator vector of functions used by P . Let D := CA − CP be the number of redundant operations added into the active set until all operations from P have been added. Example 1. Suppose the labels, as sorted by decreasing predicted marginal probabilities f(x), are as follows: 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Then the solution P contains CP = 6 functions, but the active set needs to grow to size CA = 11 to include all of them, adding D = 5 redundant functions along the way. Note that the rank loss of the predictions f(x) is Lr(yP , f(x)) = 2 + 5 = 7, as it double counts the two redundant functions which are scored higher than two relevant labels. Noting that in general Lr(yP , f(x)) ≥ D, the previous upper bound on the runtime of Sort and add can be further upper bounded as follows: CTA = (CP +D) T ≤ const + const×DT ≤ const + const× Lr(yP , f(x))T Hence we see that for a constant value of T , this upper bound can be minimized by optimizing the Rank loss of the predictions f(x). Note also that Lr(yP , f(x)) = 0 would imply D = 0, in which case CA = CP . F DOMAIN SPECIFIC LANGUAGE OF DEEPCODER Here we provide a description of the semantics of our DSL from Sect. 4.1, both in English and as a Python implementation. Throughout, NULL is a special value that can be set e.g. to an integer outside the working integer range. First-order functions: \u2022 HEAD :: [int] -> int lambda xs: xs[0] if len(xs)>0 else Null Given an array, returns its first element (or NULL if the array is empty). \u2022 LAST :: [int] -> int lambda xs: xs[-1] if len(xs)>0 else Null Given an array, returns its last element (or NULL if the array is empty). \u2022 TAKE :: int -> [int] -> int lambda n, xs: xs[:n] Given an integer n and array xs, returns the array truncated after the n-th element. (If the length of xs was no larger than n in the first place, it is returned without modification.) 17 Under review as a conference paper at ICLR 2017 \u2022 DROP :: int -> [int] -> int lambda n, xs: xs[n:] Given an integer n and array xs, returns the array with the first n elements dropped. (If the length of xs was no larger than n in the first place, an empty array is returned.) \u2022 ACCESS :: int -> [int] -> int lambda n, xs: xs[n] if n>=0 and len(xs)>n else Null Given an integer n and array xs, returns the (n+1)-st element of xs. (If the length of xs was less than or equal to n, the value NULL is returned instead.) \u2022 MINIMUM :: [int] -> int lambda xs: min(xs) if len(xs)>0 else Null Given an array, returns its minimum (or NULL if the array is empty). \u2022 MAXIMUM :: [int] -> int lambda xs: max(xs) if len(xs)>0 else Null Given an array, returns its maximum (or NULL if the array is empty). \u2022 REVERSE :: [int] -> [int] lambda xs: list(reversed(xs)) Given an array, returns its elements in reversed order. \u2022 SORT :: [int] -> [int] lambda xs: sorted(xs) Given an array, return its elements in non-decreasing order. \u2022 SUM :: [int] -> int lambda xs: sum(xs) Given an array, returns the sum of its elements. (The sum of an empty array is 0.) Higher-order functions: \u2022 MAP :: (int -> int) -> [int] -> [int] lambda f, xs: [f(x) for x in xs] Given a lambda function f mapping from integers to integers, and an array xs, returns the array resulting from applying f to each element of xs. \u2022 FILTER :: (int -> bool) -> [int] -> [int] lambda f, xs: [x for x in xs if f(x)] Given a predicate f mapping from integers to truth values, and an array xs, returns the elements of xs satisfying the predicate in their original order. \u2022 COUNT :: (int -> bool) -> [int] -> int lambda f, xs: len([x for x in xs if f(x)]) Given a predicate f mapping from integers to truth values, and an array xs, returns the number of elements in xs satisfying the predicate. \u2022 ZIPWITH :: (int -> int -> int) -> [int] -> [int] -> [int] lambda f, xs, ys: [f(x, y) for (x, y) in zip(xs, ys)] Given a lambda function f mapping integer pairs to integers, and two arrays xs and ys, returns the array resulting from applying f to corresponding elements of xs and ys. The length of the returned array is the minimum of the lengths of xs and ys. \u2022 SCANL1 :: (int -> int -> int) -> [int] -> [int] Given a lambda function f mapping integer pairs to integers, and an array xs, returns an array ys of the same length as xs and with its content defined by the recurrence ys[0] = xs[0], ys[n] = f(ys[n-1], xs[n]) for n ≥ 1. The INT→INT lambdas (+1), (-1), (*2), (\/2), (*(-1)), (**2), (*3), (\/3), (*4), (\/4) provided by our DSL map integers to integers in a self-explanatory manner. The INT→BOOL lambdas (>0), (<0), (%2==0), (%2==1) respectively test positivity, negativity, evenness and oddness of the input integer value. Finally, the INT→INT→INT lambdas (+), (-), (*), MIN, MAX apply a function to a pair of integers and produce a single integer. As an example, consider the function SCANL1 MAX, consisting of the higher-order function SCANL1 and the INT→INT→INT lambda MAX. Given an integer array a of length L, this function computes 18 Under review as a conference paper at ICLR 2017 the running maximum of the array a. Specifically, it returns an array b of the same length L whose i-th element is the maximum of the first i elements in a. 19 Introduction Background on Inductive Program Synthesis Learning Inductive Program Synthesis (LIPS) DeepCoder Domain Specific Language and Attributes Data Generation Machine Learning Model Search Training Loss Function Experiments DeepCoder Compared to Baselines Generalization across program lengths Alternative models Related Work Discussion and Future Work Example Programs Experimental Results The Neural Network Depth-First Search Training Loss Function Domain Specific Language of DeepCoder ","flair":"three\tResearch"}
{"author":"syllogism_","created":"Thu Nov 10 13:47:05 EST 2016","text":" Blog About Request Kemal Şanlı Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models by Matthew Honnibal on November 10, 2016Over the last six months, a powerful new neural network playbook has come together for Natural Language Processing. The new approach can be summarised as a simple four-step formula: embed, encode, attend, predict. This post explains the components of this new approach, and shows how they're put together in two recent systems. When people think about machine learning improvements they usually think about efficiency and accuracy, but the most important dimension is generality. If you want to write a program to flag abusive posts on your social media platform, you should be able to generalise the problem to \"I need to take text and predict a class ID\". It shouldn't matter whether you're flagging abusive posts or tagging emails that propose meetings. If two problems take the same type of input and produce the same type of output, we should be able to reuse the same model code, and get a different behaviour by plugging in different data \u2014 like playing different games that use the same engine. Implementation Example I've implemented the decomposable attention model for natural language inference with spaCy and Keras. You can find it here on GitHub. Let's say you have a great technique for predicting a class ID from a dense vector of real values. You're convinced you can solve any problem that has that particular input\/output \"shape\". Separately, you also have a great technique for predicting a single vector from a vector and a matrix. Now you have the solutions to three problems, not two. If you start with a matrix and a vector, and you need a class ID, you can obviously compose the two techniques. Most NLP problems can be reduced to machine learning problems that take one or more texts as input. If we can transform these texts to vectors, we can reuse general purpose deep learning solutions. Here's how to do that. A four-step strategy for deep learning with text Embedded word representations, also known as \"word vectors\", are now one of the most widely used natural language processing technologies. Word embeddings let you treat individual words as related units of meaning, rather than entirely distinct IDs. However, most NLP problems require understanding of longer spans of text, not just individual words. There's now a simple and flexible solution that is achieving excellent performance on a wide range of problems. After embedding the text into a sequence of vectors, bidirectional RNNs are used to encode the vectors into a sentence matrix. The rows of this matrix can be understood as token vectors \u2014 they are sensitive to the sentential context of the token. The final piece of the puzzle is called an attention mechanism. This lets you reduce the sentence matrix down to a sentence vector, ready for prediction. Here's how it all works. Step 1: Embed An embedding table maps long, sparse, binary vectors into shorter, dense, continuous vectors. For example, imagine we receive our text as a sequence of ASCII characters. There are 256 possible values, so we can represent each value as a binary vector with 256 dimensions. The value for a will be a vector of 0s, with a 1 at column 97, while the value for b will be a vector of zeros with a 1 at column 98. This is called the \"one hot\" encoding scheme. Different values receive entirely different vectors. Most neural network models begin by tokenising the text into words, and embedding the words into vectors. Other models extend the word vector representation with other information. For instance, it's often useful to pass forward a sequence of part-of-speech tags, in addition to the word IDs. You can then learn tag embeddings, and concatenate the tag embedding to the word embedding. This lets you push some amount of position-sensitive information into the word representation. However, there's a much more powerful way to make the word representations context-specific. Step 2: Encode Given a sequence of word vectors, the encode step computes a representation that I'll call a sentence matrix, where each row represents the meaning of each token in the context of the rest of the sentence. The technology used for this purpose is a bidirectional RNN. Both LSTM and GRU architectures have been shown to work well for this. The vector for each token is computed in two parts: one part by a forward pass, and another part by a backward pass. To get the full vector, we simply stick the two together. Here's what's being computed: Bidirectional RNNdef encode(fwd_rnn, bwd_rnn, word_vectors): fwd_out = ndarray((len(word_vectors), fwd_rnn.nr_hidden, dtype='float32') bwd_out = ndarray((len(word_vecors), bwd_rnn.nr_hidden), dtype='float32') fwd_state = fwd_rnn.initial_state() bwd_state = bwd_rnn.initial_state() for i in range(len(word_vectors)): fwd_state = fwd_rnn(word_vectors[i], fwd_state) bwd_state = bwd_rnn(word_vectors[-(i+1)], bwd_state) fwd_out[i] = fwd_state bwd_out[-(i+1)] = bwd_state return concatenate([fwd_state, bwd_state]) I think bidirectional RNNs will be one of those insights that come to feel obvious with time. However, the most direct application of an RNN is to read the text and predict something from it. What we're doing here is instead computing an intermediate representation \u2014 specifically, per token features. Crucially, the representation we're getting back represents the tokens in context. We can learn that the phrase \"pick up\" has a different meaning from the phrase \"pick on\", even if we processed the two phrases into separate tokens. This has always been a huge weakness of NLP models. Now we have a solution. Step 3: Attend The attend step reduces the matrix representation produced by the encode step to a single vector, so that it can be passed on to a standard feed-forward network for prediction. The characteristic advantage of an attention mechanism over other reduction operations is that an attention mechanism takes as input an auxiliary context vector: By reducing the matrix to a vector, you're necessarily losing information. That's why the context vector is crucial: it tells you which information to discard, so that the \"summary\" vector is tailored to the network consuming it. Recent research has shown that the attention mechanism is a flexible technique, and new variations of it can be used to create elegant and powerful solutions. For instance, Parikh et al. (2016) introduce an attention mechanism that takes two sentence matrices, and outputs a single vector: Yang et al. (2016) introduce an attention mechanism that takes a single matrix and outputs a single vector. Instead of a context vector derived from some aspect of the input, the \"summary\" is computed with reference to a context vector learned as a parameter of the model. This makes the attention mechanism a pure reduction operation, which could be used in place of any sum or average pooling step. Step 4: Predict Once the text or pair of texts has been reduced into a single vector, we can learn the target representation \u2014 a class label, a real value, a vector, etc. We can also do structured prediction, by using the network as the controller of a state machine such as a transition-based parser. Interestingly, most NLP models usually favour quite shallow feed-forward networks. This has meant that some of the most important recent technologies for computer vision, such as residual connections and batch normalisation have so far had relatively little impact in the NLP community. Example 1: A Decomposable Attention Model for Natural Language Inference Natural language inference is a problem of predicting a class label over a pair of sentences, where the class represents the logical relationship between them. The Stanford Natural Language Inference corpus uses three class labels:About the SNLI corpusThe statements in the SNLI corpus are crowd-sourced captions collected as part of the Flickr 30k corpus. In other words, the sentences being marked as entailment were typed in by workers on Mechanical Turk. This makes the data rather artificial, so absolute accuracies on the task are little indication of how \"ready\" these technologies really are. You should try to be neither impressed nor disappointed to hear that the state of the art is 88%. What matters is that standard bag-of-words techniques perform much worse. Entailment: If the first sentence is true, the second sentence must be true. Contradiction: If the first sentence is true, the second sentence must be false. Neutral: Neither entailment nor contradiction. Bowman et al. (2015) give the following examples from the corpus: Text Hypothesis Label A man inspects the uniform of a figure in some East Asian country. The man is sleeping contradiction An older and younger man smiling. Two men are smiling and laughing at the cats playing on the floor. neutral A black race car starts up in front of a crowd of people. A man is driving down a lonely road. contradiction A soccer game with multiple males playing. Some men are playing a sport. entailment A smiling costumed woman is holding an umbrella. A happy woman in a fairy costume holds an umbrella. neutral One of the motivations for the corpus is to provide a new, reasonably sized corpus for developing models that encode sentences into vectors. For example, Bowman et al. (2016) describe an interesting transition-based model that reads a sentence sequentially to construct a tree-structured internal representation. Bowman et al. were able to achieve an accuracy of 83.2, a substantial improvement over previous work. Less than six months later, Parikh et al. (2016) presented a model that achieved 86.8% accuracy, with around 10% of the parameters of Bowman et al.'s model. Soon after, Chen et al. (2016) published a system that performed even better \u2014 88.3%. When I first read Parikh et al.'s paper, I couldn't understand how their model performed so well. The answer is in the way the model mixes the two sentence matrices, using their novel attention mechanism: The crucial advantage is that the sentence-to-vector reduction operates over he sentences jointly, while Bowman et al. (2016) encode the sentences into vectors independently. Remember Vapnik's princple: When solving a problem of interest, do not solve a more general problem as an intermediate step. Vladimir Vapnik Parikh et al. (2016) take the natural language inference task to be the problem of interest. They structure their problem to solve it directly, and therefore have a big advantage over models which encode the sentences separately. Bowman et al. are more interested in the general problem, and structure their model accordingly. Their model is therefore useful in situations where Parikh et al.'s is not. For instance, with Bowman et al.'s model, you cache the sentence vector, making it much more efficient to compute a similarity matrix. Example 2: Hierarchical Attention Networks for Document Classification Document classification was the first NLP application I ever worked on. The Australian equivalent of the SEC funded a project to crawl Australian websites and automatically detect financial scams. While the project was a little ahead of its time, document classification changed surprisingly little over most of the next ten years. That's why I find the hierarchical attention networks model that Yang et al. (2016) recently published so exciting. It's the first paper I've seen to offer a really compelling general improvement over older bag-of-words models. Here's how it works. The model receives as input a document, consisting of a sequence of sentences, where each sentence consists of a sequence of word IDs. Each word of each sentence is separately embedded, to produce two sequences of word vectors, one for each sentence. The sequences are then separately encoded into two sentence matrices. An attention mechanism then separately reduces the sentence matrices to sentence vectors, which are then encoded to produce a document matrix. A final attention step reduces the document matrix to a document vector, which is then passed through the final prediction network to assign the class label. This model uses the attention mechanism as a pure reduction step: it learns to take a matrix as input, and summarise it into a vector. It does this by learning context vectors for the two attention transformations, which can be understood as representing words or sentences that the model would find ideally relevant. Alternatively, you can see the whole reduction step as a feature extraction procedure. Under this view, the context vector is just another opaque parameter. Author Methods Yelp '13 Yelp '14 Yelp '15 IMDB Yang et al. (2016) HN-ATT 68.2 70.5 71 49.4 Yang et al. (2016) HN-AVE 67 69.3 69.9 47.8 Tang et al. (2015) Paragraph Vector 57.7 59.2 60.5 34.1 Tang et al. (2015) SVM + Bigrams 57.6 61.6 62.4 40.9 Tang et al. (2015) SVM + Unigrams 58.9 60 61.1 39.9 Tang et al. (2015) CNN-word 59.7 61 61.5 37.6 An interesting comparison can be drawn between the Yang et al. (2016) model and a convolutional neural network (CNN). Both models are able to automatically extract position-sensitive features. However, the CNN model is both less general and less efficient. With the bidirectional RNN, each sentence only needs to be read twice \u2014 once forwards, and once backwards. The LSTM encoding can also extract features of arbitrary length, because any aspect of the sentence context might be mixed into the token's vector representation. The procedures for reducing the sentence matrix to a vector is also simple and efficient. To construct the document vector, the same procedure is simply applied again. The main factor that drives the model's accuracy is the bidirectional LSTM encoder, to create the position-sensitive features. The authors demonstrate this by swapping the attention mechanism out for average pooling. With average pooling, the model still outperforms the previous state-of-the-art on all benchmarks. However, the attention mechanism improves performance further on all evaluations. I find this especially interesting. The implications are quite general \u2014 there are after all plenty of situations where you want to reduce a matrix to a vector for further prediction, without reference to any particular external context. Next Steps I've implemented the entailment model for use with our NLP library, spaCy, and I'm working on an implementation of the text classification system. We're also planning to ship a general-purpose bidirectional LSTM model for use with spaCy, to make it easy to use pre-trained token vectors on your problems. If you have a clear idea of the function you're trying to learn \u2014 if you know your inputs and outputs of your problem \u2014 there's probably a simple and compelling solution to your problem. We can help you find it, or build it for you entirely \u2014 just get in touch. Bibliography Decomposable Attention Model for Natural Language Inference Parikh, Ankur P.; Tackström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (2016) Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference Chen, Qian; Zhu, Xiaodan; Ling, Zhenhua; Wei, Si (2016) Hierarchical Attention Networks for Document Classification Yang, Zichao; Yang, Diyi; Dyer, Chris; He, Xiaodong; Smola, Alex; Hovy, Eduard (2016) A Fast Unified Model for Parsing and Sentence Understanding Bowman, Samuel R.; Gauthier, Jon; Rastogi, Abhinav; Gupta, Raghav; Manning, Christopher D.; Potts, Christopher (2016) A large annotated corpus for learning natural language inference Bowman, Samuel R.; Angeli, Gabor; Potts, Christopher; Manning, Christopher D. (2015) Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding Johnson, Rie; Zhang, Tong (2015) About the Author Matthew Honnibal Matthew is a leading expert in AI technology, known for his research, software and writings. He completed his PhD in 2009, and spent a further 5 years publishing research on state-of-the-art natural language understanding systems. Anticipating the AI boom, he left academia in 2014 to develop spaCy, an open-source library for industrial-strength NLP. Join our mailing list Stay in the loop! Sign up Explosion.ai Explosion AI is a digital studio specialising in Artificial Intelligence and Natural Language Processing. We design custom algorithms, applications and data assets. We're the makers of spaCy, the leading open source NLP library. Read more... Legal \/ Imprint Latest Blog Posts Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models The spaCy user survey: results and analysis Building your bot's brain with Node.js and spaCy spaCy v1.0: Deep Learning with custom pipelines and Keras ","flair":"three\tResearch"}
{"author":"[deleted]","created":"Tue Sep 27 14:21:32 EDT 2016","text":"If you found this interesting or useful, please use the links to the services below to share it with other readers. You will need a free account with each service to share an item via that service.","flair":"null\tnull"}
{"author":"undefdev","created":"Sun Oct 23 03:44:00 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.02391 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization Authors: Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra (Submitted on 7 Oct 2016) Abstract: We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are \"important\" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a \"stronger\" deep network from a \"weaker\" one even when both networks make identical predictions. Comments: 17 pages, 16 figures Subjects: Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG) Cite as: arXiv:1610.02391 [cs.CV]   (or arXiv:1610.02391v1 [cs.CV] for this version) Submission history From: Ramprasaath Ramasamy Selvaraju [view email] [v1] Fri, 7 Oct 2016 19:54:24 GMT (8245kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"lopespm","created":"Thu Oct 06 09:40:47 EDT 2016","text":"Above is the built deep Q-network (DQN) agent playing Out Run, trained for a total of 1.8 million frames on a Amazon Web Services g2.2xlarge (GPU enabled) instance. The agent was built using python and tensorflow. The Out Run game emulator is a modified version of Cannonball. All source code for this project is available on GitHub. The agent learnt how to play by being rewarded for high speeds and penalized for crashing or going off road. It fetched the game\u2019s screens, car speed, number of off-road wheels and collision state from the emulator and issued actions to it such as pressing the left, right, accelerate or brake virtual button. Agent trainer implements the deep Q-learning algorithm used by Google\u2019s DeepMind Team to play various Atari 2600 games. It uses a reward function and hyperparameters that fit best for Out Run, but could potentially be used to play other games or solve other problems. There is a wealth of good information about this reinforcement learning algorithm, but I found some topics difficult to grasp or contextualize solely from the information available online. I will attempt to add my humble contribution by tackling these and also provide details about the project\u2019s implementation, results and how it can be used\/modified\/deployed. Let\u2019s start by one of its main gears: Q-learning At the heart of deep Q-learning lies Q-learning, a popular and effective model-free algorithm for learning from delayed reinforcement. Jacob Schrum has made available a terse and accessible explanation which takes around 45 minutes to watch and serves as a great starting point for the paragraphs below. Let\u2019s take the canonical reinforcement learning example presented by Jacob (grid world): To implement this algorithm, we need to build the Q-function (one of the forms of the Bell-Equation) by using the Q-value iteration update: In the above grid, there are 9 actionable states, 2 terminal states and 4 possible actions (left, right, up, down), resulting in 36 (9 actionable states x 4 possible actions) Q-values. This project aims to train an agent to play Out Run via its game screens, so for the sake of argument, let´s consider that each game screen is transformed into a 80x80 greyscale image (each pixel value ranging from 0 to 255), and that each transformed image represents a state. 6400 pixels (80x80) and 256 possible values per pixel translate to 256 possible states. This value alone is a good indicator of how inflated the number of possible Q-values will be. Multiplying 9 possible actions by 256 possible states results in 256 x 9 possible Q-values. If we use multiple and\/or colored images for state representation, then this value will be even higher. Quite unwieldy if we want to store these values in a table or similar structure. Artificial neural networks work quite well for inferring the mapping implied by data, giving them the ability to predict an approximated output from an input that they never saw before. No longer do we need to store all state\/action pair\u2019s q-values, we can now model these mappings in a more general, less redundant way. This is perfect. We can now use a neural network to model the Q-function: the network would accept a state\/action combination as input and would output the corresponding Q-value. Training-wise, we can feed in the state\/action combo to get the network\u2019s Q-value output, and calculate the expected Q-value using the formula above. With these two values, we can perform a gradient step on the (for example) squared difference between the expected value and the network\u2019s output. This is perfect, but there is still room for improvement. Imagine we have 5 possible actions for any given state: to get the optimal future value estimate (consequent state\u2019s maximum Q-value) we need to ask (forward pass) our neural network for a Q-value 5 times per learning step. Another approach (used in DeepMind\u2019s network) would be to feed in the game\u2019s screens and have the network output the Q-value for each possible action. This way, a single forward pass would output all the Q-values for a given state, translating to one forward pass per optimal future value estimate. Q-learning and neural networks are the center pieces of a deep Q-network reinforcement learning agent and I think that by understanding them and how they fit together, it can be easier to picture how the algorithm works as a whole. Above is an overall representation of how the different components relate during a play evaluation, centered around the , the main decision component. Each game screen is resized to a desaturated 80x80 pixels image (opposed to 84x84 on DeepMind\u2019s papers), and if you might be wondering why each state is a sequence of four game screens instead of one, that is because the agent\u2019s history is used for better motion perception. Achieving this requires a sequence of preprocessed images to be stacked in channels (like you would stack RGB channels on a colored image) and fed to the network. Note that RGB channels and agent history could be used simultaneously for state representation. For example, with three channels per (RGB) image and an agent history length of four, the network would be fed twelve channels per input state. The network\u2019s architecture is essentially the same used by DeepMind, except for the first convolutional neural network\u2019s input (80x80x4 instead of 84x84x4, to account for the different input sizes) and the linear layer\u2019s output (9 instead of 18, to account for the different number of actions available) The algorithm used to train this network is well described here (page 7) and here, but I would like to present it graphically, to hopefully provide some further intuition. Below is agent trainer´s implementation of the aforementioned algorithm. It adds some new concepts which were not approached by this article: The reward function\u2019s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. April Yu et al. have an interesting paper on simulated autonomous vehicle control which details a DQN agent used to drive a game that strongly resembles Out Run (JavaScript Racer). Based on their reward function experiments, I\u2019ve built a function which rewards logarithmically based on speed and penalizes when the car is off-road, crashed or stopped. Run the trainer and emulator on your local machine by following the guide available on agent-trainer\u2019s readme. It is also possible to deploy the agent to an AWS EC2 instance or generic Linux remote machine by using a set of bash scripts offered by agent-trainer-deployer. Amazon allows you to bid on spare EC2 computing capacity via spot instances. These can cost a fraction of on-demand ones, and for this reason were chosen as the prime method for training in this project, leading to the need for mid-training instance termination resilience. To accommodate this scenario, the deployment scripts and agent-trainer are designed to support train session resumes. To persist results and decrease boot up time between sessions, a long-lived EBS volume is attached to the live instance. The volume contains the training results, agent-trainer´s source code, cannonball\u2019s source code, dockerfiles and their respective docker images. The hyperparameters used on all sessions mimic the ones used on DeepMind\u2019s Human-Level Control through Deep Reinforcement Learning paper, except for the number of frames skipped between actions, which are spaced apart by 450ms (equivalent to 13 frames) on agent-trainer. The Out Run game, as you would play it in an arcade, clutters the road with various cars in order to make the game more challenging. In-game traffic was disabled for both training and evaluation plays, rendering a more achievable starting point for these experiments. Training with random traffic could be an interesting posterior experiment. Some experiments were made by increasing the discount factor up its final value during training, as proposed on \u201CHow to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies\u201D, but did not achieve better stability or rewards when compared to a fixed 0.99 discount factor. The aforementioned paper also proposes decaying the learning rate during training, which increased stability and performance significantly. Decaying the learning rate without minimum value clipping yielded the best results. Another improvement was to train the game without a time limit, meaning that the training episode would only finish when the car reached the last stage\u2019s end. This allowed for a broader replay memory training set, since the agent traversed a wide range of different tracks and settings. Play evaluation was the same between all experiments, this is, the agent was evaluated by playing on the default 80 second, easy mode. Here is a summary of the most relevant training sessions (you can find their models, metrics and visualizations on agent-trainer-results): Training sessions summary: session names are formed by <session ID>_<number of episodes trained>(M)achine used: a) AMD Athlon(tm) II X2 250 Processor @ 3GHz; 2GB RAM DDR3-1333 SDRAM; SSD 500 GB: Samsung 850 EVO (CPU only training); b) AWS EC2 g2.2xlarge (GPU enabled instance), 200 GB General Purpose SSD (GP2) Notice on the videos above how the timed mode trained session reaches the first checkpoint about five seconds earlier than the unlimited time mode trained session , but proceeds to drive off-road two turns after. Its stability gets increasingly compromised as more episodes are trained, which can be observed by the rampant loss increase before 7300 episodes are reached (): Both and timed trained sessions mostly drive off-road and stall after the first stage, whereas the unlimited time mode trained session can race through all the stages crashing only three times (as shown on the article\u2019s first video) and is able to match the performance of a timed trained session when evaluated on the default easy timed mode. Below is the loss plot for and , both trained using the game\u2019s unlimited time mode, difference being that keeps a fixed learning rate and decays it every 50100 steps: Agent-trainer was not built from the get-go to train games or problems other than Out Run, but I think it would be interesting to perform a thought exercise on what would be necessary to do so. There are three main areas in which agent-trainer has domain knowledge about Out Run: Training another game would require the creation of a new wrapper with the same interface as , a new enumerator specific to the game, a new RewardCalculator with a different reward function and the removal\/replacement of the metric. Apart from the previously mentioned steps, solving generic problems would require the preprocessor to be changed\/replaced if images were not to be used for state representation. An option would be to create a new preprocessor class with a process(input) method, tweak the hyperparameters as required (so that the network knows which dimensions to expect on its input), and finally inject the newly created class in EpisodeRunner, replacing the old preprocessor class. I am not a machine learning expert, but from my learner\u2019s point of view, if you are interested in getting your feet wet, Andrew Ng\u2019s Machine Learning Course is as a great starting point. It is freely available on the Coursera online learning platform. This was my first solid contact with the subject and served as a major stepping stone for related topics such as Reinforcement Learning. Udacity Google Deep Learning: this free course tackles some of the popular deep learning techniques, all the while using tensorflow. I did this right after Andrew Ng\u2019s course and found it to leave the student with less support during lessons - less hand-holding if you will - and as result I spent a good amount of time dabbling to reach a solution for the assignments. As a side note, I started building this project by the end of the Deep Learning course, mostly because I wanted to apply and consolidate the concepts I learnt into something more practical and to share this knowledge further, so it could hopefully help more people who are interested in this. All source code is available on GitHub:","flair":"null\tnull"}
{"author":"wjbianjason","created":"Sat Nov 05 04:15:46 EDT 2016","text":"To be specific,when I input some isolate words ,then the model generate one nature sentence which contains these input words.For example,I input \"girl white beautiful\",then generate this sentence \"that girl in white shirt looks beautiful\".I'm confused to solve this problem,so please anybody help me.","flair":"one\tDiscussion"}
{"author":"deeprnn","created":"Mon Oct 03 21:49:17 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.00527 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Video Pixel Networks Authors: Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu (Submitted on 3 Oct 2016) Abstract: We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects. Comments: 16 pages Subjects: Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG) Cite as: arXiv:1610.00527 [cs.CV]   (or arXiv:1610.00527v1 [cs.CV] for this version) Submission history From: Nal Kalchbrenner [view email] [v1] Mon, 3 Oct 2016 13:06:40 GMT (5347kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"rantana","created":"Tue Nov 08 00:10:29 EST 2016","text":"Looking at the latest spat of submissions (most of them to ICLR 2017), I can't help but notice how much hype is put into the title. Words like 'outrageous', 'frustratingly', 'rebirth' and videos that play like bad startup ads.\n\nHas the hype finally spilled over into academia?\n\nWhy are academics are doing hard sells of pretty incremental research.","flair":"one\tDiscussion"}
{"author":"im-the-stig","created":"Sun Oct 02 12:17:05 EDT 2016","text":"Nvidia has been hyping up the [news](http:\/\/qz.com\/797752\/nvidia-self-driving-car-neural-network\/) that their car is self taught after watching humans drive. How do other systems like Google, Tesla, Uber learn to drive. IIRC, I read that all of Tesla's cars are connected to the hive, which learns only the terrain, not sure about driving.","flair":"null\tnull"}
{"author":"__lava__","created":"Sun Nov 13 19:26:36 EST 2016","text":"Implementation of Spatial Contrasting Network in Keras. Included are three models: Images of each network and the weights from their first layers are in ( and sub-directories, respectively). Please help me reproduce the results from the paper!","flair":"four\tProject"}
{"author":"AutoModerator","created":"Wed Sep 28 11:52:19 EDT 2016","text":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n","flair":"null\tnull"}
{"author":"NichG","created":"Fri Oct 07 22:03:35 EDT 2016","text":"Generative Adversarial Networks (and the related DCGAN) are a relatively new type of neural network architecture which pits two sub-networks against each-other in order to learn very realistic generative models of high-dimensional data (mostly used for image synthesis, though extensions to sound, text, and other media have been constructed). When one generates something like an image, the pixels themselves are not individually so responsible for the realism of the image as the correlations and relatively arrangements of pixels, and so hand-crafted loss functions to measure reconstruction accuracy have a tendency towards blurriness or other perceptual artefacts. But when you allow two networks to compete with each-other: one to make a plausible fake, the other to distinguish forgeries from reality, then the networks (in principle) eliminate the noticeable differences from reality one-by-one. A blurry image can be detected, so it eliminates blurriness, and so on. The results of this approach are visually quite impressive, but because the two sub-networks are trained against different, opposed target functions, there is a wealth of new instabilities and problems that can crop up compared to training traditional neural networks which have a single unified loss function. In this post, I\u2019m going to look at extremely simple adversarial networks \u2013 ones with only one parameter for each sub-network \u2013 so that we can exhaustively explore the parameter space, and hopefully get some intuition for how these adversarial networks behave when trained. When training the usual neural network architecture, there is some kind of objective which the network is tuned to optimize (the loss function), and training proceeds by performing gradient descent on the parameters of the network to try to increase or decrease this function. Since it\u2019s just a scalar function, it defines a kind of landscape  \u2013 a given set of network parameters maps to a certain value of the function \u2013 potentially expensive to compute, but very well-defined. If that function changes in the right direction, you know that training is proceeding correctly, and if it goes in the wrong direction you know that you should stop, reduce the learning rate, etc. If you make the learning rate small enough, be careful enough, you have a pretty strong guarantee that you can make it so that the loss function is always going in the right direction up until the point where it finds some local optimum. You can ultimately be sure that the network can\u2019t possibly get any worse at the task than it already is (at least, on the training data), so the worst thing that can happen is for it to get stuck somewhere. Even in that case, if you have two networks trained completely separately from different initial conditions and so on, you can always compare them and see which is better just by comparing their loss functions. For adversarial networks, the structure is a bit different. The first sub-network, the Generator, takes a random variable and outputs what it thinks an example from the data set that can fool the other network looks like. The second network, the Discriminator, takes as inputs both real examples and fake examples, and is tasked to classify them. So the Discriminator has a loss function , but the Generator has a different loss function . These loss functions are chosen to guarantee the theoretical result that if the generator can output samples from an arbitrary probability distribution and the discriminator can assign an arbitrary probability to a sample , then for data distributed according to a true distribution there should be a fixed point when \u2013 that is, the correct answer to the problem is a point at which neither the discriminator nor generator can improve their individual loss functions. However, this is a much weaker guarantee than you have in gradient descent on a landscape \u2013 you don\u2019t know whether this fixed point is stable, or if one even exists if the generator and discriminator are finite networks. When training both networks in parallel, there isn\u2019t guaranteed to be some objective way to measure the improvement of the network as a whole \u2013 you can\u2019t read out improvements or convergence by examining a training curve. As this is a generalized dynamical system, you can get limit cycles and strange attractors, meaning that training can go in circles or become chaotic. That said, GANs work shockingly well. Even if they can be hard to tune and to get working, there are many papers and open-source implementations showing impressive, stable results from GANs. When GANs work, they learn fast \u2013 producing realistic images even after a single epoch of training. So even if these kinds of troublesome behaviors are possible, they are either not guaranteed to be a problem or often they simply don\u2019t matter when it comes to GANs producing reasonable outputs. At the same time, tuning GANs is currently somewhat of an art form, and requires significant human intervention and inspection of the results, making it hard to extend to tasks where a human cannot readily judge gradations in quality of different sets of outputs.There was a recent paper which tried to impose a scalar landscape onto the GAN training, essentially treating the adversarial part as a perturbation on top of a more traditional, well-defined training problem. So even if GANs are working, there\u2019s reason to try to figure out what\u2019s going on, objectively evaluate their performance, and move back towards scalar optimization if possible or beneficial. If I want to plot how a GAN would train in a way that we can evaluate by eye, it really needs to be in 2D (or 3D at most). So that leaves me with two trainable parameters, across two sub-networks. This is going to be a very simple GAN. Given how few variables I have, lets try to make a GAN which takes random numbers from a 1D Gaussian as input, and generates samples from a 1D Gaussian with the same standard deviation but potentially with a shifted mean as output. And actually, lets make it a trivial problem \u2013 the mean is the same too, so the best generator should just pass its input through without modification. Keeping it as simple as possible, my generator \u2018network\u2019 is going to be: where is the latent variable input, a unit-standard deviation zero-mean Gaussian. So this network just scales its input by , applies a constant shift , and has no nonlinearity. But this is going to be too many parameters to visualize, so lets just restrict for now and let vary. My discriminator \u2018network\u2019 is going to be: where is a sigmoid function and is a sample either from the generator, or from data (which is a unit Gaussian with zero mean). Again, too many parameters to visualize, so lets just fix for now. The loss function for the discriminator is the log loss: where if x is a real sample, and if x is a fake. The loss function for the generator is just the negative of the loss for the discriminator (it wants to output a sample that fools the discriminator), but for the generator all examples are going to be fake, so this is just . Given this set of networks, we can calculate the gradient vector, if we were just training these two networks together via gradient descent against their respective loss functions , and plot that as a function of and . The results are a bit worrying: there doesn\u2019t appear to be any kind of finite fixed point, so this training will never converge. Instead, it looks as though the generator parameter will become positive infinity, and the discriminator parameter goes to negative infinity. What gives? It turns out that this discriminator has a critical blind spot. It\u2019s using a sigmoid as the output nonlinearity, which is pretty normal for making a network with a probabilistic output on a single binary question. But in this case, the discriminator is too weak to properly classify both and as unrealistic samples. It has to be one or the other. If the discriminator decides that is unrealistic, then has to be the \u2018most realistic sample\u2019, and vice versa. Even worse, in this case because is fixed, the discriminator can never declare a larger value of x less realistic than a smaller one! So in order to always win, the generator just has to hide out at , and there\u2019s nothing the discriminator can do about it. Because the discriminator lacks the capacity to properly model the probability distribution that the network is being asked to generate, the result is a fundamental instability of the training process in this case \u2013 no matter how slow you go or how careful you are, you won\u2019t converge to the data distribution. Fortunately, this kind of problem may only exist in these artificially tiny networks. It\u2019s known that if you give a neural network enough nodes, they can act as universal function approximators. That is to say, if a given network can\u2019t model a particular function well enough, you can in principle always just make the network bigger. So presumably this kind of problem is one that goes away when you\u2019re working with large enough networks. But, correspondingly, it suggests that a discriminator network which is too small may actually be unstable for a GAN, whereas it would simply underfit in a standard neural network) For our tiny network, the asymptotic behavior of the sigmoid function is the source of the problem, but we can replace the sigmoid nonlinearity with a Gaussian. So that way we know the discriminator can exactly, accurately model the true distribution: . This appears to work much better. There\u2019s now a clear stable fixed point at (0,0), though the approach to the fixed point is oscillatory. It turns out that even if we force to the wrong value, e.g. or , we still get the stable fixed point in the middle \u2013 so not all errors or limitations of the discriminator lead to a fundamental training instability. It appears that the biases are doing the right thing now \u2013 this network correctly discerns the mean of the data Gaussian, albeit in a somewhat round-about way. So what if we do the same thing with fixed biases (), but now look at training the weights and ? In principle, this should have a fixed point at (1,1) and at (-1,-1) (because the Gaussian with zero mean is symmetric under reflection). What actually happens? It\u2019s a bit hard to see from the plot, but something is going wrong. The horizontal axis () seems to be doing alright \u2013 there are fixed points close to , so that\u2019s okay. But the vertical axis () appears to be collapsing to zero! That means that the generator is outputting a delta function at , not a Gaussian with the same standard deviation as the data distribution! Why? Again, this is a problem with the discriminator not being powerful enough. In the theoretical picture on GANs, if the generator deviates at all from the true , there\u2019s always some function the discriminator can come up with to get some extra advantage. But that function in this case would not be a Gaussian, it would be something bi-modal with a dip in the center where the generator has decided to focus. So even if the discriminator is capable of modeling the true distribution exactly, that may not be sufficient \u2013 it needs to be able to cover some set of fluctuations around the true distribution in order to prevent the GAN from exploiting those vulnerabilities. Essentially, the discriminator needs to be able to model the true distribution plus every possible perturbation around it that the generator could come up with. The universal function approximation properties of neural networks can actually work against things here \u2013 if the generator is too powerful compared to the discriminator, it could just find some function the discriminator can never answer. The outcome mirrors a problem that does actually occur in higher-dimensional GANs. If we look at what happens to the data generated here during this malfunction, the generator is only outputting copies of the single most likely data point \u2013 the diversity of the generated set has collapsed. This kind of collapse happens in image-generation GANs as well, where if the generator gets too far ahead it just memorizes one example from the training set and only outputs that regardless of the variations in the noise source. Once the network has fallen into that hole, it\u2019s usually very hard to recover the entropy of the outputs with further training. The usual protocol is to adjust the learning rates to keep the error of both the generator and discriminator from getting too small, but perhaps the reason why this kind of careful management is necessary is that there are true fixed points in the parameter space corresponding to this collapse, and unless those fixed points are somehow destabilized then the networks will always be at risk of going there if you continue training indefinitely. Another question we can ask with these visualizations is, how much data is needed and how does the vector field respond to having insufficient data? In a standard single network case, the usual result is overfitting, and the signature in the loss function landscape is that the local and global minima become very sharp. So far, these plots have been generated using 2500 samples. What if we go down to 100 samples? We\u2019ll examine the plane for this one, since training is ostensibly stable and correct in that set of parameters. With only 100 samples, we get a much more ragged set of contours. On a \u2018landscape\u2019 like this, the wrong step-size could easily end up running off to somewhere weird, getting further away from the fixed point, getting trapped in parasitic cycles, or accidentally escaping the approach to the fixed point and having a \u2018collapse\u2019. 100 samples is tiny for a data-set, but keep in mind that this is a 1D data set, so what constitutes small enough to create snags might be a much larger number of samples in higher dimensions where you can\u2019t possibly cover the entire space. We can reduce the amount of data further, down to 5 samples, and this becomes a real nightmare. It\u2019s unclear whether gradient descent will find the fixed point anymore, no matter how careful you are. The regions at (-4,4) and (4,-4) become essentially noise patterns without any clear escape trajectory. An additional saddle point has appeared around (1,-2), and it looks as though there may be a number of local minima. Of course, asking anything from 5 samples is unreasonable, but this gives an idea of how things look when they break. Is there a reasonable way to fix this kind of mess? In classic neural networks, one would use regularization \u2013 usually dropout. We can\u2019t do that here (we only have one link to turn off!), but what we can do instead is add noise to the parameters. Even as we\u2019re looking at these very low-data vector fields, its mostly the local structure that has become messy due to the small amount of data. The overall global structure remains quite similar in all cases, even the 5 sample case. So if we could somehow take a local average of the gradient direction over some radius, it might actually fix these local snags and make the training behavior more stable. To do this, we sample each gradient 30 times, but adding Gaussian random noise with standard deviation 0.2 to the parameters . We keep the same data and latent vectors (100 samples for this plot), but basically just average over the local neighborhood in the parameter space. The results are much smoother (of course), and also much more directly approach the fixed point. So the lesson for larger GANs is: Sample a lot of random variations of the parameters (e.g. through dropout), apply normalization to the weights and biases to shut down fixed points at infinity, etc \u2013 all of those things should help with the instabilities and problems we found here. Can we do anything like this analysis in full-sized GANs? After all, everything here could just be an artefact of the tiny, extremely weak \u2018networks\u2019 we\u2019re forced to use to make 2D visualizations. Well, its hard to say. One thing we can in principle do is to take 2D slices out of the parameter space of the larger network, and look for fixed points, saddles, and run-off-to-infinity behaviors on those slices. On the left is a random cut taken out of the bias space of a randomly initialized GAN with two 100 neuron hidden layers for both the generator and discriminator, and it seems to show some of the same characteristics as our tiny network \u2013 in this case, it looks like the first bias plot with things appearing to run off to infinity. However, unlike our 2-parameter network, there is significant out-of-plane motion here which is not visualized \u2013 that means that something appearing to run off to infinity could just be on an oscillatory approach to a fixed point somewhere out-of-plane (which is probably the case here, as training this kind of 1D GAN tends to be relatively well-behaved for larger networks). So some caution in directly applying the visualization technique is probably necessary. On the other hand, perhaps the features of these vector field landscapes as seen from the point of view of a network-in-training will be more robust and reliable. In dynamical systems theory, one metric which can be used to evaluate the overall behavior of a dynamical system is the Lyapunov exponent \u2013 it measures the rate at which nearby trajectories diverge from each-other, and detects the presence (and structure) of chaos in the system. So the idea is, by training a cluster of nearby networks together, one should be able to detect when the training process passes near saddle points, whether it\u2019s sitting near a fixed point or just drifting off to infinity. I think that this must ultimately be related to Hessian-based update rules \u2013 the cluster of nearby trajectories is essentially locally sampling the second derivatives of the loss function along a random set of directions. The interesting aspect of it for GANs is that this higher-order gradient information won\u2019t just determine the stable step size, but also might be able to tell you if you\u2019re stuck in a limit cycle or crashing or things like that, and may be able provide the information needed to bias the training updates in such a way as to damp out those oscillations. For example, one can generically decompose vector fields into a gradient of a scalar field and a curl of another vector field: . If that decomposition can be done locally (which requires knowledge of the derivatives of , e.g. second-order derivatives of the loss function), then its possible to only follow the part of the field, recovering a scalar optimization problem. So I think it might be interesting to look at efficient ways to approximate the Hessian as a way to get information to stabilize the training updates of GANs. But for now, that extends beyond the scope of this post.","flair":"three\tResearch"}
{"author":"sybilckw","created":"Mon Nov 14 08:20:56 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01989 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: DeepCoder: Learning to Write Programs Authors: Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, Daniel Tarlow (Submitted on 7 Nov 2016) Abstract: We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites. Comments: Submitted to ICLR 2017 Subjects: Learning (cs.LG) Cite as: arXiv:1611.01989 [cs.LG]   (or arXiv:1611.01989v1 [cs.LG] for this version) Submission history From: Marc Brockschmidt [view email] [v1] Mon, 7 Nov 2016 11:09:45 GMT (140kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"bjornsing","created":"Sun Oct 23 12:33:56 EDT 2016","text":"TLDR: Bayes rule is cool. Stable Wi\u2011Fi rules. The former can give us the latter. Transmitting Wi\u2011Fi frames over noisy radio spectrum is far from trivial. You see, every second or so the Wi\u2011Fi driver in your router needs to make thousands of nitty-gritty little decisions: Which modulation rate should the Wi\u2011Fi chipset use for this particular data frame? Should several frames be aggregated together and sent in a single radio burst? Should the transmission be protected with a RTS-CTS handshake? How many times should the Wi\u2011Fi chipset retry the transmission if no acknowledgement frame is received back? And if all the transmission attempts fail, then what? Should we try again with different parameters, or simply drop the frame? Most of the time the Wi\u2011Fi driver in your router makes sensible decisions, and you enjoy stable and fast Wi\u2011Fi. But, as we\u2019ve all experienced, there are times when your Wi\u2011Fi turns less than stellar. In my experience this is often because the Wi\u2011Fi driver in your router has become \u201Coverwhelmed\u201D by a challenging radio environment, one in which the usual \u201Crules of thumb\u201D no longer work very well. So what can we do to improve the stability and performance of Wi\u2011Fi? How about replacing the \u201Crules of thumb\u201D with the one rule that rules them all: Bayes rule. This is the first post in a series that will explore this opportunity to advance the frontiers of knowledge and push the human race forward. :P With a bit of luck we will some day arrive at an implementation of a Bayesian Wi\u2011Fi rate control algorithm for the Linux kernel, so that anyone can run it on a low-cost consumer Wi\u2011Fi router by flashing it with OpenWrt firmware. The Linux kernel defines an interface for rate control algorithms in net\/mac80211.h. So in very practical terms the problem is to implement the operations needed to fill in a . The main challenges are which, as the name implies, is called for each frame before it is transferred to the Wi\u2011Fi chipset for transmission over the air, and which is called after transmission to report back the result. The former should pick the modulation rates and other radio parameters to use for transmission, and pack them into an array of ieee80211_tx_rates, each with its own mac80211_rate_control_flags. The latter should evaluate the result of the transmission and learn from it; so that a better selection can be made the next time is called. The default rate control algorithm in Linux is called Minstrel. Most of its implementation is in the file net\/mac80211\/rc80211_minstrel_ht.c. It\u2019s widely cited in the literature and often used as a benchmark when evaluating novel algorithms. In most radio environments it performs reasonably well, but it\u2019s based on very hand-wavy statistical reasoning. Essentially it just keeps an exponentially weighted moving average (EWMA) of the packet error rate (PER) for each combination of radio parameters supported by the communicating Wi\u2011Fi chipsets. It then constructs retry chains of radio parameters (corresponding to R through R in the sketch above) based on very simple \u201Crules of thumb\u201D: From time to time Minstrel will try some unexpected radio parameters in an attempt to learn. It\u2019s this behavior that has given it its name: like a wandering minstrel the algorithm will wander around the different rates and sing wherever it can. I see a number of problems with these \u201Crules of thumb\u201D and I suspect that they lead to suboptimal performance in more challenging radio environments. I could go on and on about these problems, but I think it suffices to say that Minstrel is not based on sound statistical inference. So what would a rate control algorithm based on sound statistical inference look like? Fortunately I\u2019m not the first person to ask that question; there\u2019s a large body of research papers on the subject out there. To me the most promising theoretical approach seems to be a Bayesian one where we cast the rate adaptation problem in the shape of a multi-armed bandit problem: different radio parameters are like different slot machines in a casino, each with an unknown (but potentially very different) reward distribution, and the task of the rate control algorithm is similar to that of a gambler in trying to determining which slot machine to play in order to maximize reward in some sense. There\u2019s an excellent paper by Richard Combes, Alexandre Proutiere, Donggyu Yun, Jungseul Ok and Yung Yi titled \u201COptimal Rate Sampling in 802.11 Systems\u201D that takes this approach. So we have a research paper and we need an implementation. How is that research? Not so fast, Combes et al makes one unfortunate simplifying assumption: This may sound like a safe assumption but it is simply not true in the complex world of MIMO, collisions and interference. Just one example is that higher rates often have a higher success probability than lower rates when errors are primarily caused by collisions or interference (as is common in busy radio environments), simply because a packet sent at a higher rate stays in the air for a shorter period of time and is therefore less likely to be \u201Cshot down\u201D by sporadic interference. So what can we do? Is there a way to salvage Combes et als results? Probably, but I\u2019m not sure I\u2019m the person to do it. On the other hand I feel rather sure that we could do something in the same general direction. For example Pedro A. Ortega has done some interesting work on Thompson sampling and Bayesian control that I think could be applied to Wi-Fi rate adaptation. There\u2019s also a large body of research on so-called contextual bandit problems and restless bandit problems that could be applicable. I can see a couple of ways forward. One is very theoretical and includes reading up more on the contextual bandit problem. Another is more practical and includes modeling the rate adaptation problem as a Dynamic Bayesian Network and applying Thompson sampling. A first prototype using existing code for approximate inference in DBNs (like libDAI or Mocapy++) and a network simulator like NS-3 could probably be put together relatively quickly. This could be an interesting thesis project, for example. Yet another approach is even more practical and can be described as \u201Cwriting a better Minstrel\u201D, directly targeting the Linux kernel. The focus would be on simplifying assumptions like working with conjugate priors. This would be of less scientific interest, but perhaps more useful in the short term. Which approach to take depends on who wants to join in the fun. :) We need everything from theoreticians to testers, so please don\u2019t be shy! Want to join in the fun, and get your name on a research paper and\/or some pretty cool open source code? Shoot me an email at bjorn@openias.org or fill in the form below!","flair":"four\tProject"}
{"author":"downtownslim","created":"Wed Nov 23 19:19:32 EST 2016","text":"At least five members of Twitter\u2019s Cortex artificial intelligence group have left this month, at a time when companies are rushing to accumulate AI talent. Earlier today, Kevin Swersky tweeted out a photo of his laptop and Twitter badge, indicating his departure. Two days earlier, Jasper Snoek and Alex Wiltschko had done the same thing. On Monday, Google announced that a key member of the Twitter Cortex team, Hugo Larochelle, had been selected to head up the company\u2019s new artificial intelligence and deep learning division in Montreal. Larochelle shared his badge and MacBook photo on November 11. And Ryan Adams did that, too, on the same day. All five joined Twitter following the social networking company\u2019s acquisition of startup Whetlab in 2015. Apple, Amazon, Facebook, Google, and Microsoft have all been hiring people from other technology companies and universities to bolster their AI capabilities.","flair":"two\tNews"}
{"author":"onekonek","created":"Tue Oct 11 23:27:26 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.03017 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: Fully Character-Level Neural Machine Translation without Explicit Segmentation Authors: Jason Lee, Kyunghyun Cho, Thomas Hofmann (Submitted on 10 Oct 2016 (v1), last revised 1 Nov 2016 (this version, v2)) Abstract: Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment. Comments: 15 pages, 2 figures Subjects: Computation and Language (cs.CL); Learning (cs.LG) Cite as: arXiv:1610.03017 [cs.CL]   (or arXiv:1610.03017v2 [cs.CL] for this version) Submission history From: Jason Lee [view email] [v1] Mon, 10 Oct 2016 18:19:34 GMT (380kb,D) [v2] Tue, 1 Nov 2016 17:51:32 GMT (415kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"jhartford","created":"Mon Nov 14 13:03:58 EST 2016","text":"For details and a link to the paper see www.cs.ubc.ca\/~jasonhar\/Video contains vector graphics from Freepik (http:\/\/www.freepik.com\/) and the audio backtrack came from Bensound (http:\/\/www.bensound.com\/).","flair":"three\tResearch"}
{"author":"downtownslim","created":"Fri Nov 11 14:51:43 EST 2016","text":" Under review as a conference paper at ICLR 2017 VARIATIONAL RECURRENT ADVERSARIAL DEEP DOMAIN ADAPTATION Sanjay Purushotham*, Wilka Carvalho*, Tanachat Nilanon, Yan Liu Department of Computer Science University of Southern California Los Angeles, CA 90089, USA {spurusho,wcarvalh,nilanon,yanliu.cs}@usc.edu ABSTRACT We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain- invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model\u2019s ability to create domain- invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches. 1 INTRODUCTION Many real-world applications demand effective machine learning algorithms that can learn invariant representations across related time series datasets. For example, precision medicine for patients of various age groups, mobile application recommendation for users based on locations, and so on. In both examples, while the domains (i.e. age group and location) may vary, there exist common predictive patterns that can aid in inferring knowledge from one domain to another. Most often than not, some domains has significantly larger number of observations (e.g., respiratory failure in adults) than others (e.g., respiratory failure in children). Therefore effective domain adaption from time series data are in great demand. The general approach to tackle the domain adaptation has been explored under many facets which include reducing the domain discrepancy between the source and target domains(Ben-David et al. (2007)), instance re-weighting (Jiang & Zhai (2007)), subspace alignment (Fernando et al. (2013)), deep learning (Tzeng et al. (2015); Ganin & Lempitsky (2014)). Many of these approaches work very well for non-sequential data; but however are not suitable for multivariate time series data as they usually not capture the temporal dependencies present in the data. For sequential data, some earlier works successfully used dynamic Bayesian Networks(Huang & Yates (2009)) or Recurrent Neural Networks (Socher et al. (2011)) to learn latent feature representations which were domain-invariant. Unfortunately, these works are not flexible enough to model non-linear dynamics or they do not explicitly capture and transfer the complex latent dependencies needed for domain adaptation of the time series data. In this paper, we address this problem with a model that learns temporal latent dependencies (i.e. dependencies between the latent variables across timesteps) which can be transferred across domains that experience different distributions in their features. It achieves this by using variational methods to produce a latent representation that captures underlying temporal latent dependencies inspired by the Variational Recurrent Neural Network (Chung et al. (2016)) and - motivated by the theory of domain adaptation (Ben-David et al. (2010)) - performs adversarial training on this representation similarly to *: Co-first authors 1 Under review as a conference paper at ICLR 2017 Figure 1: A Story of Temporal Dependency and Domain Invariance (a) DNN (b) DANN (c) VRADA t-SNE projections for the latent representations of DNN, DANN, and our VRADA model for domain adaptation from Adult-AHRF to Child-AHRF dataset. Source data is represented with red circles and target data with blue circles. From left to right, one can see that domain adaptation results in mixing the source and target domain data distributions. We can also see a story of how more temporal dependency being encoded (from left to right) into the latent representation induces more domain-invariant representations (VRADA has better distribution mixing than DANN). As models capture more underlying factors of variation, post domain adaptation representations gradually smoothen and become evenly dispersed, indicating that temporal dependency acts synergestically with domain adaptation. the Domain Adversarial Neural Network (DANN) (Ganin et al. (2016)) to make the representations invariant across domains. We call our model the Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) model. As far as we know, this is the first model capable of accomplishing unsupervised domain adaptation while transferring temporal latent dependencies for the complex multivariate time series data. Figure 1 shows an example of the domain invariant representations learned by different deep learning models including our VRADA model. From this figure, we can see that our model (VRADA) shows better mixing of the domain distributions than the competing models indicating that it learns better domain invariant representations. In order to prove the efficacy of our model, we perform domain adaptation using real-world healthcare time-series data. We choose healthcare data for two primary reasons. (1) Currently, a standard protocol in healthcare is to build, evaluate and deploy machine learning models for particular datasets (e.g. models for particular age groups or particular diseases) which may perform poorly on unseen datasets with different distributions. For example, models built around patient data from particular age groups perform poorly on other age groups because the features used to train the models have different distributions across the groups (Alemayehu & Warner (2004); Lao et al. (2004); Seshamani & Gray (2004))\u2013i.e. knowledge learned from one group is not transferrable to the other group. This is essentially a domain adaptation problem where knowledge needs to be transferred across domains which share features that exhibit different distributions. (2) Healthcare data has multiple attributes recorded per patient visit, and it is longitudinal and episodic in nature. Thus, healthcare data is a suitable platform on which to study a model which seeks to capture complex temporal representations and transfer this knowledge across domains. The rest of the paper is structured as follows. In the following section, we briefly discuss the current state-of-the-art deep domain adaptation approaches. Afterwards, we present our model mathematically, detailing how it captures temporal latent dependencies at the same time and learns domain-invariant representations by adversarial training. In Section 4, we compare and contrast the performance of proposed approach on two real-world health care datasets; and provide analysis on our domain-invariant representations. 2 RELATED WORK Domain adaptation is a specific instance of transfer learning in which the feature spaces are shared but their marginal distributions are different. A good survey on the two has been done in several previous works Pan & Yang (2009); Jiang (2008); Patel et al. (2015). Domain adaptation has been well studied in computer vision(Saenko et al. (2010); Gong et al. (2012); Fernando et al. (2013)) and natural language processing (NLP) (Blitzer (2007); Foster et al. (2010)) applications. Recently, deep learning paradigm has become popular in domain adaptation (Chen et al. (2012); Tzeng et al. (2015); Yang & Eisenstein; Long & Wang (2015)) due to its ability to learn rich, flexible, non-linear domain-invariant 2 Under review as a conference paper at ICLR 2017 h1 h2 h3 ht . . . . . . . . . x1 x2 x3 xt z1 z2 z3 zt Gy Gd Figure 2: Block diagram of VRADA. Blue lines show the inference process, qθe(zt|x≤t, z<t). Brown lines show the generation process, pθg (xt|z≤t, x<t). Red lines show the recurrence process where ht is informed by ht−1, which is informed by zt−1 and xt−1. Black lines indicate classification. representations. Here, we briefly discuss two deep domain adaptation approaches which are closely related to our proposed model. Domain Adversarial Neural Networks (DANN) Ganin et al. (2016) is a deep domain adaptation model which uses two core components to create domain-invariant representations, a feature extractor that produces the data\u2019s latent representation, and an adversarial domain labeler that attempts to classify that data\u2019s domain to help the feature extractor produce latent representations which are domain-invariant. In Louizos et al. (2015), the authors propose Variational Fair AutoEncoder, which uses Variational Autoencoding architecture Kingma & Welling (2013) to learn latent representations where most of the information about certain known factors of variation are purged from the representation while still retaining as much information about the data as possible. While, these deep learning approaches learn domain-invariant representations, they fail to capture and transfer the underlying complex temporal latent relationships from one domain to another as they use convolutional or feed forward neural networks which we claim are not suitable for multivariate time series data. Other works such as Huang & Yates (2009); Xiao & Guo (2013) have used distributed representations for domain adaptation in sequence labeling tasks in NLP. However, they either induce hidden states as latent features using dynamic Bayesian networks (DBNs) (such as hidden Markov Models(HMM)) or learn generalizable distributed representations of words using Recurrent Neural Networks (RNN) (Socher et al. (2011)) to enable domain adaptation. These works either model the highly non-linear dynamics (like RNN) or capture the complex latent dependencies present in sequential data (like DBNs) but not both. To overcome the challenges of DBNs and RNNs, Variational Recurrent Neural Network (VRNN)( Chung et al. (2016)) was proposed recently, to capture the complex relationship between the underlying hidden factors of model variation and the output variables at different time- steps. VRNN uses Variational Autoencoders (VAEs)( Kingma & Welling (2013); Goodfellow et al. (2016)) at each time-step and learns complex relationship of latent hidden factors along time and are thus well-suited for multimodal sequential data such as multivariate time series. In the following section, we discuss our approach, Variational Adversarial Deep Domain Adaptation (VRADA), which uses VRNN to model and transfer complex temporal latent relationships while learning the domain invariant representations for unsupervised domain adaptation in multivariate time series. 3 VARIATIONAL RECURRENT ADVERSARIAL DEEP DOMAIN ADAPTATION In this section, we present our Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) model for the purpose of capturing and transferring temporal latent dependencies across domains via domain-invariant representations. First, we introduce the notations used in this paper and then discuss our VRADA model in detail. 3.1 NOTATIONS Let us denote a multivariate variable-length time series with N data samples as {xi = (xit)T i t=1}Ni=1, where xit ∈ RD. (Note: in our experiments, for all data samples T i = τ , but for generality we maintain T i). We denote {xiS}ni=1 as source domain data and {xiT }Ni=n+1 as target domain data. We assume that each source domain data sample xiS comes with L labels yi ∈ {0, 1}L (for example, these labels may correspond to a clinical outcome such as mortality or ICD9 diagnosis codes), while 3 Under review as a conference paper at ICLR 2017 target domain has no labeled data samples, i.e. we are interested in unsupervised domain adaptation problem. We assign a domain label di ∈ {0, 1} to each data sample to indicate if it comes from the source or target domain. di will be used for adversarial training. 3.2 VRADA The block diagram of our VRADA model is shown in Figure 2. To explicitly model the dependencies between the latent random variable across time steps, the VRADA model utilizes Variational Recurrent Neural Networks (VRNN) (Chung et al. (2016)). VRNN effectively contain Variational Auto- Encoders (Kingma & Welling (2013)) at every time step, which are conditioned on previous auto- encoders via the hidden state ht−1 of an RNN, such as an LSTM (Hochreiter & Schmidhuber (1997)). Therefore, for each time-step of xit, we infer a latent random variable z i t via zit|xit ∼ N (µz,t, diag(σz,t)), where [µz,t, σz,t] = ϕencτ (ϕxτ (xit), ht−1) with prior zit ∼ N (µ0,t, diag(σ0,t)), where [µ0,t, σ0,t] = ϕpriorτ (ht−1) where µ∗,t, σ∗,t denote parameters of a generating distribution, and ϕ∗τ can be any highly flexible function such as deep neural networks. For each zit, x i t is generated via xit|zit ∼ N (µx,t, diag(σx,t)), where [µx,t, σx,t] = ϕdecτ (ϕzτ (zit), ht−1) and learned by optimizing the VRNN objective function: Lr(xit; θe, θg) = Eqθe (zi≤Ti |xi≤Ti )[ T i∑ t=1 (−D(qθe(zit|xi≤t, zi<t)||p(zit|xi<t, zi<t))+log pθg (xit|zi≤t, xi<t))]) where qθe(z i t|xi≤t, zi<t) is the inference model, p(zit|xi<t, zi<t) is the posterior, pθg (xit|zi≤t, xi<t) is the generative model, θe is the parameters of the VRNN\u2019s encoder, θg the parameters of the VRNN\u2019s decoder, and D(·||·) refers to KL-Divergence. Note: z≤T refers to the set of all zt such that t ≤ T , likewise for z<T . For each xi, we use z̃i ∼ qθe(ziT i |x i ≤T i , z i <T i), as our feature representation for source domain classification task since it captures temporal latent dependencies across the time-steps. Training the VRNN for the source domain classification involves solving the following optimization: min θe,θg,θy 1 n n∑ i=1 1 T i Lr(xi; θe, θg) + 1 n n∑ i=1 Ly(xi; θy, θe) + λR(θe) (1) whereR(θe) is a regularizer for the parameters of VRNN encoder (which is also the feature extractor of VRADA) with a tuning hyperparameter λ. As we are interested in achieving domain adaptation via the latent representation z̃i (i.e. to make z̃i domain-invariant), we can adversarially train the above objective function (equation 1) by employing the domain adaptation idea proposed in Ganin et al. (2016). Let Gy(z̃i; θy) and Gd(z̃i; θd) represent the source label classifier (to predict source labels yi) and domain label classifier (to predict domain labels di) respectively with parameters θy and θd for a given input z̃i. Here, Gy(.) and Gd(.) can be deep neural networks. Let us denote their loss functions respectively as Ly(xi; θy, θe) = LB(Gy(Ve(xi; θe); θy), yi); Ld(xi; θd, θe) = LB(Gd(Ve(xi; θe); θd), di) where LB is the classification loss such as a binary or categorical cross-entropy loss function and Ve(x i; θe) is the VRNN encoder that maps input xi to z̃i. Now, for adversarial training, we consider the following domain adaptation term as the regularizer of equation 1. R(θe) = max θd [ − 1 n n∑ i=1 Ld(xi; θd, θe)− 1 n\u2032 N∑ i=n+1 Ld(xi; θd, θe) ] (2) where n\u2032 is the number of target domain samples. As shown in Ganin et al. (2016),R is the domain regularizer and it is derived from the empiricalH−divergence between the source domain and target domain samples( Ben-David et al. (2010)). 4 Under review as a conference paper at ICLR 2017 Combining the joint optimization problem of equations 1 and 2 leads to our VRADA model, where we minimize the source classification risk and at the same time achieve domain adaptation. Mathe- matically, we optimize the following complete objective function: E(θe, θg, θy, θd) = 1 N N∑ i=1 1 T i Lr(xi; θe, θg)+ 1 n n∑ i=1 Ly(xi; θy)−λ( 1 n n∑ i=1 Ld(xi; θd)+ 1 n\u2032 N∑ i=n+1 Ld(xi; θd))) (3) where λ is a trade-off between optimizing on making domain-invariant representations and optimiz- ing source classification accuracy. Our optimization involves minimization with respect to some parameters, and maximization with respect to the others, i.e., we iteratively solve the following: (θ̂g, θ̂y, θ̂e) = arg min θg,θy,θe E(θe, θg, θy, θ̂d) θ̂d = argmax θd E(θ̂e, θ̂g, θ̂y, θd) with the gradient updates calculated as: θe ← θe − η(∂Lr∂θe + ∂Ly ∂θy − λ∂Ld∂θd ) (4) θg ← θg − η ∂Lr∂θg (5) θd ← θd − η ∂Ld∂θd (6) θy ← θy − ηλ∂Ly∂θy (7) where η is the learning rate. We can use stochastic gradient descent (SGD) to solve the equations (5-7). To solve equation (4), we can use SGD and the gradient reversal layer (GRL)(Ganin et al. (2016)). The role of GRL is to reverse the gradient sign while performing backpropagation. This ensures that the domain classification loss is maximized which makes the feature representations domain-invariant. Thus, VRADA results in learning feature representations which are domain-invariant (due to domain regressorR) and which capture the temporal latent dependencies (due to optimizing VRNN objective function Lr). These things combine to allow the VRADAs\u2019 discriminative power on the source domain to transfer to the target domain. 4 EXPERIMENTS We conduct experiments on two real-world health care datasets to answer the following questions: (a) How does our VRADA model perform when compared to the state-of-the-art domain adaptation and non-adaptation approaches? (b) How different are the domain-invariant representations learned by various domain adaptation methods? (c) How do we show that the temporal latent dependencies are transferred between domains? In the remainder of this section, we will describe the datasets, methods, empirical results, and show visualizations to answer the above questions. 4.1 DATASET DESCRIPTION We conduct experiments on two health care datasets, including the MIMIC-III dataset and a Pediatric ICU dataset from Children\u2019s Hospital Los Angeles. MIMIC-III( Johnson et al. (2016)) is a public dataset with deidentified clinical care data collected at Beth Israel Deaconess Medical Center from 2001 to 2012. It contains over 58,000 hospital admission records of 38,645 adults and 7,875 neonates. For our experiments, we extracted the following two datasets: \u2022 Adult-AHRF dataset: To study domain adaptation for adult patients with acute hypoxemic respiratory failure (AHRF), we extracted 20 time series features (such as Base excess, pH value, Mean Air Pressure, PaO2, etc.) from 5527 admission records based on Khemani 5 Under review as a conference paper at ICLR 2017 et al. (2009). We grouped the patients into 4 groups\/cohorts based on their age[1] - Group 2: working-age adult (20 to 45 yrs, 508 patients); Group 3: old working-age adult (46 to 65 yrs, 1888 patients); Group 4: elderly (66 to 85 yrs, 2394 patients); Group 5: old elderly (85 yrs and up, 437 patients). We treated each group as a separate domain with which we could perform domain adaptation. For each patient, we used the first 4 day after admission (with each day serving as a single time-step) as time series data for training and testing our models. \u2022 ICD9 dataset: For this dataset we extracted 99 time series features from 19714 admission records from 4 modalities including input-events (fluids into patient, e.g., insulin), output- events (fluids out of the patient, e.g., urine), lab-events (lab test results, e.g., pH values and platelet count) and prescription-events (drugs prescribed by doctors, e.g., aspirin and potassium chloride). These modalities are known to be extremely useful for monitoring ICU patients. All the time series are of more than 48 hours of duration, and only the first 24 hours (after admission) 2-hourly sampled time series data is used for training and testing our models. We use this dataset to predict the ICD9 Diagnosis code categories for each patient\u2019s admission record. Child-AHRF dataset: This is a PICU dataset which contains health records of 398 children patient with acute hypoxemic respiratory failure in the intensive care unit at Children\u2019s Hospital Los Angeles (CHLA)(Khemani et al. (2009)). Similar to Adult-AHRF, this dataset has 20 time series features collected for 4 days after ICU admission. This dataset is considered as one group (Group 1: children, age 0 to 19 yrs) and represents one domain. 4.1.1 PREDICTION AND DOMAIN ADAPTATION TASKS Mortality Prediction: For Adult-AHRF and Child-AHRF datasets, we are interested in predicting mortality, i.e. whether a patient dies from AHRF during their hospital stay. 20.10% of all the patients in Child-AHRF and 13.84% of all patients in Adult-AHRF have a positive mortality label (i.e. the patients who die in hospital). ICD9 Code Prediction: Each admission record in MIMIC-III dataset has multiple ICD-9 diagnosis codes. We group all the occurrences of the ICD-9 codes into 20 diagnosis groups[2] . For the ICD9 dataset, we are interested in predicting these 20 ICD-9 Diagnosis Categories for each admission record. We treat this as a multi-task prediction problem. Domain Adaptation Tasks: We study unsupervised domain adaptation (i.e. target domain labels are unavailable during training and validation) task with-in age groups of Adult-AHRF dataset, ICD9 dataset and across Adult and Child-AHRF datasets. For Adult-AHRF and ICD9 datasets, we created 12 source-target domain pairs using the age groups, pairing up each domain Di with another domain Dj 6=i, for example, the source-target pair 2-5 was used for adapting from group 2 (working-age adult) to group 5 (old elderly). We also created 4 source-target pairs for performing domain adaptation from 4 adult age-groups to 1 child age-group. 4.2 METHODS AND IMPLEMENTATION DETAILS We categorize the methods use in our main experiments into the following groups: \u2022 Non-adaptive baseline methods: Logistic Regression (LR), Adaboost with decision regres- sors (Adaboost), and feed forward deep neural networks (DNN) \u2022 Deep Domain adaptation methods: Domain Adversarial Neural Networks (DANN) (Ganin et al. (2016)); DANN with an RNN (LSTM) as feature extractor (R-DANN); Variational Fair Autocoder (VFAE)(Louizos et al. (2015)) \u2022 Our method: Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) In all our experiments, we conducted unsupervised domain adaptation where target domain labels are unavailable during training and validation. For R-DANN, we used LSTM(Hochreiter & Schmidhuber [1]:https:\/\/www.cms.gov\/Research-Statistics-Data-and-Systems\/ Statistics-Trends-and-Reports\/NationalHealthExpendData\/ [2]:\u201CConditions Originating in the Perinatal Period\u201D is not shown in the preprocessed dataset. http: \/\/tdrdata.com\/ipd\/ipd_SearchForICD9CodesAndDescriptions.aspx. 6 https:\/\/www.cms.gov\/Research-Statistics-Data-and-Systems\/Statistics-Trends-and-Reports\/NationalHealthExpendData\/ https:\/\/www.cms.gov\/Research-Statistics-Data-and-Systems\/Statistics-Trends-and-Reports\/NationalHealthExpendData\/ http:\/\/tdrdata.com\/ipd\/ipd_SearchForICD9CodesAndDescriptions.aspx http:\/\/tdrdata.com\/ipd\/ipd_SearchForICD9CodesAndDescriptions.aspx Under review as a conference paper at ICLR 2017 (1997)) as the feature extractor network instead of the feed-forward neural networks used in DANN. For VFAE, DANN and all the non-domain adaptive approaches we flattened the time series along time axis and treat it as the input to the model. For fairness, the classifier and feature extractors of the VRADA and R-DANN were equivalent in depth and both had the same model capacity. We also ensure that the size of latent feature representation z̃i are similar for VRADA and DANN models. The model capacity of VFAE was chosen to be similar to VRADA. All the deep domain adaptation models including ours had depth of size 8 (including output classifier layers). We used the Adam optimizer ( Kingma & Ba (2014)) and ran all models for 500 epochs with a learning rate of 3e−4. We set an early stopping criteria that the model does not experience a decrease in the validation loss for 20 epochs. Source domain data was split into train\/validation subsets with a 70\/30 ratio and target domain data into train\/validation\/test subsets with a 70\/15\/15 ratio. In order to compare all the methods, we report AUC scores on the entire target domain set, and the test subset for each target domain data of a source-target pair. 4.3 QUANTITATIVE RESULTS In Table 1, we compare non domain adaptation and domain adaptation models\u2019 performance on the target domain test subset for the AHRF mortality prediction task. It is immediately clear that domain adaptation methods consistently outperform non domain adaptation methods. We see that generally the VRADA outperforms both variants of the DANN with it consistently seeing scores ∼ 4% higher. While the standard deviation for the VRADA was about 1%, it was about 2% for the R-DANN, further showing our models efficacy as it converges to more stable local optima. Our model VRADA beats state-of-the-art DANN(Ganin et al. (2016)) and VFAE(Louizos et al. (2015)) on all the source-pair domain adaptation tasks for Adult-AHRF dataset. For the domain adaptation from Adult-AHRF to Child-AHRF dataset, we observe that VRADA mostly outperforms all the competing models. This shows that our model can perform well even for smaller target domain datasets. Table 1: AUC Comparison for AHRF Mortality Prediction task with and without Domain Adaptation Source-Target LR Adaboost DNN DANN VFAE R-DANN VRADA 3- 2 0.555 0.562 0.569 0.572 0.615 0.603 0.654 4- 2 0.624 0.645 0.569 0.589 0.635 0.584 0.656 5- 2 0.527 0.554 0.551 0.540 0.588 0.611 0.616 2- 3 0.627 0.621 0.550 0.563 0.585 0.708 0.724 4- 3 0.681 0.636 0.542 0.527 0.722 0.821 0.770 5- 3 0.655 0.706 0.503 0.518 0.608 0.769 0.782 2- 4 0.585 0.591 0.530 0.560 0.582 0.716 0.777 3- 4 0.652 0.629 0.531 0.527 0.697 0.769 0.764 5- 4 0.689 0.699 0.538 0.532 0.614 0.728 0.738 2- 5 0.565 0.543 0.549 0.526 0.555 0.659 0.719 3- 5 0.576 0.587 0.510 0.526 0.533 0.630 0.721 4- 5 0.682 0.587 0.575 0.548 0.712 0.747 0.775 5- 1 0.502 0.573 0.557 0.563 0.618 0.563 0.639 4- 1 0.565 0.533 0.572 0.542 0.668 0.577 0.636 3- 1 0.500 0.500 0.542 0.535 0.570 0.591 0.631 2- 1 0.520 0.500 0.534 0.559 0.578 0.630 0.637 In the above table, we test classification without adaptation using Logistic Regression (LR), Adaboost with decision tree classifiers and Feed forward Deep Neural Networks (DNN); and with adaptation using Deep Domain Adversarial Neural Networks (DANN), a DANN with an LSTM in its feature extractor (R-DANN), Variational Fair Autoencoder (VFAE) and our Variational Adversarial Domain Adaptation Model (VRADA). All results are reported on the target domain test subset dataset. As the AHRF mortality prediction task made it clear that domain adaptation is necessary for inter- group adaptation, for the ICD9 multi-task prediction task that involved data with time-steps of length 12, we focused strictly on adaptive models (i.e. the DANN, R-DANN, and VRADA). Table 2 shows the aggregated AUC scores on the entire target domain dataset and test data of the target domain for the 20 tasks of the ICD9 Code Prediction task. Here we clearly see that our VRADA model outperforms competing domain adaptive models by significant margins. On numerous adaptation settings the VRADA gets a AUC score of > 0.76 whereas R-DANN obtains AUC score around 0.68 i.e. VRADA outperforms R-DANN by ∼ 8% when averaged over all the source-target domain pairs. 7 Under review as a conference paper at ICLR 2017 Table 2: AUC Comparison for ICD9 Diagnosis Code Prediction task Model 23 24 25 32 34 35 42 43 45 52 53 54 DANN entire targettarget test 0.523 0.530 0.509 0.557 0.513 0.540 0.520 0.571 0.518 0.520 0.524 0.515 0.510 0.561 0.515 0.553 0.535 0.531 0.524 0.534 0.515 0.530 0.518 0.526 R-DANN entire targettarget test 0.691 0.672 0.679 0.677 0.597 0.575 0.704 0.585 0.708 0.732 0.717 0.637 0.754 0.618 0.759 0.806 0.753 0.697 0.688 0.567 0.702 0.720 0.729 0.727 VRADA entire targettarget test 0.741 0.740 0.753 0.755 0.753 0.751 0.716 0.718 0.793 0.792 0.792 0.790 0.708 0.712 0.764 0.765 0.807 0.807 0.703 0.707 0.760 0.762 0.801 0.799 Here, we compare results for the ICD9 Diagnosis Code Prediction task on the ICD9 dataset. For each model, the top row corresponds to the performance on the entire target domain dataset and the bottom row corresponds to performance on the test subset (15%) of the target domain dataset. Further, for difficult adaptation settings such as transferring knowledge learned from young adults to the elderly (2-5), where the DANN and R-DANN perfom poorly, the VRADA consistently performs well on both data (target data) it was exposed to during training (but without labels) and data it never learned to adapt for (the target domain test subset). 4.4 DISCUSSION Figure 3 shows the temporal latent dependencies captured by our VRADA as compared to the R-DANN for 3-4 source-target pair. While both models learn temporal latent dependencies fairly well, the VRADA outperforms the R-DANN in two ways. First, the VRADA\u2019s neurons learned stronger predictions of whether features are relevant towards modeling the data. If we look at the VRADA row, for both AHRF and ICD9 we see that the neural activation patterns are more consistent across time-steps than for R-DANN. Figure 4 shows the unrolled memory cell states (in the form Examples× Time ∗ Neurons) for all the source and target domain data points. We see a consistent activation firing patterns across all these data points for VRADA but not for R-DANN. Together with the stronger performance on 34 for AHRF and 25 for ICD9, this potentially indicates that VARDA is better learning the temporal dependencies. Second, nuanced values are consistent across time-steps for the VRADA, exhibiting a gradual transition towards stronger activation with time, whereas the temporal activation pattern of the R- DANN seems somewhat sporadic. While activation gradients across time are consistent for both the R-DANN and VRADA, more consistent inhibitory and excitatory neuron firing patterns indicate that the VRADA better transfers knowledge. Another indication of domain adaptation was shown in Figure 1c. Looking at the t-SNE projections of feature representations of DNN, R-DANN, and VRADA we can see that the addition of temporal latent dependencies might help in better mixing of the domain distributions since we observe that the data is more evenly spread out. Figure 1c and Figure 3 together indicate that the VRADA\u2019s temporal latent dependency capturing power and ability to create domain-invariant representations act synergistically. 5 SUMMARY Because of the diverse range of patients healthcare data is collected for and its episodic and longitudal nature, healthcare data provides a good platform to test domain adaptation techniques for temporal data. With it as our example, we showcase the Variational Recurrent Adversarial Domain Adaptation (VRADA) model\u2019s ability to learn temporal latent representations that are domain-invariant. By comparing our model\u2019s latent representations to others\u2019, we show its ability to use variational methods to capture hidden factors of variation and produce more robust domain-invariant representations. We hope this work serves as a bedrock for future work capturing and adapting temporal latent representations across domains. ACKNOWLEDGMENTS This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1418060. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also acknowledge Thailand\u2019s Development and 8 Under review as a conference paper at ICLR 2017 R-DANN 0.5 1 1.5 2 2.5 3 3.5 4 4.5 2 4 6 8 10 12 14 16 18 20 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0.5 1 1.5 2 2.5 3 3.5 4 4.5 2 4 6 8 10 12 14 16 18 20 -3 -2 -1 0 1 2 3 VRADA 0.5 1 1.5 2 2.5 3 3.5 4 4.5 2 4 6 8 10 12 14 16 18 20 -3 -2 -1 0 1 2 3 0.5 1 1.5 2 2.5 3 3.5 4 4.5 2 4 6 8 10 12 14 16 18 20 -3 -2 -1 0 1 2 3 Source Target 2 4 6 8 10 12 5 10 15 20 25 30 35 40 -4 -3 -2 -1 0 1 2 3 4 2 4 6 8 10 12 5 10 15 20 25 30 35 40 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2 4 6 8 10 12 5 10 15 20 25 30 35 40 -10 -8 -6 -4 -2 0 2 4 6 8 10 2 4 6 8 10 12 5 10 15 20 25 30 35 40 -10 -8 -6 -4 -2 0 2 4 6 8 10 Source Target AHRF, 3-4 ICD9, 2-5 Figure 3: Cell states of memory cell for R-DANN and VRADA showing temporal latent dependencies captured by neurons of the R-DANN and VRADA for the source domain and transferred to the target domain. Each step along the y-axis refers to the activation of a single neuron with blue for strong inhibition and yellow for strong excitation. Step along the x-axis refers to activation per time-step. The left shows a single example in adapting 3-4 and the right for adapting 2-5. 50 100 150 200 250 300 350 400 450 50 100 150 200 250 300 350 400 450 -8 -6 -4 -2 0 2 4 6 8 10 50 100 150 200 250 300 350 400 450 50 100 150 200 250 300 350 400 -10 -8 -6 -4 -2 0 2 4 6 8 10 50 100 150 200 250 300 350 400 450 50 100 150 200 250 300 350 400 450 -10 -8 -6 -4 -2 0 2 4 6 8 10 50 100 150 200 250 300 350 400 450 50 100 150 200 250 300 350 400 -10 -8 -6 -4 -2 0 2 4 6 8 10 R-DANN VRADA Figure 4: Cell states of memory cell for R-DANN and VRADA showing activation for all ICD9 2-5 adaptation examples. Here, we show temporal dependencies learned across time, feature pairs for examples in a domain. The y-axis values refer to values per data point and the x-axis shows activation at time, feature pairs with the time and feature dimensions being flattened. Promotion of Science and Technology Talents Project for financial support. We thank Dr. Robinder Khemani for sharing the Child-AHRF dataset. REFERENCES Berhanu Alemayehu and Kenneth E Warner. The lifetime distribution of health care costs. Health services research, 39(3):627\u2013642, 2004. S Ben-David, J Blitzer, and K Crammer. Analysis of representations for domain adaptation. Advances in Neural . . . , pp. 137\u2013144, 2007. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151\u2013175, 2010. John Blitzer. Domain adaptation of natural language processing systems. PhD thesis, University of Pennsylvania, 2007. Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, 2012. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua Bengio. A Recurrent Latent Variable Model for Sequential Data. arXiv.org, May 2016. 9 Under review as a conference paper at ICLR 2017 Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual domain adaptation using subspace alignment. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2960\u20132967, 2013. George Foster, Cyril Goutte, and Roland Kuhn. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 451\u2013459. Association for Computational Linguistics, 2010. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. arXiv preprint arXiv:1409.7495, 2014. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2016. Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2066\u20132073. IEEE, 2012. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. Mit Press, December 2016. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997. Fei Huang and Alexander Yates. Distributional representations for handling sparsity in supervised sequence-labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pp. 495\u2013503. Association for Computational Linguistics, 2009. Jing Jiang. A literature survey on domain adaptation of statistical classifiers. URL: http:\/\/sifaka. cs. uiuc. edu\/jiang4\/domainadaptation\/survey, 2008. Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL, volume 7, pp. 264\u2013271, 2007. AEW Johnson, TJ Pollard, L Shen, L Lehman, M Feng, M Ghassemi, B Moody, P Szolovits, LA Celi, and RG Mark. Mimic-iii, a freely accessible critical care database. Scientific Data, 2016. Robinder G Khemani, David Conti, Todd A Alonzo, Robert D Bart III, and Christopher JL Newth. Effect of tidal volume in children with acute hypoxemic respiratory failure. Intensive care medicine, 35(8):1428\u20131437, 2009. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs\/1412.6980, 2014. URL http:\/\/arxiv.org\/abs\/1412.6980. Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv.org, December 2013. Zhiqiang Lao, Dinggang Shen, Zhong Xue, Bilge Karacali, Susan M Resnick, and Christos Da- vatzikos. Morphological classification of brains via high-dimensional shape transformations and machine learning methods. Neuroimage, 21(1):46\u201357, 2004. Mingsheng Long and Jianmin Wang. Learning transferable features with deep adaptation networks. CoRR, abs\/1502.02791, 1:2, 2015. Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair auto encoder. arXiv preprint arXiv:1511.00830, 2015. Sinno Jialin Pan and Qiang Yang. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2009. Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53\u201369, 2015. 10 http:\/\/arxiv.org\/abs\/1412.6980 Under review as a conference paper at ICLR 2017 Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213\u2013226. Springer, 2010. Meena Seshamani and Alastair M Gray. A longitudinal study of the effects of age and time to death on hospital costs. Journal of health economics, 23(2):217\u2013235, 2004. Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129\u2013136, 2011. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4068\u20134076, 2015. Min Xiao and Yuhong Guo. Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model. In ICML (1), pp. 293\u2013301, 2013. Yi Yang and Jacob Eisenstein. Unsupervised multi-domain adaptation with feature embeddings. 11 Introduction Related Work Variational Recurrent Adversarial Deep Domain Adaptation Notations VRADA Experiments Dataset Description Prediction and Domain Adaptation tasks Methods and Implementation Details Quantitative Results Discussion Summary ","flair":"three\tResearch"}
{"author":"sybilckw","created":"Wed Oct 12 05:44:17 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.01983 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs cs.RO References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks? Authors: Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Ram Vasudevan (Submitted on 6 Oct 2016) Abstract: Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have however relied upon massive amounts of human annotated training data. This time-consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, this paper illustrates that real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor based classification problems like those that appear in self-driving cars. Comments: 8 pages Subjects: Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO) Cite as: arXiv:1610.01983 [cs.CV]   (or arXiv:1610.01983v1 [cs.CV] for this version) Submission history From: Matthew Johnson-Roberson [view email] [v1] Thu, 6 Oct 2016 18:26:43 GMT (8066kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Staturecrane","created":"Tue Nov 01 16:00:18 EDT 2016","text":"Full code for the model can be found here Stumbling onto Alec Radford's deep convolutional generative adversarial network, or DCGAN, was one of the few genuinely jaw-dropping moments I've experienced in my life. It needs no further introduction; visit the webpage and see the madness. Or check it out right here: Those images, clearly of bedrooms, were not captured by cameras, but by statistics. They are generated from what you might call the \"probability space\" of bedroom pictures; the statistical distribution that contains all the various features you would expect to find in a picture of a bedroom, neatly separated so that a random sample from said distribution can be \"morphed\" into something that resembles a photograph. The way this is done is both shockingly simple and dreadfully complicated. It's simple in that it doesn't require a ton of code, appears to work on various kinds of datasets, and can be run on a medium-grade GPU to start producing interesting results in a few hours. It's complicated because modifying, debugging or adding to the model requires understanding the myriad kinks of generative adversarial networks, which are slightly odd beasts. I have stitched together the Torch implementation of DCGAN and Kaixhin's variational autoencoder, the latter of which performs variational inference on complicated integrals like the ones that define photographic probability distributions and can be trained with vanilla stochastic gradient descent. By tinkering with the heuristics, I was able to successfully generate fine-art-like images from the wikiart.org dataset, which was compiled by Small Yellow Duck and hosted on Kaggle. The examples below are picked from thousands upon thousands of samples generated by the model and do not exist in the training data. While the base of the DCGAN remains the same, I have changed a few things, as well as added the variational autoencoder. Compelled by a post on stabilizing GANs, I have both set limits on the generator and discriminator, so they only update when they or their adversary is performing either particularly well or particularly poorly, as well as added noise to the inputs feeding into the discriminator. Simulated annealing then lowers the amount of that noise as training progresses to encourage convergence of the GAN. The variational autoencoder uses a modified version of the discriminator to produce the latent variables, which then feed into the generator, so the entire model is convolutional. All three networks contribute equally to the loss, although the generator and discriminator are not always updating their gradients (due to the balancing act that must be played to keep them from outperforming one another). All these tricks combined seem to work consistently and stably for up to 64x64 color images, and although I suspect convergence is possible for larger dimensions, I have yet to successfully do it myself.","flair":"four\tProject"}
{"author":"Kaixhin","created":"Mon Oct 24 19:24:00 EDT 2016","text":"Agent interacts with the environment and learns through trial-and-error Agent affects its own observations (no i.i.d.) Learn a policy $\\pi$ that maps states to actions to maximise $\\mathbb{E}[R]$ (allows bootstrapping instead of Monte Carlo estimates) If we had $Q^*$, $\\pi^*(\\mathbf{s}_t) = \\arg\\!\\max\\limits_{\\mathbf{a}}Q^*(\\mathbf{s}_t, \\mathbf{a})$ Therefore, $Q^\\pi$ can be improved by bootstrapping Learn from experience: $Q'(\\mathbf{s}_t, \\mathbf{a}_t) = Q(\\mathbf{s}_t, \\mathbf{a}_t) + \\alpha \\delta$,where $\\alpha$ is the learning rate and $\\delta$ is the TD-error $Y$ is reward received + discounted max Q-value of next state Loss is Mean Squared Error (over batch): $\\mathcal{L}(\\delta) = \\frac{1}{N}\\sum\\limits_{n=1}^{N}(\\delta_n)^2$ DL Note: RL updates are usually formulated for gradient ascent","flair":"three\tResearch"}
{"author":"rmltestaccount","created":"Sat Oct 08 12:18:13 EDT 2016","text":" Google Research Blog The latest news from Research at Google Equality of Opportunity in Machine Learning Friday, October 07, 2016 Posted by Moritz Hardt, Research Scientist, Google Brain Team As machine learning technology progresses rapidly, there is much interest in understanding its societal impact. A particularly successful branch of machine learning is supervised learning. With enough past data and computational resources, learning algorithms often produce surprisingly effective predictors of future events. To take one hypothetical example: an algorithm could, for example, be used to predict with high accuracy who will pay back their loan. Lenders might then use such a predictor as an aid in deciding who should receive a loan in the first place. Decisions based on machine learning can be both incredibly useful and have a profound impact on our lives. Even the best predictors make mistakes. Although machine learning aims to minimize the chance of a mistake, how do we prevent certain groups from experiencing a disproportionate share of these mistakes? Consider the case of a group that we have relatively little data on and whose characteristics differ from those of the general population in ways that are relevant to the prediction task. As prediction accuracy is generally correlated with the amount of data available for training, it is likely that incorrect predictions will be more common in this group. A predictor might, for example, end up flagging too many individuals in this group as \u2018high risk of default\u2019 even though they pay back their loan. When group membership coincides with a sensitive attribute, such as race, gender, disability, or religion, this situation can lead to unjust or prejudicial outcomes. Despite the need, a vetted methodology in machine learning for preventing this kind of discrimination based on sensitive attributes has been lacking. A naive approach might require a set of sensitive attributes to be removed from the data before doing anything else with it. This idea of \u201Cfairness through unawareness,\u201D however, fails due to the existence of \u201Credundant encodings.\u201D Even if a particular attribute is not present in the data, combinations of other attributes can act as a proxy. Another common approach, called demographic parity, asks that the prediction must be uncorrelated with the sensitive attribute. This might sound intuitively desirable, but the outcome itself is often correlated with the sensitive attribute. For example, the incidence of heart failure is substantially more common in men than in women. When predicting such a medical condition, it is therefore neither realistic nor desirable to prevent all correlation between the predicted outcome and group membership. Equal Opportunity Taking these conceptual difficulties into account, we\u2019ve proposed a methodology for measuring and preventing discrimination based on a set of sensitive attributes. Our framework not only helps to scrutinize predictors to discover possible concerns. We also show how to adjust a given predictor so as to strike a better tradeoff between classification accuracy and non-discrimination if need be. At the heart of our approach is the idea that individuals who qualify for a desirable outcome should have an equal chance of being correctly classified for this outcome. In our fictional loan example, it means the rate of \u2018low risk\u2019 predictions among people who actually pay back their loan should not depend on a sensitive attribute like race or gender. We call this principle equality of opportunity in supervised learning. When implemented, our framework also improves incentives by shifting the cost of poor predictions from the individual to the decision maker, who can respond by investing in improved prediction accuracy. Perfect predictors always satisfy our notion, showing that the central goal of building more accurate predictors is well aligned with the goal of avoiding discrimination. Learn more To explore the ideas in this blog post on your own, our Big Picture team created a beautiful interactive visualization of the different concepts and tradeoffs. So, head on over to their page to learn more. Once you\u2019ve walked through the demo, please check out the full version of our paper, a joint work with Eric Price (UT Austin) and Nati Srebro (TTI Chicago). We\u2019ll present the paper at this year\u2019s Conference on Neural Information Processing Systems (NIPS) in Barcelona. So, if you\u2019re around, be sure to stop by and chat with one of us. Our paper is by no means the final word on this important and complex topic. It joins an ongoing conversation with a multidisciplinary focus of research. We hope to inspire future research that will sharpen the discussion of the different achievable tradeoffs surrounding discrimination and machine learning, as well as the development of tools that will help practitioners address these challenges. Google Labels: Google Brain , Machine Learning , Publications , Supervised Learning    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"three\tResearch"}
{"author":"NotAHomeworkQuestion","created":"Mon Oct 10 13:22:00 EDT 2016","text":"Suppose I've got a dataset with repeated measures within research subjects. I want to use a random intercept rather than fixed effects for the intercept term specific to the research subject. If I'd used fixed effects I'd have one parameter per research subject, but using random effects I'd have only two parameters total (the mean and SD of the normal distribution for the random effects). However, in the end, with random effects I still get one intercept estimate per subject along with standard errors for these intercept terms. Granted these have been shrunken toward the global mean, but shouldn't I still essentially have one parameter per subject?","flair":"one\tDiscussion"}
{"author":"plasmalightwave","created":"Sun Oct 09 13:35:47 EDT 2016","text":"Hello! I've just started learning ML. I've been learning about Logistic Regression using Gradient descent. I followed two tutorials:\n\nhttps:\/\/www.coursera.org\/learn\/machine-learning\/home\/week\/3 http:\/\/nbviewer.jupyter.org\/github\/tfolkman\/learningwithdata\/blob\/master\/Logistic%20Gradient%20Descent.ipynb\n\nI used the spambase dataset from UC Irvine's repo: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Spambase\n\nThe steps in my program are as follows:\n\n    1. read data from the text file; num_features = 57, classes = 1 (spam=1, not-spam=0)\n    2. Normalize feature matrix using (X-mean)\/Std_Deviation\n    3. Initialize parameters theta to zeros, learning rate to 0.01\n    4. Train theta by running gradient descent while calculating cost; terminate when cost does not \n       change by more than 0.0001 AND at least 1500 iterations have run\n    5. Test the classifier on the same input data set (predict output=1 if probability is &gt;=0.5)\n\nHowever, my model\/classifier has low accuracy (~52%) in it's prediction. I calculated accuracy by comparing how many data points the classifier got right and calculating the mean. I experimented with different learning rates\/number of iterations but always get the same accuracy.\n\nI'd really appreciate it if someone familiar with logistic regression could review my code and point where I'm going wrong:\nhttps:\/\/github.com\/codewarrior07\/ml\/blob\/master\/LogisticRegression_spambase.py\n\nI also plotted the iterations vs cost using pyplot; I get the following result:\nhttp:\/\/imgur.com\/a\/kjB09","flair":"four\tProject"}
{"author":"BafflesSean","created":"Sat Nov 12 14:29:02 EST 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose The Future Of Artificial Intelligence || Demis Hassabis - DeepMind Founder A.I. SubscribeSubscribedUnsubscribe Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 4,529 views 100 Like this video? Sign in to make your opinion count. Sign in 101 3 Don't like this video? Sign in to make your opinion count. Sign in 4 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 12, 2016 The Future Of Artificial Intelligence - Demis Hassabis - DeepMind Founder Click here to subscribe to A.I. : http:\/\/bit.ly\/29YHS5r Watch Other Videos : Current State Of Artificial Intelligence | World Economic Forum | Davos 2016 : https:\/\/www.youtube.com\/watch?v=exMZb... Stephen Hawking's Thoughts On Artificial Intelligence : https:\/\/www.youtube.com\/watch?v=xzi3n... Category Science & Technology License Standard YouTube License Show more Show less Loading... Advertisement Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Fujitsu Forum 2016 - Artificial Intelligence - Duration: 58:03. Fujitsu Forum 3,145 views 58:03 Artificial Intelligence And The Future | Demis Hassabis - DeepMind Founder - Duration: 48:23. A.I. 816 views New 48:23 How Neural Networks Actually Work || Geoffrey Hinton - Google's A.I. Chief - Duration: 21:12. A.I. 1,819 views New 21:12 Google s Deep Mind Explained! Self Learning A I - Duration: 13:45. THE F_YOU_TURE 178 views 13:45 DealBook 2016: Artificial Intelligence - Duration: 32:58. The New York Times Conferences 8,122 views 32:58 The Future Of Artificial Intelligence | Prof. Fei Fei Li - Duration: 17:42. A.I. 603 views New 17:42 New research about Artificial Intelligence And Robotics - A.I Documentary 2016 HD - Duration: 46:02. Documentary Documentary 1,457 views 46:02 Demis Hassabis How Deep Learning Can Give Birth to General Artificial Intelligence - Duration: 50:45. Scott Theil 290 views 50:45 Current State Of Artificial Intelligence | World Economic Forum | Davos 2016 - Duration: 55:33. A.I. 921 views 55:33 Google's Deep Mind Explained! - Self Learning A.I. - Duration: 13:45. ColdFusion 775,585 views 13:45 S9 Ep.12 - The Amazing Progress in Artificial Intelligence Part 1- TechTalk With Solomon - Duration: 29:02. TechTalkWithSolomon 1,440 views 29:02 Artificial Intelligence and the future | AndrÃ© LeBlanc | TEDxMoncton - Duration: 10:07. TEDx Talks 193,810 views 10:07 Unfriendly Artificial Intelligence || Kurzweil Interviews Minsky - Duration: 2:44. A.I. 510 views New 2:44 Deep Neural Network Learns To Paint Like Picasso - Duration: 4:20. A.I. 436 views New 4:20 Microsoft's Artificial Intelligence Meltdown - Duration: 6:24. Journeyman Pictures 150,562 views 6:24 Demis Hassabis, CEO, DeepMind Technologies - The Theory of Everything - Duration: 16:41. ZeitgeistMinds 186,363 views 16:41 Demis Hassabis, Google DeepMind - Artificial Intelligence and the Future - Duration: 50:19. Singularity Lectures 31,549 views 50:19 Demis Hassabis: Towards General Artificial Intelligence - Duration: 1:07:30. Center for Brains, Minds and Machines (CBMM) 18,509 views 1:07:30 Neil deGrasse Tyson Interviews Grant Imahara - Robots, Driverless Car Tech, AI and More! - Duration: 22:56. Mouser Electronics 26,321 views 22:56 Demis Hassabis: Artificial Intelligence and the Future - Duration: 47:30. Singularity Lectures 16,458 views 47:30 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"one\tDiscussion"}
{"author":"benjaminwilson","created":"Tue Sep 27 11:03:03 EDT 2016","text":" Skip to content Blog Lateral Newsletter Blog FAQ Docs Labs Publishers Enterprise The Lateral Blog All Data Demos Development Machine Learning UX Newsletter A fastText-based hybrid recommender 27th September 201620th October 2016 by Stephen Introduction Using Facebook Research\u2019s new fastText library in supervised mode, I trained a hybrid recommender system, to recommend articles to users, given as training data both the text in the articles and the user\/article interaction matrix. The labels attached to a document were both its id, and the ids of all users who viewed it. I\u2019ve not finished testing it, but early signs are that it learns quite well. This post is a short progress report, perhaps of interest to others building hybrid recommenders. About fastText What is it? Recently, Facebook Research released the C++ source code for fastText, a software library for learning word embeddings using a shallow neural network. It can learn either unsupervised, training on unstructured text, or in a supervised manner, from a series of labelled documents. The code is accompanied by two research preprints: Enriching Word Vectors with Subword Information (unsupervised) Bag of Tricks for Efficient Text Classification (supervised), explaining fastText\u2019s workings and performance on benchmark datasets in each case. The authors state its performance is comparable with much more complex \u201Cdeep learning\u201D algorithms, but the training time is significantly lower. How does it work? In unsupervised mode, fastText trains by sliding a window over the input text and either learning the centre word from the remaining context (\u201Ccontinuous bag of words\u201D or \u201CCBOW\u201D), or all the context words from the centre word (\u201CSkipgram\u201D), and learning can be viewed as a series of updates to a neural network with two layers of weights and three tiers of neurons, in which the two outer tiers each have one neuron for each word in the vocabulary, and the middle tier has as many neurons as there are dimensions in the embedding space. In this way, it is similar to word2vec (in fact, one of the fastText coauthors, Tomas Mikolov, is the creator of word2vec). Unlike word2vec, however, fastText can also learn vectors for sub-parts of words  \u2014 so-called character n-grams \u2014 ensuring e.g. that the words \u201Csupervised\u201D, \u201Csupervise\u201D and \u201Csupervisor\u201D all have similar vectorisations, even if they tend to appear in different contexts. This feature enhances learning on heavily inflected languages, according to the authors. In supervised mode, the first two neuron tiers of the network are the same, but the neurons in the output tier are in bijection with the labels, rather than the words in the vocabulary. The learning task on a given window is to predict the labels attached to the current document, given the words in the window. Training a hybrid recommender Defining the model Supervised fastText trains on a text file in which each line is a series of space-separated labels, followed by the text of single document. The labels are prepended by a crib, by default \u201C__label__\u201D, so they can be differentiated from words in the text. In my case, each line of the training file contained one document, with no repetitions. The labels attached to a document were its document id, and the ids of all users who had interacted with it. Here\u2019s a fictitious example: __label__doc_id_21231 __label__user_id_aadsf89asdf __label__user_id_87sdfsdf __label__user_id_45jfkasf novak djokovic won yet another tennis match last friday ... After training, the score associated to a user\/document pair is the cosine similarity of their associated vectors. (Note that the vectors learned by fastText are not of unit length, so computing cosine similarity means normalising.) The document yielding the highest score is defined to be the recommendation. Data preparation and hyperparameters After throwing out users and documents with too few interactions, my training set consisted of around 22k documents and 85k users, with 1 million interactions, and my test set consisted of 30,000 user\/document interaction pairs. The training set was 56MB in size. Here are the hyperparameters I specified for the best-performing model (all others were default): -dim 100 -loss ns -thread 32 -minCount 5 -neg 10 -ws 20 -epoch 100000 -lr 0.075 The training took around 5 hours on a 32-core, 120 GB AWS instance. Varying the window size, dimension and minCount made little difference to model quality. Increasing \u201Cneg\u201D \u2014 the number of negative samples \u2014 improved the quality of the model uniformly in the range I checked: values from 5 to 50 in intervals of 5. (The downside is that more negative samples means training takes longer.) Varying learning rate affected the model unpredictably: larger values seemed better when the number of epochs was small, but I can\u2019t see a consistent pattern. Using a very large learning rate with very many epochs lead to segfault errors, perhaps because of arithmetic overflow. Increasing the number of epochs steadily improved model quality. I stopped at 100,000 epochs because the improvements were becoming marginal, and my patience ran out. A small \u201Cgotcha\u201D In the version of the code I downloaded in late August, the documentation stated the default value for the \u201Closs\u201D hyperparameter was \u201Cns\u201D, meaning negative sampling. However, this was not true at the time: reading the code revealed the true default value was \u201Csoftmax\u201D, which perhaps leads to a better model, but is much slower to train. This meant that my first attempts to train a model were unsuccessful, because fixing a large enough value of \u201Cepoch\u201D to ensure the model learned something would\u2019ve meant waiting days or weeks for the model to train. After a few days\u2019 frustration, I read the code and noticed the error in the documentation. Happily, fastText coauthor Edouard Grave was extremely helpful, responding immediately to my bug report, and fixing the problem within a day. He was also kind enough to suggest I use hierarchical softmax instead of negative sampling for the \u201Closs\u201D parameter, since it is, apparently, faster still to train. I haven\u2019t yet experimented with hierarchical softmax systematically. A big thank you to Edouard for his help! Rationale: why should this work? The first layer of weights in the network encodes the word vectors; the second layer the label vectors. During training, at any given text window, each word vector is moved closer to a randomly chosen label vector for the current document. Each document id label is seen only once per training epoch, but if we train over many epochs, then: We expect the label vector associated to a document id to be pushed closer to the average of the word vectors this document contains, and We expect the vector associated to a user id to be pushed closer to the average of the vectors of all words mentioned in each document seen by that user. If both the above hold, then label vectors of document ids and label vectors of user ids will be tend to be moved closer to one another, provided the given document has been seen by the given user. For the same reason, it will also be the case that any two users with similar reading tastes, and any two documents attracting users with similar reading tastes, will have similar vectorisations. Results We measured the performance of the model as follows: For each user_id\/doc_id pair(u, d) in the test set, compute the cosine similarity of vec(u) with vec(d\u2019), where d\u2019 varies across all documents in the test set; this is by definition the score s(u, d\u2019) of (u, d\u2019). Write these scores s(u, d\u2019) in descending order in a list L, and record the position in which s(u, d) occurs. Now compute the \u201Cdecimal percentile\u201D of s(u, d) in L: if it is in first place, assign it a score of 0.0, if it is in last place, give it 1.0; in the middle is 0.5. Call this number the \u201Cpercentile rank\u201D pr(u, d) of the pair (u, d). We measure the performance of the model as the mean of pr(u, d) across all 30,000 pairs (u, d). The best model trained in this way achieved a score of around 0.093. A model that understood nothing would achieve a score of 0.5 \u2014 meaning that the score s(u, d) would be half-way down the list of test scores on average \u2014 so this is significantly better than random. However, this result is not as good as our current state-of-the-art model (CSOA), which achieves a score of less than 0.01. However, the CSOA is also trained on around 100 times more data. The next step is to train a fastText model on this larger data set. The experiment continues! Category:Machine Learning The Author Stephen I'm a data scientist at Lateral. Comments Please enable JavaScript to view the comments powered by Disqus. Previous Post Similar pages for Wikipedia Wikipedia is one of the most widely used websites globally. We built a simple extension to that displays similar pages at the top of every Wikipedia page! Next Post Developers Get API Key Documentation Confluence add-on Jobs Labs Company About Contact Cookies Policy Imprint Privacy Policy Terms of Use Channels Twitter Medium GitHub Newsletter Subscribe © Lateral 2016. All rights reserved. Shares ","flair":"null\tnull"}
{"author":"NihilisticFool","created":"Wed Oct 05 09:46:03 EDT 2016","text":"So, I have been going through the above problem, which I find quite daunting. I am not looking for a solution, but rather a discussion on the approaches one could take to model this. ","flair":"null\tnull"}
{"author":"nivwusquorum","created":"Mon Oct 03 18:53:53 EDT 2016","text":" Skip to content All gists GitHub Sign up for a GitHub account Sign in Create a gist now Instantly share code, notes, and snippets. Star 38 Fork 9 nivwusquorum\/tf_lstm.py Last active Nov 12, 2016 Embed What would you like to do? Embed Embed this gist in your website. Embed Share Copy sharable URL for this gist. Share Clone via HTTPS Clone with Git or checkout with SVN using the repository's web address. HTTPS Learn more about clone URLs Download ZIP Code Revisions 4 Stars 38 Forks 9 Simple implementation of LSTM in Tensorflow in 50 lines (+ 130 lines of data generation and comments) Raw tf_lstm.py \"\"\"Short and sweet LSTM implementation in Tensorflow. Motivation: When Tensorflow was released, adding RNNs was a bit of a hack - it required building separate graphs for every number of timesteps and was a bit obscure to use. Since then TF devs added things like `dynamic_rnn`, `scan` and `map_fn`. Currently the APIs are decent, but all the tutorials that I am aware of are not making the best use of the new APIs. Advantages of this implementation: - No need to specify number of timesteps ahead of time. Number of timesteps is infered from shape of input tensor. Can use the same graph for multiple different numbers of timesteps. - No need to specify batch size ahead of time. Batch size is infered from shape of input tensor. Can use the same graph for multiple different batch sizes. - Easy to swap out different recurrent gadgets (RNN, LSTM, GRU, your new creative idea) \"\"\" import numpy as np import random import tensorflow as tf import tensorflow.contrib.layers as layers map_fn = tf.python.functional_ops.map_fn ################################################################################ ## DATASET GENERATION ## ## ## ## The problem we are trying to solve is adding two binary numbers. The ## ## numbers are reversed, so that the state of RNN can add the numbers ## ## perfectly provided it can learn to store carry in the state. Timestep t ## ## corresponds to bit len(number) - t. ## ################################################################################ def as_bytes(num, final_size): res = [] for _ in range(final_size): res.append(num % 2) num \/\/= 2 return res def generate_example(num_bits): a = random.randint(0, 2**(num_bits - 1) - 1) b = random.randint(0, 2**(num_bits - 1) - 1) res = a + b return (as_bytes(a, num_bits), as_bytes(b, num_bits), as_bytes(res,num_bits)) def generate_batch(num_bits, batch_size): \"\"\"Generates instance of a problem. Returns ------- x: np.array two numbers to be added represented by bits. shape: b, i, n where: b is bit index from the end i is example idx in batch n is one of [0,1] depending for first and second summand respectively y: np.array the result of the addition shape: b, i, n where: b is bit index from the end i is example idx in batch n is always 0 \"\"\" x = np.empty((num_bits, batch_size, 2)) y = np.empty((num_bits, batch_size, 1)) for i in range(batch_size): a, b, r = generate_example(num_bits) x[:, i, 0] = a x[:, i, 1] = b y[:, i, 0] = r return x, y ################################################################################ ## GRAPH DEFINITION ## ################################################################################ INPUT_SIZE = 2 # 2 bits per timestep RNN_HIDDEN = 20 OUTPUT_SIZE = 1 # 1 bit per timestep TINY = 1e-6 # to avoid NaNs in logs LEARNING_RATE = 0.01 USE_LSTM = True inputs = tf.placeholder(tf.float32, (None, None, INPUT_SIZE)) # (time, batch, in) outputs = tf.placeholder(tf.float32, (None, None, OUTPUT_SIZE)) # (time, batch, out) ## Here cell can be any function you want, provided it has two attributes: # - cell.zero_state(batch_size, dtype)- tensor which is an initial value # for state in __call__ # - cell.__call__(input, state) - function that given input and previous # state returns tuple (output, state) where # state is the state passed to the next # timestep and output is the tensor used # for infering the output at timestep. For # example for LSTM, output is just hidden, # but state is memory + hidden # Example LSTM cell with learnable zero_state can be found here: # https:\/\/gist.github.com\/nivwusquorum\/160d5cf7e1e82c21fad3ebf04f039317 if USE_LSTM: cell = tf.nn.rnn_cell.BasicLSTMCell(RNN_HIDDEN, state_is_tuple=True) else: cell = tf.nn.rnn_cell.BasicRNNCell(RNN_HIDDEN) # Create initial state. Here it is just a constant tensor filled with zeros, # but in principle it could be a learnable parameter. This is a bit tricky # to do for LSTM's tuple state, but can be achieved by creating two vector # Variables, which are then tiled along batch dimension and grouped into tuple. batch_size = tf.shape(inputs)[1] initial_state = cell.zero_state(batch_size, tf.float32) # Given inputs (time, batch, input_size) outputs a tuple # - outputs: (time, batch, output_size) [do not mistake with OUTPUT_SIZE] # - states: (time, batch, hidden_size) rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, time_major=True) # project output from rnn output size to OUTPUT_SIZE. Sometimes it is worth adding # an extra layer here. final_projection = lambda x: layers.linear(x, num_outputs=OUTPUT_SIZE, activation_fn=tf.nn.sigmoid) # apply projection to every timestep. predicted_outputs = map_fn(final_projection, rnn_outputs) # compute elementwise cross entropy. error = -(outputs * tf.log(predicted_outputs + TINY) + (1.0 - outputs) * tf.log(1.0 - predicted_outputs + TINY)) error = tf.reduce_mean(error) # optimize train_fn = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(error) # assuming that absolute difference between output and correct answer is 0.5 # or less we can round it to the correct output. accuracy = tf.reduce_mean(tf.cast(tf.abs(outputs - predicted_outputs) < 0.5, tf.float32)) ################################################################################ ## TRAINING LOOP ## ################################################################################ NUM_BITS = 10 ITERATIONS_PER_EPOCH = 100 BATCH_SIZE = 16 valid_x, valid_y = generate_batch(num_bits=NUM_BITS, batch_size=100) session = tf.Session() # For some reason it is our job to do this: session.run(tf.initialize_all_variables()) for epoch in range(1000): epoch_error = 0 for _ in range(ITERATIONS_PER_EPOCH): # here train_fn is what triggers backprop. error and accuracy on their # own do not trigger the backprop. x, y = generate_batch(num_bits=NUM_BITS, batch_size=BATCH_SIZE) epoch_error += session.run([error, train_fn], { inputs: x, outputs: y, })[0] epoch_error \/= ITERATIONS_PER_EPOCH valid_accuracy = session.run(accuracy, { inputs: valid_x, outputs: valid_y, }) print \"Epoch %d, train error: %.2f, valid accuracy: %.1f %%\" % (epoch, epoch_error, valid_accuracy * 100.0) ramseydsilva commented Oct 3, 2016 Neat, thanks! Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"null\tnull"}
{"author":"downtownslim","created":"Wed Oct 19 21:14:25 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.05755 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.CR cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data Authors: Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, Kunal Talwar (Submitted on 18 Oct 2016 (v1), last revised 7 Nov 2016 (this version, v3)) Abstract: Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as \"teachers\" for a \"student\" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy\/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. Comments: Submitted to ICLR 17 Subjects: Machine Learning (stat.ML); Cryptography and Security (cs.CR); Learning (cs.LG) Cite as: arXiv:1610.05755 [stat.ML]   (or arXiv:1610.05755v3 [stat.ML] for this version) Submission history From: Nicolas Papernot [view email] [v1] Tue, 18 Oct 2016 19:37:37 GMT (199kb,D) [v2] Wed, 2 Nov 2016 13:18:56 GMT (199kb,D) [v3] Mon, 7 Nov 2016 00:18:03 GMT (199kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"TheCriticalSkeptic","created":"Tue Nov 01 06:06:17 EDT 2016","text":"As I've been playing with char-nn in Tensorflow I've noticed that there is a local minima (of sorts) where simply outputting spaces gives the best loss minimisation. It takes very few batches to get to predicting only spaces and then it slowly learns which other characters may be useful before moving on to short words.\n\nWith a relatively small network, fewer layers, and\/or shorter sequence length it can start to output semi-coherent text pretty quickly. Making the leap from semi-coherent to coherent seems like it will take a larger network with more layers. But the larger the network the harder it seems to be to push past the local minima of outputting spaces.\n\nI'm not sure if it's the corpus I'm using or if anyone else has experienced this?\n\nDoes anyone have a good guide or can recommend a paper that suggests how to fine tune hyper parameters for char-nn? ","flair":"one\tDiscussion"}
{"author":"thesameoldstories","created":"Mon Oct 31 11:05:34 EDT 2016","text":"I was reading [this interview](http:\/\/mlconf.com\/tryolabs-interview\/) where it talks about applying Deep Learning techniques to product categorization in e-commerce.\n\nIn there Nick (the interviewer) asks Agustin if he had co-trained RNN &amp; CNN models to improve performance for his project. He answered he didn't due to computational cost \/ benefit relation.\n\nQuestion: does any of you have practical experience co-training RNN &amp; CNN and can tell in which context is a good idea to do it?\n\nThanks!!","flair":"one\tDiscussion"}
{"author":"ktbyte","created":"Tue Oct 04 20:25:14 EDT 2016","text":"I have a GTX 960 and Titan X Pascal. Would anyone like to see any keras benchmarks, and if so, what kind of models (e.g. link to a script for me to compare)?","flair":"null\tnull"}
{"author":"SarcasticMetaName","created":"Wed Oct 26 10:46:33 EDT 2016","text":"Yoshua Bengio, one of the leading figures behind the rise of deep learning, is launching a Silicon Valley-style startup incubator dedicated to this enormously influential form of artificial intelligence. The incubator, Element AI, will help build companies from AI research that emerges from the University of Montreal, where Bengio is a professor, and nearby McGill University, and he says this is just part of his efforts to develop an \u201CAI ecosystem\u201D in Montreal. Bengio says the Canadian city offers \u201Cthe biggest concentration in the world\u201D of academic researchers exploring deep learning, the breed of AI that now plays such an important role inside the likes of Google, Facebook, and Microsoft. \u201CElement AI will help entrepreneurs get started in that high-growth area, with a team of experts\u2014and my help\u2014to steer those companies in the right direction,\u201D he says. According to Bengio, about 100 researchers are exploring deep learning at the University of Montreal and about 50 others are doing similar work at McGill. Whether this amounts to an unparalleled concentration of researchers is debatable\u2014Europe is also a hotbed of research\u2014but the importance of deep learning is undeniable. Last week, Microsoft researchers unveiled a computing system that recognizes conversational speech as accurately as humans do. When transcribing audio from old telephone conversations, the system makes fewer errors than professional transcribers. This research carries caveats, but it underscores how much deep learning has accelerated AI in the last five years. Using deep neural networks\u2014networks of hardware and software that learn by analyzing vast amounts of data\u2014several tech giants have pushed speech recognition to human-like levels, including Google, IBM, and Baidu. Using similar technology, these and other companies have built systems that accurately recognize faces and objects in photos. And now, deep learning is rapidly reinventing everything from machine translation to the Google search engine. The deep learning movement grew out of a relatively small community of academics, and so many of these researchers have now funneled into the tech sector. One of the movement\u2019s founding fathers, Geoff Hinton, works for Google. Another pioneer, Yann LeCun, runs the AI lab at Facebook. And in recent years, the giants of Silicon Valley have acquired one deep learning startup after another. Most notably, Google now owns DeepMind, the London-based think tank that recently used deep learning and other technologies to crack the ancient game of Go. Hinton, LeCun, and Bengio are the three people most responsible for nurturing deep learning through 1980s, \u201990s, and early 2000s when few others saw its potential. But Bengio hasn\u2019t followed the other two into the private sector. Microsoft AI researcher XD Huang calls him \u201Ca great educator,\u201D and that\u2019s typically how the deep learning community describes him. That said, Bengio is an advisor to IBM, and with Element.AI, he\u2019s creating a pipeline through which AI research can flow into the commercial world. This is significant because deep learning talent remains relatively rare. Silicon Valley\u2019s biggest players are engaged in an arms race for such talent that extends well beyond the usual fight for engineers. The enormity of the arms race was highlighted last year when OpenAI, a startup funded by Elon Musk and others, lured several key AI researchers away from Google and Facebook. It\u2019s also important that Bengio is building an incubator\u2014something along the lines of Y Combinator\u2014because this can address the gap that often exists between expertise in AI and the know-how needed to build a company or product. \u201CA lot of technologists, particularly in deep learning, are very skilled at constructing powerful algorithms but don\u2019t always know how to use them to solve real world problems,\u201D says Chris Nicholson, the founder of a deep learning startup called Skymind. \u201CThese very skilled technologists need a bit a guidance.\u201D For researchers given the right guidance, the market for their skills is enormous. Deep learning is now technology that every big company needs. And there are only so many researchers to go around. Update: Though Yoshua Bengio originally referred to his incubator as Element.AI, the official name is now Element AI.","flair":"null\tnull"}
{"author":"huyhcmut","created":"Tue Oct 04 06:59:50 EDT 2016","text":"After some months reading about deep learning research paper, i feel that there are not many original works like in (bayesian learning, kernel learning). I mean there are many papers just *improve* something, or *combine* algorithms, appear a lot even if at Top-venues. What do your think about that? Sorry if i made something wrong. :D","flair":"null\tnull"}
{"author":"vincentvanhoucke","created":"Sat Nov 05 18:42:31 EDT 2016","text":"A TensorFlow implementation of the STREET model described in the paper: \"End-to-End Interpretation of the French Street Name Signs Dataset\" The STREET model is a deep recurrent neural network that learns how to identify the name of a street (in France) from an image containing upto four different views of the street name sign. The model merges information from the different views and normalizes the text to the correct format. For example: The French Street Name Signs (FSNS) dataset is split into subsets, each of which is composed of multiple files. Note that these datasets are very large. The approximate sizes are: Here is a list of the download paths: The above files need to be downloaded individually, as they are large and downloads are more likely to succeed with the individual files than with a single archive containing them all. The datasets download includes a directory that contains some small datasets that are big enough to test that models can actually learn something. Assuming that you have put the downloads in directory alongside then you can run the following tests: Depending on your machine, this should run in about 1 minute, and should obtain error rates below 50%. Actual error rates will vary according to random initialization. Depending on your machine, this should run in about 1-2 minutes, and should obtain a label error rate between 50 and 80%, with word error rates probably not coming below 100%. Actual error rates will vary according to random initialization. Depending on your machine, the background training should run for about 3-4 minutes, and should obtain a label error rate between 10 and 50%, with correspondingly higher word error rates and even higher sequence error rate. Actual error rates will vary according to random initialization. The background eval will run for ever, and will have to be terminated by hand. The tensorboard command will run a visualizer that can be viewed with a browser. Go to the link that it prints to view tensorboard and see the training progress. See the Tensorboard introduction for more information. You can test the actual STREET model on a small FSNS data set. The model will overfit to this small dataset, but will give some confidence that everything is working correctly. Note that this test runs the training and evaluation in parallel, which is something that you should do when training any substantial system, so you can monitor progress. Depending on your machine, the training should finish in about 1-2 hours. As with the CTC testset above, the eval and tensorboard will have to be terminated manually. After running the tests above, you are ready to train the real thing! Note that you might want to use a train_dir somewhere other than \/tmp as you can stop the training, reboot if needed and continue if you keep the data intact, but \/tmp gets deleted on a reboot. Training will take a very long time (probably many weeks) to reach minimum error rate on a single machine, although it will probably take substatially fewer iterations than with parallel training. Faster training can be obtained with parallel training on a cluster. Since the setup is likely to be very site-specific, please see the TensorFlow documentation on Distributed TensorFlow for more information. Some code changes may be needed in the function in . With 40 parallel training workers, nearly optimal error rates (about 25% sequence error on the validation set) are obtained in about 30 million steps, although the error continues to fall slightly over the next 30 million, to perhaps as low as 23%. With a single machine the number of steps could be substantially lower. Although untested on this problem, on other problems the ratio is typically 5 to 1 so low error rates could be obtained as soon as 6 million iterations, which could be reached in about 4 weeks. The STREET model makes use of a graph specification language (VGSL) that enables rapid experimentation with different model architectures. The language defines a Tensor Flow graph that can be used to process images of variable sizes to output a 1-dimensional sequence, like a transcription\/OCR problem, or a 0-dimensional label, as for image identification problems. For more information see vgslspecs","flair":"two\tNews"}
{"author":"airalcorn2","created":"Mon Nov 14 10:57:14 EST 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 29 Fork 7 airalcorn2\/Recurrent-Convolutional-Neural-Network-Text-Classifier Code Issues 0 Pull requests 0 Projects 0 Pulse Graphs My (modified) Keras implementation of the Recurrent Convolutional Neural Network (RCNN) described here: http:\/\/www.aaai.org\/ocs\/index.php\/AAAI\/AAAI15\/paper\/view\/9745. 8 commits 1 branch 0 releases Fetching contributors MIT Python 100.0% Python Clone or download Clone with HTTPS Use Git or checkout with SVN using the web URL. Download ZIP Find file Branch: master Switch branches\/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit db1ae08 Nov 14, 2016 airalcorn2 committed on GitHub Remove unnecessary modules. Permalink Failed to load latest commit information. LICENSE Initial commit Nov 14, 2016 README.md Create README.md Nov 14, 2016 recurrent_convolutional_keras.py Remove unnecessary modules. Nov 14, 2016 README.md Recurrent-Convolutional-Neural-Network-Text-Classifier My (modified) Keras implementation of the Recurrent Convolutional Neural Network (RCNN) described here: http:\/\/www.aaai.org\/ocs\/index.php\/AAAI\/AAAI15\/paper\/view\/9745. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"Mandrathax","created":"Mon Oct 10 09:58:44 EDT 2016","text":"This is a place to share machine learning research papers, journals, and articles that you're reading this week.\nIf it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  \n\n|Previous weeks|\n|--------------|\n|[Week 1](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4qyjiq\/machine_learning_wayr_what_are_you_reading_week_1\/)|  \n|[Week 2](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4s2xqm\/machine_learning_wayr_what_are_you_reading_week_2\/)|  \n|[Week 3](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4t7mqm\/machine_learning_wayr_what_are_you_reading_week_3\/)|  \n|[Week 4](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4ub2kw\/machine_learning_wayr_what_are_you_reading_week_4\/)| \n|[Week 5](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4xomf7\/machine_learning_wayr_what_are_you_reading_week_5\/)| \n|[Week 6](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4zcyvk\/machine_learning_wayr_what_are_you_reading_week_6\/)|\n|[Week 7](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/52t6mo\/machine_learning_wayr_what_are_you_reading_week_7\/)|\n|[Week 8](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/53heol\/machine_learning_wayr_what_are_you_reading_week_8\/)|\n|[Week 9](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/54kvsu\/machine_learning_wayr_what_are_you_reading_week_9\/)|\n\nMost upvoted papers last week : \n\n[Recursive Deep Learning for Natural Language Processing and Computer Vision](http:\/\/nlp.stanford.edu\/~socherr\/thesis.pdf)\n\n[The Controlled Thermodynamic Integral for Bayesian Model Comparison](https:\/\/arxiv.org\/abs\/1404.5053)\n\nBesides that, there are no rules, have fun.","flair":"one\tDiscussion"}
{"author":"MetricSpade007","created":"Thu Nov 03 15:22:42 EDT 2016","text":"Do people have a favorite use case or paper that uses sequence to sequence models and it works well? \n\nMachine translation is the most obvious case, but other applications would be cool to see!","flair":"one\tDiscussion"}
{"author":"quickhook","created":"Tue Nov 22 16:47:36 EST 2016","text":" Amazon Try Prime Books All Departments Alexa Skills Amazon Video Amazon Warehouse Deals Appliances Apps & Games Arts, Crafts & Sewing Automotive Parts & Accessories Baby Beauty & Personal Care Books CDs & Vinyl Cell Phones & Accessories Clothing, Shoes & Jewelry    Women    Men    Girls    Boys    Baby Collectibles & Fine Art Computers Courses Credit and Payment Cards Digital Music Electronics Gift Cards Grocery & Gourmet Food Handmade Health, Household & Baby Care Home & Business Services Home & Kitchen Industrial & Scientific Kindle Store Luggage & Travel Gear Luxury Beauty Magazine Subscriptions Movies & TV Musical Instruments Office Products Patio, Lawn & Garden Pet Supplies Prime Pantry Software Sports & Outdoors Tools & Home Improvement Toys & Games Vehicles Video Games Wine Go Departments Hello. Sign inYour AccountSign inYour AccountTry PrimeListsCart Your Amazon.comCyber MondayGift Cards & RegistrySellHelp Books Advanced Search New Releases Best Sellers The New York Times® Best Sellers Children's Books Textbooks Textbook Rentals Sell Us Your Books Best Books of the Month Kindle eBooks Books \u203A Computers & Technology \u203A Computer Science Enter your mobile number or email address below and we'll send you a link to download the free Kindle App. Then you can start reading Kindle books on your smartphone, tablet, or computer - no Kindle device required. Apple Android Windows Phone Android To get the free app, enter your mobile phone number. or Download to your computer Mac Windows 8, 8 RT and Modern UI Windows 8 desktop, Windows 7, XP & Vista Kindle Cloud Reader Read instantly in your browser Share Facebook Twitter Pinterest Buy New $67.86 Qty:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Qty:1 List Price: $80.00 Save: $12.14 (15%) FREE Shipping. In stock on December 1, 2016. Order it now. Ships from and sold by Amazon.com. Gift-wrap available. Add to Cart Turn on 1-Click ordering for this browser Have one to sell? Sell on Amazon Flip to back Flip to front Listen Playing... Paused   You're listening to a sample of the Audible audio edition. Learn more See this image Deep Learning (Adaptive Computation and Machine Learning series) Hardcover \u2013 November 18, 2016 by Ian Goodfellow (Author) \u203A Visit Amazon's Ian Goodfellow Page Find all the books, read about the author, and more. See search results for this author Are you an author? Learn about Author Central Ian Goodfellow (Author), Yoshua Bengio (Author) \u203A Visit Amazon's Yoshua Bengio Page Find all the books, read about the author, and more. See search results for this author Are you an author? Learn about Author Central Yoshua Bengio (Author), Aaron Courville (Author) \u203A Visit Amazon's Aaron Courville Page Find all the books, read about the author, and more. See search results for this author Are you an author? Learn about Author Central Aaron Courville (Author) & 0 more 4.8 out of 5 stars 6 customer reviews #1 Best Sellerin Artificial Intelligence & Semantics See all formats and editions Hide other formats and editions Price New from Used from Hardcover \"Please retry\" $67.86 $67.86 $2,121.00 Hardcover $67.86 1 Used from $2,121.00 1 New from $67.86 \"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors. Read more Read less Add to Cart Deep Learning (Adaptive Computation and Machine Learning series) $67.86 FREE Shipping. In stock on December 1, 2016. Order it now. Ships from and sold by Amazon.com. Gift-wrap available. click to open popover Frequently Bought Together + + Total price: $147.94 Add all three to Cart Add all three to List One of these items ships sooner than the other. Show details Buy the selected items together This item:Deep Learning (Adaptive Computation and Machine Learning series) by Ian Goodfellow Hardcover $67.86 In stock on December 1, 2016. Ships from and sold by Amazon.com. FREE Shipping. Details Python Machine Learning by Sebastian Raschka Paperback $40.49 In Stock. Ships from and sold by Amazon.com. FREE Shipping. Details Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas C. Müller Paperback $39.59 In Stock. Ships from and sold by Amazon.com. FREE Shipping. Details Customers Who Bought This Item Also Bought Page 1 of 1 Start overPage 1 of 1 This shopping feature will continue to load items. In order to navigate out of this carousel please use your heading shortcut key to navigate to the next or previous heading. Back Computer Age Statistical Inference: Algorithms, Evidence, and Data Science (Institute of Mathematical Statistics Monographs) Bradley Efron 4.5 out of 5 stars 4 Hardcover $65.54 Prime Python Machine Learning Sebastian Raschka 4.3 out of 5 stars 80 #1 Best Seller in Data Modeling & Design Paperback $40.49 Prime Neural Network Design (2nd Edition) Martin T Hagan 4.7 out of 5 stars 26 Paperback $28.50 Prime Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series) Kevin P. Murphy 4.0 out of 5 stars 64 Hardcover $97.61 Prime Introduction to Machine Learning with Python: A Guide for Data Scientists Andreas C. Müller 3.5 out of 5 stars 6 Paperback $39.59 Prime Machine Learning: The New AI (The MIT Press Essential Knowledge series) Ethem Alpaydin 4.0 out of 5 stars 3 Paperback $10.63 Prime Next Special Offers and Product Promotions Get $10 off book purchases over $25. Use promo code HOLIDAYBOOK at checkout. Restrictions apply. Learn more | Shop now Editorial Reviews Review Written by three experts in the field, Deep Learning is the only comprehensive book on the subject. It provides much-needed broad perspective and mathematical preliminaries for software engineers and students entering the field, and serves as a reference for authorities. (Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX) This is the definitive textbook on deep learning. Written by major contributors to the field, it is clear, comprehensive, and authoritative. If you want to know where deep learning came from, what it is good for, and where it is going, read this book. (Geoffrey Hinton FRS, Emeritus Professor, University of Toronto; Distinguished Research Scientist, Google) Deep learning has taken the world of technology by storm since the beginning of the decade. There was a need for a textbook for students, practitioners, and instructors that includes basic concepts, practical aspects, and advanced research topics. This is the first comprehensive textbook on the subject, written by some of the most innovative and prolific researchers in the field. This will be a reference for years to come. (Yann LeCun, Director of AI Research, Facebook; Silver Professor of Computer Science, Data Science, and Neuroscience, New York University) Read more About the Author Ian Goodfellow is Research Scientist at OpenAI. Yoshua Bengio is Professor of Computer Science at the Université de Montréal. Aaron Courville is Assistant Professor of Computer Science at the Université de Montréal. Read more NO_CONTENT_IN_FEATURE Tell the Publisher! I'd like to read this book on Kindle Don't have a Kindle? Get your Kindle here, or download a FREE Kindle Reading App. New York Times best sellers Browse the New York Times best sellers in popular categories like Fiction, Nonfiction, Picture Books and more. See more Product Details Series: Adaptive Computation and Machine Learning series Hardcover: 800 pages Publisher: The MIT Press (November 18, 2016) Language: English ISBN-10: 0262035618 ISBN-13: 978-0262035613 Product Dimensions: 7 x 1 x 9 inches Shipping Weight: 2.9 pounds (View shipping rates and policies) Average Customer Review: 4.8 out of 5 stars  See all reviews (6 customer reviews) Amazon Best Sellers Rank: #697 in Books (See Top 100 in Books) #1 in Books > Computers & Technology > Computer Science > AI & Machine Learning > Intelligence & Semantics #4 in Books > Textbooks > Computer Science #9 in Books > Education & Teaching > Schools & Teaching  Would you like to update product info or give feedback on images? Customer Reviews 4.8 out of 5 stars 6 4.8 out of 5 stars 5 star 83% 4 star 17% 3 star 0% 2 star 0% 1 star 0% See all 6 customer reviews Share your thoughts with other customers Write a customer review Top Customer Reviews 5.0 out of 5 starsA constant theme here is that 'this works better than that' for practical reasons not for underlying theoretical ... By S. Matthews on November 18, 2016 Format: Hardcover This is, to invoke a technical reviewer cliché, a 'valuable' book. Read it and you will have a detailed and sophisticated practical understanding of the state of the art in neural networks technology. Interestingly, I also suspect it will remain current for a long time, because reading it I came to more and more of an impression that neural network technology (at least in the current iteration) is plateauing. Why? Because this book also makes very clear - is completely honest - that neural networks are a 'folk' technology (though they do not use those words): Neural networks work (in fact they work unbelievably well - at least, as Geoffrey Hinton himself has remarked, given unbelievably powerful computers), but the underlying theory is very limited and there is no reason to think that it will become less limited, and the lack of a theory means that there is no convincing 'gradient', to use an appropriate metaphor, for future development. A constant theme here is that 'this works better than that' for practical reasons not for underlying theoretical reasons. Neural networks are engineering, they are not applied mathematics, and this is very much, and very effectively, an engineer's book. 4 Comments 45 people found this helpful. Was this review helpful to you? Yes No Sending feedback... Thank you for your feedback. Sorry, we failed to record your vote. Please try again Report abuse 5.0 out of 5 starsBroad, Mathematical Survey By MsCurious on November 24, 2016 Format: Hardcover This book is a valuable addition to my machine learning library. It's mostly easy to read given enough statistical maturity, say at the level of All of Statistics: A Concise Course in Statistical Inference (Springer Texts in Statistics). Definitely easier than The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics) I like the fact that they use clear logical arguments and reference papers to justify certain choices like the RELU unit. I remember reading the classical approximation theorems by Hornik and getting an impression that the unit has to be bounded for universal approximation, which RELU is certainly not, but apparently it has good approximation properties too. I am also glad that they discuss issues of regularization and not just blindly assume that deeper is better as some literature and hype seems to suggest. About 1\/3 of the book is background in relevant machine learning basics and optimization, mostly part I. Convex optimization is very much an active research field now, and they describe some new interesting algorithms. The rest of the book surveys various deep learning architectures. Based on the organization of the material, I would guess they consider convolutional and recurrent networks to be the main architectures that should be used in certain specific application. I'm surprised that they consider Boltzmann machines to be research rather then current practice, but they are the experts.Read more \u203A 1 Comment 7 people found this helpful. Was this review helpful to you? Yes No Sending feedback... Thank you for your feedback. Sorry, we failed to record your vote. Please try again Report abuse 5.0 out of 5 starsAwesome Deep Learning Textbook By Justin Chen on November 25, 2016 Format: Hardcover Verified Purchase I was pretty excited when I heard earlier this year that Yoshua Bengio, Ian Goodfellow, and Aaron Courville were writing this textbook. Just got my copy in the mail today. Read a few chapters and skimmed through some of the book. This is a very comprehensive, well-written, and easy-to-understand textbook on the theoretical foundations, current research, and applications of deep learning. I've read a lot of research papers (DeepMind, Google Brain, Facebook, NYU, Stanford, etc.), blogs (Nervana Systems, Indico, Colah, Otoro's Blog, etc.), lecture notes (Stanford cs231n, cs224d, cs229), and tutorials (Quoc Le's tutorial, TensorFlow, etc.), and have watched a lot of videos (Hugo Larochelle's tutorials, Stanford cs229, TedTalks, lectures by Yann LeCun <3, etc.) to teach myself this topic. Despite the abundance of great resources floating around on the internet, there hasn't been any single thoroughly compiled resource like this. I'm wicked excited to finally own a copy. Many thanks. Shouts from Boston University! :D Comment 4 people found this helpful. Was this review helpful to you? Yes No Sending feedback... Thank you for your feedback. Sorry, we failed to record your vote. Please try again Report abuse See all 6 customer reviews (newest first) Write a customer review Most Recent Customer Reviews 5.0 out of 5 starsFive Stars Nice blend of foundation and cutting edge. Published 2 days ago by Amazon Customer 5.0 out of 5 starsFive Stars good quality. Published 4 days ago by cxxchen 4.0 out of 5 starsFour Stars The book looks good, but, when will the book be available for kindle? Published 9 days ago by Flávio Search Customer Reviews Search What Other Items Do Customers Buy After Viewing This Item? Python Machine Learning Paperback Sebastian Raschka 4.3 out of 5 stars 80 $40.49 Prime Computer Age Statistical Inference: Algorithms, Evidence, and Data Science (Institute of Mathematical Statistics Monographs) Hardcover Bradley Efron 4.5 out of 5 stars 4 $65.54 Prime Real-World Machine Learning Paperback Henrik Brink 4.7 out of 5 stars 3 $40.38 Prime The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics) Hardcover Trevor Hastie 4.1 out of 5 stars 79 $75.54 Prime There's a problem loading this menu right now. Learn more about Amazon Prime. Get fast, free shipping with Amazon Prime Prime members enjoy FREE Two-Day Shipping and exclusive access to music, movies, TV shows, original audio series, and Kindle books. > Get started Back to top Get to Know Us Careers About Amazon Investor Relations Amazon Devices Make Money with Us Sell on Amazon Sell Your Services on Amazon Sell on Amazon Business Sell Your Apps on Amazon Become an Affiliate Advertise Your Products Self-Publish with Us Become an Amazon Vendor \u203ASee all Amazon Payment Products Amazon.com Rewards Visa Card Amazon.com Store Card Amazon.com Corporate Credit Line Shop with Points Credit Card Marketplace Reload Your Balance Amazon Currency Converter Let Us Help You Your Account Your Orders Shipping Rates & Policies Amazon Prime Returns & Replacements Manage Your Content and Devices Amazon Assistant Help Australia Brazil Canada China France Germany India Italy Japan Mexico Netherlands Spain United Kingdom 6pm Score deals on fashion brands AbeBooks Rare Books & Textbooks ACX Audiobook Publishing Made Easy Alexa Actionable Analytics for the Web Amazon Business Everything For Your Business AmazonFresh Groceries & More Right To Your Door AmazonGlobal Ship Orders Internationally   Home Services Handpicked Pros Happiness Guarantee Amazon Inspire Free Digital Educational Resources Amazon Rapids Fun stories for kids on the go Amazon Video Direct Video Distribution Made Easy Amazon Web Services Scalable Cloud Computing Services Audible Download Audio Books BeautyBar.com Prestige Beauty Delivered   Book Depository Books With Free Delivery Worldwide Casa.com Kitchen, Storage & Everything Home ComiXology Thousands of Digital Comics CreateSpace Indie Print Publishing Made Easy Diapers.com Everything But The Baby DPReview Digital Photography East Dane Designer Men's Fashion   Fabric Sewing, Quilting & Knitting Goodreads Book reviews & recommendations IMDb Movies, TV & Celebrities Junglee.com Shop Online in India Kindle Direct Publishing Indie Digital Publishing Made Easy Prime Now FREE 2-Hour Delivery on Everyday Items Prime Photos Unlimited Photo Storage Free With Prime   Shopbop Designer Fashion Brands Soap.com Health, Beauty & Home Essentials TenMarks.com Math Activities for Kids & Schools Wag.com Everything For Your Pet Warehouse Deals Open-Box Discounts Whispercast Discover & Distribute Digital Content Woot! Deals and Shenanigans       Yoyo.com A Happy Place To Shop For Toys Zappos Shoes & Clothing       Conditions of Use Privacy Notice Interest-Based Ads © 1996-2016, Amazon.com, Inc. or its affiliates ","flair":"two\tNews"}
{"author":"The_Man_of_Science","created":"Wed Nov 16 22:54:17 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.05397 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Reinforcement Learning with Unsupervised Auxiliary Tasks Authors: Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu (Submitted on 16 Nov 2016) Abstract: Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth. Subjects: Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1611.05397 [cs.LG]   (or arXiv:1611.05397v1 [cs.LG] for this version) Submission history From: Max Jaderberg [view email] [v1] Wed, 16 Nov 2016 18:21:29 GMT (8149kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"DarkTriadBAMN","created":"Sun Oct 23 17:57:42 EDT 2016","text":"Hey there\n\nI'm trying to decide my schedule for the next semester and my main goal is to develop a toolset for self studying machine learning techniques over the summer. \n\n\nI've already checked that these 6 classes fit into one, non-overlapping schedule and that I have all pre-reqs. I would prefer to take 4 of the 6 classes but 5 might be possible (without being too overwhelmed).\n\nhere are my options:\n\n----------------------------\n\nApplied Differential Equations II\n-----\nDescription of class:\n\nWave, heat and Laplace equations. Solutions by separation of variables and expansion in Fourier Series or other appropriate orthogonal sets. Sturm-Liouville problems. Introduction to methods for solving some classical partial differential equations.Use of power series as a tool in solving ordinary differential equations.\n\n------------------\n\n\nMathematics of Scientific Computing\n-----\nDescription of class:\n\nThis course will provide an overview of methods to solve quantitative problems and analyze data. The tools to be introduced are mathematical in nature and have links to Algebra, Analysis, Geometry, Graph Theory, Probability and Topology. Students will acquire an appreciation of (I) the fundamental role played by mathematics in countless applications and (II) the exciting challenges in mathematical research that lie ahead in the analysis of large data and uncertainties. Students will work on a project for each unit. While this is not a programming class, the students will do some programming through their projects. \n\n------------------------\n\nIntroduction to Modern Algebra for Mathematics Majors\t\n-----\nDescription of class:\n\nElementary number theory, equivalence relations, groups, homomorphisms, cosets, Cayley's Theorem, symmetric groups, rings, polynomial rings, quotient fields, principal ideal domains, Euclidean domains. \n\n------------\n\nIntroduction to Combinatorics\n-----\nDescription of class:\n\nBasic principles of counting: addition and multiplication principles, generating functions, recursive methods, inclusion-exclusion, pigeonhole principle; basic concepts of graph theory: graphs, digraphs, connectedness, trees; additional topics from:Polya theory of counting, Ramsey theory; combinatorial optimization - matching and covering, minimum spanning trees, minimum distance, maximum flow; sieves; mobius inversion; partitions; Gaussian numbers and q-analogues; bijections and involutions; partially ordered sets. \n\n------------------------\n\nMathematical Analysis I\t\n-----\nDescription of class:\n\nReal number system, functions and limits, topology on the real line, continuity, differential and integral calculus for functions of one variable. Infinite series, uniform convergence. \n\n-----------------\n\n\nIntroduction to Mathematical Statistics II\t\n-----\nDescription of class:\n\nSecond of a two-semester sequence of mathematical statistics, primarily for undergraduate majors and graduate minors in Statistics. Random samples, point and interval estimators and their properties, methods of moments, maximum likelihood, tests ofhypotheses, elements of nonparametric statistics and elements of general linear model theory. \n\n------------------------\n\n\nThanks for any feedback or advice :)!","flair":"one\tDiscussion"}
{"author":"SJTUzhangzhe","created":"Fri Oct 14 23:44:11 EDT 2016","text":" Yuandong's Blog Thursday, October 13, 2016 Notes on DeepMind's 3rd Nature paper Recent DeepMind published their 3rd paper in Nature \"Hybrid computing using a neural network with dynamic external memory\". They devise a recurrent network structure (deep LSTM) that iteratively sends new reading\/writing commands to the external memory, as well as the action output, based on previous reading from the memory and current input. They called it DNC (Differential Neural Computer). The hope here is that the network could perform reasoning based on the given information. They experiment their model on bAbI reasoning task, network traversal\/shortest path prediction, deduction of relationship in the family tree and playing block puzzle games, showing its performance is much better than LSTM without external memory. Here are some comments: 1. Overall, it seems that they implicitly learn a heuristic function for search-based reasoning. As they mentioned in the paper, \"Visualization of a DNC trained on shortest-path suggests that it progressively explored the links radiating out from the start and end nodes until a connecting path was found (Supplementary Video 1).\". We could also see this behavior in London Underground task (Fig. 3). This could be efficient for experiments with small search space, but not necessarily a good strategy for real problems. 2. There seems to be lots of manual tunable knobs in the network. The network is to give the next set of operations on the external memory. There are many types of operations on the external memory, with different kind of attention mechanism (content-based attention, consequent writing attention, and \"usage\" mechanism built in reading and writing). Not sure which components are more important. Ideally, there should be a more automatic or principled approach. 3. Interesting details: (1) Training a sequential structural prediction model directly with the ground truth answers is not necessary good,  since when the prediction deviates from the ground truth, the model might fail easily. In this paper, they use DAgger [1] in structure prediction that blends the ground truth distribution with current predicted distribution. This makes the prediction robust. (2) For block puzzle games, they use actor-critic-like model. In this scenario, DNC outputs policy and value functions, conditioned on the configuration of the game, taken as inputs at the beginning. This coincides with our experience in Doom AI, that the actor-critic model converges faster than Q-learning. (3) Curriculum training (i.e., training the model from easy tasks first) plays an important role. This agrees with our experience when training our Doom AI (We will release the paper soon). References. [1] Ross et al, A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning, AISTATS 2011 Posted by yuandong at 12:13 PM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest No comments: Post a Comment Older Post Home Subscribe to: Post Comments (Atom) About Me yuandong A Research Scientist in Facebook AI Research working on Deep Learning Chinese blog: https:\/\/zhuanlan.zhihu.com\/yuandong View my complete profile Blog Archive ▼  2016 (3) ▼  October (3) Notes on DeepMind's 3rd Nature paper Doom AI competition Some Notes on Quantum Field Theory Simple template. Powered by Blogger. ","flair":"one\tDiscussion"}
{"author":"kendrick__","created":"Fri Oct 07 10:12:57 EDT 2016","text":"Blog Post detailing how the hardware and software communicate - Communicating between RC Car and the On-board Computer - Jabelone Neural Network architecture was based on NVIDIA's Self-driving car paper - End-To-End Learning for Self-Driving Cars","flair":"null\tnull"}
{"author":"sybilckw","created":"Wed Nov 16 19:18:19 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.04581 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: How to scale distributed deep learning? Authors: Peter H. Jin, Qiaochu Yuan, Forrest Iandola, Kurt Keutzer (Submitted on 14 Nov 2016) Abstract: Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes). Comments: Extended version of paper accepted at ML Sys 2016 (at NIPS 2016) Subjects: Learning (cs.LG) Cite as: arXiv:1611.04581 [cs.LG]   (or arXiv:1611.04581v1 [cs.LG] for this version) Submission history From: Peter Jin [view email] [v1] Mon, 14 Nov 2016 20:59:54 GMT (670kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"flyforlight","created":"Sun Oct 09 22:40:31 EDT 2016","text":"This is the repository for Fully Convolutional Instance-aware Semantic Segmentation (FCIS), which won COCO segmentation challenge 2016 by a large margin. An arxiv tech report describing FCIS is available here. If you find FCIS useful for your research, please consider citing","flair":"three\tResearch"}
{"author":"lxpz","created":"Wed Nov 23 11:04:54 EST 2016","text":" STDP-based spiking deep neural networks for object recognition Saeed Reza Kheradpisheh1,2,∗, Mohammad Ganjtabesh1, Simon J. Thorpe2, and Timothée Masquelier2 1 Department of Computer Science, School of Mathematics, Statistics, and Computer Science, University of Tehran, Tehran, Iran 2 CerCo UMR 5549, CNRS \u2013 Université Toulouse 3, France Abstract Previous studies have shown that spike-timing- dependent plasticity (STDP) can be used in spik- ing neural networks (SNN) to extract visual fea- tures of low or intermediate complexity in an un- supervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demon- strated \u2013 using rate-based neural networks trained with back-propagation \u2013 that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a tem- poral coding scheme where the most strongly acti- vated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progres- sively learned features corresponding to prototyp- ical patterns that were both salient and frequent. Only a few tens of examples per category were re- quired and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first ∗Corresponding author. Email addresses: kheradpisheh@ut.ac.ir (SRK), mgtabesh@ut.ac.ir (MG), simon.thorpe@cnrs.fr (ST) timothee.masquelier@cnrs.fr (TM). layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activ- ity of a single higher-order neuron. More gener- ally, the activity of a few hundreds of such neurons contained robust category information, as demon- strated using a classifier on Caltech 101, ETH-80, and MNIST databases. We think that the combi- nation of STDP with latency coding is key to un- derstanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions. 1 Introduction The primate\u2019s visual system solves the object recognition task through hierarchical processing along the ventral pathway of the visual cortex [1]. Through this hierarchy, the visual preference of neurons gradually increases from oriented bars in primary visual cortex (V1) to complex objects in inferotemporal cortex (IT), where neural activity provides a robust, invariant, and linearly-separable object representation [1, 2]. Despite the extensive feedback connections in the visual cortex, the first feed-forward wave of spikes in IT (∼ 100 − 150 ms post-stimulus presentation) appears to be sufficient 1 ar X iv :1 61 1. 01 42 1v 1 [ cs .C V ] 4 N ov 2 01 6 for crude object recognition [3, 4, 5]. During the last decades, various computational models have been proposed to mimic this hier- archical feed-forward processing [6, 7, 8, 9, 10]. Despite the limited successes of the early mod- els [11, 12], recent advances in deep convolutional neural networks (DCNNs) led to high perform- ing models [13, 14, 15]. Beyond the high preci- sion, DCNNs can tolerate object variations as hu- mans do [16, 17], use IT-like object representa- tions [18, 19], and match the spatio-temporal dy- namics of the ventral visual pathway [20]. Although the architecture of DCNNs is somehow inspired by the primate\u2019s visual system [21] (a hi- erarchy of computational layers with gradually in- creasing receptive fields), they totally neglect the actual neural processing and learning mechanisms in the cortex. The computing units of DCNNs send floating- point values to each other which correspond to their activation level, while, biological neurons commu- nicate to each other by sending electrical impulses (i.e., spikes). The amplitude and duration of all spikes are almost the same, so they are fully charac- terized by their emission time. Interestingly, mean spike rates are very low in the primate visual sys- tems (perhaps only a few of hertz [22]). Hence, neu- rons appear to fire a spike only when they have to send an important message, and some information can be encoded in their spike times. Such spike- time coding leads to a fast and extremely energy- efficient neural computation in the brain (the whole human brain consumes only about 10-20 Watts of energy [23]). The current top-performing DCNNs are trained with the supervised back-propagation algorithm which has no biological root. Although it works well in terms of accuracy, the convergence is rather slow because of the credit assignment problem [24]. Furthermore, given that DCNNs typically have mil- lions of free parameters, millions of labeled exam- ples are needed to avoid over-fitting. However, pri- mates, specially humans, can learn from far fewer examples while most of the time no label is avail- able. They may be able to do so thanks to Spike- timing-dependent plasticity (STDP), an unsuper- vised learning mechanism which occurs in mam- malian visual cortex [25, 26, 27]. According to STDP, synapses through which a presynaptic spike arrived before (respectively after) a postsynaptic one are reinforced (respectively depressed). To date, various spiking neural networks (SNNs) have been proposed to solve object recognition tasks. A group of these networks are actually the converted versions of traditional DCNNs [28, 29, 30]. The main idea is to replace each DCNN com- puting unit with a spiking neuron whose firing rate is correlated with the output of that unit. The aim of these networks is to reduce the energy consump- tion in DCNNs. However, the inevitable drawbacks of such spike-rate coding are the need for many spikes per image and the long processing time. Be- sides, the use of back-propagation learning algo- rithm and having both positive (excitatory) and negative (inhibitory) output synapses in a neuron are not biologically plausible. On the other hand, there are SNNs which are originally spiking net- works and learn spike patterns. First group of these networks exploit learning methods such as auto- encoder [31, 32] and back-propagation[33] which are not biologically plausible. The second group consists of SNNs with bioinspired learning rules which have shallow architectures [34, 35, 36, 37, 38] or only one trainable layer [9, 39, 40]. In this paper we proposed a STDP-based spik- ing deep neural network (SDNN) with a spike- time neural coding. The network is comprised of a temporal-coding layer followed by a cascade of consecutive convolutional (feature extractor) and pooling layers. The first layer converts the input image into an asynchronous spike train, where the visual information is encoded in the temporal or- der of the spikes. Neurons in convolutional layers integrate input spikes, and emit a spike right after reaching their threshold. These layers are equipped with STDP to learn visual features. Pooling layers provide translation invariance and also compact the visual information [8]. Through the network, visual features get larger and more complex, where neu- rons in the last convolutional layer learn and detect object prototypes. At the end, a classifier detects the category of the input image based on the activ- ity of neurons in the last pooling layer with global receptive fields. 2 We evaluated the proposed SDNN on Caltech face\/motorbike and ETH-80 datasets with large- scale images of various objects taken form differ- ent viewpoints. The proposed SDNN reached the accuracies of 99.1% on face\/motorbike task and 82.8% on ETH-80, which indicates its capability to recognize several natural objects even under se- vere variations. Based on our knowledge, there is no other spiking deep network which can recognize large-scale natural objects. We also examined the proposed SDNN on the MNIST dataset which is a benchmark for spiking neural networks, and in- terestingly, it reached 98.4% recognition accuracy. In addition to the high performance, the proposed SDNN is highly energy-efficient and works with a few number of spikes per image, which makes it suitable for neuromorphic hardware implementa- tion. 2 Proposed Spiking Deep Neural Network A sample architecture of the proposed SDNN with three convolutional and three pooling layers is shown in Figure 1. Note that the architectural properties (e.g., the number of layers and recep- tive field sizes) and learning parameters should be optimized for the desired recognition task. The first layer of the network uses Difference of Gaussians (DoG) filters to detect contrasts in the input image. It encodes the strength of these contrasts in the latencies of its output spikes (the higher the contrast, the shorter the latency). Neu- rons in convolutional layers detect more complex features by integrating input spikes from the pre- vious layer which detects simpler visual features. Convolutional neurons emit a spike as soon as they detect their preferred visual feature which depends on their input synaptic weights. Through the learn- ing, neurons that fire earlier perform the STDP and prevent the others from firing via a winner-take- all mechanism. In this way, more salient and fre- quent features tend to be learned by the network. Pooling layers provide translation invariance using maximum operation, and also help the network to compress the flow of visual data. Neurons in pool- ing layers propagate the first spike received from neighboring neurons in the previous layer which are selective to the same feature. Convolutional and pooling layers are arranged in a consecutive order. Receptive fields gradually increase through the net- work and neurons in higher layers become selective to complex objects or object parts. It should be noted that the internal potentials of all neurons are reset to zero before processing the next image. Also, learning only happens in convo- lutional layers and it is done layer by layer. Since the calculations of each neuron is independent of other adjacent neurons, to speed-up the computa- tions, each of the convolution, pooling, and STDP operations are performed in parallel on GPU. 2.1 DoG and temporal coding The important role of the first stage in SNNs is to encode the input signal into discrete spike events in the temporal domain. This temporal coding determines the content and the amount of infor- mation carried by each spike, which deeply affects the neural computations in the network. Hence, using efficient coding scheme in SNNs can lead to fast and accurate responses. Various temporal cod- ing schemes can be used in visual processing (see ref. [41]). Among them, rank-order coding is shown to be efficient for rapid processing (even possibly in retinal ganglion cells) [42, 43]. Cells in the first layer of the network apply a DoG filter over their receptive fields to detect positive or negative contrasts in the input image. DoG well approximates the center-surround properties of the ganglion cells of the retina. When presented with an image, these DoG cells detect the contrasts and emit a spike; the more strongly a cell is activated (higher contrast), the earlier it fires. In other word, the order of the spikes depends on the order of the contrasts. This rank-order coding is shown to be efficient for obtaining V1 like edge detectors [44] as well as complex visual features [9, 40] in higher cortical areas. DoG cells are retinotopically organized in two ON-center and OFF-center maps which are respec- tively sensitive to positive and negative contrasts. A DoG cell is allowed to fire if its activation is 3 Input Image DoG (Temporal Coding) Conv 1 Conv 2 Conv 3 Pool 1 Pool 2 Global pooling Classifier ? 𝑤2 𝐷 2 𝑤1 𝐷 𝑤1 𝑐1 𝑛1 𝑛1 𝑤2 𝑐1 𝑤1 𝑝1 𝑤2 𝑝1 𝑤1 𝑐2 𝑤2 𝑐2 𝑤1 𝑝2 𝑤2 𝑝2 𝑤1 𝑐3 𝑤2 𝑐3 Conv-window (real-value) Conv-window (spiking) Pooling window Spiking synapse Real-value synapse 𝑛2 𝑛2 𝑛3 𝑛3 Figure 1: A sample architecture of the proposed SDNN with three convolutional and three pooling layers. The first layer applies ON- and OFF-center DoG filters of size wD1 × wD2 on the input image and encode the image contrasts in the timing of the output spikes. The ith convolutional layer, Conv i, learns combinations of features extracted in the previous layer. The ith pooling layer, Pool i, provides translation invariance for features extracted in the previous layer and compress the visual information using a local maximum operation. Finally the classifier detects the object category based on the feature values computed by the global pooling layer. The window size of the ith convolutional and pooling layers are indicated by wci1,2 and w pi 1,2, respectively. The number of the neuronal maps of the ith convolutional and pooling layer are also indicated by ni below each layer. above a certain threshold. Note that this scheme grantees that at most one of the two cells (posi- tive or negative) can fire in each location. As men- tioned above, the firing time of a DoG cell is in- versely proportional to its activation value. For efficient GPU-based parallel computing, the input spikes are grouped into equal-size sequential pack- ets. At each time step, spikes of one packet are propagated simultaneously. In this way, a packet of spikes with near ranks (carrying similar visual information) are propagated in parallel, while, the next spike packet will be processed in the next time step. 2.2 Convolutional layers A convolutional layer contains several neuronal maps. Each neuron is selective to a visual feature determined by its input synaptic weights. Neurons in a specific map detect the same visual feature but at different locations. To this end, synaptic weights of neurons belonging to the same map should al- ways be the same (i.e., weight sharing). Within a map, neurons are retinotopically arranged. Each neuron receives input spikes from the neurons lo- cated in a determined window in all neuronal maps of the previous layer. Hence, a visual feature in a convolutional layer is a combination of several sim- pler feature extracted in the previous layer. Note that the input windows of two adjacent neurons are highly overlapped. Hence, the network can detect the appearance of the visual features in any loca- tion. Neurons in all convolutional layers are non- leaky integrate-and-fire neurons, which gather in- put spikes from presynaptic neurons and emit a spike when their internal potentials reach a prespecified threshold. Each presynaptic spike increases the neuron\u2019s potential by its synaptic weight. At each time step, the internal potential of the ith neuron is updated as follows: Vi(t) = Vi(t− 1) + ∑ j Wj,iSj(t− 1), where Vi(t) is the internal potential of the ith con- volutional neuron at time step t, Wj,i is the synaptic weight between the jth presynaptic neuron and the ith convolutional neuron, and Sj is the spike train of the jth presynaptic neuron (Sj(t− 1) = 1 if the neuron has fired at time t − 1, and Sj(t − 1) = 0 4 otherwise). If Vi exceeds its threshold, Vthr, then the neuron emits a spike and Vi is reset: Vi(t) = 0 and Si(t) = 1, if Vi(t) ≥ Vthr. Also, there is a lateral inhibition mechanism in all convolutional layers. When a neuron fires, in an specific location, it inhibits other neurons in that location belonging to other neuronal maps and does not allow them to fire in the following time steps. In addition, neurons are not allowed to fire more than once. These together provides an sparse but highly informative coding, because, there can be at most one spike at each location which indicates the existence of a particular visual feature in that location. 2.3 Local pooling layers Pooling layers help the network to gain invariance by doing a nonlinear max pooling operation over a set of neighboring neurons with the same pre- ferred feature. Some evidence suggests that such a max operation occurs in complex cells in visual cortex [8]. Thanks to the rank-order coding used in the proposed network, the maximum operation of pooling layers simply consists of propagating the first spike emitted by the afferents [45]. A neuron in a neuronal map of a pooling layer performs the maximum operation over a window in the corresponding neuronal map of the previous layer. Pooling neurons are integrate-and-fire neu- rons whose input synaptic weights and threshold are all set to one. Hence, the first input spike acti- vates them and leads to an output spike. Regarding to the rank-order coding, each pooling neuron is al- lowed to fire at most once. It should be noted that no learning occurs in pooling layers. Another important role of pooling layers is to compress the visual information. Regarding to the maximum operation performed in pooling lay- ers, adjacent neurons with overlapped inputs would carry redundant information (each spike is sent to many neighboring pooling neurons). Hence, in the proposed network, the overlap between the input windows of two adjacent pooling neurons (belong- ing to the same map) is set to be very small. It helps to compress the visual information by elim- inating the redundancies, and also, to reduce the size of subsequent layers. 2.4 STDP-based learning As mentioned above, learning occurs only in convo- lutional layers which should learn to detect visual features by combining simpler features extracted in the previous layer. The learning is done layer by layer, i.e., the learning in a convolutional layer starts when the learning in the previous convolu- tional layer is finalized. When a new image is pre- sented, neurons of the convolutional layer compete with each other and those which fire earlier trigger STDP and learn the input pattern. A simplified version of STDP [9] is used:{ ∆wij = a +wij(1 − wij), if tj − ti ≤ 0, ∆wij = a −wij(1 − wij), if tj − ti > 0, where i and j respectively refer to the index of post- and presynaptic neurons, ti and tj are the corre- sponding spike times, ∆wij is the synaptic weight modification, and a+ and a− are two parameters specifying the learning rate. Note that the exact time difference between two spikes does not affect the weight change, but only its sign is considered. Also, it is assumed that if a presynaptic neuron does not fire before the postsynaptic one, it will fire later. These simplifications are equivalent to assuming that the intensity-latency conversion of DoG cells compresses the whole spike wave in a rel- atively short time interval (say, 20−30 ms), so that all presynaptic spikes necessarily fall close to the postsynaptic spike time, and the time lags are neg- ligible. The multiplicative term wij(1−wij) ensures the weights remain in the range [0,1] and thus main- tains all synapses in an excitatory mode in adding to implementing soft-bound effect. During the learning of a convolutional layer, neu- rons in the same map, detecting the same feature in different locations, integrate input spikes and com- pete with each other to do the STDP. The first neuron which reaches the threshold and fires, if any, is the winner (global intra-map competition). The winner triggers the STDP and updates its synaptic 5 weights. As mentioned before, neurons in differ- ent locations of the same map have the same input synaptic weights (i.e., weight sharing) to be selec- tive to the same feature. Hence, the winner neuron prevents other neurons in its own map to do STDP and duplicates its updated synaptic weights into them. Also, there is a local inter-map competition for STDP. When a neuron is allowed to do the STDP, it prevents the neurons in other maps within a small neighborhood around its location from do- ing STDP. This competition is crucial to encourage neurons of different maps to learn different features. Because of the discretized time variable in the proposed model, it is probable that some competi- tor neurons fire at the same time step. One pos- sible scenario is to pick one randomly and allow it to do STDP. But a better alternative is to pick the one which has the highest potential indicating higher similarity between its learned feature and input pattern. Synaptic weights of convolutional neurons initi- ate with random values drown from a normal dis- tribution with the mean of 0.8 and STD of 0.05. As the learning of a specific layer progresses, its neurons gradually converge to different visual fea- tures which are frequent in the input images. Also, the complexity of the visual features gradually in- creases through the layers in such a way that neu- rons in the highest layer converge to object proto- types. 2.5 Global pooling and classifica- tion The global pooling layer is only used in the clas- sification phase. Neurons of the last layer perform a global max pooling over their corresponding neu- ronal maps in the last convolutional layer. Such a pooling operation provides a global translation in- variance for prototypical features extracted in the last convolutional layer. Hence, there is only one output value for each feature, which indicates the presence of that feature in the input image. The output of the global pooling layer over the training images is used to train a classifier, say an SVM. In the testing phase, the test object image is pro- cessed by the network and the output of the global pooling layer is fed to the classifier to determine its category. To compute the output of the global pooling layer, first, the threshold of neurons in the last con- volutional layer were set to be infinite, and then, their final potentials (after propagating the whole spike train generated by the input image) were measured. These final potentials can be seen as the number of early spikes in common between the current input and the stored prototypes in the last convolutional layer. Finally, the global pooling neu- rons compute the maximum potential at their cor- responding neuronal maps, as their output value. 3 Results 3.1 Caltech face\/motorbike dataset We evaluated our SDNN on the face and motor- bike categories of the Caltech 101 dataset available at http:\/\/www.vision.caltech.edu (see Figure 3 for sample images). The training set contains 200 ran- domly selected images per category, and remain- ing images constitute the test set. The test images are not seen during the learning phase but used af- terward to evaluate the performance on novel im- ages. This standard cross-validation procedure al- lows measuring the system\u2019s ability to generalize, as opposed to learning the specific training exam- ples. All images were converted to grayscale values and rescaled to be 160 pixels in height (preserving the aspect ratio). Here, we used a network similar to Figure 1, with three convolutional layers each of which followed by a pooling layer. For the first layer, only ON-center DoG filters of size 7 × 7 and standard deviations of 1 and 2 pixels are used. The first, second and third convolutional layers consists of 4, 20, and 10 neuronal maps with conv-window sizes of 5 × 5, 16×16×4, and 5×5×20 and firing thresholds of 10, 60, and 2, respectively. The pooling window sizes of the first and second pooling layers are 7×7 and 2×2 with the strides of 6 and 2, correspondingly. The third pooling layer performs a global max pooling operation. The learning rates of all convolutional 6 Figure 2: The synaptic changes of some neuronal maps in different layers through the learning with the Caltech face\/motorbike dataset. A) The first convolutional layer becomes selective to oriented edges. B) The second convolutional layer converges to object parts. C) The third convolutional layer learns the object prototype and respond to whole objects. layers are set to a+ = 0.004 and a− = 0.003. In addition, each image is processed for 30 time steps. Figure 2 shows the preferred visual features of some neuronal maps in the first, second and third convolutional layers through the learning process. To visualize the visual feature learned by a neuron, a backward reconstruction technique is used. In- deed, the visual features in the current layer can be reconstructed as the weighted combinations of the visual features in the previous layer. This backward process continues until the first layer, whose pre- ferred visual features are computed by DoG func- tions. As shown in Figure 2A, interestingly, each of the four neuronal maps of the first convolutional layer converges to one of the four orientations: π\/4, π\/2, 3π\/4, and π. This shows how efficiently the as- sociation of the proposed temporal coding in DoG cells and unsupervised learning method (the STDP and learning competition) led to highly diverse edge detectors which can represent the input image with 7 Figure 3: The spiking activity of the convolutional layers with the face and motorbike images. The preferred features of neuronal maps in each convolutional layer are shown on the right. Each feature is coded by a specific color border. The spiking activity of the convolutional layers, accumulated over all the time steps, is shown in the corresponding panels. Each point in a panel indicates that a neuron in that location has fired at a time step, and the color of the point indicates the preferred feature of the activated neuron. edges in different orientations. These edge detec- tors are similar to the simple cells in primary visual cortex (i.e., V1 area) [44] Figure 2B shows the learning progress for the neuronal maps of the second convolutional layer. As mentioned, the first convolutional layer detects edges with different orientations all over the im- age, and due to the used temporal coding, neu- rons corresponding to edges with higher contrasts (i.e., salient edges) will fire earlier. On the other hand, STDP naturally tends to learn those combi- nation of edges that are consistently repeating in the training images (i.e., common features between the target objects). Besides, the learning competi- tion tends to prevent neuronal the maps from learn- ing similar visual features. Consequently, neurons in the second convolutional layer learn the most salient, common, and diverse visual features of the target objects, and do not learn the backgrounds that drastically change between images. As seen in Figure 2B, each of the maps gradually learns a different visual feature (combination of oriented 8 edges) representing a face or motorbike feature. The learning progress for two neuronal maps of the third convolutional layer are shown in Fig- ure 2C. As seen, one of them gradually becomes selective to a complete motorbike prototype as a combination of motorbike features such as back wheel, middle body, handle, and front wheel de- tected in the second layer. Also, the other map learns a whole face prototype as a combination of facial features. Indeed, the third convolutional layer learns the whole object prototypes using in- termediate complexity features detected in the pre- vious layer. Neurons in the second layer compete with each other and send spikes toward the third layer as they detect their preferred visual features. Since, different combinations of these features are detected for each object category, neuronal maps of the third layer will learn different prototypes of different categories. Therefore, the STDP and the learning competition mechanism direct neu- ronal maps of the third convolutional layer to learn highly category specific prototypes. Figure 3 shows the accumulated spiking activity of the DoG and the following three convolutional layers over all time steps, for two face and motor- bike sample images. For each layer, the preferred features of some neuronal maps with color coded borders are demonstrated on top, and their corre- sponding spiking activity are shown in panels below them. Each colored point inside a panel indicates the neuronal map of the neuron which has fired in that location at a time step. As seen, neurons of the DoG layer detect image contrasts, and edge de- tectors in the first convolutional layer detect the orientation of edges. Neurons in the second convo- lutional layer, which are selective to intermediate complexity features, detect their preferred visual feature by combining input spikes from edge detec- tor cells in the first layer. Finally, the coincidence of these features activates neurons in the third con- volutional layer which are selective to object pro- totypes. As seen, when a face (motorbike) image is presented, neurons in the face (motorbike) maps fire. To better illustrate the learning progress of all the layers as well as their spiking activity in the temporal domain, we prepared a short video (see Video S1 in Supplementary Information). Figure 4: Recognition accuracies (mean ± std) of the proposed SDNN for different number of training images per category. As mentioned in the previous section, the output of the global pooling layer is used by a SVM classi- fier to specify the object category of the input im- ages. We trained the proposed SDNN on training images and evaluated it over the test images, where the model reached the categorization accuracy of 99.1 ± 0.2%. It shows how the object prototypes, learned in the highest layer, can well represent the object categories. Furthermore, we also calculated the single neuron accuracy. In more details, we separately computed the recognition accuracy of each neuron in the global pooling layer. Surpris- ingly, some single neurons reached an accuracy of 93%, and the mean accuracy was 89.8%. Hence, it can be said that single neurons in the highest layer are highly class specific, and different neurons carry complementary information which altogether pro- vide robust object representations. In a further experiment, we changed the number of training samples (from 5 to 200 images per cat- egory) and calculated the recognition accuracy of the proposed SDNN. As seen in Figure 4, with 5 im- ages per category it reached the accuracy of 78.2%, and only 40 images from each category are sufficient to reach 95.1% recognition accuracy. Although having more training samples leads to higher accu- racies, the proposed SDNN can extract diagnostic 9 Figure 5: Some sample images of different object categories of ETH-80 in different viewpoints. For each image, the preferred feature of an activated neuron in the third convolutional layer is shown in below. features and reach reasonable accuracies even using a few tens of training images. Due to the unsuper- vised nature of STDP, the proposed SDNN does not suffer much from the overfitting challenge caused by small training set size in supervised learning algo- rithms such as back-propagation. 3.2 ETH-80 dataset The ETH-80 dataset contains eight different ob- ject categories: apple, car, toy cow, cup, toy dog, toy horse, pear, and tomato (10 instances per cat- egory). Each object is photographed from 41 view- points with different view angles and different tilts. Some examples of objects in this dataset are shown in Figure 5. ETH-80 is a good benchmark to show how the proposed SDNN can handle multi-object categorization tasks with high inter-instance vari- ability, and how it can tolerate huge view-point variations. Five randomly chosen instances of each object category are selected for the training set used in the learning phase. The remaining instances constitute the testing set, and are not seen during the learn- ing phase. All the object images were converted to grayscale values. To evaluate the proposed SDNN Table 1: Recognition accuracies of the proposed SDNN and some other methods over the ETH-80 dataset. Method Accuracy (%) HMAX [40] 69.0 Logistic regression [46] 77.0 Convolutional SNN [40] 81.1 Imagenet pre-trained DCNN [40] 79.5 Proposed SDNN 82.8 on ETH-80, we used a network architecturally sim- ilar to the one used for Caltech face\/motorbike dataset. The other parameters are also similar, ex- cept for the number of neuronal maps in the second and third convolutional and pooling layers. Here we used 400 neuronal maps in each of these layers. Similar to the caltech dataset, neuronal maps of the first convolutional layer converged to the four oriented edges. Neurons in the second and third convolutional layers also became selective to intermediate features and object prototypes, re- spectively. Figure 5 shows sample images from the ETH-80 dataset and the preferred features of some neuronal maps in the third convolutional layer which are activated for those images. As seen, neu- rons in the highest layer respond to different views of different objects, and altogether, provide an in- 10 Figure 6: The confusion matrix of the proposed SDNN over the ETH- 80 dataset. variant object representation. Thus, the network learns 2D and view-dependent prototypes of each object category to achieve 3D representations. As mentioned before, we evaluated the proposed SDNN over the test instances of each object cat- egory which are not shown to the network during the training. The recognition accuracy of the pro- posed SDNN along with some recently reported results on ETH-80 dataset are presented in Ta- ble 1. Note that, the other methods were also trained and tested with the same protocol, five dis- tinct instances per object category being used for each of the training and testing phases. The re- sults indicate that the proposed SDNN outperforms the other methods with a recognition accuracy of 82.8%. This demonstrates the ability of the pro- posed SDNN to achieve an invariant object repre- sentation for multiple object categories. In a subsequent analysis, we computed the con- fusion matrix, to see which categories are mostly confused with each other. Figure 6 illustrates the confusion matrix of the proposed SDNN over the ETH-80 dataset. As seen, most of the errors are due to the miscategorization of dogs, horses, and cows. We checked whether these errors belong to the some specific viewpoints or not. We found out that the errors are uniformly distributed between different viewpoints. Hence, it can be concluded that these categorization errors are due to the over- all shape similarity between these object categories. Figure 7: The Gabor-like features learned by the neuronal maps of the first convolutional layer from the MNIST images. The red and green colors receptively indicate the strength of input synapses from ON- and OFF-center DoG cells. The other important aspect of the proposed SDNN is the computational efficiency of the net- work. For each ETH-80 image, on average, about 9100 spikes are emitted in all the layers, i.e., about 0.02 spike per neuron per image. This points to the fact that the proposed SDNN can recognize objects with high precision but low computational cost. This efficiency is caused by the association of the proposed temporal coding and STDP learning rule which led to a sparse but informative visual coding. 3.3 MNIST dataset MNIST [47] is a benchmark dataset for SNNs which has been widely used [48, 37, 36, 49, 38, 50]. We also evaluated our SDNN on the MNIST dataset which contains 60,000 training and 10,000 test handwritten single-digit images. Each image is of size 28 × 28 pixels and contains one of the digits 0\u20139. For the first layer, ON- and OFF-center DoG filters with standard deviations of 1 and 2 pixels are used. The first and second convolutional layers re- spectively consist of 30 and 100 neuronal maps with 5 × 5 convolution-window and firing thresholds of 15 and 10. The pooling-window of the first pooling layer was of size 2×2 with the stride of 2. The sec- ond pooling layer performs a global max operation. Note that the learning rates of all convolutional lay- ers were set to a+ = 0.004 and a− = 0.003. Figure 7 shows the preferred features of some 11 Table 2: Recognition accuracies of the proposed SDNN and some other SNNs over the MNIST dataset. Architecture Neural coding Learning-type Learning-rule Accuracy (%) Dendritic neurons [48] Rate-based Supervised Morphology learning 90.3 Convolutional SNN [37] Spike-based Supervised Tempotron rule 91.3 Two layer network [36] Spike-based Unsupervised STDP 93.5 Spiking RBM [49] Rate-based Supervised Contrastive divergence 94.1 Two layer network [38] Spike-based Unsupervised STDP 95.0 Convolutional SNN [50] Rate-based Supervised Back-propagation 99.1 Proposed SDNN Spike-based Unsupervised STDP 98.4 neuronal maps in the first convolutional layer. The green and red colors correspond to ON- and OFF- center DoG filters. Interestingly, this layer con- verged to Gabor-like edge detectors with different orientations, phase and polarity. These edge fea- tures are combined in the next layer and provide easily separable digit representation. Recognition performance of the proposed method and some recent SNNs on the MNIST dataset are provided in Table 2. As seen, the proposed SDNN outperforms unsupervised SNNs by reaching 98.4% recognition accuracy. Besides, the accuracy of the proposed SDNN is close to the 99.1% accuracy of the totally supervised rate-based SDNN [50] which is indeed the converted version of a traditional DCNN trained by back-propagation. The important advantage of the proposed SDNN is the use of much fewer spikes. Our SDNN uses only about 600 spikes for each MNIST images in total for all the layers, while the supervised rate- based SDNN uses thousands of spikes per layer [50]. Also, because of using rate-based neural coding in such networks, they need to process images for hun- dreds of time steps, while our network process the MNIST images in 30 time steps only. As stated in Section 2, the proposed SDNN uses a temporal code which encodes the information of the input image in the spike times, and each neuron in all layers, is allowed to fire at most once. This tempo- ral code associated with unsupervised STDP rule leads to a fast, accurate, and efficient processing. 4 Discussion Recent supervised DCNNs have reached high accu- racies on the most challenging object recognition datasets such as Imagenet. Architecture of theses networks are largely inspired by the deep hierar- chical processing in the visual cortex. For instance, DCNNs use retinotopically arranged neurons with restricted receptive fields, and the receptive field size and feature complexity of the neurons grad- ually increase through the layers. However, the learning and neural processing mechanisms applied in DCNNs are inconsistent with the visual cortex, where neurons communicate using spikes and learn the input spike patterns in a mainly unsupervised manner. Employing such mechanisms in DCNNs can improve their energy consumption and decrease their need for an expensive supervised learning with millions of labeled images. A popular approach in previous researches is to convert pre-trained supervised DCNNs into equiva- lent spiking network. To simulate the floating-point calculations in DCNNs, they have to use the firing rate as the neural code, which in result increases the number of required spikes and the processing time. For instance, the converted version of a sim- ple two-layer DCNN for the MNIST dataset with images of 28×28 pixels requires thousands of spikes and hundreds of time steps per image. On the other hand, there are some SDNNs [31, 32, 33] which are originally spiking network but employ firing rate coding and biologically implausible learning rules such as autoencoders and back-propagation. Here, we proposed a STDP-based SDNN with a spike-time coding. Each neuron was allowed to fire at most once, where its spike-time indicates the 12 significance of its visual input. Therefore, neurons that fire earlier are carrying more salient visual in- formation, and hence, they were allowed to do the STDP and learn the input patterns. As the learn- ing progresses, each layer converged to a set of di- verse but informative features, and the feature com- plexity gradually increases through the layers from simple edge features to object prototypes. The proposed SDNN was evaluated on several image datasets and reached high recognition accuracies. This shows how the proposed temporal coding and learning mechanism (STDP and learning competi- tion) lead to discriminative object representations. The proposed SDNN has several advantages to its counterparts. First, our proposed SDNN is the first spiking neural network with more than one learnable layer which can process large-scale nat- ural object images. Second, due to the use of an efficient temporal coding, which encodes the visual information in the time of the first spikes, it can process the input images with a low number of spikes and in a few processing time steps. Third, the proposed SDNN exploits the bio-inspired and totally unsupervised STDP learning rule which can learn the diagnostic object features and neglect the irrelevant backgrounds. Our SDNN could be efficiently implemented in parallel hardware (e.g., FPGA [51]) using address event representation (AER) [52] protocol. With AER, spike events are represented by the addresses of sending and receiving neurons, and time is rep- resented by the asynchronous occurrence of spike events. Since these hardware are much faster than biological hardware, simulations could run several order of magnitude faster than real time [53]. The primate visual system extracts the rough content of an image in about 100 ms [3, 4, 54, 5]. We thus speculate that some dedicated hardware will be able to do the same in the order of a millisecond or less. Also, the proposed SDNN can be modified to use spiking retinal models [55, 56] as the input layer. These models mimic the spatiotemporal filtering of the retinal ganglion cells with center\/surround receptive fields. Alternatively, we could use neu- romorphic asynchronous event-based cameras such as dynamic vision sensor (DVS), which generate output events when they capture transients in the scene [57]. Finally, due to the DoG filtering in the input layer of the proposed SDNN, some visual in- formation such as texture and color are lost. Hence, future studies should focus on encoding these addi- tional pieces of information in the input layer. Biological evidence indicate that in addition to the unsupervised learning mechanisms (e.g., STDP), there are also dopamine-based reinforce- ment learning strategies in the brain [58]. Besides, although it is still unclear how supervised learn- ing is implemented in biological neural networks, it seems that for some tasks (e.g., motor control and sensory inputs prediction) the brain must con- stantly learn temporal dynamics based on error feedback [59]. Employing such reinforcement and supervised learning strategies could improve the proposed SDNN in different aspects which are in- evitable with unsupervised learning methods. Par- ticularly, they can help to reduce the number of required features and to extract optimized task- dependent features. Supplementary Information Video S1. The learning progress and neu- ral activity over the Caltech face\/motorbike task. Here we presented the face and motorbike training examples, propagated the corresponding spike waves, and applied the STDP rule. The in- put image is presented at the top-left corner of the screen. The output spikes of the input layer (i.e., DoG layer) at each time step is presented in the top-middle panel, and the accumulation of theses spikes is shown in the top-right panel. For each of the subsequent convolutional layers, the preferred features, the output spikes at each time step, and the accumulation of the output spikes are presented in the corresponding panels. Note that 4, 8, and 2 features from the first, second and third convolu- tional layers are selected and shown, respectively. As mentioned, the learning occurs layer by layer, thus, the label of the layer which is currently do- ing the learning is specified by the red color. As seen, the first layer learns to detect edges, the sec- ond layer learns intermediate features, and finally 13 the third layer learns face and motorbike prototype features. The video is available at https:\/\/youtu.be\/ u32Xnz2hDkE Acknowledgment This research received funding from the European Research Council under the European Unions 7th Framework Program (FP\/2007-2013) \/ ERC Grant Agreement n.323711 (M4 project). References [1] J. J. DiCarlo, D. Zoccolan, N. C. Rust, How does the brain solve visual object recognition?, Neuron 73 (3) (2012) 415\u2013434. [2] J. J. DiCarlo, D. D. Cox, Untangling invariant object recognition, Trends in Cognitive Sciences 11 (8) (2007) 333\u2013341. [3] S. Thorpe, D. Fize, C. Marlot, et al., Speed of pro- cessing in the human visual system, Nature 381 (6582) (1996) 520\u2013522. [4] C. P. Hung, G. Kreiman, T. Poggio, J. J. DiCarlo, Fast readout of object identity from macaque inferior tem- poral cortex, Science 310 (5749) (2005) 863\u2013866. [5] H. Liu, Y. Agam, J. R. Madsen, G. Kreiman, Tim- ing, timing, timing: fast decoding of object informa- tion from intracranial field potentials in human visual cortex, Neuron 62 (2) (2009) 281\u2013290. [6] K. Fukushima, Neocognitron : a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position., Biological Cybernetics 36 (4) (1980) 193\u2013202. [7] Y. LeCun, Y. Bengio, Convolutional networks for im- ages, speech, and time series, in: The Handbook of Brain Theory and Neural Networks, Cambridge, MA: MIT Press, 1998, pp. 255\u2013258. [8] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, T. Pog- gio, Robust object recognition with cortex-like mecha- nisms, IEEE Transactions on Pattern Analysis Machine Intelligence 29 (3) (2007) 411\u2013426. [9] T. Masquelier, S. J. Thorpe, Unsupervised learning of visual features through spike timing dependent plastic- ity, PLoS Computational Biology 3 (2) (2007) e31. [10] H. Lee, R. Grosse, R. Ranganath, A. Y. Ng, Convolu- tional deep belief networks for scalable unsupervised learning of hierarchical representations, ACM Press, New York, New York, USA, 2009, pp. 1\u20138. [11] N. Pinto, Y. Barhomi, D. D. Cox, J. J. DiCarlo, Com- paring state-of-the-art visual features on invariant ob- ject recognition tasks, in: IEEE workshop on Appli- cations of Computer Vision (WACV), Kona, Hawaii, USA, 2011, pp. 463\u2013470. [12] M. Ghodrati, A. Farzmahdi, K. Rajaei, R. Ebrahim- pour, S.-M. Khaligh-Razavi, Feedforward object-vision models only tolerate small image variations compared to human, Frontiers in Computational Neuroscience 8 (74) (2014) 1\u201317. [13] A. Krizhevsky, I. Sutskever, G. Hinton, Imagenet clas- sification with deep convolutional neural networks., in: Neural Information Processing Systems (NIPS), Lake Tahoe, Nevada, USA, 2012, pp. 1\u20139. [14] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European Conference on Computer Vision (ECCV), Zurich, Switzerland, 2014, pp. 818\u2013833. [15] K. Simonyan, A. Zisserman, Very deep convolu- tional networks for large-scale image recognition, arXiv:1409.1556. [16] S. R. Kheradpisheh, M. Ghodrati, M. Ganjtabesh, T. Masquelier, Deep networks resemble human feed- forward vision in invariant object recognition, Scientific Reports 6 (2016) 32672. [17] S. R. Kheradpisheh, M. Ghodrati, M. Ganjtabesh, T. Masquelier, Humans and deep networks largely agree on which kinds of variation make object recogni- tion harder, Frontiers in Computational Neuroscience 10 (2016) 92. [18] C. F. Cadieu, H. Hong, D. L. Yamins, N. Pinto, D. Ardila, E. A. Solomon, N. J. Majaj, J. J. DiCarlo, Deep neural networks rival the representation of pri- mate it cortex for core visual object recognition, PLoS Computational Biology 10 (12) (2014) e1003963. [19] S.-M. Khaligh-Razavi, N. Kriegeskorte, Deep super- vised, but not unsupervised, models may explain it cortical representation, PLoS Computational Biology 10 (11) (2014) e1003915. [20] R. M. Cichy, A. Khosla, D. Pantazis, A. Torralba, A. Oliva, Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual ob- ject recognition reveals hierarchical correspondence, Scientific Reports 6. [21] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436\u2013444. [22] S. Shoham, D. H. OConnor, R. Segev, How silent is the brain: is there a dark matter problem in neuroscience?, Journal of Comparative Physiology A 192 (8) (2006) 777\u2013784. 14 https:\/\/youtu.be\/u32Xnz2hDkE https:\/\/youtu.be\/u32Xnz2hDkE [23] W. Maass, Computing with spikes, Special Is- sue on Foundations of Information Processing of TELEMATIK 8 (1) (2002) 32\u201336. [24] E. T. Rolls, G. Deco, Computational neuroscience of vision, Oxford university press, Oxford, UK, 2002. [25] C. D. Meliza, Y. Dan, Receptive-field modification in rat visual cortex induced by paired visual stimulation and single-cell spiking, Neuron 49 (2) (2006) 183\u2013189. [26] S. Huang, C. Rozas, M. Treviño, J. Contreras, S. Yang, L. Song, T. Yoshioka, H.-K. Lee, A. Kirkwood, Associa- tive hebbian synaptic plasticity in primate visual cor- tex, The Journal of Neuroscience 34 (22) (2014) 7575\u2013 7579. [27] D. B. McMahon, D. A. Leopold, Stimulus timing- dependent plasticity in high-level vision, Current Bi- ology 22 (4) (2012) 332\u2013337. [28] Y. Cao, Y. Chen, D. Khosla, Spiking deep con- volutional neural networks for energy-efficient object recognition, International Journal of Computer Vision 113 (1) (2015) 54\u201366. [29] E. Hunsberger, C. Eliasmith, Spiking deep networks with lif neurons, arXiv:1510.08829. [30] P. U. Diehl, G. Zarrella, A. Cassidy, B. U. Pedroni, E. Neftci, Conversion of artificial recurrent neural net- works to spiking neural networks for low-power neuro- morphic hardware, in: IEEE International Conference on Rebooting Computing, San Diego, California, USA, 2016, pp. 1\u20138. [31] P. Panda, K. Roy, Unsupervised regenerative learn- ing of hierarchical features in spiking deep networks for object recognition, in: IEEE International Joint Conference on Neural Networks (IJCNN), Vancouver, Canada, 2016, pp. 1\u20138. [32] K. S. Burbank, Mirrored stdp implements autoencoder learning in a network of spiking neurons, PLoS Com- putational Biology 11 (12) (2015) e1004566. [33] Y. Bengio, D.-H. Lee, J. Bornschein, Z. Lin, Towards biologically plausible deep learning, arXiv:1502.04156. [34] J. M. Brader, W. Senn, S. Fusi, Learning real-world stimuli in a neural network with spike-driven synaptic dynamics, Neural computation 19 (11) (2007) 2881\u2013 2912. [35] S. Habenschuss, J. Bill, B. Nessler, Homeostatic plas- ticity in bayesian spiking networks as expectation max- imization with posterior constraints, in: Advances in Neural Information Processing Systems, Lake Tahoe, Nevada, USA, 2012, pp. 773\u2013781. [36] D. Querlioz, O. Bichler, P. Dollfus, C. Gamrat, Immu- nity to device variations in a spiking neural network with memristive nanodevices, IEEE Transactions on Nanotechnology 12 (3) (2013) 288\u2013295. [37] B. Zhao, R. Ding, S. Chen, B. Linares-Barranco, H. Tang, Feedforward categorization on aer motion events using cortex-like features in a spiking neural network, IEEE Transactions on Neural Networks and Learning Systems 26 (9) (2015) 1963\u20131978. [38] P. U. Diehl, M. Cook, Unsupervised learning of digit recognition using spike-timing-dependent plastic- ity, Frontiers in computational neuroscience 9 (2015) 99. [39] M. Beyeler, N. D. Dutt, J. L. Krichmar, Categoriza- tion and decision-making in a neurobiologically plau- sible spiking network using a stdp-like learning rule, Neural Networks 48 (2013) 109\u2013124. [40] S. R. Kheradpisheh, M. Ganjtabesh, T. Masquelier, Bio-inspired unsupervised learning of visual features leads to robust invariant object recognition, Neurocom- puting 205 (2016) 382\u2013392. [41] S. Thorpe, A. Delorme, R. Van Rullen, Spike-based strategies for rapid processing, Neural Networks 14 (6) (2001) 715\u2013725. [42] R. Van Rullen, S. J. Thorpe, Rate coding versus tem- poral order coding: what the retinal ganglion cells tell the visual cortex, Neural Computation 13 (6) (2001) 1255\u20131283. [43] G. Portelli, J. M. Barrett, G. Hilgen, T. Masquelier, A. Maccione, S. Di Marco, L. Berdondini, P. Korn- probst, E. Sernagor, Rank order coding: a retinal infor- mation decoding strategy revealed by large-scale multi- electrode array retinal recordings, Eneuro 3 (3) (2016) ENEURO\u20130134. [44] A. Delorme, L. Perrinet, S. J. Thorpe, Networks of integrate-and-fire neurons using rank order coding b: Spike timing dependent plasticity and emergence of orientation selectivity, Neurocomputing 38 (2001) 539\u2013 545. [45] G. A. Rousselet, S. J. Thorpe, M. Fabre-Thorpe, Tak- ing the max from neuronal responses, Trends in Cog- nitive Sciences 7 (3) (2003) 99\u2013102. [46] F. Zou, Y. Wang, Y. Yang, K. Zhou, Y. Chen, J. Song, Supervised feature learning via l 2-norm regularized lo- gistic regression for 3d object recognition, Neurocom- puting 151 (2015) 603\u2013611. [47] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient- based learning applied to document recognition, Pro- ceedings of the IEEE 86 (11) (1998) 2278\u20132324. [48] S. Hussain, S.-C. Liu, A. Basu, Improved margin multi- class classification using dendritic neurons with mor- phological learning, in: IEEE International Symposium on Circuits and Systems (ISCAS), Melbourne, VIC, Australia, 2014, pp. 2640\u20132643. 15 [49] P. O\u2019Connor, D. Neil, S.-C. Liu, T. Delbruck, M. Pfeif- fer, Real-time classification and sensor fusion with a spiking deep belief network, Frontiers in Neuroscience 7 (2013) 178. [50] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, M. Pfeiffer, Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing, in: IEEE International Joint Conference on Neural Net- works (IJCNN), Killarney, Ireland, 2015, pp. 1\u20138. [51] A. Yousefzadeh, T. Serrano-Gotarredona, B. Linares- Barranco, Fast Pipeline 128??128 pixel spiking convo- lution core for event-driven vision processing in FP- GAs, in: Proceedings of 1st International Conference on Event-Based Control, Communication and Signal Processing (EBCCSP), 2015. [52] M. Sivilotti, Wiring considerations in analog VLSI sys- tems with application to field-programmable networks, Ph.D. thesis, Comput. Sci. Div., California Inst. Tech- nol., Pasadena, CA (1991). [53] T. Serrano-Gotarredona, T. Masquelier, T. Prodro- makis, G. Indiveri, B. Linares-Barranco, STDP and STDP variations with memristors for spiking neuro- morphic learning systems., Frontiers in Neuroscience 7 (2013) 2. [54] H. Kirchner, S. J. Thorpe, Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited, Vision Research 46 (11) (2006) 1762\u20131776. [55] A. Wohrer, P. Kornprobst, Virtual Retina: a biological retina model and simulator, with contrast gain control. 26 (2) (2009) 219\u201349. [56] P. Mart́ınez-Cañada, C. Morillas, B. Pino, E. Ros, F. Pelayo, A Computational Framework for Realistic Retina Modeling, International Journal of Neural Sys- tems 26 (7) (2016) 1650030. [57] P. Lichtsteiner, C. Posch, T. Delbruck, An 128x128 120dB 15us-latency temporal contrast vision sensor, IEEE J. Solid State Circuits 43 (2) (2007) 566\u2013576. [58] M. Pignatelli, A. Bonci, Role of dopamine neurons in reward and aversion: a synaptic plasticity perspective, Neuron 86 (5) (2015) 1145\u20131157. [59] K. Doya, Complementary roles of basal ganglia and cerebellum in learning and motor control, Current Opinion in Neurobiology 10 (6) (2000) 732\u2013739. 16 1 Introduction 2 Proposed Spiking Deep Neural Network 2.1 DoG and temporal coding 2.2 Convolutional layers 2.3 Local pooling layers 2.4 STDP-based learning 2.5 Global pooling and classification 3 Results 3.1 Caltech face\/motorbike dataset 3.2 ETH-80 dataset 3.3 MNIST dataset 4 Discussion ","flair":"three\tResearch"}
{"author":"pcaquestions","created":"Sun Oct 16 18:08:30 EDT 2016","text":"I've been working on a project and I'm not sure how to approach using pca on a certain data set. The way I used PCA before was when given say m examples(of faces or something labelled) with n features, the # of features was reduced to a certain # of principle components.\nThe current dataset I'm looking to reduce has data on currency markets in the following format:\nhttp:\/\/imgur.com\/a\/td7CP\n\nSo 1st column deals w\/ dates, the rest of the columns have the  exchange rate at the date given by the 1st column\nI haven't dealt with time series so I'm not sure how to approach this. Would I be reducing the dimensions of each year? Doesn't feel right to me .\n\nedit: the full dataset has about 3k dates and 18 currencies\nand its the returns on the exchange rate, not the exchange rate","flair":"one\tDiscussion"}
{"author":"whateverr123","created":"Tue Oct 25 17:12:05 EDT 2016","text":"This directory contains CMake files for building TensorFlow on Microsoft Windows. CMake is a cross-platform tool that can generate build scripts for multiple build systems, including Microsoft Visual Studio. N.B. We provide Linux build instructions primarily for the purpose of testing the build. We recommend using the standard Bazel-based build on Linux. The CMake files in this directory can build the core TensorFlow runtime, an example C++ binary, and a PIP package containing the runtime and Python bindings. Note: Windows support is in an alpha state, and we welcome your feedback. The Python package supports Python 3.5 only, because that is the only version for which standard Python binaries exist and those binaries are compatible with the TensorFlow runtime. (On Windows, the standard Python binaries for versions earlier than 3.5 were compiled with older compilers that do not have all of the features (e.g. C++11 support) needed to compile TensorFlow. We welcome patches for making TensorFlow work with Python 2.7 on Windows, but have not yet committed to supporting that configuration.) The following Python APIs are not currently implemented: Loading custom op libraries via . In order to use your custom op, please put the source code under the tensorflow\/core\/user_ops directory, and a shape function is required (not optional) for each op. Path manipulation functions (such as ) are not functional. The libraries are not currently included in the PIP package. The following operations are not currently implemented: Google Cloud Storage support is not currently implemented. The GCS library currently depends on and , and the Windows version could use standard Windows APIs for making HTTP requests and cryptography (for OAuth). Contributions are welcome for this feature. We are actively working on improving CMake and Windows support, and addressing these limitations. We would appreciate pull requests that implement missing ops or APIs. Install the pre-requisites detailed above, and set up your environment. The following commands assume that you are using the Windows Command Prompt (). You will need to set up your environment to use the appropriate toolchain, i.e. the 64-bit tools. (Some of the binary targets we will build are too large for the 32-bit tools, and they will fail with out-of-memory errors.) The typical command to do set up your environment is: When building with GPU support after installing the CUDNN zip file from NVidia, append its bin directory to your PATH environment variable. In case TensorFlow fails to find the CUDA dll's during initialization, check your PATH environment variable. It should contain the directory of the CUDA dlls and the directory of the CUDNN dll. For example: We assume that and are installed and in your . If for example is not in your path and it is installed in , you can add this directory to your as follows: Clone the TensorFlow repository and create a working directory for your build: N.B. This assumes that is in your environment variable. The other paths are for illustrative purposes only, and may be different on your platform. The character is a line continuation and must be the last character on each line. To build with GPU support add \"^\" at the end of the last line above following with: Note that the flag must match the build configuration that you choose when invoking . The known-good values are and . The build type is not currently supported, because it relies on a library for Python () that is not distributed by default. There are various options that can be specified when generating the solution and project files: : Note that the option must match the build configuration that you choose when invoking MSBuild in step 4. The known-good values are and . The build type is not currently supported, because it relies on a library for Python () that is not distributed by default. . Defaults to . You can build a small subset of the kernels for a faster build by setting this option to . . Defaults to . Generate project files for a simple C++ example training program. . Defaults to . Generate project files for building a PIP package containing the TensorFlow runtime and its Python bindings. . Defaults to . Include gRPC support and the distributed client and server code in the TensorFlow runtime. . Defaults to . Include SSL support (for making secure HTTP requests) in the TensorFlow runtime. This support is incomplete, and will be used for Google Cloud Storage support. . Defaults to . Include GPU support. If GPU is enabled you need to install the CUDA 8.0 Toolkit and CUDNN 5.1. CMake will expect the location of CUDNN in -DCUDNN_HOME=path_you_unziped_cudnn. . Defaults to . This builds cc unit tests. There are many of them and building will take a few hours. After cmake, build and execute the tests with . Defaults to . This enables python kernel tests. After building the python wheel, you need to install the new wheel before running the tests. To execute the tests, use This build requires Docker to be installed on the local machine.","flair":"two\tNews"}
{"author":"reworksophie","created":"Mon Nov 14 11:27:23 EST 2016","text":"This interview took place at the RE\u2022WORK Deep Learning Summit in London, on 22-23 September 2016. Oriol Vinyals is a Research Scientist at DeepMind. Previously he was a member of the Google Brain team. He holds a Ph.D. in EECS from University of California, Berkeley, and a Masters degree from University of California, San Diego.","flair":"null\tnull"}
{"author":"nightshade_7","created":"Fri Sep 30 18:52:30 EDT 2016","text":"I've been looking at the forums and through the examples but trying to do binary classification seems to be difficult on MatConvNet for some reason. I'm not able to justify why that happens. Have any of you implemented a binary model on the library? Any help would be greatly appreciated.","flair":"null\tnull"}
{"author":"urish","created":"Sun Nov 20 23:29:29 EST 2016","text":"This page is a companion to a recent paper by Hardt, Price, Srebro, which discusses ways to define and remove discrimination by improving machine learning systems. As machine learning is increasingly used to make important decisions across core social domains, the work of ensuring that these decisions aren't discriminatory becomes crucial. Here we discuss \"threshold classifiers,\" a part of some machine learning systems that is critical to issues of discrimination. A threshold classifier essentially makes a yes\/no decision, putting things in one category or another. We look at how these classifiers work, ways they can potentially be unfair, and how you might turn an unfair classifier into a fairer one. As an illustrative example, we focus on loan granting scenarios where a bank may grant or deny a loan based on a single, automatically computed number such as a credit score. In the diagram above, dark dots represent people who would pay off a loan, and the light dots those who wouldn't. In an ideal world, we would work with statistics that cleanly separate categories as in the left example. Unfortunately, it is far more common to see the situation at the right, where the groups overlap. A single statistic can stand in for many different variables, boiling them down to one number. In the case of a credit score, which is computed looking at a number of factors, including income, promptness in paying debts, etc., the number might correctly represent the likelihood that a person will pay off a loan, or default. Or it might not. The relationship is usually fuzzy\u2014it's rare to find a statistic that correlates perfectly with real-world outcomes. This is where the idea of a \"threshold classifier\" comes in: the bank picks a particular cut-off, or threshold, and people whose credit scores are below it are denied the loan, and people above it are granted the loan. (Obviously real banks have many additional complexities, but this simple model is useful for studying some of the fundamental issues. And just to be clear, Google doesn't use credit scores in its own products.) The diagram above uses synthetic data to show how a threshold classifier works. (To simplify the explanation, we're staying away from real-life credit scores or data--what you see shows simulated data with a zero-to-100 based \"score\".) As you can see, picking a threshold requires some tradeoffs. Too low, and the bank gives loans to many people who default. Too high, and many people who deserve a loan won't get one. So what is the best threshold? It depends. One goal might be to maximize the number of correct decisions. (What threshold does that in this example?) Another goal, in a financial situation, could be to maximize profit. At the bottom of the diagram is a readout of a hypothetical \"profit\", based on a model where a successful loan makes $300, but a default costs the bank $700. What is the most profitable threshold? Does it match the threshold with the most correct decisions? The issue of how \u201Cthe correct\u201D decision is defined, and with sensitivities to which factors, becomes particularly thorny when a statistic like a credit score ends up distributed differently between two groups. Imagine we have two groups of people, \"blue\" and \"orange.\" We are interested in making small loans, subject to the following rules: Everyone has a credit score between 0 and 100 No constraints Blue and orange thresholdsare the same Same fractions blue \/ orange loans to people who can pay them off The most profitable, since there are no constraints. But the two groups have different thresholds, meaning they are held to different standards. Both groups have the same threshold, but the orange group has been given fewer loans overall. Among people who would pay back a loan, the orange group is also at a disadvantage. The number of loans given to each group is the same, but among people who would pay back a loan, the blue group is at a disadvantage. Among people who would pay back a loan, blue and orange groups do equally well. This choice is almost as profitable as demographic parity, and about as many people get loans overall. In this case, the distributions of the two groups are slightly different, even though blue and orange people are equally likely to pay off a loan. If you look for pair of thresholds that maximize total profit (or click the \"max profit\" button) you'll see that the blue group is held to a higher standard than the orange one. That could be a problem\u2014and one obvious solution for the bank not to just pick thresholds to make as much money as possible. Another approach would be to implement a group-unaware, which holds all groups to the same standard. Is this really the right solution, though? For one thing, if there are real differences between two groups, it might not be fair to ignore them\u2014for example, women generally pay less for life insurance than men, since they tend to live longer. But there are other, mathematical problems with a group-unaware approach even if both groups are equally loan-worthy. In the example above, the differences in score distributions means that the orange group actually gets fewer loans when the bank looks for the most profitable group-unaware threshold. If the goal is for the two groups to receive the same number of loans, then a natural criterion is demographic parity, where the bank uses loan thresholds that yield the same fraction of loans to each group. Or, as a computer scientist might put it, the \"positive rate\" is the same across both groups. In some contexts, this might be the right goal. In the situation in the diagram, though, there's still something problematic: a demographic parity constraint only looks at loans given, not rates at which loans are paid back. In this case, the criterion results in fewer qualified people in the blue group being given loans than in the orange group. To avoid this situation, the paper by Hardt, Price, Srebro defines a concept called equal opportunity. Here, the constraint is that of the people who can pay back a loan, the same fraction in each group should actually be granted a loan. Or, in data science jargon, the \"true positive rate\" is identical between groups. A key result in the paper by Hardt, Price, and Srebro shows that\u2014given essentially any scoring system\u2014it's possible to efficiently find thresholds that meet any of these criteria. In other words, even if you don't have control over the underlying scoring system (a common case) it's still possible to attack the issue of discrimination. For organizations that do have control over the scoring system, using these definitions can help clarify core issues. If a classifier isn't as effective for some groups as others, it can cause problems for the groups with the most uncertainty. Restricting to equal opportunity thresholds transfers the \"burden of uncertainty\" away from these groups and onto the creators of the scoring system. Doing so provides an incentive to invest in better classifiers. This work is just one step in a long chain of research. Optimizing for equal opportunity is just one of many tools that can be used to improve machine learning systems\u2014and mathematics alone is unlikely to lead to the best solutions. Attacking discrimination in machine learning will ultimately require a careful, multidisciplinary approach. Thanks to Meredith Whittaker, Daniel Smilkov, Jimbo Wilson, and our other colleagues at Google for their help with this piece.","flair":"four\tProject"}
{"author":"kvdveer","created":"Tue Sep 27 12:39:45 EDT 2016","text":"A client of mine works through documents, and selects a few good ones for further processing. I've built him a system to pre-assess whether a document matches his criteria of being a good document. This system is trained using is decisions in the past, and is remarkably accurate. The system needs to be trained continuously; the criteria for a 'good' document shift over time, to allow for advances in the field, and to compensate for applicants trying to game the system. Currently the system is only used to assign priorities - eventually all documents get reviewed.\n\nThe success of the project is now about to work against me. The clients wants to use the model to filter out the bad documents, and not spend any time on those. This means that the predicted bad documents no longer get reviewed, and my training set will develop a bias towards the good documents.\n\nI'm now looking for ways to prevent this bias. I've come up with a few solutions:\n\n* I could randomly ignore the prediction, and use those as training input. This will annoy the client for having to deal with 'obviously bad' documents.\n* I could treat a bad prediction as a bad review, but I'm afraid that such a solution would eventually become a large echo-chamber incapable of learning anything new\n* I could ignore the entire bias concern, and let the system balance itself out, despite the circular setup. Similarly, I'm afraid that, over time, many 'good' documents get classified as garbage, and never reach the human review stage.\n\nNone of these options feel particularly good. How would you deal with this?","flair":"null\tnull"}
{"author":"BafflesSean","created":"Sun Oct 02 12:02:02 EDT 2016","text":"Does anyone know roughly how much training time is required to train a WaveNet to a similar level of quality to the examples presented in the Deep Mind blog post?\n\nhttps:\/\/deepmind.com\/blog\/wavenet-generative-model-raw-audio\/\n\nI haven't seen a public implementation of WaveNet achieve the results described in the blog post.\n\nEdit: I'm asking because I'm considering a psychophysics project that requires audio synthesis as one of it's core components.","flair":"null\tnull"}
{"author":"youngChange","created":"Fri Nov 25 12:24:52 EST 2016","text":"I am a 3rd year CS student trying to make an application that will auto-group faces of the same person like google photos does.\nI used OpenFace, an open source implementation of Google's FaceNet paper, which uses triplet loss to generate a [1 x 127] representation of an person's face. \nNow that I have a representation for each photo in my data-set, I would like to cluster photos of the same person together. The tricky part here is that I do not know exactly how many unique people are in the dataset.\n\nCould anyone provide any guidance to cluster photos of the same person without knowing the number of clusters?. \n\nThank you for your time.","flair":"one\tDiscussion"}
{"author":"HumbleNoob","created":"Sat Oct 15 06:59:21 EDT 2016","text":"Hi, \nI'm an undergraduate student working on a text generation task. I am unable to train a network using my pretrained word embeddings as weights for input layer to LSTM. My word2vec embedding is trained on a larger corpus and the training corpus is a subset of it. I'm mapping word vectors to embedding weights using word2vec model. My vocabulary for the task consists of some word2vec_vocab+additional words in corpus. The model is as follows:\n\nTrain data: vector with indices mapped using a dictionary consisting of word2vec model indices + additional word indices = full dict\n\nTest data: one hot vector with 1 at position w.r.t corpus(I am not using full dict for mapping, thus position indices differ.)\n\n**Model**:\n\n w2v_dim= 200\n\nseq_length= 7\n\nvocab_size= # of unique words in corpus\n\n\nmodel.add(Embedding(corpus_size, w2v_dim, mask_zero=False, weights=[embedding], input_length=seq_length)) \n\nmodel.add(LSTM(memory_units, return_sequences=True, init= \"orthogonal\"))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(TimeDistributed(Dense(vocab_size, activation='softmax', init= \"orthogonal\")))\n\n\n\n**Problem**: Model overfitting on training data with increasing loss on validation set. \n\n\nWhat am I missing? What else can I do to improve the model? Thanks.\n\n ","flair":"one\tDiscussion"}
{"author":"jacky0812","created":"Sun Oct 09 23:29:13 EDT 2016","text":"Pretty much most of the RNN related papers that I read are on text, speech or images data, is there any reason that we don't see much research on using RNN on time series data like stock market or IOT, sensor data?","flair":"one\tDiscussion"}
{"author":"kmrocki","created":"Wed Oct 26 14:33:04 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.07675 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.AI cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Surprisal-Driven Zoneout Authors: Kamil Rocki, Tomasz Kornuta, Tegan Maharaj (Submitted on 24 Oct 2016 (v1), last revised 24 Nov 2016 (this version, v5)) Abstract: We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods. Comments: To be published at the Continual Learning and Deep Networks Workshop NIPS 2016 Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.07675 [cs.LG]   (or arXiv:1610.07675v5 [cs.LG] for this version) Submission history From: Kamil Rocki [view email] [v1] Mon, 24 Oct 2016 22:38:52 GMT (1234kb,D) [v2] Fri, 28 Oct 2016 19:55:16 GMT (1234kb,D) [v3] Mon, 31 Oct 2016 15:18:11 GMT (1234kb,D) [v4] Thu, 3 Nov 2016 17:09:23 GMT (1234kb,D) [v5] Thu, 24 Nov 2016 06:40:26 GMT (1242kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"max_likelihood","created":"Wed Nov 09 20:36:41 EST 2016","text":"Hi! I am trying to fine-tune an already fine-tuned model (was fine-tuned from imagenet to PASCAL VOC 2007+2012 trainval) to MS COCO. Unfortunately, my caffe-based implementation (py-faster-rcnn) is running into dimensionality issues concerning the input and output layers, which are not trivial to solve. I have done quite some extensive research, but now I am wondering, if this is at all possible (only ML demi-gods seem to have accomplished this) or at all desirable? What is your intuition? Will an aggregated, big dataset with a subset of shared categories outperform a model that has been fine-tuned from dataset to dataset...to...dataset? Thanks a lot!\n\nupdate: there are two basic directions:\nA) fine-tuning an already fine-tuned dataset: \nthe pipeline would look like this: imagenet --&gt; pascal voc --&gt; ms coco --&gt; Caltech --&gt; KITTI. During this pipeline, for each dataset, I would change the last layers, while keeping the lower layers static. Also, for each dataset, there would be different categories. \nB) Giant dataset:\nPutting all images from all datasets together to create a giant dataset, which I train on the small intersection of categories of each dataset (car, pedestrian).","flair":"one\tDiscussion"}
{"author":"onewugtwowugs","created":"Thu Oct 13 07:56:20 EDT 2016","text":"I'm interested in comparing the performance of various word embedding models, and my work would be greatly simplified if there already was a collection of these trained on various popular models and large corpora out there somewhere. I have found a few repositories containing pre-trained models, but nothing where the models have all been trained on the same corpora. Have you seen anything like this?\n\nThanks","flair":"one\tDiscussion"}
{"author":"modeless","created":"Fri Sep 30 19:52:41 EDT 2016","text":"An explanation of the ground truth is that the dashed line first goes to the left, then to the right, and then on both sides, and also changes from single to double, hence the ground truth should have double dashed lines on both the sides. On the corners, the number of slanted lines increase by one after every two images, hence the ground truth should have four slant lines on both the corners. GANs have been shown to be useful in several image generation and manipulation tasks and hence it was a natural choice to prevent the model make fuzzy generations. In Context-RNN-GAN, 'context' refers to the adversary receiving previous images (modeled as an RNN) and the generator is also an RNN. The name distinguishes it from our simpler RNN GAN model where the adversary is not contextual (as it only uses a single image) and only the generator is an RNN. The discriminator is modeled as a GRU-RNN which gets all the preceding images to decide whether the generation by the Generator is the correct image for the timestep. The generator is modeled as a GRU-RNN which tries to generate an image using the preceding images. It is guided by the contextual discriminator to produce real looking images. The above figure corresponds to our Context-RNN-GAN model, where the generator G and the discriminator D (where D represents its ith timestep snapshot) are both RNNs. G generates an image at every timestep while D receives all the preceding images as context to decide whether the current image output by G is real vs generated for that particular timestep. x are the input images. When using an L-2 loss function, some of the generated images were superimpositions of the component parts and were too cluttered. When using an L-1 loss function, although it was sharper than using an L-2 loss, it was missing some components of the actual diagrams. A weighted combination of L-1 and adversarial loss (as defined for the context based discriminator model described above) was used for the Context-RNN-GAN model to produce the best results based on empirical evaluation. Context-RNN-GAN with features obtained from Siamese CNN is competitive with humans in 10th grade in the sense that it is able to achieve accuracy of 35.4% when the generated features are compared with the features obtained from actual answer images. It needs to be noted that humans can see the options to get the best possible overall sequence of six images and hence can select the best choice while our model is just comparing the generations (obtained using sequence of five images in the problem) with options to get the best option. So, we can say that our model is very good generator and comparable to even 10th grade humans. An interesting aspect is that the model is never trained on the correct answers, it is just trained on multiple sequences from the problem images and still performs remarkably well. First Generation In the first example generation it is interesting to note that the model correctly predicted elements in off diagonal while faltered in the shape of the elements in leading diagonals. Second Generation Second example shows an interesting case whereby the image generated by our model is also plausible (if by symmetry it is considered that first and third the semicircular ring is solid and hence fourth and sixth should be solid) while the actual answer is of course plausible according to the reasoning that the (solid vs hollow) flipped in the first two cases then stayed the same for the next two timesteps. Even more interesting is its analysis of the spatial dynamics of the ball and the semicircular ring which it almost correctly captured. Third Generation Another very interesting case is the generation which it gets correct. However, in this case the answer figure is exactly similar to the second figure in the sequence. Therefore, it is not illustrative of the ability of the model to generate the sequence of the pattern. The model can be applied to video prediction tasks as illustrated by above figure. The Moving MNIST task consists of videos of two moving MNIST digits and the next frame has to be predicted from the preceding frames.","flair":"null\tnull"}
{"author":"mimighost","created":"Thu Nov 03 05:51:20 EDT 2016","text":"Creating font is a hard business, creating a Chinese font is an even harder one. To make a GBK (a character set standardized by Chinese government) compatible font, designers will need to design unique looks for more than 26,000 Chinese characters, a daunting effort that could take years to complete. What about the designer just creates a subset of characters, then let computer figures out what the rest supposed to look like? After all, Chinese characters are consisting of a core set of radicals(偏旁部首), and the same radical looks pretty similar on different characters. This project is an explorational take on this using deep learning. Specifically, the whole font design process is formulated as a style transfer problem from a standard look font, such as SIMSUN, to an stylized target font. A neural network is trained to approximate the transformation in between two fonts given a subset of pairs of examples. Once the learning is finished, it can be used to infer the shape for the rest of characters. Box below illustrated this general idea: This project is heavily inspired by the awesome blog Analyzing 50k fonts using deep neural networks from Erik Bernhardsson, and great paper Learning Typographic Style from Shumeet Baluja. After trying various different architectures, including more sophisticated ones with residuals and deconvolution, I ended up with a more traditional flavour top-down CNN structure, as shown below. The image on the top shows the progress of model made on validation set during training, on various fonts. All of the them are trained on 2000 examples with layer number set to 3. It is pretty interesting to see how the model slowly converges from random noise, to first capture the recognizable shape of a character, then to more nuanced details. Below is the progress captured for a single font during training. The image below shows the predicted result versus ground truth. For each font, we take 2000 most used characters as training set, running for 3000 iterations. A test set with 100 characters are used for inference. For all the fonts, the source font is SIMSUN. For majority of the fonts, the network succeeds to make reasonable guesses. Some of them are actually very close to the ground truth. Also worth noting, the tiny but distinguishable details are preserved by the network, like the curly end of radicals. But just like many other neural network powered applications, when the network fails, it fails spectacularly. For some fonts, especially the ones that are light in weight, it merely comes up some blurry inky blob. On the other hand, for those heavy fonts, it loses critical details of blanks to make the character distinguishable, only capturing the overall outline. Even in the successful cases, the loss of parts of radicals seems like a common problem. Also, network seems to do better on SongTi family(宋体 or 明朝体) of fonts, but didn't do well on KaiTi(楷体), which may due to the fact that SIMSUN itself is a SongTi font. Because the limitation of space, for each font, we randomly sample one character from test set. If you wish to see the result on bigger test set of characters, please refer to this 2000 characters may be 10% of the whole GBK standard, but these are still a lot of characters. I choose the number out of my instinct, and it seems work well for many fonts. But is it really necessary? To figure this out, I pick one font(it takes way too much time to carry this experiment on every font, sadly), run an experiment with number of training examples, ranging from 500 to 2000, and ask the model to render characters on a common test set, below shows the result. The above image shows from top to bottom, the effect of increasing size of training set from 500 to 2000. The improvement becomes smaller between training 1500 and 2000, which indicates that the sweet point is somewhere in between. To use this package, TensorFlow is required to be installed (tested on 0.10.0 version). Other python requirements are listed in the requirements.txt. Also a GPU is highly recommended, if you expect to see the result in reasonable amount of time. All experiments run on one Nvidia GTX 1080. For 3000 iterations with batch size 16, it takes the small model about 20 minutes to finish, while 80 minutes for the medium one, and 2 hours for big model. Prior to training, you need to run the preprocess script to generate character bitmaps for both source and target fonts: The preprocess script accepts both TrueType and OpenType fonts, take a list of characters (some common charsets are builtin in the charsets directory in this repo, like the top 3000 most used simplified Chinese characters) then save the bitmaps of those characters in .npy format. By default, for the source font, each character will be saved with font size 128 on a 160x160 canvas, and target font with size 64 on 80x80 canvas, with respect. No special alignment is needed here, just make sure characters don't get truncated. After the preprocess step, assume you already have the bitmaps for both source and target fonts, noted as src.npy and tgt.npy, run the below command to start the actual training: For other options, you can use the -h checkout the exact usage case. Suppose we finish training (finally!), now we can use the infer mode mentioned previously to see how the model is doing on unseen characters. You can refer to the captured frames in the frame_dir to help you pick the model that you are most satisfied with (Spoiler: it is usually not the one with minimum error). Run the following command Note the source_font can be different from the one used in training. In fact, it can even be any other font. But it is better to choose the same or similar font for inference for good output. After the inference, you will find series of images for all output characters and a npy file that contains the inferred character bitmaps. This project started out as a personal project to help me learning and understanding TensorFlow, but shapes up to be something more interesting, that I think worthy sharing with more people. Currently, the network can only manages to learn one style at a time, it will be interesting to see how to extend it to handle multiple styles at once. 2000 characters are fewer than 10% of the complete GBK sets, but it is still a lot of characters, is it possible to learn style with something fewer than 100 characters? My guess is GAN(Generative Adversarial Network) could be really useful to try for this purpose. On network design, this architecture is proven effective on different fonts, but what is the optimal number of layers for each size of convolutions remains to be figured out, or whether some convolution layers are necessary at all. Another interesting direction I would love to explore is create font with mixed styles. Simply combining two fonts in the loss function didn't work well. Maybe we should train a VGG network just for fonts, then hijacking the feature maps? Or potentially more novel change in network design is required to tackle this. Last but not least, this project demonstrates a possibility of applying deep learning, more specifically, CNN in helping accelerating design process for Chinese fonts. The results are promising and encouraging, but not quite there to magically create new font out of box, nor it is part of the claim of this project.","flair":"four\tProject"}
{"author":"m3wm3wm3wm","created":"Sun Nov 27 11:25:05 EST 2016","text":"By data-to-text I mean a black box which takes numbers and spits out natural language. This is a spacial case of NLG.\n\nThere are papers floating around on this subject, but they do not publish their code (**how is this still a thing?**)\n\nEDIT:\n\nNot really.\n\n[SimpleNLG](https:\/\/github.com\/simplenlg\/simplenlg) is an example if what I'm looking for, but it's not an incomplete implementation of an old NLG method.\n\nThis paper explains data-to-text: http:\/\/homepages.abdn.ac.uk\/e.reiter\/pages\/papers\/enlg07.pdf","flair":"one\tDiscussion"}
{"author":"yoav_hollander","created":"Sun Oct 16 14:50:43 EDT 2016","text":" Skip to content The Foretellix Blog Menu Home About Privacy Policy Terms of Use Why this blog Terminology All posts Misc stuff: The verification gap, ML training and more Posted on October 16, 2016October 17, 2016 by Yoav Hollander This post covers recent updates in machine learning, autonomous systems and verification. It has four sections: Automation \/ ML keep accelerating, but verification of automation \/ ML seems to lag behind HVC is coming, and I plan to attend (and even present) The idea of training an ML-based system using synthetic inputs (which I like) has just been tried by some folks at U of Michigan Reverse Reinforcement Learning only seems to imply that we can\u2019t do \u201Cnormal\u201D verification The speed of it all  I know it is a cliché to say that everything is speeding up: You could argue that this has been going on since life started (3.8 million years ago next Wednesday) \u2013 just plot it on a logarithmic scale and you\u2019ll see. Still, the last few weeks somehow made me realize that all those things that I am interested in verifying (AVs, autonomous systems in general, ML etc.) are moving faster than even most believers of that cliché assumed just a year ago. Here are some examples: The US seems to be officially behind AVs as a mission. Almost all car companies (and many others) now have concrete AV plans, and some are actually deploying experimentally President Obama just gave a thoughtful interview (with MIT Media Lab\u2019s Joi Ito) about AI, ML, the future of employment and all that When the topic of the dangers of AGI come up (see this post), people often still say \u201Coh, it is still X years away\u201D, but the X keeps shrinking. I hear 20-to-30-years more and more, and that\u2019s pretty soon. The rate of inventions in ML seems to be accelerating. For instance, an article about \u201CDeep Symbolic Reinforcement Learning\u201D just came out (long summary here), which may help bring together the (previously very separate) symbolic-AI and ML fields. The above (somewhat arbitrary) snapshot just reminded me that those technologies are speeding up, but the verification side of things is not speeding up at the same rate (though everybody says it is important). This should change, and I assume it will. HVC is coming  On a related note, the Haifa Verification Conference is happening on 15..17 of November (in Haifa, Israel). It is a pretty decent conference devoted to all-things-verification (I blogged about last year\u2019s conference here). I plan to be there on the first two days. Towards the end of the first day, I\u2019ll give a presentation about the current state of autonomous systems verification (including verification of ML-based systems). I consider the conference to be slightly too formal-heavy, so to balance that I plan to provide mainly light entertainment, gossip and sweeping generalizations about the current state of this increasingly-important field. I promise to blog about my impressions. And if you are there by any chance, I\u2019d love to chat. Using a verification environment to train an ML-based system I mentioned this idea before, e.g. here: This (somewhat-oddball) direction essentially says: If we already have a verification environment capable of producing rare corner cases, why not use it to actually train the ML-based system (since otherwise rare events will not appear in enough training data). This assumes a CDV-based VE, capable of creating many constrained-random variants of diverse scenarios, as I described e.g. here. Such an environment can produce an essentially-infinite training set. There are some potential problems with this idea, though. As I said here: It is pretty hard to create realistic, completely-synthetic inputs. This is indeed a big problem: For instance, creating a synthetic, believable LIDAR stream with the matching video stream is pretty hard. So (at least currently) you may need to use pre-recorded streams, and e.g. superimpose people-and-animals-jumping-in. \u2026 This is what I think Google is doing. They talk here about doing \u201Cthree million miles of testing in our simulators every single day\u201D, which (assuming 1:1 simulation speed) translates to a few thousand cores constantly running simulations. That was said in the context of building a VE for verification, but the same problem (let\u2019s call it \u201Csynthetic-multi-streams\u201D) obviously applies to training as well. Another problem, unique for training-via-synthetic-inputs, is that this may cause overfitting to some artifacts of the scenario generation algorithm or the display engine. For instance, suppose we use this train-via-VE technique just to train the system on extreme and dangerous cases. If the display engine makes the sky too uniformly blue, the system could learn to be extra-careful only when the sky looks like that. Finally, using the exact same VE for both training and verifying a system may carry some risks Still, I think this is a pretty good direction, and the problems will eventually be solved. So I was happy to see a new article by some folks from the U of Michigan talking about a variant of this: They describe training a vehicle-detection ML-system (which maps from video inputs to vehicle-bounding-boxes) using synthetic inputs created by capturing scenes from runs of the Grand Theft Auto computer game. This is not the full thing I want (e.g. it is not a reuse of a verification environment, they don\u2019t seem to have the ability to control everything, and they explicitly ignore non-visual stuff). Nevertheless, the results seem pretty encouraging. In general, I am fairly bullish on this idea (and hope to see more research directed at it). Here\u2019s why: As the article says, this gives you a potentially-infinite, very diverse set of inputs to train on. In the context of vehicle detection (assuming a good VE) this would give you cars-in-snow, cars-in-India, cars-on-hilly-road, cars-in-all-directions and so on (and any combination of the above). You can create rare, potentially-dangerous situations, which may be completely absent from the non-synthetic training set. Note that it is probably a good idea to train your system on both non-synthetic and synthetic inputs. The stimulus-generation part of the VE will supply inputs for training. The checker part of the VE may supply \u201Canswers\u201D (for supervised learning) or a reward signal (for Reinforcement Learning, see next chapter). The coverage part will help us track that we went through all needed scenarios and their parameters. As CDV-based VEs for autonomous systems improve (as I assume they must), VE-based-training capabilities will also automatically improve. For instance, when that thorny synthetic-multi-streams problem is eventually solved for verification, it can be used for training as well. In the meantime, one can carefully work around some of the problems. For instance, to solve the synthetic-multi-stream problem, one can use a mix of recording and synthetic inputs (as many are doing now), perhaps enhanced with the ability to choose and stitch together recorded segments in interesting ways. Note that the VE in question does not have to be directed specifically at an ML-component: It could be a full-AV VE, and yet you can still use it for training an ML component within the VE. But perhaps the biggest plus of using VE-based-training is that it lets you impose some structure and modularity on an ML-based system which otherwise lacks those qualities. For instance, there may be no way to separate out (in the ML-based system) the \u201Cweather aspect\u201D, i.e. the handling of different weather conditions. But in the VE you absolutely can do that: have a \u201Cweather aspect\u201D module, and as you do the training, methodically take all scenarios and add weather conditions to them, making sure the system gets trained on them all. This goes for many other aspects, such as the \u201Clocation aspect\u201D: Suppose your ML-based AV should behave differently in different countries. With VE-based training, the team responsible for \u201Cdriving in India\u201D can modularize the India-related part of the training (driving on the left, much more chaotic traffic, \u2026) into a separate, inspectable module of the VE. Then, they can make sure to do \u201Cenough training\u201D with this aspect super-imposed on the \u201Cnormal training scenarios\u201D. This, of course, assumes your VE is good at aspect-merging. [Added 17-Oct-2016: A new paper from DeepMind talks about new ways to apply simulation-based training to the real world, and mentions another paper which says that training on synthetic data often fails because of \u201Cthe discrepancy in visual cues between simulation and reality\u201D] Reverse Reinforcement Learning and verification In a previous post I said ML-based systems should (mostly) be verified using \u201Ctraditional\u201D rule-based techniques (like CDV): While I will be talking at length about using-ML-to-verify-ML, I urge you again, oh ML practitioners and researchers, to not be guided by the beauty of your weapons (as Leonard Cohen used to say). For serious, safety-critical systems, one needs serious, systematic verification. You would have thought that me asserting that (on the Internet!) would be enough to end all discussion😉. Well, no. Some people did question that assertion, and that got me thinking again: Can we really expect rule-based verification to work for everything? To understand the limits of rule-based verification, consider Reverse Reinforcement Learning: Reinforcement Learning (RL) is a fairly-popular ML technique: Unlike supervised learning, in RL (as you probably know better than me) we don\u2019t train the system by giving it examples with the \u201Cright\u201D answers. Rather, we give it a \u201Creward signal\u201D from time to time (e.g. when we teach it to play Go, we give it a +1 reward when it wins a game and a -1 reward when it loses). We then let the algorithm derive the \u201Cvalue function\u201D for intermediate actions by itself. For instance, if experience shows that doing action X when in state Y has a high chance of getting the system to the final \u201Cwin the game\u201D state, then the algorithm will assign a high value to doing X in state Y. And so on. Once the value function is established, it is a simple matter to choose, in any state, the action with the highest value function. But what if the problem is so complex that we can\u2019t even specify the right reward signal? Well, one idea is to use \u201CReverse Reinforcement Learning\u201D (RRL). In RRL, the algorithm tracks, say, a (human) expert solving the same problems, and tries to derive the value function the expert is using when making decisions. Deriving a value function from actions is of course tricky (\u201Cdid the expert exit through the green door because green doors are better or because the green door is closer to the garage and getting to the garage is better?\u201D) but is apparently doable. As problems get more complex, RRL (or something like it) will probably keep getting more important: To take an extreme example, I have talked before about Artificial General Intelligence (AGI) and its dangers. One possible, partial way to deal with that danger is for the AGI to keep asking people about what to do in novel \/ unusual cases, which sounds a bit like RRL. OK, back to verification: RRL-based systems are probably really hard to verify in a rule-based way: There is no spec, no rules, not even a user-defined reward signal: The system simply has to \u201Cdo its best to find what the human expert would have done, given enough time and information\u201D. But (putting aside the extreme, full-AGI case) I think regulatory bodies, the public (and common sense) will still demand rule-based verification: Just saying \u201COh, I verified the learning algorithm, and it works fine\u201D will not cut it. People will want to know what scenarios \/ parameters were executed during verification, and what requirements have been tracked. Even if some \/ most things are decided using demonstration-only, the system should still follow some basic requirements. And yes, checking may be hard (see my post about probabilistic checking), but people will still want to understand it. So, at least for the foreseeable future, even for RRL-base systems we still need something like CDV for verification. Notes I\u2019d like to thank Ziv Binyamini, Amiram Yehudai and Sandeep Desai for commenting on previous versions of this post. Share this: Twitter Facebook Google Like this: Like Loading... Post navigation Verification implications of the new US AV policy HVC\u20192016 trip report 2 thoughts on \u201CMisc stuff: The verification gap, ML training and more\u201D Gil Amid says: October 24, 2016 at 5:58 pm Hi Yoav, I am sure we can continue chat on HVC site ( I plan to be there ). However \u2013 are you suggesting ( or have you considered ) using gaming s\/w as stimuli generators ? It seems gaming engines have enough randomness and enough scenarios options to cover a wide set of input and extreme conditions. ) It may be that the gaming engine may be help to support\/help checking the output \u2013 based on your score of the game. How deeply have you looked at this ? LikeLike Reply Yoav Hollander says: October 24, 2016 at 7:09 pm Hi Gil Did not look very deeply, but I do consider game engines to be a promising direction (assuming you look mainly at visual sensors). My current favorite is Unity [1]: It is portable and fairly advanced. I don\u2019t know enough about available hooks to assess how hard it will be to add monitoring \/ coverage \/ checking. Because there is so much money and excitement in game design, there are other things we can learn from that field, e.g. from game design languages. I talked about that a bit in [2]. See you at HVC. [1] https:\/\/unity3d.com\/ [2] https:\/\/blog.foretellix.com\/2015\/10\/16\/misc-stuff-hvc-game-design-languages-and-more\/ LikeLike Reply Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Search for: Archives Archives Select Month November 2016  (1) October 2016  (1) September 2016  (3) August 2016  (1) July 2016  (4) June 2016  (3) May 2016  (1) April 2016  (2) March 2016  (1) January 2016  (2) December 2015  (1) November 2015  (2) October 2015  (2) September 2015  (1) August 2015  (3) July 2015  (10) Recent Posts HVC\u20192016 trip report Misc stuff: The verification gap, ML training and more Verification implications of the new US AV policy Using Machine Learning to verify Machine Learning? Machine Learning for Coverage Maximization Machine Learning verification and Explainable AI About faults and bugs Checking probabilistic components Future of verification: Better ways to predict behavior Recent Comments Yoav Hollander on Misc stuff: The verification g\u2026 Gil Amid on Misc stuff: The verification g\u2026 Steve Ross on Verification implications of t\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Using Machine Learning to veri\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Verification implications of t\u2026 Steve Ross on Verification implications of t\u2026 Mugur Tatar on Checking probabilistic compone\u2026 Recent Posts HVC\u20192016 trip report Misc stuff: The verification gap, ML training and more Verification implications of the new US AV policy Using Machine Learning to verify Machine Learning? Machine Learning for Coverage Maximization Machine Learning verification and Explainable AI About faults and bugs Checking probabilistic components Future of verification: Better ways to predict behavior Recent Comments Yoav Hollander on Misc stuff: The verification g\u2026 Gil Amid on Misc stuff: The verification g\u2026 Steve Ross on Verification implications of t\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Using Machine Learning to veri\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Verification implications of t\u2026 Steve Ross on Verification implications of t\u2026 Mugur Tatar on Checking probabilistic compone\u2026 Archives Archives Select Month November 2016  (1) October 2016  (1) September 2016  (3) August 2016  (1) July 2016  (4) June 2016  (3) May 2016  (1) April 2016  (2) March 2016  (1) January 2016  (2) December 2015  (1) November 2015  (2) October 2015  (2) September 2015  (1) August 2015  (3) July 2015  (10) Meta Register Log in Entries RSS Comments RSS WordPress.com Follow Blog via Email Enter your email address to follow this blog and receive notifications of new posts by email. Join 64 other followers Follow The Foretellix Blog on WordPress.com Blog at WordPress.com. %d bloggers like this: ","flair":"null\tnull"}
{"author":"evc123","created":"Tue Oct 11 04:51:21 EDT 2016","text":"JavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload. Sign in Neuroscience and AI Notes      Share The version of the browser you are using is no longer supported. Please upgrade to a supported browser.Dismiss File Edit View Tools Help Accessibility Debug See new changes           Accessibility     View only           Toggle screen reader support","flair":"one\tDiscussion"}
{"author":"deepPurpleHaze","created":"Mon Nov 14 09:06:08 EST 2016","text":"Using a deep convolutional neural network with residual connections, Miles Deep quickly classifies each second of a pornographic video into 6 categories based on sexual act with 95% accuracy. Then it uses that classification to automatically edit the video. It can remove all the scenes not containing sexual contact, or edit out just a specific act. Unlike Yahoo's recently released NSFW model, which uses a similar architecture, Miles Deep can tell the difference between nudity and various explicit sexual acts. As far as I know this is the first and only public pornography classification or editing tool. This program can also be viewed as a general framework for classifying video with a Caffe model, using batching and threading in C++. By replacing the weights, model definition, and mean file it can immediately be used to edit videos with other classes without recompiling. See below for an example. For GPU usage you need an Nvidia GPU and CUDA 8.0 drivers. Highly recommended; increases speed 10x. This can be installed from a package or by downloading from NVIDIA directly. Additional drivers from NVIDIA that make the CUDA GPU support even faster. Download here. (Requires registration) Download the model too. Put miles-deep in the same location as the model folder (not in it). I'm working on a version for Windows. Sorry, I don't have a Mac but it should run on OSX with few changes. Compilations instructions below. I'll accept pull requests related to OSX or other linux compatibility. Windows will likely require anothe repository to link with Caffe for windows. This finds the scenes sex from the back or front and outputs the result in This edits out all the non-sexual scenes from and outputs the result in . This reduces the batch size to 16 (default 32). Finds only scenes with cunnilingus, outputs result in . NOTE: Reduce the batch size if you run out of memory Tested on an Nvidia GTX 960 with 4GB VRAM and a 24.5 minute video file. At batch_size 32 it took approximately 0.6 seconds to process 1 minute of input video or about 36 seconds per hour. In addition to batching, Miles Deep also uses threading, which allows the screenshots to be captured and processed while they are classified. By popular demand I added this option, which outputs : The file contains the cuts for each target, ordered as they occur in the movie. The first lines gives the movie name, the labels, the total movie time, and the total seconds for each label. Then for each cut it list the start time, end time, average score, and coverage. Because of the threshold and the gaps, these cuts may overlap and aren't guaranteed to cover every second. Here is an example of the predictions for each second of a video: Here's an example of how to use the program with your own model (or a pre-trained one): This finds the scenes in with a tabby cat and returns with only those parts. n02123045 is the category for tabby cats. You can find the category names in . You can use a pre-trained model from the model zoo instead. Note: This example is just to show the syntax. It performs somewhat poorly in my experience, likely due to the 1000 classes. This program is ideally suited to models with a smaller number of categories with an 'other' category too. The model is a CNN with residual connections created by pynetbuilder. These models are pre-trained on ImageNet. Then the final layer is changed to fit the new number of classes and fine-tuned. As Karpathy et al suggest, I train the weights for the top-3 layers not just the top-layer, which improves the accuracy slightly: Below are the results for fine-tuning the top3 layers with different models, tested on 2500 training images, taken from different videos than the training set. Of all the models tested, the resnet50_1by2 provides the best balance between runtime, memory, and accuracy. I believe the full resnet50's low accuracy is due to overfitting because it has more parameters, or perhaps the training could be done differently. The results above were obtained with mirroring but not cropping. Using cropping slightly improves the results on the resnet50_1by2 to 95.2%, therefore it is used as the final model. Fine-tuning the Inception V3 model with Tensorflow also only achieved 80% accuracy. However, that is with a 299x299 image size instead of 224x224 with no mirroring or cropping, so the result is not directly comparable. Overfitting may be a problem with this model too. Given the predictions for a frame each second, it takes the argmax of those predictions and creates cut blocks of the movie where argmax equals the target and the score is greater than some threshold. The gap size, the minimum fraction of frames matching the target in each block, and the score threshold are all adjustable. FFmpeg supports a lot of codecs including: mp4, avi, flv, mkv, wmv, and many more. This model doesn't make use of any temporal information since it treats each image separately. Karpathy et al showed that other models which use multiple frames don't perform much better. They have difficulty dealing with camera movement. It would still be interesting to compare their slow fusion model with the results here. The training database consists of 36,000 (and 2500 test images) images divided into 6 categories: The images are resized to 256x256 with horizontal mirroring and random cropping to 224x224 for data augmentation. A lot of the experiments were done without cropping but it slightly improves the results for the resnet50_1by2. For now the dataset is limited to two heterosexual performers. But given the success of this method, I plan to expand the number of categories. Due to the nature of the material, I will not be releasing the database itself; only the trained model. Sex front and back are defined by the position of the camera, instead of the orientation of the performers. If the female's body is facing the camera so the front of the vagina is shown, it's sex front. If the female's rear is shown instead, it's sex back. This creates two visually distinct classes. No distinction is made between vaginal and anal intercourse; sex back or sex front could include either. Code licensed under GPLv3, including the trained model. Caffe is licensed under BSD 2. If you have problems, suggestions, or thoughts open an issue or send me email nipplerdeeplearning at gmail.","flair":"four\tProject"}
{"author":"sbt_","created":"Mon Oct 03 10:33:56 EDT 2016","text":"Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset. In Part I and Part II, we saw different holdout and bootstrap techniques for estimating the generalization performance of a model. We learned about the bias-variance trade-off, and we computed the uncertainty of our estimates. In this third part, we will focus on different methods of cross-validation for model evaluation and model selection. We will use these cross-validation techniques to rank models from several hyperparameter configurations and estimate how well they generalize to independent datasets. Previously, we used the holdout method or different flavors of bootstrapping to estimate the generalization performance of our predictive models. We split our dataset into two parts: a training and a test dataset. After the machine learning algorithm fit a model to the training set, we evaluated it on the independent test set that we withheld from the machine learning algorithm during model fitting. While we were discussing challenges such as the bias-variance trade-off, we used fixed hyperparameter settings in our learning algorithms, such as the number of k in the K-nearest neighbors algorithm. We defined hyperparameters as the parameters of the learning algorithm itself, which we have to specify a priori \u2014 before model fitting. In contrast, we refered to the parameters of our resulting model as the model parameters. So, what are hyperparameters, exactly? Considering the k-nearest neighbors algorithm, one example of a hyperparameter is the integer value of k. If we set k=3, the k-nearest neighbors algorithm will predict a class label based on a majority vote among the 3-nearest neighbors in the training set. The distance metric for finding these nearest neighbors is yet another hyperparameter of the algorithm. Now, the k-nearest neighbors algorithm may not be an ideal choice for illustrating the difference between hyperparameters and model parameters, since it is a lazy learner and a nonparametric method. In this context, lazy learning (or instance-based learning) means that there is no training or model fitting stage: A k-nearest neighbors model literally stores or memorizes the training data and uses it only at prediction time. Thus, each training instance represents a parameter in a k-nearest neighbors model. In short, nonparametric models are models that cannot be described by a fixed number of parameters that are being adjusted to the training set. The structure of parametric models is not decided by the training data rather than being set a priori; nonparamtric models do not assume that the data follows certain probability distributions unlike parametric methods (exceptions of nonparametric methods that make such assumptions are Bayesian nonparametric methods). Hence, we may say that nonparametric methods make fewer assumptions about the data than parametric methods. In contrast to k-nearest neighbors, a simple example of a parametric method would be logistic regression, a generalized linear model with a fixed number of model parameters: a weight coefficient for each feature variable in the dataset plus a bias (or intercept) unit. These weight coefficients in logistic regression, the model parameters, are updated by maximizing a log-likelihood function or minimizing the logistic cost. For fitting a model to the training data, a hyperparameter of a logistic regression algorithm could be the number of iterations or passes over the training set (epochs) in a gradient-based optimization. Another example of a hyperparameter would be the value of a regularization parameter such as the lambda-term in L2-regularized logistic regression: Changing the hyperparameter values when running a learning algorithm over a training set may result in different models. The process of finding the best-performing model from a set of models that were produced by different hyperparameter settings is called model selection. In the next section, we will look at an extension to the holdout method that helps us with this selection process. In Part I, we learned that resubstituion validation is a bad approach for estimating of the generalization performance. Since we want to know how well our model generalizes to new data, we used the holdout method to split the dataset into two parts, a training set and an independent test set. Can we use the holdout method for hyperparameter tuning? The answer is \u201Cyes!\u201D However, we have to make a slight modification to our initial approach, the \u201Ctwo-way\u201D split, and split the dataset into three parts: a training, a validation, and a test set. We can regard the process of hyperparameter tuning and model selection as a meta-optimization task. While the learning algorithm optimizes an objective function on the training set (with exception to lazy learners), hyperparameter optimization is yet another task on top of it; here, we typically want to optimize a performance metric such as classification accuracy or the area under a Receiver Operating Characteristic curve. After the tuning stage, selecting a model based on the test set performance seems to be a reasonable approach. However, reusing the test set multiple times would introduce a bias and our final performance estimate and likely result in overly optimistic estimates of the generalization performance \u2014 we can say that \u201Cthe test set leaks information.\u201D To avoid this problem, we could use a three-way split, dividing the dataset into a training, validation, and test dataset. Having a training-validation pair for hyperparameter tuning and model selections allows us to keep the test set \u201Cindependent\u201D for model evaluation. Now, remember our discussion of the \u201C3 goals\u201D of performance estimation? The \u201Cthree-way holdout method\u201D is one way to tackle points 1 and 2 (more on point 3 in the next article, Part IV). Though, if we are only interested in point 2, selecting the best model, and do not care so much about an \u201Cunbiased\u201D estimate of the generalization performance, we could stick to the two-way split for model selection. Thinking back of our discussion about learning curves and pessimistic biases in Part II, we noted that a machine learning algorithm often benefits from more labeled data; the smaller the dataset, the higher the pessimistic bias and the variance \u2014 the sensitivity of our model towards the way we partition the data. \u201CThere ain\u2019t no such thing as a free lunch.\u201D The three-way holdout method for hyperparameter tuning and model selection is not the only \u2014 and certainly often not the best \u2014 way to approach this task. In later sections, we will learn about alternative methods and discuss their advantages and trade-offs. However, before we move on to the probably most popular method for model selection, k-fold cross-validation (or sometimes also called \u201Crotation estimation\u201D in older literature), let us have a look at an illustration of the 3-way split holdout method: Since there\u2019s a lot going on in this figure, let\u2019s walk through it step by step. We start by splitting our dataset into three parts, a training set for model fitting, a validation set for model selection, and a test set for the final evaluation of the selected model. The second step illustrates the hyperparameter tuning stage. We use the learning algorithm with different hyperparameter settings (here: three) to fit models to the training data. Next, we evaluate the performance of our models on the validation set. This step illustrates the model selection stage; after comparing the performance estimates, we choose the hyperparameters settings associated with the best performance. Note that we often merge steps two and three in practice: we fit a model and compute its performance before moving on to the next model in order to avoid keeping all fitted models in memory. As discussed in Part I and Part II, our estimates may suffer from pessimistic bias if the training set is too small. Thus, we can merge the training and validation set after model selection and use the best hyperparameter settings from the previous step to fit a model to this larger dataset. Now, we can use the independent test set to estimate the generalization performance our model. Remember that the purpose of the test set is to simulate new data that the model has not seen before. Re-using this test set may result in an overoptimistic bias in our estimate of the model\u2019s generalization performance. Finally, we can make use of all our data \u2014 merging training and test set \u2014 and fit a model to all data points for real-world use. It\u2019s about time to introduce the probably most common technique for model evaluation and model selection in machine learning practice: k-fold cross-validation. The term cross-validation is used loosely in literature, where practitioners and researchers sometimes refer to the train\/test holdout method as a cross-validation technique. However, it might make more sense to think of cross-validation as a crossing over of training and validation stages in successive rounds. Here, the main idea behind cross-validation is that each sample in our dataset has the opportunity of being tested. K-fold cross-validation is a special case of cross-validation where we iterate over a dataset set k times. In each round, we split the dataset into k parts: one part is used for validation, and the remaining k-1 parts are merged into a training subset for model evaluation as shown in the figure below, which illustrates the process of 5-fold cross-validation: Just as in the \u201Ctwo-way\u201D holdout method, we use a learning algorithm with fixed hyperparameter settings to fit models to the training folds in each iteration \u2014 if we use the k-fold cross-validation method for model evaluation. In 5-fold cross-validation, this procedure will result in five different models fitted; these models were fit to distinct yet partly overlapping training sets and evaluated on non-overlapping validation sets. Eventually, we compute the cross-validation performance as the arithmetic mean over the k performance estimates from the validation sets. We saw the main difference between the \u201Ctwo-way\u201D holdout method and k-fold cross validation: k-fold cross-validation uses all data for training and testing. The idea behind this approach is to reduce the pessimistic bias by using more training data in contrast to setting aside a relatively large portion of the dataset as test data. And in contrast to the repeated holdout method, which we discussed in Part II, test folds in k-fold cross-validation are not overlapping. In repeated holdout, the repeated use of samples for testing results in performance estimates that become dependent between rounds; this dependence can be problematic for statistical comparisons, which we will discuss in Part IV. Also, k-fold cross-validation guarantees that each sample is used for validation in contrast to the repeated holdout-method, where some samples may never be part of the test set. In this section, we introduced k-fold cross-validation for model evaluation. In practice, however, k-fold cross-validation is more commonly used for model selection or algorithm selection. K-fold cross-validation for model selection is a topic that we will cover later in this article, and we will talk about algorithm selection in detail throughout the next article, Part IV. At this point, you may wonder why we chose k=5 to illustrate k-fold cross-validation in the previous section. One reason is that it makes it easier to illustrate k-fold cross-validation compactly. Moreover, k=5 is also a common choice in practice, since it is computationally less expensive compared to larger values of k. If k is too small, though, we may increase the pessimistic bias of our estimate (since less training data is available for model fitting), and the variance of our estimate may increase as well since the model is more sensitive to how we split the data (later, we will discuss experiments that suggest k=10 as a good choice for k). In fact, there are two prominent, special cases of k-fold cross validation: k=2 and k=n. Most literature describes 2-fold cross-validation as being equal to the holdout method. However, this statement would only be true if we perform the holdout method by rotating the training and validation set in two rounds (i.e., using exactly 50% data for training and 50% of the samples for validation in each round, swapping these sets, repeating the training and evaluation procedure, and eventually computing the performance estimate as the arithmetic mean of the two performance estimates on the validation sets). Given how the holdout method is most commonly used though, I like to describe the holdout method and 2-fold cross-validation as two different processes as illustrated in the figure below: Now, if we set k=n, that is, if set the number of folds as being equal to the number of training instances, we refer to the k-fold cross-validation process as Leave-one-out cross-validation (LOOCV). In each iteration during LOOCV, we fit a model to n-1 samples of the dataset and evaluate it on the single, remaining data point. Although this process is computationally expensive, given that we have n iterations, it can be useful for small datasets, cases where withholding data from the training set would be too wasteful. Several studies compared different values of k in k-fold cross-validation, analyzing how the choice of k affects the variance and the bias of the estimate. Unfortunately, there is no Free Lunch though as shown by Yohsua Bengio and Yves Grandvalet in \u201CNo unbiased estimator of the variance of k-fold cross-validation.\u201D However, we may still be interested in finding a \u201Csweet spot,\u201D a value that seems to be a good trade-off between variance and bias in most cases, and we will continue the bias-variance trade-off discussion in the next section. For now, let\u2019s conclude this section by looking at an interesting research project where Hawkins and others compared performance estimates via LOOCV to the holdout method and recommend the LOOCV over the latter \u2014 if computationally feasible. These conclusions are partly based on the experiments carried out in this study using a 469-sample dataset. The following table summarizes the finding in a comparison of different Ridge Regression models: In rows 1-4, Hawkins and others used 100-sample training sets to compare different methods of model evaluation. The first row corresponds to an experiment where the researchers used LOOCV and fit regression models to 100-sample training subsets. The reported \u201Cmean\u201D refers to the averaged difference between the true coefficiants of determination and the coefficients of determination obtained via LOOCV (here called q2) after repeating this procedure on different 100-sample training sets. In rows 2-4, the researchers used the holdout method for fitting models to 100-sample training sets, and they evaluated the performances on holdout sets of sizes 10, 20, and 50 samples. Each experiment was repeated 75 times, and the mean column shows the average difference between the estimated R2 and true R2 values. As we can see, the estimate obtained via LOOCV (q2) is closest to the true R2. The estimates obtained from the 50-test sample holdout method are also passable, though. Based on these particular experiments, I agree with the researchers\u2019 conclusion: One reason why we may prefer the holdout method may be concerns about computational efficiency, if our dataset is sufficiently large. As a rule of thumb, we can say that the pessimistic bias and large variance concerns are less problematic the larger the dataset. Moreover, it is not uncommon to repeat the k-fold cross-validation procedure with different random seeds in hope to obtain a \u201Cmore robust\u201D estimate. For instance, if we repeated a 5-fold cross-validation run 100 times, we would compute the performance estimate for 500 test folds report the cross-validation performance as the arithmetic mean of these 500 folds. (Although this is commonly done in practice, we note that the test folds are now overlapping.) However, there\u2019s no point in repeating LOOCV, since LOOCV always produces the same splits. Based on the experimental evidence that we saw in the previous section, we may prefer LOOCV over single train\/test splits via the holdout method for small and moderately sized datasets. In addition, we can think of the LOOCV estimate as being approximately unbiased: the pessimistic bias of LOOCV (k=n) is intuitively lower compared k<n-fold cross-validation, since almost all (for instance, n-1) training samples are available for model fitting. While LOOCV is almost unbiased, one downside of using LOOCV over k-fold cross-validation with k<n is the large variance of the LOOCV estimate. First, we have to note that LOOCV is defect when using a discontinuous loss-function such as the 0-1 loss in classification or even in continuous loss functions such as the mean-squared-error. Often, it is said that LOOCV These statements are certainly true if we refer to the variance between folds. Remember that if we use the 0-1 loss function (the prediction is either correct or not), we could consider each prediction as a Bernoulli trial, and the number of correct predictions is following a binomial distribution , where ; the variance of a binomial distribution is defined as We can estimate the variability of a statistic (here: the performance of the model) from the variability of that statistic between subsamples. Obviously though, the variance between folds is a poor estimate of the variance of the LOOCV estimate \u2014 the variability due to randomness in our training data. Now, when we are talking about the variance of LOOCV, we typically mean the difference in the results that we would get if we repeated the resampling procedure multiple times on different data samples from the underlying distribution. Thus, a more interesting point has been made by Hastie, Tibshirani, and Friedman: Or in other words, we can attribute the high variance to the well-known fact that the mean of highly correlated variables has a higher variance than the mean of variables that are not highly correlated. Maybe, this can intuitively be explained by looking at the relationship between covariance ( And the relationship between covariance and correlation (X and Y are random variables) is defined as The large variance that is often associated with LOOCV has also been observed in empirical studies \u2014 for example, I really recommend reading the excellent paper A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection (Kohavi, 1995) by Ron Kohavi. Now that we established that LOOCV estimates are generally associated with a large variance and a small bias, how does this method compare to k-fold cross-validation with other choices for k and the bootstrap method? In Part II, we mentioned the pessimistic bias of the standard bootstrap method, where the training set asymptotically (only) contains 0.632 of the samples from the original dataset; 2- or 3-fold cross-validation has about the same problem. We discussed the 0.632 Bootstrap that was designed to address this pessimistic bias issue. However, Kohavi also observed in his experiments (Kohavi, 1995) that the bias in bootstrap was still extremely large for certain real-world datasets (now, optimistically biased) compared to k-fold cross-validation. Eventually, Kohavi\u2019s experiments on various real-world datasets suggest that 10-fold cross-validation offers the best trade-off between bias and variance. Furthermore, other researchers found that repeating k-fold cross-validation can increase the precision of the estimates while still maintaining a small bias (Molinaro and others, 2005; Kim, 2009). Before moving on to model selection, let\u2019s summarize this discussion of the bias-variance trade-off by listing the general trends when increasing the number of folds or k: Previously, we used k-fold cross-validation for model evaluation. Now, we are going to take things further and use the k-fold cross-validation method for model selection. Again, the key idea is to keep an independent test dataset, that we withhold from during training and model selection, to avoid the leaking of test data in the training stage: Although, the figure above might seem somewhat complicated at first glance, the process is quite simple and similar to the \u201Cthree-way holdout\u201D workflow that we discussed at the beginning of this article. Let\u2019s walk through it step by step. Similar to the holdout method, we split our dataset into two parts, a training and an independent test set; we tuck away the test set for the final model evaluation step at the end. In the second step, we can now experiment with various hyperparameter settings; we could use Bayesian Optimization, Randomized Search, or plain old Grid Search. For each hyperparameter configuration, we apply the k-fold cross-validation on the training set, resulting in multiple models and performance estimates. Taking the hyperparameter settings that correspond to the best-performing model, we can then use the complete training set for model fitting. Now it\u2019s time to make use of the independent test set that we withheld; we use this test set to evaluate the model that we obtained from step 3. Finally, when we completed the evaluation stage, we can fit a model to all our data, which could be the model for (the so-called) deployment. When we browse the deep learning literature, we often find that that the 3-way holdout method is the method of choice when it comes to model evaluation; it is also common in older (non-deep learning literature) as well. As mentioned earlier, the three-way holdout may be preferred over k-fold cross-validation since the former is computationally cheap in comparison. Aside from computational efficiency concerns, we only use deep learning algorithms when we have relatively large sample sizes anyway, scenarios where we don\u2019t have to worry about high variance \u2014 due to sensitivity of our estimates towards how we split the dataset for training, validation, and testing \u2014 so much. Now that we discussed model selection in the previous section, let us take a moment and consider the Law of Parsimony aka Occam\u2019s Razor: Or to say it with other words, using one of my favorite quotes: In model selection practice, we can apply Occam\u2019s razor using one-standard error method as follows: Although, we may prefer simpler models for several reasons, Pedro Domingos made a good point regarding the performance of \u201Ccomplex\u201D models. Here\u2019s an excerpt from his his recent article, \u201CTen Myths About Machine Learning:\u201D Again, there are several reasons why we may prefer a simpler model if its performance is within a certain, acceptable range \u2014 for example, using the one-standard error method. Although a simpler model may not be the most \u201Caccurate\u201D one, it may be computationally more efficient, easier to implement, and easier to understand and reason with compared to more complicated alternatives. To see how the one-standard error method works in practice, let us apply it to a simple toy dataset: 300 datapoints, cocentric circles, and a uniform class distribution (150 samples from class 1 and 150 samples from class 2). First, we split the dataset into two parts, 70% training data and 30% test data, using stratification to maintain equal class proportions. The 210 samples from the training dataset are shown below: Say we want to optimize the Gamma hyperparameter of a Support Vector Machine (SVM) with a non-linear Radial Basis Function-kernel (RBF-kernel), where is the free parameter of the Gaussian RBF: (Intuitively, we can think of the Gamma as a parameter that controls the influence of single training samples on the decision boundary.) When I ran the RBF-kernel SVM algorithm with different Gamma values over the training set, using stratified 10-fold cross-validation, I obtained the following performance estimates, where the error bars are the standard errors of the cross-validation estimates: (The code for producing the plots shown in this article can be found in this Jupyter Notebook on GitHub.) We can see that Gamma values between 0.1 and 100 resulted in a prediction accuracy of 80% or more. Furthermore, we can see that results in a fairly complex decision boundary, and results in a decision boundary that is too simple to separate the two classes. In fact, seems like a good trade-off between the two aforementioned models \u2014 the performance of the corresponding model falls within one standard error of the best performing model with or There are many ways for evaluating the generalization performance of predictive models. So far, we have seen the holdout method, different flavors of the bootstrap approach, and k-fold cross-validation. In my opinion, the holdout method is absolutely fine for model evaluation when working with relatively large sample sizes. If we are into hyperparameter tuning, we may prefer 10-fold cross-validation, and Leave-One-Out cross-validation is a good option if we are working with small sample sizes. When it comes to model selection, again, the \u201Cthree-way\u201D holdout method may be a good choice due to computational limitations; a good alternative is k-fold cross-validation with an independent test set. An even better method for model selection or algorithm selection is nested cross-validation, a method that we will discuss in Part IV. In the next part of this series, we will discuss hypothesis tests and methods for algorithm selection in more detail. Say we want to hire a stock market analyst. To find a good stock market analyst, let\u2019s assume we asked our candidates to predict whether certain stock prices go up or down in the next 10 days, prior to the interview. A good candidate should get at least 8 out of these 10 predictions correct. Without having any knowledge about how stocks work, I would say that our probability of correctly predicting the trend each day is 50% \u2014 that\u2019s a coin-flip each day. So, if we just interviewed one coin-flipping candidate, her chance of being right 8 out of 10 times would be 0.0547: In other words, we can say that this candidate\u2019s predictive performance is unlikely due to chance. However, say we didn\u2019t just invite one single interviewee: we invited 100. If we\u2019d asked these 100 interviewers for their predictions. Assuming that no candidate has a clue about how stocks work, and everyone was guessing randomly, the probability that at least one of the candidates got 8 out of 10 predictions correct is: So, shall we assume that a candidate who got 8 out of 10 predictions correct was not simply guessing randomly? We will continue this discussion on hypothesis tests, comparisons between learning algorithms in Part IV.","flair":"null\tnull"}
{"author":"pplonski","created":"Thu Nov 17 05:56:03 EST 2016","text":"This is a bunch of code to port Keras neural network model into pure C++. Neural network weights and architecture are stored in plain text file and input is presented as in case of image. The code is prepared to support simple Convolutional network (from MNIST example) but can be easily extended. There are implemented only ReLU and Softmax activations. It is working regardless the Keras backend.","flair":"four\tProject"}
{"author":"iamaroosterilluzion","created":"Tue Oct 25 19:58:57 EDT 2016","text":"For example, if you're an ML \/ software engineer at an ecommerce company and you're tasked with building a product recommendation engine, what might your software architecture look like?\n\n- Does your data come in from an ETL-like process?\n- Where do you store your data? Postgres, Hadoop, a csv file?\n- How do you manage the training and prediction processes for the model? Do you run them as cron processes or synchronously as new data comes in? Does the model \"live\" on a server?\n- How does the ecommerce app get recommendations from the model? Do you build a REST API on top of the model to serve the recommendations?\n\nAnother example might be a lead scoring engine, would the architecture look completely different or are there a set of best practices?","flair":"one\tDiscusssion"}
{"author":"nasimrahaman","created":"Thu Nov 03 05:04:50 EDT 2016","text":"Inspired by [this](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5asl74\/discussion_discriminator_converging_to_0_loss_in\/) discussion, I imagine it'd be helpful to gather some collective wisdom. If this goes somewhere, we could summarize to a Github FAQ for future reference. ","flair":"one\tDiscussion"}
{"author":"mbierly","created":"Thu Nov 03 11:52:03 EDT 2016","text":"Clinton and Trump are two of the most unpopular presidential nominees in U.S. history. That made us wonder: Would we be better off if a machine wrote their speeches? United States presidential acceptance speeches follow a predictable formula: thank the American people, congratulate the opponent on a hard-fought campaign, and promise good things to come. Are they predictable enough that a machine could learn to produce a presidential acceptance speech of its own? To find out, we assembled a dataset with every presidential acceptance speech from 1960-2012. We trained a long short-term memory (LSTM) Recurrent Neural Network on the dataset using TensorFlow, and asked the model to write an acceptance speech. Below is the result, with some light formatting to aid readability. American community, our volunteers, the new country can feel a nation! Yes, I will do great, parents. My victory tonight is superb. First, thank you. I will concede to our nation on this pavement to America. We have a life, and you the great people, have an enemy. Meet Americans. We say, \u201CAt noon some sacrifice for Washington.\u201D States, the election consensus, you differed all two-hundred. And nation, with the journey is America. In the defenses save no good thing: the best for our country when we can\u2019t, and harmony toward Bob governor. From 4 telephone States, I had not sleeves and humility\u2014 ask the institutions. And Lieberman must thank hundreds of us and not serve Texas. We end in depths and common. The Governor worked his young potential. I live in our day, America. Together, I grow - among all in destiny. Joe would forget God and others\u2014yet seizing a supporter\u2019s time. Tonight, however, the politics are alive. What you in our society need, you never do\u2014 hope, love. All people forward! There are mean New States. Within the fought America, from the voted, its future. \u2026 in its purpose. Yes generations. Together you said the victory of this human\u2013 grateful to serve 18\u2010hour Dan. And I want new Security, founded for peace. And easily we say that she\u2019s always in the Carolina of opportunities. This record expressed the tremendous victory for our country, for being cynical, and honest, and tapped an absence\u2014 to be the best at watching on the phone. My victory today, together, won the office. All the bombs true tasks each have hope. Bush and scorching George, shall reform 4 nation, the matter threatening my daughters. Iraq and all, the same chance for hope. Country nations, for you, magnificent opportunities. A new world\u2019s record without a campaign divided. Forget people that campaigned passions, again, who pay with their families to every United This America. And the problems today, their mortgage, inflation in disagreements too, disappointed they\u2019re not a house of politics. The nation ladies saw their complete world office. We will think through all God until behind. His chance teachers far. Let the great no phone Obama build us radios. Your spending states and Birmingham seize this family. We wish you, warrior girl America, humbled who press and seek her much. Brave American steps, on the harbor of here, have some who worked. Friends\u2026 county! Together with politics\u2014you make my obligations build in our congratulations. It\u2019s the country that I have. Americans started the quarter of dangerous tonight. I will do our command because that\u2019s America. The campaign of joyous government like her chairman parents. I will start pulling the victory troubles under joyous Republicans. All, all to joy, Americans on this President! Tonight with you, summon God\u2019s feel, rather genius wisdom but cleaner, the message to our tolerant pride. From a nation of confidence. Witness my love spoken in America. Towards the storm. If you\u2019re interested in learning more, Max Deutsch created an awesome guide for writing with artificial intelligence.","flair":"four\tProject"}
{"author":"downtownslim","created":"Fri Nov 11 12:03:21 EST 2016","text":" Under review as a conference paper at ICLR 2017 OUT-OF-CLASS NOVELTY GENERATION: AN EXPERI- MENTAL FOUNDATION Mehdi Cherti & Balázs Kégl LAL\/LRI CNRS\/Université Paris-Saclay {mehdi.cherti, balazs.kegl}@gmail.com Akın Kazakçı MINES ParisTech, PSL Research University, CGS-I3 UMR 9217 akin.kazakci@mines-paristech.fr ABSTRACT Recent advances in machine learning have brought the field closer to computa- tional creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hy- potheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architec- tures and hyperparameter combinations which lead to out-of-distribution novelty. 1 INTRODUCTION Recent advances in machine learning have renewed interest in artificial creativity. Studies such as deep dream (Mordvintsev et al., 2015) and style transfer (Gatys et al., 2015) have aroused both general public interest and have given strong impetus to use deep learning models in computational creativity research (ICC, 2016). Although creativity has been a topic of interest on and off through- out the years in machine learning (Schmidhuber, 2009), it has been slowly becoming a legitimate sub-domain with the appearance of dedicated research groups such as Google\u2019s Magenta and re- search work on the topic (Nguyen et al., 2015; Lake et al., 2015). There is a large body of work studying creativity by computational methods. A large variety of tech- niques, from rule-based systems to evolutionary computation has been used for a myriad of research questions. Compared to these methods, machine learning methods provide an important advantage: they enable the study of creativity in relation with knowledge (i.e., knowledge-driven creativity; Kazakçı et al. (2016b)). Nevertheless, to better highlight the points of interest in computational creativity research for the machine learning community and to allow machine learning researchers to provide systematic and rigorous answers to computational creativity problems, it is important to precisely answer three questions: 1. What is meant by the generation of novelty? 2. How can novelty be generated? 3. How can a model generating novelty be evaluated? Within the scope of machine learning, it would be tempting to seek answers to these questions in the sub-field on generative modeling. Mainstream generative modeling assumes that there is a phe- nomena generating the observed data and strive to build a model of that phenomena, which would, for instance, allow generating further observations. Traditional generative modeling considers only in-distribution generation where the goal is to generate objects from the category or categories of 1 https:\/\/magenta.tensorflow.org\/welcome-to-magenta Under review as a conference paper at ICLR 2017 already observed objects. In terms of novelty generation, this can be considered as generating look- a-likes of known types of objects. Although there is considerable value in in-distribution generation (e.g., for super-resolution (Freeman et al., 2002; Dong et al., 2014; Ledig et al., 2016) or in-painting (Xie et al., 2012; Cho, 2013; Yeh et al., 2016)), this perspective is limited from a strict point of view of creativity: it is unlikely to come up with a flying ship by generating samples from a distribution of ships and flying objects. Researchers in creativity research (Runco & Jaeger, 2012) have argued that the crux of creative pro- cess is the ability to build new categories based on already known categories. However, creativity is beyond a simple combination exploration: it is about generating previously unknown but meaningful (or valuable) new types of objects using previously acquired knowledge (Hatchuel & Weil, 2009; Kazakçı, 2014). Under this perspective, novelty generation aims at exhibiting an example from a new type. This objective, which we shall call out-of-distribution generation, is beyond what can be formalized within the framework of traditional learning theory, even though learning existing types is a crucial part of the process. From a machine learning point of view, generating an object from an unknown type is not a well- defined problem, and research in generative modeling usually aims at eliminating this possibility altogether, as this is seen as a source of instability (Goodfellow et al., 2014; Salimans et al., 2016) leading to spurious samples (Bengio et al., 2013). In a way, sampling procedures are designed to kill any possibility of sampling out of the distribution, which is a problem for studying the generation of novelty by machine learning methods. Arguably, the most important problem is the evaluation of what constitutes a good model for gen- erating out-of-distribution. On the one hand, we are seeking to generate meaningful novelty, not trivial noise. On the other hand, we aim at generating unknown objects, so traditional metrics based on the concept of likelihood are of no use since novelty in the out-of-distribution sense is unlikely by definition. This lack of metrics hinders answering the first two questions. Without a clear-cut eval- uation process, the utility of extending the definition of novelty generation to out-of-sample seems pointless. This paper argues that for a wider adoption of novelty generation as a topic for scientific study within machine learning, a new engineering principle is needed, which would enable such evaluation, and consequently, rigorous experimental research. In the traditional supervised context, the main engi- neering design principle is the minimization of the error on a hold-out test set. The paper proposes a simple setup where the generative potential of models can be evaluated by holding out entire classes, simulating thus unknown but meaningful novelty. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time. The main contributions of this paper: \u2022 We design an experimental framework based on hold-out classes to develop and to analyze out-of-distribution generators. \u2022 We review and analyze the most common evaluation techniques from the point of view of measuring out-of-distribution novelty. We argue that likelihood-based techniques inher- ently limit exploration and novelty generation. We carefully select a couple of measures and demonstrate their applicability for out-of-distribution novelty detection in experiments. \u2022 We run a large-scale experimentation to study the ability of novelty generation of a wide set of different autoencoders and GANs. The goal here is to re-evaluate existing architectures under this new goal in order to open up exploration. Since out-of-distribution novelty generation is arguably a wider (and softer) objective than likelihood-driven sampling from a fixed distribution, existing generative algorithms, designed for this latter goal, constitute a small subset of the algorithms able to generate novelty. The goal is to motivate the reopening some of the closed design questions. The paper is organized as follows. We review some of the seminal work at the intersection of machine learning and out-of-distribution generation in Section 2. We discuss the conceptual frame- work of out-of-distribution generation and its relationship with likelihood-based generative models in Section 3. We outline the families of evaluation metrics, focusing on those we use in the paper in Section 4. In Section 4.3 we describe the gist of our experimental setup needed to understand the 2 https:\/\/www.youtube.com\/watch?v=BRmsSEiwwEA Under review as a conference paper at ICLR 2017 metrics described in Section 4.4, designed specifically for the out-of-distribution setup. We describe the details of the experimental setup and analyze our results in Section 5. Finally, we conclude in Section 6. 2 MACHINE LEARNING AND NOVELTY GENERATION: THE INNOVATION ENGINE, \u201CZERO-SHOT\u201D LEARNING, AND DISCOVERING NEW TYPES There are three important papers that consider novelty generation in a machine learning context. Nguyen et al. (2015) propose an innovation engine (Figure 1(a)). They generate images using a neural net that composes synthetic features. The generator is fed back with an entropy-based score (similar to objectness; Section 4.2) coming from an Imagenet classifier, and the feedback is used in an evolutionary optimization loop to drive the generation. An important contribution of the paper is to demonstrate the importance of the objectness score. They show that interesting objects are not generated when asking the machine to generate from a single given class. The generation paths often go through objects from different classes, \u201Cstepping stones\u201D which are seemingly unrelated to the final object. The main conceptual difference between our approaches is that Nguyen et al. (2015) do not ground their generative model in learned knowledge: their generation process is not learned model, rather a stochastic combinatorial engine. On the one hand, this makes the generation (evolutionary optimization) rather slow, and on the other, the resulting objects reflect the style of the (preset) synthetic features rather than features extracted from existing objects. The main goal of Lake et al. (2015) and Rezende et al. (2016) is one-shot learning and generation: learn to classify objects given a small number (often one) of examples coming from a given cate- gory, and learn to generate new objects given a single example (Figure 1(b)). One-shot generation is definitely an intermediate step towards out-of-distribution generation. The extremely low num- ber of examples conceptually limits likelihood-based learning\/fitting\/generation. Lake et al. (2015) circumvents this problem by learning strong Bayesian top-down models (programs) that capture the structural properties of known objects which are generalizable across classes. They also consider unconstrained (\u201Czero-shot\u201D) generation as an extension of their approach, and show that the model can generate new symbols from scratch. They make no attempt to conceptualize the goal of uncon- strained generation outside the top-down Bayesian framework, or to design evaluation metrics to assess the quality of these objects, but their intriguing results are one of the strongest motivations of our paper. Kazakçı et al. (2016a) show that symbols of new types can be generated by carefully tuned au- toencoders, learned entirely bottom-up, without imposing a top-down Bayesian architecture (Fig- ure 1(c)). They also make a first step of defining the conceptual framework of novelty generation by arguing the goal of generating objects from new types, unknown at the time of training. They design a technique for finding these new types semi-automatically (combining clustering and human label- ing). They argue the importance of defining the value of these new types (and of out-of-distribution generation in general), but they make no attempt to design evaluation metrics, thus limiting the exploration and the development of out-of-distribution generative architectures. 3 PROBABILISTIC VS. CONSTRUCTIVE GENERATIVE MODELS The generative process is commonly framed in a probabilistic setup: it is assumed that an un- derlying unknown likelihood model P(·) should first be learned on an i.i.d. training sample D = {x1, . . . ,xn}, assumed to be generated from P(·), and then a sampler S should sample from the learned P̂(·). The first step, estimating P(·) using D, is a classical function learning problem that can be studied through the usual concepts of overfitting and regularization, and algorithms can be designed using the classical train\/test principle. The second step, designing S for sampling from P̂(·) is also a classical domain of random sampling with a conceptual framework and a plethora of methods. Technically both steps are notoriously hard for the high-dimensional distributions and the complex dependencies we encounter in interesting domains. Hence, most of the recent and successful meth- ods get rid of the two-step procedure at the level of algorithmic design, and short-cut the procedure from the probabilistic D → P → S to the constructive D → A, where A(D) is a generator, tasked to produce sample objects similar to elements of D but not identical to them. A is fundamentally 3 Under review as a conference paper at ICLR 2017 (a) \u201CSynthetic\u201D objects from imagenet categories from Figure 7 of Nguyen et al. (2015) (b) \u201CUnconstrained\u201D symbols from Figure 7 of Lake et al. (2015) (c) New types of symbols from Figure 6 of Kazakçı et al. (2016a) Figure 1: Examples of generating new objects or types. different from (P,S) in that there is no explicit fitting of a function, we use D to directly design an algorithm or a program. When the probabilistic setup is still kept for analysis, we face a fundamental problem: if we as- sume that we are given the true likelihood function P(·), the likelihood of the training sample 1 n ∑n i=1 logP(xi) is a random variable drawn independently from the distribution of log-likelihoods of i.i.d. samples of size n, so the trivial generatorA which resamplesD will have the same expected log-likelihood as an optimal i.i.d. sampler. The resampling \u201Cbug\u201D is often referred to as \u201Coverfitting\u201D. While it makes perfect sense to talk about overfitting in theD → P → S paradigm (when P is fitted on D), it is somewhat conceptually misleading when there is no fitting step, we propose to call it \u201Cmemorizing\u201D. When a generator A is trained on D without going through the fitting step D → P , the classical tools for avoiding memorizing (regularization, the train\/test framework) may be either conceptually inadequate or they may not lead to an executable engineering design principle. The conceptual problem of analyzing constructive algorithms in the probabilistic paradigm is not unrelated to our argument of Section 1 that the probabilistic generative framework is too restrictive for studying novelty generation and for designing out-of-distribution generative models. In our view, this flaw is not a minor nuisance which can be fixed by augmenting the likelihood to avoid resam- pling, rather an inherent property which cannot (or rather, should not) be fixed. The probabilistic framework is designed for generating objects from the distribution of known objects, and this is in an axiomatic contradiction with generating out-of-distribution novelty, objects that are unknown at the moment of assembling a training sample. Resampling (generating exact copies) is only the most glaring demonstration of a deeper problem which is also present in a more subtle way when attempting to generate new types of objects. We are not arguing that the probabilistic generative framework should be banished, it has a very important role in numerous use cases. Our argument is that it is not adequate for modeling out-of- distribution novelty generation. What follows from this on the algorithmic level is not revolutionary: the design of most successful generative algorithms already moved beyond the probabilistic frame- work. On the other hand, moving beyond the probabilistic generative framework at a conceptual level is a paradigm change which will require groundwork for laying the foundations, including revisiting ideas from a domain larger than machine learning. At the algorithmic\/computational level the machine learning community has already started to move beyond likelihood. The overfitting problem is often solved by implicitly constrainingA not to resam- ple. Another common solution is to design tractable likelihood surrogates that implicitly penalize memorization. These surrogates then can be used at the training phase (to obtain non-resampling generators explicitly) and\/or in the evaluation phase (to eliminate generators that resample). The ingenious idea of using discriminators in GANs (Goodfellow et al., 2014; Salimans et al., 2016) is a concrete example; although the setup can be analyzed through the lens of probabilistic sam- pling, one does not have to fall back onto this framework. If we drop the underlying conceptual probabilistic framework, the constructive GAN idea may be extended beyond generating from the 4 Under review as a conference paper at ICLR 2017 set which is indistinguishable from the set of existing objects. In Section 4.4 we will use discrim- inators to assess the quality of generators whose very goal is to generate novelty: objects that are distinguishable from existing objects. The main challenge is to avoid the trivial novelty generator, producing uninteresting noise. This challenge is structurally similar to avoiding the trivial memoriz- ing\/resampling generator in in-distribution sampling. The two main elements that contribute to the solution is i) to ground the generator strongly in the structure of existing knowledge, without overly fixating it on existing classes, and ii) use a discriminator which knows about out-of-class novelty to steer architectures towards novelty generation. 4 EVALUATION OF GENERATIVE MODELS In this section we outline the families of evaluation metrics, focusing on those we use in the paper. In Section 4.3 we describe the gist of our experimental setup needed to understand the metrics described in Section 4.4, designed specifically for the out-of-distribution setup. 4.1 INDIRECT SUPERVISED METRICS When generative models are used as part of a pipeline with a supervised goal, the evaluation is based on the evaluation of the full pipeline. Examples include unsupervised pre-training (Hinton et al. (2006); Bengio et al. (2007); the original goal that reinvigorated research in neural nets), semi- supervised learning (Kingma et al., 2014; Rasmus et al., 2015; Maaløe et al., 2016; Salimans et al., 2016), in-painting (Xie et al., 2012; Cho, 2013; Yeh et al., 2016), or super-resolution (Freeman et al., 2002; Dong et al., 2014; Ledig et al., 2016). The design goal becomes straightforward, but the setup is restricted to improving the particular pipeline, and there is no guarantee that those objectives can be transferred between tasks. In our case, the objective of the supervised pipeline may actually sup- press novelty. In a certain sense, GANs also fall into this category: the design goal of the generator is to fool a high-quality discriminator, so the generator is asked not to generate new objects which can be easily discriminated from known objects. In our experiments, surprisingly, we found that GANs can be still tuned to generate out-of-distribution novelty, probably due to the deficiencies of both the generator and the discriminator. Our goal in this paper can also be understood as designing a pipeline that turns novelty generation into a supervised task: that of generating objects from classes unknown at training time. 4.1.1 PARZEN DENSITY ESTIMATOR Parzen density estimators are regularly used for estimating the log-likelihood of a model (Breuleux et al., 2009). A kernel density estimator is fit to generated points, and the model is scored by log- likelihood of a hold-out test set under the kernel density. The metrics can be easily fooled (Theis et al., 2015), nevertheless, we adopted it in this paper for measuring both the in-distribution and out-of-distributions quality of our generators. 4.2 OBJECTNESS Salimans et al. (2016) proposed a new entropy-based metrics to measure the \u201Cobjectness\u201D1 of the generated set of objects. As GANs, the metrics uses a trained discriminator, but unlike GANs, it is not trained for separating real objects and generated objects, rather to classify real objects into existing categories. The goal of the generator is create objects which belong confidently to a low number (typically one) of classes. To penalize generators fixating onto single objects or categories, they also require that the set of objects has a high entropy (different objects span the space of the categories represented by the discriminator). The metrics is only indirectly related to classical log- likelihood: in a sense we measure how likely the objects are through the \u201Ceye\u201D of a discriminator. Formally, objectness is defined as 1 N n∑ i=1 K∑ `=1 pi,` log pi,` p` , 1They also call it \u201Cinception score\u201D but we found the term objectness better as it is more general than the single model used in their paper. 5 Under review as a conference paper at ICLR 2017 where K is the number of classes, pi,` = P(`|xi) is the posterior probability of category ` given the generated object xi, under the discriminator P trained on a set with known labels, and p` = 1 n n∑ i=1 pi,`, are the class marginals. Salimans et al. (2016) proposed this metric as one of the \u201Ctricks\u201D to stabilize GANs, but, interest- ingly, a similar measure was also used in the context if evolutionary novelty generation (Nguyen et al., 2015). 4.3 ASSESSING OUT-OF-DISTRIBUTION NOVELTY BY OUT-OF-CLASS SCORING As the classical supervised validation setup simulates past (training) and future (test) by randomly partitioning an existing data set, we can simulate existing knowledge and novelty by partitioning existing data sets holding out entire classes. The goal of the novelty generator is then to use train- ing classes to build a model that can generate objects from future (hold-out) classes, unknown at training. In our first experiments we tried to leave out single classes of MNIST, but the label noise \u201Cleaked\u201D hold-out classes which made the evaluation tricky. To avoid this, we decided to challenge the generator, trained on MNIST, to generate letters. We pre-trained various discriminators using different setups, only on digits (MNIST), only on letters (Google fonts), or on a mixture of digits and letters, and used these discriminators to evaluate novelty generators in different ways. For example, we measure in-class objectness and in-class Parzen using a discriminator trained on MNIST, and out-of-class objectness and out-of-class Parzen by a discriminator trained on (only) Google fonts. 4.4 OUT-OF-CLASS SCORES Naturally, letter discriminators see letters everywhere. Since letters are all they know, they classify everything into one of the letter classes, quite confidently (this \u201Cblind spot\u201D phenomenon is exploited by Nguyen et al. (2015) for generating \u201Csynthetic\u201D novelty), the letter objectness of an in-distribution digit generator can sometimes be high. For example, a lot of 6s were classified as bs. To avoid this \u201Cbias\u201D, we also trained a discriminator on the union of digits and letters, allowing it to choose digits when it felt that the generated object looked more like a digit. We designed two metrics using this discriminator: out-of-class count measures the frequency of confidently classified letters in a generated set, and out-of-class max is the mean (over the set) of the probability of the most likely letter. None of these metrics penalize \u201Cfixated\u201D generators, outputting the same few letters all the time, so we combine both metrics with the entropy of the letter posterior (conditioned on being a letter). Formally, let pi,1, . . . , pi,Kin be the in-class posteriors and pi,Kin+1, . . . , pi,Kin+Kout be the out-of- class posteriors, where Kin = 10 is the number of in-class classes (digits), and Kout = 26 is the number of out-of-class classes (letters). Let `∗i = argmax ` pi,` and `∗outi = argmax Kin<`≤Kin+Kout pi,` be the most likely category overall and most likely out-of-class category, respectively. Let p̃` = ∑n i=1 I {` = `∗outi}∑n i=1 I {`∗outi > Kin} be the normalized empirical frequency of the out-of-class category `. We measure the diversity of the generated sample by the normalized entropy of the empirical frequencies diversity = − 1 logKout Kin+Kout∑ `=Kin p̃` log p̃`, 6 https:\/\/fonts.google.com\/ Under review as a conference paper at ICLR 2017 and define out-of-class count = (1− λ)× 1 n n∑ i=1 I { `∗i > Kin ∧ pi,`∗i > θ } + λ× diversity, and out-of-class max = (1− λ)× 1 n n∑ i=1 pi,`∗outi + λ× diversity. In our experiments we set the confidence level θ = 0.95 and the mixture coefficient λ = 0.5. 4.5 HUMAN REFEREEING AND THE VISUAL TURING TEST The ultimate test of l\u2019art pour l\u2019art generative models is whether humans like the generated objects. Visual inspection is often used as an evaluation principle in papers (Denton et al., 2015; Radford et al., 2015; Dosovitskiy et al., 2016), and it is sometimes even made part of the objectified pipeline by using crowdsourcing tools (Denton et al., 2015; Lake et al., 2015; Salimans et al., 2016). First, it definitely makes development (e.g., model selection and hyperparameter tuning) slow. Second, the results depend a lot on what questions are asked and how the responders are primed. For testing generative models, the usual GAN-type question to ask is whether the generated objects are gener- ated by a nature (or a human) or a machine (the visual Turing test). Even those that go the furthest in tasking machines to generate novelty (Lake et al., 2015) ask human judges to differentiate between human and machine. In our view, this question is too restrictive when the goal is out-of-distribution novelty generation. Asking whether an object is \u201Cnew\u201D is arguably too vague, but inventing adjective categories (such as \u201Csurprising\u201D or \u201Cinteresting\u201D (Schmidhuber, 2009)) that can poll our ability to detect novelty should be on the research agenda. Priming is another important issue: the answer of a human annotator can depend on the information given to her. Nevertheless, a human annotation tool with well-designed priming and questions could accelerate research in novelty generation in the same way labeling tools and standard labeled benchmark sets accelerated supervised learning. We assessed the visual quality of the set of generated objects using an in-house annotation tool. We took each model which appeared in the top ten by any of the quantitative metrics described in the previous section, and hand-labeled them into one of the following three categories: i) letters, ii) digits, and iii) bad sample (noise or not-a-symbol). Each panel consisted 26 × 15 generated objects, the fifteen most probable symbols of each letter according to the classifier trained on both letters and digits (Figure 2). The goal of this annotation exercise was i) to assess the visual quality of the generated symbols and ii) to assess the quality of the metrics in evaluating novelty. (a) The top autoencoder (b) The top GAN Figure 2: A couple of the top models according to human assessment. Top left characters of each 4 × 4 panel are the labels, letters coming from the training sample. For each letter we display the fifteen most probable symbols according to the classifier trained on both letters and digits. 7 Under review as a conference paper at ICLR 2017 5 EXPERIMENTS Our scores cannot be directly optimized because they all measure out-of-class performance, and showing out-of-class objects at training would be \u201Ccheating\u201D. All our (about 1000) models were trained for \u201Cclassical\u201D objectives: reconstruction error in the case of autoencoders, and adversarial error in the case of GANs. The out-of-class scores were used as a weak feedback for model selection and (quasi random) hyperparameter optimization. The goal is not to be statistically flawless, after all we do not have a statistical model. Rather we set our goal to analyze existing generative architectures from the point of view of novelty generation. Most of the generative models come from a large class of architectures, sometimes purposefully designed for not to \u201Cmisbehave\u201D. When possible, we turned these tricks, designed to avoid generating \u201Cspurious\u201D objects, into optional hyperparameters. 5.1 DETAILED EXPERIMENTAL SETUP We used two families of deep learning based generative models, autoencoders and GANs. The architectures and the optional features are described in the next sections. All hyperparameters were selected randomly using reasonable priors. All the ∼1000 autoencoders were trained on MNIST training data. 5.1.1 AUTOENCODER ARCHITECTURES AND GENERATION PROCEDURE We used three regularization strategies for autoencoders: sparse autoencoders (Makhzani & Frey, 2013; 2015), denoising autoencoders (Bengio et al., 2013) and contractive autoencoders (Rifai et al., 2011). Sparse autoencoders can either be fully connected or convolutional. For fully connected sparse autoencoders, we use the k-sparse formulation from Makhzani & Frey (2013), a simple way of obtaining a sparse representation by sorting hidden units and keeping only the top k%, zeroing out the others, and then backpropagating only through non-zero hidden units. For convolutional sparse architectures, we use the \u201Cwinner take all\u201D (WTA) formulation from Makhzani & Frey (2015) which obtains spatial sparsity in convolutional feature maps by keeping only the maximum activation of each feature map, zeroing out the others. We optionally combine it with channel sparsity which, for each position in the feature maps, keeps only the maximum activation across the channels and zero out the others. For contractive autoencoders, we use the fully connected version with a single hidden layer from Rifai et al. (2011). We also explore mixtures between the different autoencoder variants in the hyperparameter search. For each model we choose to enable or disable independently the denoising training procedure, the contractive criterion (parametrized by the contractive coefficient, see (Rifai et al., 2011)) and the sparsity rate k (only for fully connected architectures). Table 1 shows the hyperparameters and their priors. The generation procedure we use for autoencoders is based on Bengio et al. (2013), who proposed a probabilistic interpretation of denoising autoencoders and a way to sample from them using a Markov chain. To have a convergent procedure and to obtain fixed points, we chose to use a de- terministic generation procedure instead of a Markov chain (Bahdanau & Jaeger, 2014). As in Bahdanau & Jaeger (2014), we found that the procedure converged quickly. In initial experiments we found that 100 iterations were sufficient for the majority of models to have convergence so we chose to fix the maximum number of iterations to 100. We also chose to extend the procedure of Bahdanau & Jaeger (2014) by binarizing (using a threshold) the images after each reconstruction step, as we found that it improved the speed of the convergence and could lead to final samples with an exact zero reconstruction error. For stochastic gradient optimization of the autoencoder models, we used adadelta (Zeiler, 2012) with a learning rate of 0.1 and a batch size of 128. We used rectified linear units as an activation function for hidden layers in all models. We use the sigmoid activation function for output layers. 8 Under review as a conference paper at ICLR 2017 Table 1: Autoencoder hyperparameter priors. Name Prior Type nb layers 1, 2, 3, 4, 5 choice nb fully connected hidden units 100,200,300,...1000 choice nb conv layers 1, 2, 3, 4, 5 choice nb conv filters 8, 16, 32, 64, 128, 256, 512 choice conv layers filter size 3 or 5 choice noise corruption [0, 0.5] uniform k sparsity rate [0, 1] uniform contraction coefficient [0, 100] uniform 5.1.2 GENERATIVE ADVERSARIAL NETWORKS (GANS) For GANs, we built upon Radford et al. (2015) and used their architecture as a basis for hyperparam- eter search. We modified the code proposed here to sample new combinations of hyperparameters. Table 2 shows the hyperparameters and their priors. Name Prior Type nb discr. updates 1, 2, 3 choice l2 coeficient [10−6, 10−1] logspace gen. input dim. 10, 20, 50, 70, 100, 150, 200, 300 choice nb fully connected gen. units 8, 16, 32, 64, 128, 256, 1024, 2048 choice nb fully connected discr. units 8, 16, 32, 64, 128, 256, 1024, 2048 choice nb filters gen. 8, 16, 32, 64, 128, 256, 512 choice nb filters discr. 8, 16, 32, 64, 128, 256, 512 choice nb iterations 50, 100, 150, 200, 250, 300 choice learning rate [10−6, 10−1] on logspace, or 0.0002 logspace weight initialization Normal(0, std) where std is from [10−3, 10−1] logspace Table 2: GAN hyperparameter priors. 5.2 ANALYSIS First, we found that tuning (selecting) generative models for in-distribution generation will make them \u201Cmemorize\u201D the classes they are trained to sample from. This is of course not surprising, but it is important to note because it means that out-of-class generation is non-trivial, and the vast majority of architectures designed and tuned in the literature are not generating out-of-class novelty naturally. Second, we did succeed to find architectures and hyperparameter combinations which lead to out- of-class novelty. Most of the generated objects, of course, were neither digits nor letters (Figure 3), which is why we needed the \u201Csupervising\u201D discriminators to find letter-like objects among them. The point is not that all new symbols are letters, that would arguably be an impossible task, but to demonstrate that by opening up the range of generated objects, we do not generate noise, rather objects that can be forming new categories. Figure 3: A random selection of symbols generated by one of our best sparse autoencoder, the same as the one that generated the letters in Figure 4(b). 9 https:\/\/github.com\/Newmu\/dcgan_code\/tree\/master\/mnist Under review as a conference paper at ICLR 2017 inter-score correlations human counts oc om oo op ic im io ip out in bad out count 1 -0.03 -0.13 0.04 -0.12 0.02 -0.07 -0.11 12 0 8 out max -0.03 1 -0.07 0.01 -0.16 -0.10 0.03 -0.09 15 0 5 out objectness -0.13 -0.07 1 0.21 -0.06 0.08 0.02 -0.08 9 10 1 out Parzen 0.04 0.01 0.21 1 -0.17 0.01 -0.19 -0.20 4 13 3 in count -0.12 -0.16 -0.06 -0.17 1 0.30 0.1 0.14 - - - in max 0.02 -0.10 0.08 0.01 0.30 1 0.03 0.06 - - - in objectness -0.07 0.03 0.02 -0.19 0.1 0.03 1 0.00 - - - in Parzen -0.11 -0.09 -0.08 -0.20 0.14 0.06 0.00 1 0 17 3 Table 3: Inter-score correlations among top 10% models per score and human annotation counts among top twenty models per score. out=letters; in=digits. The quantitative goal of this study was to assess the quality of the defined metrics in evaluating out- of-distribution generators. We proceeded in the following way. We selected the top ten autoencoders and GANs according to the five metrics of out-of-class (letters) count, out-of-class max, out-of- class objectness, out-of-class Parzen, and in-class Parzen. We then annotated these models into one of the three categories of \u201Cletter\u201D (out), \u201Cdigit\u201D (in), and \u201Cbad\u201D (noise or not-a-symbol). The last three columns of Table 3 show that the out-of-class count and out-of-class max scores work well in selecting good out-of-class generators, especially with respect to in-class generators. They are relatively bad in selecting good generators overall. Symmetrically, out-of-class objectness and the Parzen measures select, with high accuracy, good quality models, but they mix out-of-class and in-class generators (digits and letters). Parzen scores are especially bad at picking good out- of-class generators. Somewhat surprisingly, even out-of-class Parzen is picking digits, probably because in-distribution digit generators generate more regular, less noisy images than out-of-class letter generators. In other words, opening the space towards non-digit like \u201Cspurious\u201D symbols come at a price of generating less clean symbols which are farther from letters (in a Parzen sense) than clean digits. We also computed the inter-score correlations in the following way. We first selected the top 10% models for each score because we were after the correlation of the best-performing models . Then we computed the Spearman rank correlation of the scores (so we did not have to deal with different scales and distributions). The first eight columns of Table 3 show that i) in-class and out-of-class measures are anti-correlated, ii) out-of-class count and max are uncorrelated, and are somewhat anti-correlated with out-of-class objectness. These results suggest that the best strategy is to use out-of-class objectness for selecting good quality models and out-of-class count and max to select models which generate letters. Figure 4 illustrates the results by pangrams (sentences containing all letters) written using the generated symbols. The models (a)-(d) were selected automatically: these were the four models that appeared in the top ten both according to out-of-class objectness and out-of-class counts. Letters of the last sentence (e) were hand-picked by us from letters generated by several top models. Among the four models, three were fully connected autoencoders with sparsity and one was a GAN. All of the three sparse autoencoders had five hidden layers and used a small noise corruption (less than 0.1). The GAN used the default learning rate of 0.0002 and a large number (2048) of fully connected hidden units for the generator, while the number of fully connected hidden units of the discriminator was significantly smaller (128). (a) (b) (c) (d) (e) Figure 4: Pangrams created (a-d) using top models selected automatically, and (e) using letters selected from several models by a human. 10 Under review as a conference paper at ICLR 2017 6 DISCUSSION AND PERSPECTIVES In this paper we have proposed a framework for designing and analysing generative models for novelty generation. The quantitative measures make it possible to systematically study the creative capacity of generative models. We believe that human evaluation will remain an important source of feedback in this domain for the foreseeable future. Nevertheless, quantitative measures, such as our out-of-class objectness and out-of-class count and max, will i) make it possible to semi-automate the search for models that exhibit creativity, and ii) allow us to study, from the point of view of novelty generation, the numerous surrogates used for evaluating generative models (Theis et al., 2015), especially those that explicitly aim at quantifying creativity or interestingness (Schmidhuber, 2009). The main focus of this paper was setting up the experimental pipeline and to analyze various quality metrics, designed to measure out-of-distribution novelty of samples and generative models. The immediate next goal is to analyze the models in a systematic way, to understand what makes them \u201Cmemorizing\u201D classes and what makes them opening up to generate valuable out-of-distribution samples. REFERENCES Dzmitry Bahdanau and Herbert Jaeger. Smart decisions by small adjust-ments: Iterating denoising autoencoders. Technical report, Technical Report 32, Jacobs University, School of Engineering and Science, 2014. Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In B. Schölkopf, J. C. Platt, and T. Hoffman (eds.), Advances in Neural Infor- mation Processing Systems 19, pp. 153\u2013160. MIT Press, 2007. URL http:\/\/papers.nips. cc\/paper\/3048-greedy-layer-wise-training-of-deep-networks.pdf. Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899\u2013907, 2013. Olivier Breuleux, Yoshua Bengio, and Pascal Vincent. Unlearning for better mixing. Universite de Montreal\/DIRO, 2009. Kyunghyun Cho. Boltzmann machines and denoising autoencoders for image denoising. arXiv preprint arXiv:1301.3468, 2013. Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a lapla- cian pyramid of adversarial networks. In Advances in Neural Information Processing Systems, pp. 1486\u20131494, 2015. Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. arXiv preprint arXiv:1501.00092, 2014. Alexey Dosovitskiy, Jost Springenberg, Maxim Tatarchenko, and Thomas Brox. Learning to gener- ate chairs, tables and cars with convolutional networks. 2016. William T Freeman, Thouis R Jones, and Egon C Pasztor. Example-based super-resolution. IEEE Computer graphics and Applications, 22(2):56\u201365, 2002. Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor- mation Processing Systems, pp. 2672\u20132680, 2014. Armand Hatchuel and Benoit Weil. Ck design theory: an advanced formulation. Research in engi- neering design, 19(4):181\u2013192, 2009. Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u20131554, 2006. 11 http:\/\/papers.nips.cc\/paper\/3048-greedy-layer-wise-training-of-deep-networks.pdf http:\/\/papers.nips.cc\/paper\/3048-greedy-layer-wise-training-of-deep-networks.pdf Under review as a conference paper at ICLR 2017 Proceedings of the International Conference on Computational Creativity, 2016. ICCC. A. Kazakçı, M. Cherti, and B. Kégl. Digits that are not: Generating new types through deep neural nets. In Proceedings of the International Conference on Computational Creativity, 2016a. Akın Kazakçı. Conceptive artificial intelligence: Insights from design theory. In International Design Conference DESIGN2014, pp. 1\u201316, 2014. Akın Kazakçı, Mehdi Cherti, and Balázs Kégl. Digits that are not: Generating new types through deep neural nets. In Proceedings of the Seventh International Conference on Computational Creativity, 2016b. Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581\u20133589, 2014. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Aitken, Alykhan Tejani, Jo- hannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802, 2016. Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep gen- erative models. arXiv preprint arXiv:1602.05473, 2016. Alireza Makhzani and Brendan Frey. k-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013. Alireza Makhzani and Brendan J Frey. Winner-take-all autoencoders. In Advances in Neural Infor- mation Processing Systems, pp. 2773\u20132781, 2015. Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. Google Research Blog. Retrieved June, 20, 2015. Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Innovation engines: Automated creativity and improved stochastic optimization via deep learning. In Proceedings of the 2015 on Genetic and Evolutionary Computation Conference, pp. 959\u2013966. ACM, 2015. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi- supervised learning with ladder networks. In Advances in Neural Information Processing Systems, pp. 3532\u20133540, 2015. Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One- shot generalization in deep generative models. In ICML, 2016. Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 833\u2013840, 2011. Mark A Runco and Garrett J Jaeger. The standard definition of creativity. Creativity Research Journal, 24(1):92\u201396, 2012. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. arXiv preprint arXiv:1606.03498, 2016. Jürgen Schmidhuber. Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes, pp. 48\u201376. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. ISBN 978-3-642-02565-5. doi: 10.1007\/978-3-642-02565-5 4. URL http:\/\/dx.doi.org\/ 10.1007\/978-3-642-02565-5_4. 12 http:\/\/dx.doi.org\/10.1007\/978-3-642-02565-5_4 http:\/\/dx.doi.org\/10.1007\/978-3-642-02565-5_4 Under review as a conference paper at ICLR 2017 Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015. Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural net- works. In Advances in Neural Information Processing Systems, pp. 341\u2013349, 2012. Raymond Yeh, Chen Chen, Teck Yian Lim, Mark Hasegawa-Johnson, and Minh N Do. Semantic image inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539, 2016. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. 13 Introduction Machine learning and novelty generation: the innovation engine, ``zero-shot'' learning, and discovering new types Probabilistic vs. constructive generative models Evaluation of generative models Indirect supervised metrics Parzen density estimator Objectness Assessing out-of-distribution novelty by out-of-class scoring Out-of-class scores Human refereeing and the visual Turing test Experiments Detailed experimental setup Autoencoder architectures and generation procedure Generative adversarial networks (GANs) Analysis Discussion and perspectives ","flair":"three\tResearch"}
{"author":"a_endurance","created":"Thu Nov 17 08:18:43 EST 2016","text":"Tensorflow implementation of model discussed in the following paper: Low-effort place recognition with WiFi fingerprints using deep learning Python 2.7 is used during development and following libraries are required to run the code provided in the notebook: The UJIIndoorLoc dataset used for model training and testing, can be downloaded from the following [link]. Note: If you see mistakes or want to suggest changes, please submit a pull request.","flair":"four\tProject"}
{"author":"Kiuhnm","created":"Sat Oct 08 08:30:10 EDT 2016","text":"I read [this article](http:\/\/blog.mrtz.org\/2015\/03\/09\/competition.html) but I really don't get it. (Yes, I know what Differential Privacy is.) The author says:\n\n&gt; In this post, I will describe a method to climb the public leaderboard without even looking at the data. The algorithm is so simple and natural that an unwitting analyst might just run it.\n\nFirst of all, I don't see the point in climbing the public leaderboard by cheating. Anyway, if you want to discourage it, why not simply indicate the number of submissions? [edit: One could use fake accounts, I guess.]\n\nSecond, why use \"wacky boosting\" when the labels are fixed?! Couldn't we just use some sort of deterministic searching? For instance, we could change a label or a small group of labels at a time and see how the score change.\n\nThe author seems quite excited about it so maybe I'm missing something here. I couldn't contact the author so I decided to ask here.","flair":"null\tnull"}
{"author":"bagelorder","created":"Wed Sep 28 08:56:24 EDT 2016","text":"I am more used that linear maps are written as \nWx = y\nDoes it have a reason that the Theano tutorials consistently use xW = y? Is this common practice in the area of neural-networks?","flair":"null\tnull"}
{"author":"downtownslim","created":"Sun Nov 06 15:12:17 EST 2016","text":" Under review as a conference paper at ICLR 2017 OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Noam Shazeer1, Azalia Mirhoseini∗\u20201, Krzysztof Maziarz∗2, Andy Davis1, Quoc Le1 and Jeff Dean1 1Google Brain, {noam,azalia,andydavis,qvl,jeff}@google.com 2Jagiellonian University, Cracow, krzysztof.maziarz@student.uj.edu.pl ABSTRACT The capacity of a neural network to absorb information is limited by its number of parameters. In this work, we present a new kind of layer, the Sparsely-Gated Mixture-of-Experts (MoE), which can be used to effectively increase model ca- pacity with only a modest increase in computation. This layer consists of up to tens of thousands of feed-forward sub-networks (experts) containing a total of up to tens of billions of parameters. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the task of language modeling, where model capacity is critical for ab- sorbing the vast quantities of world knowledge available in the training corpora. We present new language model architectures where an MoE layer is inserted between stacked LSTMs, resulting in models with orders of magnitude more pa- rameters than would otherwise be feasible. On language modeling and machine translation benchmarks, we achieve comparable or better results than state-of-the- art at lower computational cost, including test perplexity of 29.9 on the 1 Billion Word Language Modeling Benchmark and BLEU scores of 40.56 and 26.03 on the WMT\u201914 En to Fr and En to De datasets respectively. 1 INTRODUCTION The exponential growth in training data as well as the increasing size and complexity of neural net- works have been persistent trends in machine learning. It is evident that realizing the true potential of larger datasets demands training models with larger numbers of parameters. For typical deep learning models, where the entire model is activated for every example, This leads to a quadratic blow-up in training costs, as both the model size and the number of training examples increase. Unfortunately, the advances in computing power and distributed computation fall short of meeting such demand. To this end, it is imperative to design new scalable techniques that enable training ever larger models on ever larger datasets. In this work we present a new technique that enables training extremely large models without in- creasing computation. Our approach is to increase the number of parameters by introducing a new type of neural network layer: a Sparsely-Gated Mixture-of-Experts (MoE). The MoE layer consists of a number of experts, each being a neural network model, and a trainable gating network which selects a sparse combination of the experts to process each input (see Figure 1). The sparse gating allows us to make the layer extremely large (billions of parameters), while keeping computation per example manageable (millions of FLOPs) and maintaining large enough batch sizes to allow for efficient GPU matrix multiplication. While the concept of a mixture of experts has existed for some time, e.g., (Jacobs et al., 1991), our work advances prior art in several key aspects: We introduce the sparsity in the gating network which allows us increase the number of parameters while keeping the computation constant. We also pose MoEs as a general purpose neural network component which can be applied in a feed- forward, recurrent or convolutional manner. In addition, we develop novel system design practices that enable scaleable and parallel training of MoE layers with thousands of experts. ∗Equally major contributors \u2020Work done as a member of the Google Brain Residency program (g.co\/brainresidency) 1 Under review as a conference paper at ICLR 2017 Figure 1: A Mixture of Experts (MoE) layer embedded within a language model. In this case, the sparse gating function selects two experts to perform computations. Their outputs are modulated by the outputs of the gating network. While the introduced technique is generic, in this paper we focus on language modeling tasks, which are known to benefit from very large models. In particular, we apply a MoE convolutionally between stacked LSTM layers in a language model as in Figure 1. The MoE is called once for each position in the text, selecting a different combination of experts at each position. The different experts tend to become highly specialized based on syntax and semantics. We applied our approach to Language Modeling (LM) and Neural Machine Translation (NMT) tasks. We trained models with up to 30 billion parameters, one or two orders of magnitude larger than models found in the literature. This allowed us to match and exceed best published results at a fraction of the computational cost. Language Modeling: We experimented on the 1 Billion Word Language Model Benchmark (Chelba et al., 2013). On this dataset, we achieved a test perplexity of 29.9, improving on the best published result for a single model (Jozefowicz et al., 2016) while speeding up training by a factor of 8. Machine Translation: On the WMT\u201914 En to Fr dataset, we achieved a BLEU score of 40.56 which is the highest reported value on this dataset. On the WMT\u201914 En to De dataset, we achieved the BLEU score of 26.03, also the highest value on this dataset. Both results were achieved while training the model for the same amount of time as (Wu et al., 2016). We were also able to improve the state-of-the-art by 1.0 BLEU score on an internal translation dataset that is 2 orders of magnitude larger than the WMT\u201914. This was done by training a model with 35 times more parameters than what is reported in (Wu et al., 2016) while speeding up training time by a factor of 6. These results are particularly significant as our baseline is highly optimized. 2 RELATED WORK Exploiting scale in both the number of available training examples and the model sizes has been central to the success of deep learning. When datasets are sufficiently large, increasing the size of neural networks can give much better prediction accuracy. This has been shown in domains such as text (Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), images (Krizhevsky et al., 2012; Le et al., 2012), and audio (Hinton et al., 2012; Amodei et al., 2015). Thus, new mechanisms should be designed to enable scalable training of very large models on large datasets. Ever since its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994), the mixture-of-experts approach has been the subject of much research. Prior work has focused on different aspects including using different types of expert models such as SVMs (Collobert et al., 2 Under review as a conference paper at ICLR 2017 2002), Gaussian Processes (Tresp, 2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009), or different expert configurations such as a hierarchical struc- ture (Yao et al., 2009), and infinite number of experts (Rasmussen & Ghahramani, 2002). Eigen et al. (2013) extends MoE to a deep model by stacking two layers of mixture of experts (with each expert being a feed forward network) followed by a Softmax layer. The experts output are weighted by a trainable gating network. Garmash & Monz (2016) suggests an ensemble model in the for- mat of mixture of experts for machine translation. The gating network is trained on a pre-trained ensemble NMT model. The key difference between our work and prior research on MoE is that our work enables training extremely large number of experts (e.g., thousands) and parameters (e.g., multiple billions). This is possible by a trainable gating network that assigns the inputs to only a sparse number of experts. We regard our MoE as a new type of neural network layer that can be used to increase the capacity of any conventional deep learning model in a computationally tractable manner. Our experts and the gating function are learnable feed forward networks. In our experiments, we create models with tens of billions of parameters whose training time is comparable or even better than that of previous state-of-the-art models that have only millions of parameters. We show that such dramatic increase in parameter size effectively results in new state-of-the-art inference accuracy in language modeling and translation tasks. 3 THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER The Mixture-of-Experts (MoE) layer consists of a set of n \u201Cexpert networks\" E1, · · · , En, and a \u201Cgating network\" G whose output is a sparse n-dimensional vector. Figure 1 shows an overview of the MoE module. The experts are themselves neural networks, each with their own parameters. Although in principle we only require that the experts accept the same sized inputs and produce the same-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where the models are feed-forward networks with identical architectures, but with separate parameters. Let us denote by G(x) and Ei(x) the output of the gating network and the output of the i-th expert network for a given input x. The output y of the MoE unit can be written as follows: y = n∑ i=1 G(x)iEi(x) (1) We save computation based on the sparsity of the output of G(x). Wherever G(x)i = 0, we need not compute Ei(x). In our experiments, we have up to thousands of experts, but only need to evaluate a handful of them for every example. 3.1 GATING NETWORK Softmax Gating: A simple choice of non-sparse gating function (Jordan & Jacobs, 1994) is to multiply the input by a trainable weight matrix Wg and then apply the Softmax function. Gσ(x) = Softmax(x ·Wg) (2) Sparse Gating: To obtain a sparse gating vector, we multiply Gσ(x) component-wise with a sparse mask M(Gσ(x)) and normalize the output. The mask itself is a function of Gσ(x) and specifies which experts are assigned to each input example: G(x)i = Gσ(x)iM(Gσ(x))i∑n j=1 Gσ(x)jM(Gσ(x))j (3) 3 Under review as a conference paper at ICLR 2017 Top-K Mask: One choice of mask is to keep only the top k experts per example, for some small constant k such as 2. In the equation above, we let M(v) = TopK(v, k), where: TopK(v, k)i = { 1 if vi is in the top k elements of v. 0 otherwise. (4) Other Mask Functions: One can imagine experimenting with other choices of randomized and deterministic mask functions. In our larger distributed training setups, using a strictly balanced load helps ensure that we keep our computational devices across different machines equally busy. In Appendix A, we discuss an alternative mask function that enforces this property. 3.2 ENSURING EXPERT UTILIZATION We have observed that the gating network tends to converge to a state where it always produces large weights for same few experts. This imbalance is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more by the gating network. We address this problem by imposing an additional training loss to encourage the experts to be used equally across each training batch. This loss is added to the overall loss function for the model. For each batch X of inputs, we compute the `2 loss on the difference between the average output of the gating network across the batch and the uniform distribution: Lbalance(X) = 1 2 n∑ i=1 (∑ x∈X G(x)i |X| − 1 n )2 (5) 3.3 HIERACHICAL MIXTURE OF EXPERTS If the number of experts is very large, we can reduce the branching factor by using a two-level hierar- chical MoE. In a hierarchical MoE a primary gating network chooses a sparse weighted combination of \u201Cexperts\", each of which is itself a secondary mixture-of-experts with its own gating network. We have not found the need for deeper hierarchies. 4 ADDRESSING MOE SYSTEM PERFORMANCE ISSUES A major challenge that arises from a naive MoE implementation is the shrinking batch problem. If the gating network chooses k out of n experts for each example, then for a batch of b examples, each expert receives a much smaller batch of approximately kbn � b examples. The shrinking batch problem causes computational inefficiency, as computational devices such as CPUs or GPUs tend to benefit from the increased ratio between the number of arithmetic operations and memory accesses which happens when large batch sizes are used. Our solutions to the shrinking batch problem involve finding ways to increase the original batch size, which we discuss in the following. Combination Across Time Steps: In our language models, we apply the MoE to each time step of the previous layer. If we wait for the previous layer to finish, we can apply the MoE to all the time steps together as one big batch, as shown in Figure 2. Doing so increases the size of the input batch to the MoE layer by a factor of the number of time steps, which we denote by ns.1 Combination Across Batches: In addition to combination across time steps, we employ a second technique which combines examples from different training batches, as shown in Figure 2-Right. In a conventional distributed training setting, multiple copies of the model on different devices process distinct batches of data, and parameter updates are performed asynchronously by a parameter server. In our technique, these different batches run synchronously. We distribute the standard layers of the model and the gating network according to the conventional data-parallel schemes but only keep one shared copy of each expert. Each expert in the MoE layer receives a combined batch consisting of 1We could not use this trick if we were applying the MoE recurrently between time steps, which might be a direction of future work. 4 Under review as a conference paper at ICLR 2017 Figure 2: Left: Concatenate outputs of many time steps to form bigger expert batches. Right: Concatenate many minibatches from distributed data parallel models to form expert batches. the relevant examples from all of the data-parallel input batches. If the model is distributed over np devices, and each device receives a batch of size b, each expert receives a batch of approximately k·b·ns·np n examples. Thus, we achieve a factor of np× improvement in expert batch size. In the case of a hierarchical MoE (Section 3.3), the primary gating network employs data parallelism, secondary MoEs employ model parallelism, each one residing on its own device. 5 EXPERIMENTS In the following, we present our experimental results on scaling up neural language model parameter sizes. We will show how these large models improve baselines in language modeling and neural machine translation. For language modeling, we tested our method on the 1 Billion Word Language Model Benchmark (Chelba et al., 2013). We improved on the best published result for a single, word-level model (Jozefowicz et al., 2016), using a fraction of the computation. Table 1: Summary of our results against state-of-the-art. For more details see Section 5.1 Test #Parameters Computation per Word Training Time Perplexity excluding embedding excluding output and output layers layer Ours 29.9 34431 million 58 million 15 hours on 128 k40s Best Published Results 30.6 151 million 151 million 3 weeks on 32 k40s For machine translation, we experimented with the WMT\u201914 En→Fr and En→De corpora, which have 36M sentence pairs and 5M sentence pairs, respectively. On these translation datasets, we compared our method against recently published Neural Machine Translation baselines in (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016). We also experimented the performance of MoEs on Google\u2019s internal translation data for English to French language pair, which is two orders of magnitudes larger than the WMT corpora (Wu et al., 2016). A summary of our BLEU score results, computation, and model parameter sizes against state-of-the-art is shown in Table 2. All our results are based on a single model. We achieved our results by significantly increasing the model parameter sizes while incurring far less computation per word. Table 2: Summary of our results against state-of-the-art. For more details, see Tables 4, 5, and 7. WMT WMT Production Total Computation En→Fr En→De En→Fr #Parameters per Word Ours 40.56 26.03 36.57 8690 million 100 million Best Published Results 39.92 24.91 35.56 250 million 215 million 5.1 1 BILLION WORD LANGUAGE MODEL BENCHMARK Dataset: We experimented with our method on the 1 Billion Word Language Model Benchmark, introduced by (Chelba et al., 2013). The dataset contains approximately 829 million words with a vocabulary of 793,471 words, including sentence boundary markers. All the sentences are shuffled and the duplicates are removed. The words that are out of vocabulary are replaced by a special UNK token, which constitutes 0.3% of the dataset. The test data consists of 159,658 examples. 5 Under review as a conference paper at ICLR 2017 5.1.1 SMALL MODELS We investigated the effect of varying the number of experts while maintaining a very low compu- tational budget: about 6 million multiply-and-adds per word excluding the output layer, or about 4% of the per-word computation used in the state-of-the art models described in (Jozefowicz et al., 2016). Model Architecture: Our models consist of 2 recurrent Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997; Gers et al., 2000) layers, with a MoE layer between them. We added residual connections between layers to encourage gradient flow (He et al., 2015). The LSTM layers each contain 512 cells, and the experts in the MoE are each simple 512x512 matrices with no non-linearity. We varied the number of experts between models, using ordinary MoE layers with 8, 64 and 512 experts and hierarchical MoE layers with 512, 4096 and 32768 experts. For the hierar- chical MoE layers, the first level branching factor was 32. We used the Batchwise Mask described in Appendix A. The average number of experts used per word was k = 8. For the hierarchical MoE models, we used k = 2 in the primary gating network and k = 4 in the secondary gating networks. As a computation-matched baseline, we also trained a model where the MoE layer was replaced with a third LSTM. Training: The models were trained on a cluster of 32 K40 GPUs using the synchronous method described in Section 4. Each batch consisted of a set of sentences totaling roughly 220,000 words. We trained for 50,000 steps, or about 14 epochs. The step times on the order of 1 second, so training took less than a day. We used the Adam optimizer (Kingma & Ba, 2015). The learning rate was increased linearly for the first 200 training steps, held constant for the next 200 steps, and decreased after that so as to be proportional to the inverse square root of the step number. The Softmax output layer was trained efficiently using importance sampling similarly to the models in (Jozefowicz et al., 2016). We used a dropout rate of 0.2 between all layers during training. Results: We evaluated our model using perplexity on the holdout dataset, used by (Chelba et al., 2013; Jozefowicz et al., 2016). We follow the standard procedure and sum over all the words in- cluding the end of sentence symbol. We report the best measured test perplexity in Table 3. For each model, we report the test perplexity, the number of parameters, and the amount of computation (measured by the number of multiply-and-adds per word in the forward pass excluding the output layer). Test perplexity continued to improve through training step 50,000 on all models except for the largest one (32768 experts), which began to degrade after step 32,000 due to over-fitting. For that model, we report the best test perpleixty. The 8-expert model achieved test perplexity of 50.2, similar to the computation-matched baseline model. Test perplexity improved as much as 26% as the number of experts increased, down to 36.8 with 32768 experts. There was no noticeable difference between the hierarchical and flat MoEs at 512 experts. Table 3: Perplexity comparison of our method against previous state-of-art models on 1 Billion Word Language Model Benchmark. Model Test Computation per Word #Parameters Total Perplexity excluding output layer excluding embedding #Parameters (millions) and output layers (billions) (millions) LSTM-1024-512 (Jozefowicz et al., 2016) 48.2 4.7 4.7 0.8 LSTM-2048-512 (ibid.) 43.7 9.4 9.4 0.8 LSTM-8192-2048 (ibid.) 32.2 151 151 3.3 2 x LSTM-8192-1024 (ibid.) 30.6 151 151 1.8 2 x LSTM-8192-1024 + CNN inputs (ibid.) 30.0 151 151 1.0 3 x LSTM-512 50.5 6.3 6.3 0.8 2 x LSTM-512, 8 Experts 50.2 6.3 6.3 0.8 2 x LSTM-512, 64 Experts 48.4 6.3 21.0 0.8 2 x LSTM-512, 512 Experts (flat MoE) 41.9 6.6 138.7 1.0 2 x LSTM-512, 512 Experts (hierarchical MoE) 41.3 6.3 138.7 1.0 2 x LSTM-512, 4096 Experts 37.5 6.4 1080.0 1.9 2 x LSTM-512, 32768 Experts 36.8 7.4 8610.9 9.4 3 x LSTM-2048-1024 35.1 56.6 56.6 1.7 2 x LSTM-2048-1024, 32768 Experts 29.9 55.7 34431.1 36.1 6 Under review as a conference paper at ICLR 2017 5.1.2 BIG MODEL We ran one additional model with larger LSTMs (2048 units, with a 1024-dimensional projection), and a larger hierarchical MoE, consisting of 32768 experts in a 128x256 hierarchy. Each expert consisted of a 1024x1024 weight matrix. We used k = 3 for the primary gating network and k = 6 for the secondary gating networks, so on average 18 experts were used per word. We trained the model synchronously on a cluster of 128 k40 GPUs. Each training batch consisted of sentences totaling about 900,000 words. Step times were about 4 seconds. A dropout rate of 0.3 was applied between layers during training. The model was trained for a total of 13300 steps, during which time test perplexity continued to improve. The final test perplexity was 29.9, better than the best single word-level model reported in the literature (Jozefowicz et al., 2016). Our model was also much faster to train (15 hours on 128 k40s vs. 3 weeks on 32 k40s). This is in large part due to the fact that the published model needs to employ much larger LSTMs to achieve the same quality as the MoE model, incurring roughly 2.5 times as much computation per training example. As with the small models, we also trained a computation-matched baseline model with 3 LSTMs according to a similar regimen. The results are reported at the bottom of Table 3. 5.2 MACHINE TRANSLATION ON WMT\u201914 EN→ FR AND EN→ DE Dataset: We benchmarked our method on the WMT\u201914 En→Fr and En→De corpora, where the training sets have 36M sentence pairs and 5M sentence pairs, respectively. The experimental pro- tocols were also similar to those in (Wu et al., 2016): newstest2014 was used as the test set to compare against previous work (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), while the combination of newstest2012 and newstest2013 was used as the development set. Model Architecture: Our model was a modified version of the GNMT model described in (Wu et al., 2016). To reduce computation, we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We inserted MoE layers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). We used an attention mechanism between the encoder and decoder, with the first decoder LSTM receiving output from and providing input for the attention. 2 All of the layers in our model have input and output dimensionality of 512. Our LSTM layers have 2048 hidden units, with a 512-dimensional output projection. We added residual connections around all LSTM and MoE layers to encourage gradient flow (He et al., 2015). Similar to GNMT, to effectively deal with rare words, we used sub-word units (also known as \u201Cwordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our system. We used a model with a shared source and target vocabulary of 32K wordpieces. We also used the same beam search technique as proposed in (Wu et al., 2016). Each MoE layer in our model is composed of up to 2048 experts. Each expert in the MoE layer is a feed forward network with one hidden layer of size 2048 and ReLU activation. Thus, each expert contains [512 ∗ 2048] + [2048 ∗ 512] = 2M parameters. The output of the MoE layer is passed through a sigmoid function. Training: We trained our networks using the Adam optimizer and tuned the learning rate as de- scribed in previous section. Similarly to (Wu et al., 2016), we used dropout (Zaremba et al., 2014) between all layers during training. Our dropout rate was 0.4. We used longer periods of increas- ing and constant learning rate at the beginning of training (2000 steps and 8000 steps respectively). Training was done synchronously on a cluster of up to 64 GPUs as described in section 4. Metrics: We evaluated our models using the perplexity and the standard BLEU score metric. We reported tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public implementation of Moses (on Github), which was also used in (Luong et al., 2015a). Results: We first report the performance of our method as we increase the number of experts in our model. This set of results is shown in Figure 3. Each step approximately processes 200k words. As can be seen from the Figure, as we increased the number of experts to approach 2048, the test 2For performance reasons, we used a slightly different attention function from the one described in (Wu et al., 2016) - See appendix B 7 Under review as a conference paper at ICLR 2017 perplexity of our model continued to improve. This supports the argument of using MoE for training large neural translation models. 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 Number of Steps 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Pe rp le xi ty #Experts=0 #Experts=32 #Experts=512 #Experts=2048 Figure 3: Perplexity on WMT\u201914n En→ Fr data. All models are identical except for the number of experts in MoE. In all the models except for the one with no experts, the gating network selects exactly 4 experts to process each example. The results of our method on En→Fr with 2048 experts and other baselines are shown in Table 4. As can be seen from the table, our approach achieved a BLEU score of 40.56. Since our method did not use RL refinement, this result provides a significant gain of 1.34 BLEU score on top of a strong baseline (Wu et al., 2016). The perplexity is also much better given the same size of vocabulary as described in (Wu et al., 2016). Note that our best model (i.e., MoE with 2048 experts) reached to the reported BLEU and perplexities much faster than the baseline while using fewer GPUs. Table 4: Perplexity and BLEU comparison of our method against previous state-of-art methods on the WMT\u201914 En→ Fr (newstest2014). Model Test Test Computation Total Training Perplexity BLEU per Word #Parameters Time MoE with 2048 Experts 2.69 40.35 100.8M 8.690B 3 days\/64 k40s MoE with 2048 Experts (longer training) 2.63 40.56 100.8M 8.690B 6 days\/64 k40s GNMT (Wu et al., 2016) 2.79 39.22 214.2M 246.9M 6 days\/96 k80s GNMT+RL (Wu et al., 2016) 2.96 39.92 214.2M 246.9M 6 days\/96 k80s PBMT (Durrani et al., 2014) 37.0 LSTM (6-layer) (Luong et al., 2015b) 31.5 LSTM (6-layer+PosUnk) (Luong et al., 2015b) 33.1 DeepAtt (Zhou et al., 2016) 37.7 DeepAtt+PosUnk (Zhou et al., 2016) 39.2 Table 5: Perplexity and BLEU comparison of our method against previous state-of-art methods on the WMT\u201914 En→ De (newstest2014). Model Test Test Computation Total Training Perplexity BLEU per Word #Parameters Time MoE with 2048 Experts 4.64 26.03 100.8M 8.690B 1 day\/64 k40s GNMT (Wu et al., 2016) 5.25 24.91 214.2M 246.9M 1 day\/96 k80s GNMT +RL (Wu et al., 2016) 8.08 24.66 214.2M 246.9M 1 day\/96 k80s PBMT (Durrani et al., 2014) 20.7 DeepAtt (Zhou et al., 2016) 20.6 The results of our method on En→De with 2048 experts and other baselines are shown in Table 5. As can be seen from the table, our method achieved a BLEU score of 26.03 which is 1.12 better than the best model in (Wu et al., 2016). The perplexity was also much better given the same size of vocabulary as described in (Wu et al., 2016). We found that the experts indeed become highly specialized by syntax and\/or semantics, as can be seen in Table 6. For example, one expert is used when the indefinite article \u201Ca\" introduces the direct object in a verb phrase indicating importance or leadership. 8 Under review as a conference paper at ICLR 2017 Table 6: Contexts corresponding to a few of the 2048 experts in the MoE layer in the encoder portion of the WMT\u201914 En→ Fr translation model. For each expert i, we sort the inputs in a training batch in decreasing order of G(x)i, and show the words surrounding the corresponding positions in the input sentences. Expert 381 Expert 752 Expert 2004 ... with researchers , ... ... plays a core ... ... with rapidly growing ... ... to innovation . ... plays a critical ... ... under static conditions ... ... tics researchers . ... provides a legislative ... ... to swift ly ... ... the generation of ... ... play a leading ... ... to dras tically ... ... technology innovations is ... ... assume a leadership ... ... the rapid and ... ... technological innovations , ... ... plays a central ... ... the fast est ... ... support innovation throughout ... ... taken a leading ... ... the Quick Method ... ... role innovation will ... ... established a reconciliation ... ... rec urrent ) ... ... research scienti st ... ... played a vital ... ... provides quick access ... ... promoting innovation where ... ... have a central ... ... of volatile organic ... ... ... ... 5.3 MACHINE TRANSLATION ON PRODUCTION EN→FR DATA Dataset, Model Architecture, and Training: We also tested our method on our production En- glish to French data, which is 2 orders of magnitude larger than the WMT\u201914 corpus. We used the exact same model architecture and training procedure as described in previous section. Results: Figure 4 demonstrates the significance of increasing the number of parameters in the model in order to improve learning. It shows the improvement in perplexity on Google Production En→ Fr data on models with different number of experts, from a baseline of 0 expert to a MoE model of 2048 experts. If applicable, in all the models exactly 4 experts were used for processing each example. Each step approximately processes 200k words. As we increased the number of experts, from a baseline of 0 expert to the ones with 2048 experts the perplexity constantly improved. The improvement was achieved while the amount of computation per word was constant across all the models with non-zero experts. 0 20000 40000 60000 80000 100000 Number of Steps 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Pe rp le xi ty #Experts=0 #Experts=4 #Experts=32 #Experts=512 #Experts=2048 Figure 4: Perplexity on Google Production En→ Fr data. All models are identical except for the number of experts in MoE. In all the models except for the one with no experts, the gating network selects exactly 4 experts to process each example. Table 7: Perplexity and BLEU comparison of our method against previous state-of-art methods on the Google Production En→ Fr dataset. Model Eval Eval Test Test Computation Total Training Perplexity BLEU Perplexity BLEU per Word #Parameters Time MoE with 2048 Experts 2.60 37.27 2.69 36.57 100.8M 8.690B 1 day\/64 k40s GNMT (Wu et al., 2016) 2.78 35.80 2.87 35.56 214.2M 246.9M 6 days\/96 k80s In Table 7, we report the comparison between our 2048-expert model and the strong and highly optimized baseline models in (Wu et al., 2016). Our model achieved 1.01 higher BLEU score on test data while speeding up computation by a factor of 6× using fewer machines. 9 Under review as a conference paper at ICLR 2017 6 CONCLUSION This work introduces Mixture-of-Experts (MoE) as a new type of layer in neural networks. The pro- posed approach trains an outrageously large network that contains thousands of experts and billions of parameters. We propose a sparse gating approach that enables training such a large network. We describe technical details on our data- and model- parallel system that is integral to MoE layer\u2019s efficiency and scalability. On the 1 Billion Word Language Modeling Benchmark and WMT\u201914 translation benchmarks, our approach\u2019s quality surpasses all currently published results. We also show that our approach can be applied to datasets with orders of magnitude more data, to deliver high quality language modeling and translation results. We find that increasing the model capacity well beyond the conventional approaches can lead to significant improvements on learning quality while incurring comparable or faster training time to the baseline models. ACKNOWLEDGMENTS We would like to thank all of the members of the Google Brain Team and the Google Translate Team who helped us with this project, in particular Geoffrey Hinton, Zhifeng Chen, Yonghui Wu, and Melvin Johnson. REFERENCES Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jing- dong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yo- gatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english and mandarin. arXiv preprint arXiv:1512.02595, 2015. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. Ronan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large scale problems. Neural Computing, 2002. Marc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In ICML, 2015. Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh\u2019s phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014. David Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. Ekaterina Garmash and Christof Monz. Ensemble learning for multi-source neural machine transla- tion. In staff.science.uva.nl\/c.monz, 2016. Felix A. Gers, Jürgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Continual pre- diction with lstm. Neural Computation, 2000. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. IEEE Conference on Computer Vision and Pattern Recognition, 2015. Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 2012. 10 Under review as a conference paper at ICLR 2017 Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 1997. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computing, 1991. Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computing, 1994. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo- lutional neural networks. In NIPS, 2012. Quoc V. Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeffrey Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention- based neural machine translation. EMNLP, 2015a. Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare word problem in neural machine translation. ACL, 2015b. Carl Edward Rasmussen and Zoubin Ghahramani. Infinite mixtures of Gaussian process experts. NIPS, 2002. M. Schuster and K Nakajima. Japanese and Korean voice search. ICASSP, 2012. Babak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR, 2009. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014. Lucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS, 2015. Volker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John- son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Bangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical mixture of classification experts uncovers interactions between brain regions. In NIPS. 2009. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016. 11 Under review as a conference paper at ICLR 2017 APPENDICES A STRICTLY BALANCED EXPERT UTILIZATION For performance purposes, we would like the different experts (which reside on different devices) to each receive the same number of examples from each training batch. To this end, we introduce an alternative mask function, Mbatchwise(X,m), which operates over batches of input vectors. Instead of keeping the top k values per example, we keep the top m values per expert across the training batch. We generally choose m = k|X|n for some small value k, so that each example is sent to an average of k experts. Mbatchwise(X,m)j,i = { 1 if Xj,i is in the top m values for to expert i 0 otherwise (6) As our experiments suggest and also observed in (Ioffe & Szegedy, 2015), using a batchwise func- tion during training (such as Mbatchwise) requires modifications to the inference when we may not have a large batch of examples. Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask. We use the following mask at inference time: Mthreshold(x, T )i = { 1 if xi > Ti 0 otherwise (7) To learn the threshold values, we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical. Lbatchwise(X,T,m) = |X|∑ j=1 n∑ i=1 (Mthreshold(x, T )i −Mbatchwise(X,m)j,i)(Xj,i − Ti) (8) B ATTENTION FUNCTION The attention mechanism described in GNMT (Wu et al., 2016) involves a learned \u201CAttention Func- tion\" A(xi, yj) which takes a \u201Csource vector\" xi and a \u201Ctarget vector\" yj , and must be computed for every source time step i and target time step j. In GNMT, the attention function is implemented as a feed forward neural network with a hidden layer of size n. It can be expressed as: AGNMT (xi, yj) = n∑ d=1 Vdtanh((xiU)d + (yjW )d) (9) Where U and W are trainable weight matrices and V is a trainable weight vector. For performance reasons, in our models, we used a slightly different attention function: A(xi, yj) = n∑ d=1 Vdtanh((xiU)d)tanh((yjW )d) (10) With our attention function, we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications. We found little difference in quality between the two functions. 12 Introduction Related Work The Structure of the Mixture-of-Experts layer Gating Network Ensuring Expert Utilization Hierachical Mixture of Experts Addressing MoE System Performance Issues Experiments 1 Billion Word Language Model Benchmark Small Models Big Model Machine Translation on WMT'14 En Fr and En De Machine Translation on Production EnFr Data Conclusion Appendices Strictly Balanced Expert Utilization Attention Function ","flair":"three\tResearch"}
{"author":"mimighost","created":"Tue Oct 11 18:07:36 EDT 2016","text":"For some reason I decided one night I wanted to get a bunch of fonts. A lot of them. An hour later I had a bunch of scrapy scripts pulling down fonts and a few days later I had more than 50k fonts on my computer. I then decided to convert it to bitmaps. It turns out this is a bit trickier than it might seem like. You need to crop in such a way that each character of a font is vertically aligned, and scale everything to fit the bitmap. I started with 512 * 512 bitmaps of all character. For every font you find the max y and min y of the bounding box, and the same thing for each individual letter. After some more number juggling I was able to scale all characters down to 64 * 64. The result is a tensor of size 56443 * 62 * 64 * 64. Exercise for the reader: where does the number 62 come from? I stored it as a tiny little (13GB) HDF5 file that you can download here: fonts.hdf5. If you take the average of all fonts, here\u2019s what you get: Hopefully by now it should be clear where the number 62 came from. The median is a lot less blurry than the average: Both mean and median are well-formed and legible! However individual fonts are all over the place: I guess I practically begged for it, stealing fonts from various sketchy places all over the web.In particular most of the fonts don\u2019t even have lower case versions of the letters. A minority of fonts miss certain characters and will just output rectangles instead. And look at the ridiculous Power Ranger figure for the lower case \u201Cc\u201D! Now, let\u2019s train a neural network that generates characters! Specifically what I wanted to do is to create a \u201Cfont vector\u201D that is a vector in latent space that \u201Cdefines\u201D a certain font. That way we embed all fonts in a space where similar fonts have similar vectors. I built a simple neural network using Lasagne\/Theano \u2014 check out the code here. It took an insane amount of time to converge, probably because there\u2019s so much data and parameters. After weeks of running, the model converges to something that looks decent. Some notes on the model All of the code is available in the erikbern\/deep-fonts repo on Github. After convergence, we end up having a nice 40D embedding of all 50k fonts. LooksÂ like it ends up being roughly a multivariate normal \u2014 here\u2019s the distribution of each of the 40 dimensions: Playing around with the model To start with, let\u2019s recreate real font characters with characters generated from the network. Let\u2019s plot the real character together with the model outputs. For each pair below, the real character is on the left, the model output on the right. These are all characters drawn from the test set, so the network hasn\u2019t seen any of them during training. All we\u2019re telling the network is (a) what font it is (b) what character it is. The model has seen other characters of the same font during training, so what it does is to infer from those training examples to the unseen test examples. The network does a decent job at most of the characters, but gives up on some of the more difficult ones. For instance, characters with thin black lines are very hard to predict for the model, since if it renders the line just a few pixel to the side, that\u2019s twice the loss of just rendering whitespace. We can also interpolate between different fonts in continuous space. Since every font is a vector, we can create arbitrary font vectors and generate output from it. Let\u2019s sample four fonts and put them in the corners of a square, then interpolate between them! Certain characters have multiple forms that we can interpolate between, eg. lowercase g: We can also pick a font vector and generate new fonts from random perturbations: (btw internet god \u2014 please forgive me for wasting bandwidth on all the animated gifs in this blog post!) We can also generate completely new fonts. If we model the distribution of font vectors as a multivariate normal, we can sample random vectors from it and look at the fonts they generate. I\u2019m interpolating between a few of thoseÂ vectors in the picture below: An interesting thing here is that the model has learned that many fonts use upper case characters for the lower case range \u2014 the network interpolates between Q and q seamlessly. Here\u2019s an example of the network interpolating very slowly between two fonts where this is the main difference: Another cool thing we can do since we have all fonts in a continuous space is to run t-SNE on them and embed all fonts into the 2D plane. Here\u2019s a small excerpt of such an embedding: There are many other fun things you can do. It\u2019s clear that there\u2019s some room for improvement here. In particular, if I had more time, I would definitely explore generative adversarial models, which seems better at generating pictures. Another few things should be relatively easy to implement, such as batch normalization and parametric leaky rectifications. And finally the network architecture itself could probably benefit from doing deconvolutions instead of fully connected layers Feel free to download the data and play around with it if you\u2019re interested!","flair":"four\tProject"}
{"author":"cryptobionic","created":"Sat Nov 19 15:44:09 EST 2016","text":"[The repo](https:\/\/github.com\/cameronfabbri\/tensorflow_ops) and the\n[direct source code](https:\/\/github.com\/cameronfabbri\/tensorflow_ops\/blob\/master\/tensorflow_ops\/__init__.py)\n\nI noticed throughout many of my projects I was just copying and pasting a lot of code (mainly functions for convolutions and fully connected layers), so I grouped them in a simple package that can be easily imported and used. For example, the function below for computing a convolution was being used in every project, and therefore was  rewritten in every project. \n\n    def _conv_layer(inputs, kernel_size, stride, num_channels, name):\n        with tf.variable_scope(name) as scope:\n            input_channels = inputs.get_shape()[3]\n            weights = _variable_with_weight_decay('weights', shape=[kernel_size, kernel_size, input_channels, num_channels], stddev=0.1, wd=0.0005)\n            biases = _variable_on_cpu('biases', [num_channels], tf.constant_initializer(0.1))\n            conv = tf.nn.conv2d(inputs, weights, strides=[1, stride, stride, 1], padding='SAME')\n            conv_biased = tf.nn.bias_add(conv, biases)\n            return conv_biased\n\nNow all that needs to be done is:\n    from tensorflow_ops import _conv_layer\n\nFeedback and additions are welcome, an example is provided in the repository.","flair":"four\tProject"}
{"author":"fhuszar","created":"Thu Oct 20 10:09:40 EDT 2016","text":"Generative Adversarial Networks (GANs) are notoriously hard to train. In a recent paper, we presented an idea that might help remedy this. Our intern Casper spent the summer working with GANs, resulting in a paper which appeared on arXiv this week. One particular technique did us great service: instance noise. It's not the main focus of Casper's paper, so the details have been relegated to an Appendix. We thought it's a good idea to summarise it here, giving a few more details. Naturally, I think the full paper is also worth a read, there are a few more interesting things in there: There are different ways to think about GANs: you can approach it from a game theoretic view of seeking Nash equilibrium (Salimans et al, 2016), or you can treat it as an E-M like iterative algorithm where the discriminator's job is likelihood ratio estimation (Mohamed et al, 2016, Uehara et al, 2016, Nowozin et al). If you've read my earlier posts, it should come as no surprise that I subscribe to the latter view. Consider the following idealised GAN algorithm, each iteration consisting of the following steps: If $q_\\theta$ and $p$ are well-conditioned distributions in a low-dimensional space, this algorithm performs gradient descent on an approximation to the KL divergence, so it should converge. Crucially, the convergence of this algorithm relies on a few assumptions never really made explicit that don't always hold: In the paper we argued that in real-world situations neither of these holds, mainly because $q_\\theta$ and $p$ are concentrated distributions whose support may not overlap. In image modelling, distribution of natural images $p$ is often assumed to be concentrated on or around a lower-dimensional manifold. Similarly, $q_\\theta$ is often degenerate by construction. The odds that the two distributions share support in high-dimensional space, especially early in training, are very small. If $q_\\theta$ and $p$ have non-overlapping support, then The main ways to avoid these pathologies involve making the discriminator's job harder. Why? The $JS$ divergence is constant locally in $\\theta$, but it doesn't mean that the variational lower bound also has to be constant. Indeed, if you cripple the discriminator so the lower bound is not tight, you may end up with a non-constant function of $\\theta$ that will roughly guide you to the right direction. An example of this crippling is that in most GAN implementations the discriminator is only partially updated in each iteration, rather than trained until convergence. This extreme form of early stopping is a form of regularisation that prevents the discriminator from overfitting. Another way to cripple the discriminator is adding label noise, or equivalently, one-sided label smoothing as introduced by Salimans et al, (2016). In this technique the labels in the discriminator's training data are randomly flipped. Let's illustrate this technique in two figures. The top panel shows two almost perfectly separable distributions $q_\\theta$ and $p$ (it is called $p_Y$ in the paper). Notice how the large gap between the distributions means that there are large number of possible classifiers that tell the two distributions apart and achieve similar logistic loss. The Bayes-optimal classifier may not be unique, and the set of near-optimal classifiers is very large and diverse. In the middle panel we show the effect of \\emph{one sided label smoothing} or equivalently, adding label noise. In this technique, the labels of some real data samples $x\\sim p$ are flipped so the discriminator is trained thinking they were samples from $q_\\theta$. The discriminator indeed has a harder task now. However, the likelhiood ratio $\\log \\frac{p}{(1-\\pi) q\\theta + \\pi p}$ is still not well-defined. Also, although discriminators have a harder job, they are all punished evenly: there is no way for the discriminator to be smart about handling label noise. Adding label noise doesn't change the structure of the logistic regression loss landscape dramatically, it mainly just pushes everything up. Hence, there are still a large number of near-optimal discriminators. Adding label noise still does not allow us to pinpoint a single unique Bayes-optimal classifier. The JS divergence is not saturated to its maximum level anymore, but it is still locally constant in $\\theta$. The last panel shows the technique we propose, whereby we add noise to samples from both $q_\\theta$ and $p$. We use convolution $p_\\sigma \\ast q_\\theta$ to denote additive noise. As a result, the noisy distributions now overlap, the log-likelihood-ratio $\\log \\frac{p_\\sigma \\ast p}{p_\\sigma \\ast q_\\theta}$ is well-behaved, and the JS divergence between the two noisy distributions is a non-constant function of $\\theta$. An alrernative way to think about instance noise vs. label noise is via graphical models. The following three graphical models define joint distributions, parametrised by $\\theta$. The GAN algorithm tries to adjust $\\theta$ so as to minimise the mutual information between the highlighted nodes in these graphical models: Here's what the variables are: In this joint distribution, vanilla GANs minimise is the mutual information $\\mathbb{I}[x,y]$ which corresponds to JS divergence. If the distributions of $x_{fake}$ and $x_{real}$ have no overlap, $y$ is a deterministic function of $x$ and therefore the mutual information is maximal. Hence, in this scenario, the objective function is theoretically constant in $\\theta$. The second panel shows the effect of label smoothing, or, adding label noise. Now the discriminator is trained on randomly flipped labels $\\tilde{y}$ instead of the real labels $y$. The GAN algorithm can be thought of as trying to minimise $\\mathbb{I}[x,\\tilde{y}]$. It is not hard to see that, in situations where $y$ is a deterministic function of $x$, then this mutual information is also constant with respect to $\\theta$. In the instance noise trick, the discriminator sees the correct labels $y$, but its input is the noisy $\\tilde{x}$. We think that $\\mathbb{I}[\\tilde{x},y]$ is a much better objective function to target. Even when $\\mathbb{I}[y;x]$ is saturated, $\\mathbb{I}[\\tilde{x},y]$ still depends on $\\theta$ in a non-trivial and meaningful way. If the noise distribution $p_\\sigma$ is something like a Gaussian, adding noise can be thought of as a way to measure how far away $q$ and $p$ are from each when they don't overlap. Finally, there's a way to understand label noise from the perspective of minimising divergences. Using a GAN algorithm we can construct algorithms that minimise the following family of divergences: where $\\sigma$ is the parameter of the noise distribution. Importantly, these divergences are still convex and have a global minimum when $q_\\theta = p$. Obviously, as the noise level $\\sigma$ increases, the fine details of the distribution are \"blurred out\" so the divergences are expected to become less sensitive to fine patterns. We tried this technique in the context of GANs for image superresolution, and we found it stabilises training, as predicted. We used additive Gaussian white noise whose variance parameter $\\sigma$ we annealed linearly during training. The figure below shows that the discriminator's performance is kept in check by the added noise throughout training: The blue curve shows the average probability the discriminator assigns to real data, the red the probability it assigns to synthetic data. If the discriminator was winning, these probabilities would always be $0.0$ and $1.0$, but the noise makes the discriminator's job harder. The orange curve shows the probability the current discriminator $D_t$ assigns to new fake data, after the generator is updated ($G_{t+1}$). As expected, the orange curve is always above the red one, which means that the current approximation of $d_\\sigma(q_\\theta\\|p)$ had been improved. Another way to show that convergence is happening is to look at the average SSIM and MSE values during training. This is only possible because we trained the network for superresolution, so we actually always had a ground truth image to compare to. Both metrics improve steadily as the figures show: Instance noise is a theoretically motivated way to remedy the poor convergence properties of GANs. We have not tested it extensively in the context of generative modeling, but we think it should help there also. Let us know if you have any experience with similar techniques.","flair":"three\tResearch"}
{"author":"Sebubu","created":"Thu Nov 03 12:43:32 EDT 2016","text":"I have done a transfer learning project to distinguish between different mushroom species. The showcase can be accessed via **[mushreco.ml](http:\/\/mushreco.ml\/en\/)**.\n\nTechnical details:\n\n* The classifier has been trained on 208 categories of mushrooms. Every category contains between 60 and 100 training images which is definitely at the lower end. \n* All images have been scrapped via Google, Yahoo and Baidu. The images where filtered by myself to avoid too many wrong mushrooms in a category.\n* At the end, I had 18'000 images in the dataset. \n* A pretrained Resnet50 on imagenet has been used as a general image descriptor. \n* The whole work was done with Keras. Keras provides quite cool and easy to use pretrained models!\n\nMy first impression of the task was that it will never work. The diversity in only one species is enormous. A mushroom can look completely different if it is young vs. old. Also the amount of available data is critical. But hey, I gave it a try ;)\n\n\nAt the end, the classifier works impressively exact. It will not replace a expert but can help you identifying a mushroom.","flair":"four\tProject"}
{"author":"AlfonzoKaizerKok","created":"Tue Sep 27 21:57:08 EDT 2016","text":"Hi Reddit, I'm looking into alternatives to Theano. Tensorflow is the obvious choice, but I found MXNet to be another viable option. Is there any reason one should not go for MXNet in favour of Theano\/Tensorflow?","flair":"null\tnull"}
{"author":"cptai","created":"Sat Nov 26 08:09:08 EST 2016","text":"I am reading the [normalizing flow paper](https:\/\/arxiv.org\/pdf\/1505.05770v6.pdf) and am a bit confused. \n\nIt seems that being able to model complex (i.e. correlated?) posterior is one of the advantages of the proposed approach (e.g. Section 2.3, last paragraph). But for a deep latent Gaussian model with diagonal covariance, the posterior can also be arbitrarily complex. So why is normalizing flow considered to be more expressive? What did I get wrong? \n","flair":"one\tDiscussion"}
{"author":"vernik911","created":"Fri Nov 25 08:05:38 EST 2016","text":" Home Moments Search query Search Twitter Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Language: English Bahasa Indonesia Bahasa Melayu Català Čeština Dansk Deutsch English UK Español Filipino Français Hrvatski Italiano Magyar Nederlands Norsk Polski Português Română Slovenčina Suomi Svenska Tiếng Việt Türkçe Ελληνικά Български език Русский Српски Українська мова עִבְרִית العربية فارسی मराठी हिन्दी বাংলা ગુજરાતી தமிழ் ಕನ್ನಡ ภาษาไทย 한국어 日本語 简体中文 繁體中文 Have an account? Log in Have an account? Remember me · Forgot password? New to Twitter? Sign up Mushroom AI @mushroomaibot Tweets 59 Following 50 Followers 38 Likes 5   More Likes Unmute @mushroomaibot Mute @mushroomaibot Follow Following Unfollow Blocked Unblock Pending Cancel Mushroom AI @mushroomaibot Mushroom AI recognizes different mushroom species by photo using deep convolutional neural network. mushroomai.ml Joined November 2016 49 Photos and videos Photos and videos Tweets Tweets Tweets & replies Media @mushroomaibot is blocked Are you sure you want to view these Tweets? Viewing Tweets won't unblock @mushroomaibot. View Tweets Close Mushroom AI followed Pinned Tweet Mushroom AI \u200F@mushroomaibot Nov 25 Mode \"alpha\". I can recognize only selected mushroom speciespic.twitter.com\/z4K4UQyCMS 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 12h12 hours ago Paxillus involutus 73.1% Agaricus bisporus 18.4% Cantharellus cibarius 5.3% @ilgar_al #mushroom #AI #MLpic.twitter.com\/rmoBQCir2y 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Lycoperdon perlatum 48.2% Amanita phalloides 32.3% Paxillus involutus 9.9% @athst #mushroom #AI #MLpic.twitter.com\/zByRiBdqkC 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Cantharellus cibarius 82.1% Paxillus involutus 12.3% Lycoperdon perlatum 4.7% @athst #mushroom #AI #MLpic.twitter.com\/WTj0vaBDML 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Suillus luteus 88.9% Leccinum scabrum 3.5% Agaricus bisporus 2.6% @ilgar_al #mushroom #AI #MLpic.twitter.com\/pwQZUEoZG9 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Boletus edulis 85.2% Agaricus campestris 8.5% Suillus luteus 2.5% @ilgar_al #mushroom #AI #MLpic.twitter.com\/V2nH2kzTWV 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Paxillus involutus 32.4% Agaricus campestris 21.6% Suillus luteus 17.7% @ilgar_al #mushroom #AI #MLpic.twitter.com\/IYyGickFjl 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita phalloides 90.3% Agaricus campestris 3.4% Agaricus bisporus 3.3% @ilgar_al #mushroom #AI #MLpic.twitter.com\/LhrPURqCyD 0 replies 4 retweets 1 like Reply Retweet 4 Retweeted 4 Like 1 Liked 1 More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita phalloides 77.1% Leccinum scabrum 6.7% Agaricus campestris 6.2% @santichapela #mushroom #AI #MLpic.twitter.com\/WZ35tpsEfO 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Cantharellus cibarius 98.1% Lycoperdon perlatum 0.6% Agaricus bisporus 0.6% @santichapela #mushroom #AI #MLpic.twitter.com\/oLMjx5nJ7o 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Paxillus involutus 61.1% Cantharellus cibarius 18.9% Lycoperdon perlatum 10.1% @santichapela #mushroom #AI #MLpic.twitter.com\/mBAQVFX0aC 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Cantharellus cibarius 76.3% Lycoperdon perlatum 7.7% Paxillus involutus 5.2% @santichapela #mushroom #AI #MLpic.twitter.com\/bX4aQyrecs 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Leccinum scabrum 95.0% Boletus edulis 2.5% Suillus luteus 1.6% @santichapela #mushroom #AI #MLpic.twitter.com\/uKwv8vPCCF 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita muscaria 99.8% Lycoperdon perlatum 0.1% Amanita phalloides 0.0% @santichapela #mushroom #AI #MLpic.twitter.com\/DPB4Es0xJP 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Leccinum scabrum 55.5% Suillus luteus 23.8% Amanita phalloides 14.7% @santichapela #mushroom #AI #MLpic.twitter.com\/m6KWRnfz9c 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Paxillus involutus 46.8% Amanita phalloides 20.8% Leccinum scabrum 9.6% @santichapela #mushroom #AI #MLpic.twitter.com\/Ed7pEy7roB 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 21h21 hours ago Lycoperdon perlatum 51.1% Agaricus campestris 25.1% Amanita phalloides 13.2% @voodoohop #mushroom #AI #MLpic.twitter.com\/nVV2H4SKzD 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 21h21 hours ago Lycoperdon perlatum 38.8% Cantharellus cibarius 27.9% Paxillus involutus 27.4% @hedonismo_sp #mushroom #AI #MLpic.twitter.com\/Hy7A2GbV7l 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot Nov 26 Amanita phalloides 76.3% Agaricus bisporus 7.9% Agaricus campestris 7.5% @gesamoje #mushroom #AI #MLpic.twitter.com\/vCVcYTgiVl 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot Nov 26 Lycoperdon perlatum 61.2% Paxillus involutus 35.6% Cantharellus cibarius 2.2% @Antonkuznetsov5 #mushroom #AI #MLpic.twitter.com\/NjwMGT45kF 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Mushroom AI Retweeted Peter Skomoroch \u200F@peteskomoroch Nov 26 San Francisco, CA Peter Skomoroch Retweeted Evgeny Vlasenko Interesting thread - it might not be obvious, but this is a simple case where a bad deep learning prediction could get someone killedhttps:\/\/twitter.com\/mahnunchik\/status\/802183480264232961 \u2026 Peter Skomoroch added, Evgeny Vlasenko @mahnunchik [Project] Mushroom AI - twitter bot which can recognize different mushroom species by photo https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5etha4\/project_mushroom_ai_twitter_bot_which_can\/?ref=share&ref_source=twitter \u2026 via @reddit 0 replies 3 retweets 14 likes Reply Retweet 3 Retweeted 3 Like 14 Liked 14 More Copy link to Tweet Embed Tweet @mushroomaibot hasn't tweeted yet. Back to top ↑ Loading seems to be taking a while. Twitter may be over capacity or experiencing a momentary hiccup. Try again or visit Twitter Status for more information. New to Twitter? Sign up now to get your own personalized timeline! Sign up You may also like · Refresh © 2016 Twitter About Help Terms Privacy Cookies Ads info Close Choose a trend location Dismiss Close Previous Next Close Go to a person's profile Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Close Retweet this to your followers? Optional comment for Retweet   Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @   140 Retweet Tweet Close Are you sure you want to delete this Tweet? Cancel Delete Close Promote this Tweet Close Block Cancel Block Add a location to your Tweets When you tweet with a location, Twitter stores that location. You can switch location on\/off before each Tweet and always have the option to delete your location history. Learn more Turn location on Not now Close Profile summary Close Your lists Close Create a new list List name Description Under 100 characters, optional Privacy Public · Anyone can follow this list Private · Only you can access this list Save list Close Close Copy link to Tweet Here's the URL for this Tweet. Copy it to easily share with friends. Close Embed this Tweet Embed this Video Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? Include parent Tweet Include media Preview Close Log in to Twitter Remember me · Forgot password? Don't have an account? Sign up » Close Sign up for Twitter Not on Twitter? Sign up, tune into the things you care about, and get updates as they happen. Sign up Have an account? Log in » Close Two-way (sending and receiving) short codes: Country Code For customers of United States 40404 (any) Canada 21212 (any) United Kingdom 86444 Vodafone, Orange, 3, O2 Brazil 40404 Nextel, TIM Haiti 40404 Digicel, Voila Ireland 51210 Vodafone, O2 India 53000 Bharti Airtel, Videocon, Reliance Indonesia 89887 AXIS, 3, Telkomsel, Indosat, XL Axiata Italy 4880804 Wind 3424486444 Vodafone » See SMS short codes for other countries Close Confirmation Close   Close Close Buy Now Close Buy Now Hmm... Something went wrong. Please try again. Skip all Welcome home! This timeline is where you\u2019ll spend most of your time, getting instant updates about what matters to you. Tweets not working for you? Hover over the profile pic and click the Following button to unfollow any account. Say a lot with a little When you see a Tweet you love, tap the heart \u2014 it lets the person who wrote it know you shared the love. Spread the word The fastest way to share someone else\u2019s Tweet with your followers is with a Retweet. Tap the icon to send it instantly. Join the conversation Add your thoughts about any Tweet with a Reply. Find a topic you\u2019re passionate about, and jump right in. Learn the latest Get instant insight into what people are talking about now. Get more of what you love Follow more accounts to get instant updates about topics you care about. Find what's happening See the latest conversations about any topic instantly. Never miss a Moment Catch up instantly on the best stories happening as they unfold. Back Next Next Tweet from user ","flair":"four\tProject"}
{"author":"aibuff","created":"Sun Oct 16 20:07:41 EDT 2016","text":"Hi Reddit Machine Learning Community,\nI dont know if this question suits this subreddit so apologies for my ignorance.\nI am planning to get my feet wet in the deep learning and eventually using amazon\/azure for data processing purposes. But till then I want to experiment with initial Kaggle data-sets like Titanic , MNIST etc. and I am planning to buy a gaming laptop for the same. I am not in a position to buy a desktop as I dont have the space for it ( currently sharing room with roommates) and I want my workstation to be mobile. Can anyone provide me any suggestions opinions? My budget is betweeen $1000-$1500.","flair":"one\tDiscussion"}
{"author":"AnvaMiba","created":"Wed Nov 09 14:12:24 EST 2016","text":"I'm looking for datasets (possibly used by established benchmarks) for domain adaptation of the supervised kind: where you have labeled samples for both the source and target domain, but the target domain has much less training examples.\n\nSo far I've found the [Multi-Domain Sentiment Dataset](https:\/\/www.cs.jhu.edu\/~mdredze\/datasets\/sentiment\/) and some image datasets by [UC Berkeley](https:\/\/cs.stanford.edu\/~jhoffman\/domainadapt\/#datasets_code), but they are quite small. I'm interested in something a bit bigger and more challenging, but not as big and challenging that it takes one week\/GPU to run one experiment (so no machine translation or image captioning).\n\nAlso, anything for which a baseline neural network implementation in Theano\/Tensorflow\/Torch exists would make my life easier. Any suggestion?\n","flair":"one\tDiscussion"}
{"author":"Mandrathax","created":"Mon Nov 21 09:31:58 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.06080 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression Authors: Quang Minh Hoang, Trong Nghia Hoang, Kian Hsiang Low (Submitted on 18 Nov 2016) Abstract: While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models. Comments: 31st AAAI Conference on Artificial Intelligence (AAAI 2017), Extended version with proofs, 11 pages Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1611.06080 [stat.ML]   (or arXiv:1611.06080v1 [stat.ML] for this version) Submission history From: Kian Hsiang Low [view email] [v1] Fri, 18 Nov 2016 14:00:48 GMT (111kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"raw_hazard","created":"Sat Oct 15 12:34:13 EDT 2016","text":"I find Keras a very useful and well done tool. It is perfect to start using Theano and it is really easily understandable and usable.\nNow, I realised a Keras model (using Theano interface), which works perfectly well and I would like to replicate it using only Theano code.\nSince Keras is actually using Theano code, I should be able, in principle, to do this.\nThe neural net is a convolutional neural network for a one output regression task, with the following layers: conv2d, maxpool2d, conv2d, maxpool2d, dense, dense, output and using Adam optimizer.\nUnfortunately, despite it seems to me that I implemented exactly the same neural network with vanilla Theano code, the performance is consistently different.\nSo, I guess I must be wrong somewhere, but I can't see where.\n\nI will put here a link to the codes I'm using, in order to make the post too long. They are short and simple codes, do not worry =)\n\n[Keras Impl.](http:\/\/pastebin.com\/7eNubwxw)\n\n[Theano main](http:\/\/pastebin.com\/Lvdn6UAc)\n[Dense layer with MSE loss function](http:\/\/pastebin.com\/RyUH07Te)\n[Conv layer + max pooling](http:\/\/pastebin.com\/VVmXm1Uk)\n[Updates rule](http:\/\/pastebin.com\/fp5Draq7)\n\n\nKeep in mind that the Theano code is mostly adapted from the Theano tutorial found on the website, and the update rules for Adam optimizer is adapted from Keras source code.\nI have a large training set, so I usually check after a few epochs the behaviour of the code and this is what I see:\nKeras Model: the validation error keeps decreasing, and already after two or three epochs I see a very good match between prediction and true values (points quite close around the bisector in the plot at the bottom of the code)\nVanilla Theano Impl: the validation decreases at first, but then some kind of oscillating\/overfitting features appear (and the absolute value is 10 times higher than in the Keras impl.), and the match between prediction and true values is worse (points quite spread around the bisector in the plot at the bottom of the code)\n\nSo, is someone able to tell me where is the difference between the Keras model and my Theano implementation? Since I'm new to the field, I would really like to understand what I'm doing wrong that so strongly affects the performance of the network.\nAny help would be appreciated. Thanks\n\nEDIT: please note, Keras default params initialization is glorot uniform, which is exactly what I use in the Theano impl. Biases instead are initialized to zero in both cases.","flair":"three\tResearch"}
{"author":"Dytanoth","created":"Thu Oct 06 17:12:20 EDT 2016","text":"Hi all,\nI have been doing a thesis on speech recognition. Mainly about user experience, but I do want to know if I got the theory right behind the models. Please correct me if I'm wrong.\n\nThe very very basic idea I've got:\nWhen we've got an audio stream, the HMM will divide it into \"states\" which are a set period of time (like 10ms). Those states contain a certain \"wave\" that makes up the sound. From this state a speech vector is taken, which will be send to either the GMM or DNN for recognition. The results of those recognitions are phonemes which combined will make words.\n\nThe GMM plots the vector in a multi-dimensional graph, like [this](https:\/\/embed.gyazo.com\/093bf77be2d25c26d26f75d5aa3419fb.png) and compares it to the acoustic model to find the most alike plot.\n\nThe DNN recognizes the input as something it had seen before, and goes through different layers of hidden nodes to get the output data. As a DNN model was trained to get output \"B\" when input is \"A\" by backpropagation.\nI hope I got this right, as a lot of articles instantly mention different formulas. Which I really don't understand (yet).\n\nDoes anyone know if I took anything the wrong way?\n\nThanks you!","flair":"three\tResearch"}
{"author":"sharno","created":"Tue Nov 15 04:06:18 EST 2016","text":" Skip navigation UploadSign in Search Home Trending History Best of YouTube Music Sports Gaming Movies TV Shows News Live Spotlight 360Â° Video Browse channels Sign in now to see your channels and recommendations! Sign in YouTube Red Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Mohamed Elsharnouby SubscribeSubscribedUnsubscribe90 Loading... Loading... Working... Wanderer and half geek Wanderer and half geek Master student Show more Show less â\u2013º Play all Reinforcement Learning by David Silver from DeepMind Mohamed Elsharnouby 10 videos 861 views Last updated on Nov 15, 2016 Play all Share Loading... Save Sign in to YouTube Sign in Play next Play now RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning by DeepMind 1:28:13 Play next Play now RL Course by David Silver - Lecture 2: Markov Decision Process by DeepMind 1:42:05 Play next Play now RL Course by David Silver - Lecture 3: Planning by Dynamic Programming by DeepMind 1:39:09 Play next Play now RL Course by David Silver - Lecture 4: Model-Free Prediction by DeepMind 1:37:02 Play next Play now RL Course by David Silver - Lecture 5: Model Free Control by DeepMind 1:36:31 Play next Play now RL Course by David Silver - Lecture 6: Value Function Approximation by DeepMind 1:36:45 Play next Play now RL Course by David Silver - Lecture 7: Policy Gradient Methods by DeepMind 1:33:58 Play next Play now RL Course by David Silver - Lecture 8: Integrating Learning and Planning by DeepMind 1:40:13 Play next Play now RL Course by David Silver - Lecture 9: Exploration and Exploitation by DeepMind 1:39:18 Play next Play now RL Course by David Silver - Lecture 10: Classic Games by DeepMind 1:51:24 Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"one\tDiscussion"}
{"author":"RefurbishedMac","created":"Sat Nov 26 11:11:01 EST 2016","text":"[D] Is Machine Learning Growing at an Exponential Rate?\n\nIn terms of accuracy? Or any other metrics? I am researching the field. If anyone could point me to any resources about how fast ML is growing, I would greatly appreciate it!","flair":"one\tDiscussion"}
{"author":"open_nsfw","created":"Thu Oct 20 15:33:49 EDT 2016","text":" Image Synthesis from Yahoo's open_nsfw Warning: This post contains abstract depictions of nudity and may be unsuitable for the workplace Yahoo's recently open sourced neural network, open_nsfw, is a fine tuned Residual Network which scores images on a scale of to on its suitability for use in the workplace. In the documentation, Yahoo notes Defining NSFW material is subjective and the task of identifying these images is non-trivial. Moreover, what may be objectionable in one context can be suitable in another. What makes an image NSFW, according to Yahoo? I explore this question with a clever new visualization technique by Nguyen et al.. Like Google's Deep Dream, this visualization trick works by maximally activating certain neurons of the classifier. Unlike deep dream, we optimize these activations by performing descent on a parameterization of the manifold of natural images. This parametrization takes the form of a Generative Network, , trained adversarially on an unrelated dataset of natural images. The \"space of natural images\", according to , look mostly like abstract art. Unsurprisingly, these random pictures, lacking any kind of semantics, have low scores on the classifier. NSFW Images Following Nguyen et al., we perform projected gradient descent on the following problem to obtain the maximal activation for . Not surprisingly, the results of the optimization are clearly pornographic. SFW Images On there other end of the spectrum, optimizing for SFW images seem redundant, as you might just expect it to be the absence of NSFW content. If this were the case, one would expect to observe most images scoring close to . This is only kinda true. Random images generally score between . This is small, but not . We will try to push this down further by descending on in the exact same way as above. Images which maximize the this score all have a distinct pastoral quality - depictions of hills, streams and generally pleasant scenery. This is likely an artifact of the negative examples used in the training set. Synthesizing Pareidolia Lets take this even further by stripping a layer off this network. The final score, , is in fact calculated from the relative strength of two independent neurons, a \"\" neuron, and a \"\" neuron. This explains the phenomena above, as the neuron gets excited on the sight of rolling hills and running brooks, and the excitations of correlate with, well, pornography. The classifier takes in both these expert opinions, and combines them democratically by the softmax, to get the final score. Since most pornography does not take place with a Thomas Kinkade painting in the background, so this is a fair heuristic for most real world problems. But what happens if we try to excite both neurons simultaneously? This amounts to minimizing Surprisingly, from my experiments, for and , the relative strength of still dominates. However, there is enough of a contribution of to produce images of a very different flavor. Spurred on by the success above, I explore the possibility of the generation of images for which activations span two different networks. Nguyen et al. has achieved great results on the MIT scene recognition model places-CNN . What happens when we maximize neurons of places-CNN and open_nsfw together? We will refer to the places-CNN classifier's belief that an image belongs to category as . These categories are one of possible labels, such as \"marketplace\" or \"abbey\". We perform descent on this linear combination of the two objectives: (The above equation isn't strictly correct, and needs one more tweak for this to work. For details of the optimization, I refer you to the code) This program produces the most remarkable results. The images generated range from the garishly explicit to the subtle. But the subtle images are the most fascinating as to my surprise they are only seemingly innocent. These are not adversarial examples per-say. The NSFW elements are all present, just hidden in plain sight. Once you see the true nature of these images, something clicks and it becomes impossible to unsee. I've picked a few of my favorite results for show here. Beach Canyon Concert Gallery Coral Reef Desert Museum Tower Volcano The generative capacity of convolutional neural nets are, quite simply, remarkable. If you liked this project, say hi here. And you can view my badly commented code for the second part of this project here. You will need this library, and of course, open_nsfw to run it. I trust you'll figure the rest out. If you really want to, you can follow me on twitter. Follow @gabeeegoooh This is my website ","flair":"four\tProject"}
{"author":"evc123","created":"Thu Nov 03 02:19:17 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.02995 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.AI References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Extrapolation and learning equations Authors: Georg Martius, Christoph H. Lampert (Submitted on 10 Oct 2016) Abstract: In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified. Comments: 13 pages, 8 figures, 4 tables Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI) MSC classes: 68T05, 68T30, 68T40, 62J02, 65D15 ACM classes: I.2.6; I.2.8 Cite as: arXiv:1610.02995 [cs.LG]   (or arXiv:1610.02995v1 [cs.LG] for this version) Submission history From: Georg Martius [view email] [v1] Mon, 10 Oct 2016 16:47:36 GMT (1884kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"shonburton","created":"Mon Oct 10 20:16:42 EDT 2016","text":"The 2016 Machine Learning Conference in SF is scheduled for November 11th, 2016 at Hotel Nikko. Located at the heart of picturesque San Francisco, this venue boasts a large meeting space with natural light, views of the bay, and with several screens\u2013you won\u2019t miss a single slide of your favorite ML presentations.","flair":"two\tNews"}
{"author":"casperkaae","created":"Mon Oct 17 09:23:44 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.04490 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Amortised MAP Inference for Image Super-resolution Authors: Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, Ferenc Huszár (Submitted on 14 Oct 2016) Abstract: Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR. Subjects: Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1610.04490 [cs.CV]   (or arXiv:1610.04490v1 [cs.CV] for this version) Submission history From: Casper Kaae Sønderby [view email] [v1] Fri, 14 Oct 2016 14:58:44 GMT (5502kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"_bskaggs","created":"Tue Nov 01 16:08:40 EDT 2016","text":" Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 55\u201364, Austin, TX, November 1, 2016. c©2016 Association for Computational Linguistics Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions Hao Fang Hao Cheng Mari Ostendorf University of Washington {hfang,chenghao,ostendorf}@uw.edu Abstract Many social media platforms offer a mecha- nism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predict- ing community endorsement in online discus- sions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the la- tent modes can be used to weight text features thereby improving prediction accuracy. 1 Introduction Online discussion forums provide a platform for people with shared interests (online communities) to discuss current events and common concerns. Many forums provide a mechanism for readers to indicate positive\/negative reactions to comments in the dis- cussion, with up\/down votes, \u201Cliking,\u201D or indicating whether a comment is useful. The cumulative re- action, which we will refer to as \u201Ccommunity en- dorsement,\u201D can be useful to readers for prioritizing what they read or in gathering information for deci- sion making. This paper introduces the task of au- tomatically predicting the level of endorsement of a comment based on the response structure of the discussion and the text of the comment. To address this task, we introduce a neural network architecture that learns latent discussion structure (or, conversa- tion) modes and adjusts the relative dependence on text vs. structural cues in classification. The neural network framework is also useful for combining text with the disparate features that characterize the sub- mission context of a comment, i.e. relative timing in the discussion, response structure (characterized by graph features), and author indexing. The idea of conversation modes stems from the observation that regions of a discussion can be quali- tatively different: low vs. high activity, many partici- pants vs. a few, etc. Points of high activity in the dis- cussion (comments that elicit many responses) tend to have higher community endorsement, but some points of high activity are due to controversy. We hypothesize that these cases can be distinguished by the submission context, which we characterize with a vector of graph and timing features extracted from the local subgraph of a comment. The context vec- tors are modeled as a weighted combination of latent basis vectors corresponding to the different modes, where bases are learned using the weak supervision signal of community endorsement. We further hy- pothesize that the nature of the submission context impacts the relative importance of the actual text in a comment; hence, a mode-dependent gating mecha- nism is introduced to weight the contribution of text features in estimating community endorsement. The model is assessed in experiments on Red- dit discussion forum data, using karma (the differ- ence in numbers of up and down votes) as a proxy for community endorsement, showing benefits from both the latent modes and the gating. As described further below, the prediction task differs somewhat from prior work on popularity prediction in two re- spects. First, the data is not constrained to control 55 for either submission context or comment\/post con- tent, but rather the goal is to learn different context modes that impact the importance of the message. Second, the use of the full discussion thread vs. a limited time window puts a focus on participant in- teraction in understanding community endorsement. 2 Related Work The cumulative response of readers to social me- dia and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for, so the total (karma in Reddit) is a reasonable proxy for community endorsement. For all the different types of measures, a challenge in predicting the cumulative reaction is that the cases of most interest are at the tails of a Zipfian distribu- tion. Various prediction tasks have been proposed with this in mind, including regression on a log score (Bandari et al., 2012), classification into 3-4 groups (e.g. none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al., 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015). In our work, we take the approach of classification, but use a finer grain quantization with bins automat- ically determined by the score distribution. The work on cumulative reaction has mostly con- sidered two different scenarios: predicting responses before a comment\/document has been published vs. after a limited lookahead time for extracting fea- tures based on the initial response. While the frame- work proposed here could handle either scenario, the experiments reported allow the classifier to use a longer future window, until most of the discussion has played out. This provides insight into the dif- ficulty of the task and illustrates that volume of re- sponses alone does not reliably predict endorsement. A few studies investigate language factors that may impact popularity through carefully controlled experiments. To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors. Our work differs in that content is not controlled and the submission context includes the response structure and relative timing of the comment within the dis- cussion. Tan et al. (2014) futher control the author and temporal factors in addition to the topic of the content, by ranking pairs of tweets with almost iden- tical content made by the same author within a lim- ited time window. Jaech et al. (2015) control the temporal factor for ranking Reddit comments made in a time-limited window and study different lan- guage factors. Here, rather than manually control- ling the submission context, we propose a model to discover latent modes of submission context (rela- tive timing, response structure) and analyze its util- ity in predicting community endorsement. Further- more, we study how the usefulness of language in- formation in estimating the community endorsement varies depending on submission context. 3 Data and Task Data: Reddit (https:\/\/www.reddit.com) is a discussion forum with thousands of sub- communities organized as subreddits. Users can initiate a tree-structured discussion thread by mak- ing a post in a subreddit. Comments are made either directly to the root post or to other comments within the thread, sometimes triggering sub-discussions. Each comment can receive upvotes and downvotes from registered users; the difference is shown as the karma score beside the comment. The graph structure of a Reddit disccussion thread is shown in Fig. 1.1In this paper, three popular subreddits are studied: AskMen (1,057K comments), AskWomen (814K comments), and Politics (2,180K comments). 1Visualization obtained from https:\/\/whichlight. github.io\/reddit-network-vis. 56 Figure 1: Visualization of a Reddit discussion thread. The or- ange node represents the root post; other nodes are comments (size proportional to karma), which are in black unless the user comments more than once in the thread. Task: In many discussion forums, including the those explored here, community endorsement (i.e., karma in Reddit) has a heavy-tailed Zipfian distribu- tion, with most comments getting minimal endorse- ment and high endorsement comments being rare. Since the high endorsement comments are of most interest, we do not want to treat this as a regression problem using a mean squared error (MSE) objec- tive.2 Instead, we quantize the karma into J + 1 discrete levels and design a task consisting of J bi- nary classification subtasks which individually pre- dict whether a comment has karma of at least level-j for each level j = 1, . . . , J given the text of the com- ment and the structure of the full discussion thread. (All samples have karma at least level-0.) Karma scores are quantized into 8 levels of com- munity endorsement according to statistics com- puted over a large collection of comments in the subreddit. The quantization process is similar to the head-tail break rule described in (Jiang, 2013). First, comments with karma no more than 1 are la- beled as level-0, indicating that these comments re- ceive no more upvotes than downvotes.3 Then, we compute the median karma score for the rest of the comments, and label those with below-than-median karma as level-1. This process is repeated through level-6, and the remaining comments are labeled as 2A prediction error of 50 is minimal for a comment with karma of 500 but substantial for a comment with karma of 1, and the low karma comments dominate the overall MSE. 3The inital karma score of a comment is 1. 12 13 14 15 16 17 18 19 20 21 AskMen AskWomen Politics Lo g 2 (n u m b er o f sa m p le s) level-0 level-1 level-2 level-3 level-4 level-5 level-6 level-7 Figure 2: The data distribution for each subreddit. level-7. The resulting data distributions are shown in Fig. 2. Note that the quantization is subreddit de- pendent, since the distribution and range of karma tends to vary for different subreddits. Evaluation metric: Since we use a quantization scheme following a binary thresholding process, we can compute the F1 score for each level-j subtask (j = 1, 2, . . . , 7) by treating comments whose pre- dicted level is lower than j as negative samples and others as positive samples. To evaluate the overall prediction performance, the seven F1 scores are ag- gregated via a macro average, which effectively puts a higher weight on the higher endorsement levels. 4 Model Description The proposed model utilizes two kinds of infor- mation for a comment to predict its quantized karma: (1) the submission context encoded by a set of graph and timing statistics, and (2) the textual content of the comment itself. Both sources of infor- mation are first embedded in a continuous space by a neural network as illustrated in Fig. 3, where c ∈ RC and d ∈ RD encode the submission context and the textual content, respectively. As described further below, the two vectors are transformed for use in the final decision function to c̃, a linear combination of latent basis vectors, and d̃, a context-dependent weighted version of the text features. Submission context modes: Reddit discussions have a variety of conversation structures, includ- ing sections involving many contributors or just a few. Based on observations that high karma com- 57 Figure 3: Proposed model: Gray circles c and d are the pro- jected submission context features and the encoded textual con- tent vector, respectively. Blue boxes b1, · · · ,bK are latent ba- sis vectors, which are learned by the neural network. Purple diamonds a1, · · · ,aK and g represent scalers, i.e., the basis coefficients and context-dependent gate value. Red circles c̃ and d̃ are the context embedding (i.e., a linear combination of latent basis vectors) and the weighted text embedding, respec- tively. The yellow circle y is the output layer. Black arrows are connections carrying weight matrices. ⊗ and ⊕ indicate multi- plication and element-wise addition, respectively. ments seem to co-occur with active points of dis- cussions, we identify a set of features to represent the submission context of a comment, specifically aiming to characterize relative timing of the com- ment within the discussion, participant response to the comment, and whether the comment author is the original poster (see Table 1 for the full list). The features are normalized to zero mean and unit vari- ance based on the training set. In this paper, instead of controlling for the sub- mission context, we let the model learn latent modes of submission context and examine how the learned context modes relate to different levels of commu- nity endorsement. The proposed model learns K la- tent basis vectors b1, · · · ,bK ∈ RC for characteriz- ing the submission context of a particular comment in the discussion. Given the raw submission context feature vector x ∈ RN , the model computes a vector c ∈ RC as c = LReL(Px), where P ∈ RC×N is a projection matrix, and LReL(·) is the leaky recti- fied linear function (Mass et al., 2013) with 0.1 as the slope of the negative part. Coefficients for these Range Description 0\/1 Whether the comment author is the user whoinitiated the thread. Z≥0 Number of replies to the comment. Number of comments in the subtree rooted from the comment. Height of the subtree rooted from the com- ment. Depth of the comment in the tree rooted from the original post. R≥0 Relative comment time (in hours) with re- spect to the original post. Relative comment time (in hours) with re- spect to the parent comment. Table 1: Features for representing the conversation structure. K latent bases are then estimated as ak = softmax(vT tanh(U [c; bk])), where v ∈ RC and U ∈ RC×2C are parameters to be learned. The final submission context embedding is obtained as c̃ = ∑K k=1 ak · bk ∈ RC . The computation of basis coefficients is similar to the attention mechanism that has been used in the context of machine translation (Bahdanau et al., 2015), constituency parsing (Vinyals et al., 2015), question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015). To the best of our knowledge, this is the first attempt to use the attention mechanism for latent basis learning. Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016). In this paper, we use a bi- directional RNN to encode each sentence, and con- catenate the hidden layers at the last time step of each direction as the sentence embedding. For com- ments with multiple sentences, we average the sen- tence embeddings into a single vector as the textual content embedding d ∈ RD. For the t-th token in a sentence, the hidden layers of the bi-directional RNN are computed as h(l)t = GRU(zt,h (l) t−1), h (r) t = GRU(zt,h (r) t+1), where zt ∈ RD is the token input vector, h(l)t ∈ RD and h(r)t ∈ RD are the hidden layers for the left- to-right and right-to-left directions, respectively, and 58 GRU(·, ·) denotes the gated recurrent unit (GRU), which is proposed by Cho et al. (2014) as a sim- pler alternative to the long short-term memory unit (Hochreiter and Schmidhuber, 1997) for addressing the vanishing gradient issue in RNNs. For consis- tency of the model and consideration of computation speed, we replace the hyperbolic tangent function in the GRU with the LReL function. Although not shown in Fig. 3, weight matrices in the bi-directional RNN are jointly learned with all other parameters. To generate the token input vector to the RNN, we utilize the lemma and part-of-speech (POS) tag of each token (obtained with the Stanford CoreNLP toolkit (Manning et al., 2014)), in addition to its word form. A token embedding zt ∈ RD for the t-th token in a sentence is computed as zt = Ewordewordt + E posepost + E lemmaelemmat , where et\u2019s are one-hot encoding vectors for the to- ken, and E\u2019s are parameters to be learned. The di- mensions of these one-hot encoding vectors are de- termined by the size of the corresponding vocabu- laries, which include all observed types except sin- gletons. Thus, these embedding matrices E\u2019s have the same first dimension D but different second di- mensions. This type of additive token embedding has been used in (Botha and Blunsom, 2014; Fang et al., 2015) to integrate various types of informa- tion about the token. Moreover, it reduces the tuning space since we only need to make a single decision on the dimensionality of the token embedding. Gating mechanism: For estimating comment karma levels, the textual content should provide ad- ditional information beyond the submission context. However, we hypothesize that the usefulness of tex- tual content may vary under different submission contexts since structure reflects size of the reader- ship. Therefore, we design a context-dependent gat- ing mechanism in the proposed model to weight the textual factors. A scalar gate value is estimated from the submission context embedding c̃, i.e., g = sigmoid(wT c̃), where w ∈ RC is the parameter to be learned. The textual content embedding d ∈ RD is scaled by the gate value g before being fed to the output layer, i.e., d̃ = g · d. Decision function: The estimated probability dis- tribution y = [y0, . . . , y7] over all quantized karma levels is computed by the softmax output layer, i.e., y = softmax(Q [ c̃; d̃ ] ), where Q ∈ RJ×(C+D) is the weight matrix to be learned. The hypothesized level for a comment is L̂ = argmaxjyj . For each level-j subtask, both the label L and the hypothe- sis L̂ are converted to binary values by checking the condition whether they are no less than j. 5 Parameter Learning To train the proposed model, each comment is treated as an independent sample, and the objec- tive is the maximum log-likelihood of these samples. We use mini-batch stochastic gradient descent with a batch size of 32, where the gradients are computed with the back-propagation algorithm (Rumelhart et al., 1986). Specifically, the Adam algorithm is used (Kingma and Ba, 2015). The initial learning rate is selected from the range of [0.0010, 0.0100], with a step size of 0.0005, according to the log-likelhood of the validation data at the first epoch. The learning rate is halved at each epoch once the log-likelihood of the validation data decreases. The whole train- ing procedure terminates when the log-likelihood decreases for the second time. Each comment is treated as a data sample, and as- signed to a partition number in {0, 1, . . . , 9} accord- ing to the thread it belongs to. Each partition has roughly the same number of threads. We use par- titions 4\u20139 as training data, partitions 2\u20133 as valida- tion data, and partitions 0\u20131 as test data, The training data are shuffled at the beginning of each epoch. As discussed in Section 3, there are many more low-level comments than high-level comments, and the evaluation metric effectively puts more emphasis on high-level comments. Therefore, rather than us- ing the full training and validation sets, we subsam- ple the low-level comments (level-0, level-1, level- 2, level-3) such that each level has roughly the same number of samples as level-4. Since the three sub- reddits studied in this paper vary in their sizes, to eliminate the factor of training data size, we use similar amounts of training (∼90K comments) and validation (∼30K comments) data for these subred- dits. Note that we do not subsample the test data, i.e., 192K for AskMen, 463K for AskWomen, and 1,167K for Politics. 59 20% 30% 40% 50% 60% 70% 80% AskMen AskWomen Politics level>0 leve>1 level>2 level>3 level>4 level>5 level>6 Figure 4: Individual F1 scores for the full model. 6 Experiments In this section, we present the performance of the proposed model and conduct contrastive experi- ments to study model variants in two dimensions. For the submission context features, we compare representations obtained via feedforward neural net- works to that obtained by a learned combination of latent basis vectors. In terms of textual features, we compare a model which uses no text, context- independent text features, and a context-depending gating mechanism. Finally, we analyze the learned latent submission context modes, as well as context- dependent gate values that reflect the amount of tex- tual information used by the full model. 6.1 Model Configuration All parameters in the neural networks except bias terms are initialized randomly according to the Gaussian distribution N (0, 10−2). We tune the number of latent bases K and the number of hidden layer neurons C and D based on the macro F1 scores on the validation data. For the full model, the best configuration uses K = 8, C = 32 and D = 64 for all subreddits, except Politics where D = 32. 6.2 Main Results The performance of the full model on individual lev- els is presented in Fig. 4. As expected, the low- est level comments are easier to classify. Detec- tion of high-level comments is most reliable in the Politics subreddit, but still difficult. Table 2 compares models variants that only AskMen AskWomen Politics SubtreeSize 39.1 42.9 41.7 ConvStruct 43.9 41.4 42.0 Feedfwd-1 46.5 50.6 49.6 Feedfwd-2 46.8 50.9 49.8 Feedfwd-3 47.1 50.5 50.0 LatentModes 47.0 51.0 50.3 Table 2: Test macro F1 scores for models that do not use the textual content information. AskMen AskWomen Politics No text 47.0 51.0 50.3 Un-gated 48.3 52.5 49.5 Gated 48.7 53.1 51.3 Table 3: Test macro F1 scores for models with and without the gating mechanism. All models use latent modes to represent the submission context information. use the submission context features. The SubtreeSize baseline uses a multinominal lo- gistic regression model to predict the level accord- ing to the subtree size feature alone, whereas the ConvStruct uses the same model but with all conversation structure features defined in Tabel 1. All baselines are stronger than predicting based on prior distributions, which has F1 scores in the 11- 17 range. The model Feedfwd-n is a feedforward neural network with n hidden layers; it uses the sub- mission context feature c in Fig. 3 for prediction. The model LatentBases represents the submis- sion context information by a linear combination of latent bases; it uses c̃ in Fig. 3 for prediction. Com- pared with Feedfwd-1 in terms of the number of model parameters, Feedfwd-2, Feedfwd-3 and LatentBases have C2, 2C2, and (2C2+K) extra parameters, respectively. These models have simi- lar performance, but there is a slight improvement by increasing model capacity. While the proposed method does not give a significant performance gain, it leads to a more interpretable model. Table 3 studies the effect of adding text and intro- ducing the gating mechanism. The un-gated variant uses d instead of d̃ for prediction. Without the gat- ing mechanism, textual information provides signifi- cant improvement for AskMen and AskWomen but not for Politics. With the introduced dynamic gating mechanism, the textual information is used more effectively for all three subreddits. 60 level-0 level-1 level-2 level-3 level-4 level-5 level-6 level-7 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 A B C D E F G H low medium high (a) AskMen 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 A B C D E F G H low medium high (b) AskWomen 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 A B C D E F G H low medium high (c) Politics Figure 5: Empirical distributions of levels for each latent mode. Modes are grouped by dominating levels, i.e., level-0 and level-1 as low, level-6 and level-7 as high, and the rest as medium. Within each cluster, the modes are sorted by the number of samples. (a) AskMen (b) AskWomen (c) Politics Figure 6: Visualization of learned clusters. 0 5 10 15 20 25 30 35 A B C D E F G H low medium high time_since_parent time_since_root subtree_height num_children Figure 7: Mean values of four submission context features for each latent mode of AskWomen. 6.3 Analysis In this subsection, we analyze the learned submis- sion context modes and the gate values that control the amount of textual information to be used by the model for predicting comment karma level. Submission context modes: To study the submis- sion context modes, we assign each comment to a cluster according to which basis vector receives the highest weight: argmaxk=1,...,Kak. The label distribution for each cluster is shown in Fig. 5. It can be observed that some clusters are dominated by level-0 comments, and others are dominated by level-7 comments. In Fig. 6, we visualize the learned clusters by projecting the raw conversation struc- ture features x to a 2-dimensional space using the t- SNE algorithm (van der Maaten and Hinton, 2008). For purposes of illustrating cross-domain similari- ties, we group the clusters dominated by level-0 and level-1 comments into a low endorsement cluster, those dominated by level-6 and level-7 into a high endorsement cluster, and the rest as the medium en- dorsement cluster. It can be seen that the learned clusters split the comments with a consistent pattern, with the higher endorsement comments towards the 61 AskMen AskWomen Politics medium 0.87 0.87 0.85 high 0.67 0.66 0.76 Table 4: Text gate values relative to low karma modes. right and the low endorsement comments to the left. In Fig. 7, we show mean values of four selected submission context features for each latent mode of AskWomen, where units of time are in hours. High karma comments tend to be submitted early in the discussion, and the number of children (di- rect replies) is similar to or greater than the height of its subtree (corresponding to a broad subtree). Low and medium karma comments have a ratio of number of children to subtree height that is less than one. Low karma comments tend to come later in the discussion overall (time since root) but also later in terms of the group of responses to a parent com- ment (time since parent). These trends hold for all three subreddits. All subreddits have within-group differences in the mode characteristics, particularly the low-karma modes. For AskWomen, graph clus- ter B corresponds to comments made at the end of a discussion, which are more likely to be low karma because there are fewer readers and less opportu- nity for a new contribution. Cluster C comments come earlier in the discussion but have small sub- trees compared to other early comments. Text gate: In Table 4, we show the mean gate val- ues g for each group of latent modes. Since gate val- ues are not comparable across subreddits due to dy- namic range of feature values, the values shown are scaled by the value for the low-level mode. We ob- serve a consistent trend across all subreddits: lower gate values for higher karma. Recall that the high karma comments typically spawn active discussions. Thus, a possible explanation is that users may be bi- ased to endorse comments that others are endorsing, making the details of the content less important. 7 Conclusion In summary, this work has addressed the problem of predicting community endorsement of comments in a discussion forum using a new neural network architecture that integrates submission context fea- tures (including relative timing and response struc- ture) with features extracted from the text of a com- ment. The approach represents the submission con- text in terms of a linear combination of latent basis vectors that characterize the dynamic conversation mode, which gives results similar to using a deep network but is more interpretable. The model also includes a dynamic gate for the text content, and analysis shows that when response structure is avail- able to the predictor, the content of a comment has the most utility for comments that are not in active regions of the discussion. These results are based on characterizing quantized levels of karma with a series of binary classifiers. Quantized karma predic- tion could also be framed as an ordinal regression task, which would involve a straightforward change to the neural network learning objective. This work differs from related work on popularity prediction in that the task does not control for con- tent of a post\/comment, nor limit the time window of the submission. With fewer controls, it is more dif- ficult to uncover the aspects of textual content that contribute to endorsement, but by conditioning on submission context we can begin to understand herd effects of endorsement. The task described here also differs from previous work in that the full (or almost full) discussion thread is available in extracting fea- tures characterizing the response to the comment, but the modeling framework would also be useful with a limited window lookahead. The results using the full discussion tree also show the limits of using response volume to measure endorsement. A limitation of this work is that the submission context is represented only in terms of the relative timing and graph structure in a discussion thread and does not use the text within earlier or responding comments. Prior work has shown that the relevance of a comment to the preceding discussion matters (Jaech et al., 2015), and clearly the sentiment ex- pressed in responses should provide important cues. Capturing these different sources of information in a gated framework is of interest for future work. Acknowledgments This paper is based on work supported by the DARPA DEFT Program. Views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. 62 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. Int. Conf. Learning Representations (ICLR). Roja Bandari, Sitaram Asur, and Bernardo Huberman. 2012. The pulse of news in social media: forecasting popularity. In Proc. Int. AAAI Conf. Web and Social Media (ICWSM). Jan A. Botha and Phil Blunsom. 2014. Composi- tional morphology for word representations and lan- guage modelling. In Proc. Int. Conf. Machine Learn- ing (ICML). Justin Cheng, Lada Adamic, P. Alex Dow, Jon Michael Kleinberg, and Jure Leskovec. 2014. Can cascade be predicted? In Proc. Int. Conf. World Wide Web (WWW), pages 925\u2013936. Hao Cheng, Hao Fang, and Mari Ostendorf. 2015. Open- domain name error detection using a multi-task RNN. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP). Kyunghyun Cho, Bart van Merriënboer, Caglar Gul- cehre, Dzmitry Bahadanau, Fethhi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statis- tical machine translation. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1724\u20131734. Hao Fang, Mari Ostendorf, Peter Baumann, and Janet Pierrehumbert. 2015. Exponential language modeling using morphological features and multi-task learning. IEEE Trans. Audio, Speech, and Language Process., 23(12):2410\u20132421, December. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735\u2013 1780, November. Liangjie Hong, Ovidiu Dan, and Brian Davison. 2011. Predicting popular messages in Twitter. In Proc. Int. Conf. World Wide Web (WWW), pages 57\u201358. Aaron Jaech, Vicky Zayats, Hao Fang, Mari Ostendorf, and Hannaneh Hajishirzi. 2015. Talking to the crowd: What do people react to in online discussions? In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 2026\u20132031. Bin Jiang. 2013. Head\/tail break: A new classification scheme for data with a heavy-tailed distribution. The Professional Geographer, 65(3):482\u2013494. Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proc. Int. Conf. Learning Representations (ICLR). Himabindu Lakkaraju, Julian McAuley, and Jure Leskovec. 2013. What\u2019s in a name? Understanding the interplay between titles, content, and communities in social media. In Proc. Int. AAAI Conf. Web and So- cial Media (ICWSM). Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language pro- cessing toolkit. In Proc. Annu. Meeting Assoc. for Computational Linguistics: System Demonstrations, pages 55\u201360. Andrew L. Mass, Awni Y. Hannun, and Andrew Y. Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. Int. Conf. Machine Learning (ICML). Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xi- aodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long short-term memory networks: Analysis and appli- cation to information retrieval. IEEE Trans. Au- dio, Speech, and Language Process., 24(4):694\u2013707, April. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning representations by back- propogating errors. Nature, 323(6088):533\u2013536, Oc- tober. B. Suh, L. Hong, P. Pirolli, and E. H. Chi. 2010. Want to be retweeted? large scale analytics on factors impact- ing retweet in twitter network. In Proc. IEEE Inter. Conf. on Social Computing (SocialCom), pages 177\u2013 184. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 2431\u20132439. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 3104\u20133112. Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The ef- fect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter. In Proc. Annu. Meeting Assoc. for Computational Lin- guistics (ACL), pages 175\u2013186. Manos Tasgkias, Wouter Weerkamp, and Maarten de Ri- jke. 2009. Predicting the volume of comments on on- line news stories. In Proc. CIKM, pages 1765\u20131768. Alexandru Tatar, Jeremie Leguay, Panayotis Antoniadis, Arnaud Limbourg, Marcelo Dias de Amorim, and Serge Fdida. 2011. Predicting the polularity of online articles based on user comments. In Proc. Inter. Conf. on Web Intelligence, Mining and Semantics (WIMS), pages 67:1\u201367:8. Laurens van der Maaten and Geoffrey Hinton. 2008. Vi- sualizing data using t-SNE. Machine Learning Re- search, 9, Nov. 63 Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar as a foreign language. In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 2755\u20132763. Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks. In Proc. Int. Conf. Learning Rep- resentations (ICLR). Tae Yano and Noah A. Smith. 2010. What\u2019s worthy of comment? content and comment volume in political blogs. In Proc. Int. AAAI Conf. Weblogs and Social Media (ICWSM). Qingyuan Zhao, Murat A. Erdogdu, Hera Y. He, Anand Rajaraman, and Jure Leskovec. 2015. SEISMIC: A self-exciting point process model for predicting Tweet popularity. In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Mining. 64 ","flair":"three\tResearch"}
{"author":"zergling103","created":"Tue Nov 08 19:15:55 EST 2016","text":"I remember finding a paper about using NN's to compress image data, in a similar way to JPEG, where it is possible to vary the bits per pixel to get better or worse reconstructions.\n\nI found this: https:\/\/arxiv.org\/abs\/1604.08772 It used MSE or something to that effect, so as a result you'll get blurrier, indistinct results as you remove bits.\n\nHowever, there was a similar paper, which I think used adversarial techniques to ensure that the resulting sample was always natural-looking, where compressed patches always looked like real image data. Instead of getting blurry as you remove bits, the patches would look less and less like the image you gave it, but still look \"realistic\". In other words, the network hallucinated missing information.\n\nIf anyone could help me find this paper, I'd be very thankful! In the meantime I'll look through my very long list of bookmarks of other papers to see if I find it first (assuming I bookmarked it). :)\n\nEDIT: Couldn't find it in bookmarks. Googling isn't helping. :(\n\nEDIT: Thank you very much for all of the interesting papers. :)","flair":"one\tDiscussion"}
{"author":"f9d8hv3sl","created":"Sat Oct 08 12:41:41 EDT 2016","text":"press release:\nhttp:\/\/www.mitsubishielectric.co.jp\/news\/2016\/pdf\/1007.pdf\n\n\nnews article was behind a soft wall, so I copy-pasted it here:\n\nhttp:\/\/techon.nikkeibp.co.jp\/atcl\/news\/16\/100704454\/\n\n三菱電機は、ニューラルネットの中間層を自動的に生成できる技術を開発したと発表した（発表資料）。ニューラルネットの入力と出力を決めれば、人手を介さずに学習用のデータを用いて、適切な中間層を作ることが可能である。できあがったニューラルネットの性能は人手で設計した場合の優れた結果と同等以上という。この技術を使うことで、ニューラルネットを利用したい技術者自身が、人工知能（AI）の専門家がいなくても高い性能のニューラルネットを構築可能になる。例えば、これまで1年を要したニューラルネットの構築作業があったとすると、この技術で1カ月ほどに減らせるいう。\n[画像のクリックで拡大表示]\n図1　隠れ層（中間層）のノード数や重みの初期値を自動的に生成\n[画像のクリックで拡大表示]\n図2　ネオコグニトロンの開発者として知られる福島邦彦氏\n\n　同社の情報技術総合研究所 主席研究員で、ネオコグニトロンの開発者として知られる福島邦彦氏が開発した「AiS（add-if-silent）」と呼ぶ方式を利用する。まず、入力層のニューロンに対して中間層のニューロンを1つ用意し、最初に入力したデータの値に比例する重みで結合する。このネットワークに次のデータを入れた時に、中間層のニューロンが発火しなければ、中間層のニューロンを1つ増やす。用意した学習データに対してこの作業を続け、どのようなデータを入力しても中間層のいずれかのニューロンが反応するようになった時が、適切なニューロン数だと判断する。「ニューラルネットが過学習しないように、中間層のニューロンがどの程度の入力で発火するかの設計が重要になる」（福島氏）。 \n\n今回は、この中間層の後に出力層を設けた3層のネットワークで、誤差逆伝播法（バックプロパゲーション）を用いて重みの値を最適化した。学習に使ったデータは、中間層を作る際に使ったものと同じである。天候や曜日といった10次元程度の入力から、利用されるレンタル自転車の数を予測するニューラルネットに適用したところ、良好な結果が得られたという。学習に利用したデータは1000程度である。この成果は10月16～21日に京都大学で開かれる国際会議「ICONIP（International Conference on Neural Information Processing）2016」で発表する。　\n\n　今回のニューラルネットは規模が小さくデータ量も少ないが、より規模の大きなネットワークに適用することに大きな問題はないという。今後は中間層の層数を増やす方策も盛り込むもよう。例えば、上述のような3層のネットワークで学習が収束しなければ、再びAiS法を使って次の層を作成。このネットワークでも収束しなければさらに層を追加する、といった作業を繰り返す手法などが考えられるという。\n\n　開発した技術は、まずは同社の機器に組み込むニューラルネットの開発に利用する。「学習に多くのデータが必要なので、自社の中でも数が多く出ている製品が対象になるだろう」（同社 情報技術総合研究所 知能情報処理技術部長の三嶋 英俊氏）。実用化の時期は未定という。 \n ","flair":"null\tnull"}
{"author":"Mandrathax","created":"Thu Oct 27 12:31:15 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.08123 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Socratic Learning: Empowering the Generative Model Authors: Paroma Varma, Rose Yu, Dan Iter, Christopher De Sa, Christopher Ré (Submitted on 25 Oct 2016 (v1), last revised 9 Nov 2016 (this version, v2)) Abstract: Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they \"tell a story\" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts. Comments: 3 figures Subjects: Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1610.08123 [cs.LG]   (or arXiv:1610.08123v2 [cs.LG] for this version) Submission history From: Paroma Varma [view email] [v1] Tue, 25 Oct 2016 23:43:49 GMT (129kb,D) [v2] Wed, 9 Nov 2016 08:00:06 GMT (123kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"evc123","created":"Thu Oct 27 10:42:56 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.06402 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.AI < prev | next > new | recent | 1610 Change to browse by: cs cs.LG cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Artificial Intelligence Title: A Growing Long-term Episodic & Semantic Memory Authors: Marc Pickett, Rami Al-Rfou, Louis Shao, Chris Tar (Submitted on 20 Oct 2016) Abstract: The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains. Comments: Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material Subjects: Artificial Intelligence (cs.AI); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.06402 [cs.AI]   (or arXiv:1610.06402v1 [cs.AI] for this version) Submission history From: Marc Pickett [view email] [v1] Thu, 20 Oct 2016 13:29:56 GMT (216kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"HazrMard","created":"Tue Oct 11 23:24:12 EDT 2016","text":"Machine Learning on imgur. Under development. Basic interface is complete. is a set of tools that help parse, analyze, and act on textual data on imgur.com. It provides a modular interface that lets you build trivial to complex workflows. From making a random comment bot, to systemically detecting and downvoting memes and posts about cats: you can do it all. The latter was my primary motivation for this project, but that's another story. While initially written for imgur, the project has been structured so that it can be easily ported to another API. Additionally, many machine learning functions in can be universally used with any data set out of the box. provides an option to use Principal Component Analysis (PCA) as a way to reduce the dimensionality of data points. For a given set of data, it finds vectors that best describe the set's distribution. Those vectors can be used as axes for later computations. For example, posts on imgur.com may have thousands of unique words which would mean thousands of dimensions in the data. But with PCA, a few vectors can be used to distinguish posts without significant loss in accuracy. With fewer vectors needed to describe posts and users (by their comments), other computations become less costly. For example, you can try to predict: All of the machine learning methods above in parentheses have been implemented in the class, with more to come. also provides the class. It can post comments, upload pictures, send messages etc. interactively or in the backround on whatever schedule you choose. See docs folder for more details on usage. This was my first ever project on github. I had left it in suspension in favour of other things to do. I came back more than a year later to finish this. If you compare last year's version v1 with this one () you'll notice quite a change in approach. That's the lesson here folks,","flair":"four\tProject"}
{"author":"buriburisuri","created":"Sat Nov 12 21:10:23 EST 2016","text":"A tensorflow implementation of French-to-English machine translation using DeepMind's ByteNet from the paper Nal et al's Neural Machine Translation in Linear Time. This paper proposed the fancy method which replaced the traditional RNNs with conv1d dilated and causal conv1d, and they achieved fast training and state-of-the-art performance on character-level translation. The architecture ( from the paper ) I've used NLTK's comtrans English-French parallel corpus for convenience. You can easily download it as follows: to train the network. You can see the result ckpt files and log files in the 'asset\/train' directory. Launch tensorboard --logdir asset\/train\/log to monitor training process. I've trained this model on a single Titan X GPU during 10 hours until 50 epochs. If you don't have a Titan X GPU, reduce batch_size in the train.py file from 16 to 8. to translate sample French sentences to English. The result will be printed on the console. The result looks messy but promising. Though Comtrans corpus in NLTK is very small(in my experiment only 17,163 pairs used), the model have learned English words structures and syntax by character level. I think that the translation accuracy will be better if we use big corpus. You can translate French sentences to English sentences with the pre-trained model on the Comtrans corpus in NLTK. Extract the following zip file in 'asset\/train\/ckpt'. And try another sample French sentences in the 'translate.py' file. Namju Kim (buriburisuri@gmail.com) at Jamonglabs Co., Ltd.","flair":"four\tProject"}
{"author":"minimum_liklihood","created":"Tue Nov 15 09:13:26 EST 2016","text":" About Blog OpenAI Gym Requests for Research OpenAI and Microsoft Greg Brockman, Ilya Sutskever, and Sam Altman November 15, 2016 We're working with Microsoft to start running most of our large-scale experiments on Azure. This will make Azure the primary cloud platform that OpenAI is using for deep learning and AI, and will let us conduct more research and share the results with the world. One of the most important factors for accelerating our progress is accessing more and faster computers; this is particularly true for emerging AI technologies like reinforcement learning and generative models. Azure has impressed us by building hardware configurations optimized for deep learning \u2014 they offer K80 GPUs with InfiniBand interconnects at scale. We're also excited by their roadmap, which should soon bring Pascal GPUs onto their cloud. In the coming months we will use thousands to tens of thousands of these machines to increase both the number of experiments we run and the size of the models we train. We'll share the results of this partnership with everyone: along with publishing our research results, we'll continue releasing open-source software making it easier for people to run large-scale AI workloads on the cloud. We'll also be giving feedback to the Microsoft team so that Azure's capabilities keep pace with our understanding of AI. It's great to work with another organization that believes in the importance of democratizing access to AI. We're looking forward to accelerating the AI community through this partnership. ","flair":"two\tNews"}
{"author":"mimighost","created":"Thu Nov 24 03:51:50 EST 2016","text":" Home News People Publications Data and tools Student opportunities Home Humies 2014 Competition Data and tools Active Learning of Regular Expressions for Entity Extraction Annotated strings for learning text extractors Automatic Synthesis of Regular Expressions from Examples Can A Machine Replace Humans In Building Regular Expressions? A Case Study Evolutionary Inference of Attribute-based Access Control Policies Ghega-dataset Hidden fraudulent URLs dataset Inference of Regular Expressions for Text Extraction from Examples Paper citations for important Computer Science venues XML data for automatic schema generation Manage News ...and then IEEE Intelligent Systems ! Active Learning of Regular Expressions Automatic Generation of Access Control Policies Automatic Generation of Regular Expression: version 1.1 Automatic generation of regular expressions Automatic Regex Generation - IEEE Computer paper (!) Bibliometric evaluation of researchers in the Internet age Compressing Regular Expression Sets for Deep Packet Inspection Computer Aided Design of Football Teams Continuous and Non-intrusive Reauthentication of Web Sessions based on Mouse Dynamics Fraudulent pages in web sites of Italian Administrations IEEE Transactions (TKDE) paper! Lab paper \"Detection of hidden fraudulent URLs...\" Lab paper about Italian PA web pages at IAS 2012 Lab paper at GECCO 2013 Lab paper at IEEE\/WIC\/ACM WI 2012 Lab paper on GP for a medical application at ISPA Learning multiple patterns for text extraction Learning of Syntax Patterns: GECCO! New regex generator tool online! New section on data and tools Online! Our recent papers... Our work on GP for Regex at GECCO 2012 PhD School at Krakow, Poland Playing a programming competition against humans... Recent publications... Recommending the right publication venue: we'll show how at ICTAI Silver medal at the 2016 \"Human competitive\" awards!!! Source code for regex generator is now public Started large scale Twitter collection The Lab at ICWE 2012 Twitter event *tion presented at WI 2012 Two lab papers at i-Society 2012 Uncitedness of high quality work is not an exceptional case We are finalist (again) at the \"Human-Competitive\" Awards! We are finalist at the \"Human-Competitive\" Awards! News archive People Publications International Conference Publications International Journal Publications Other publications Student opportunities Sitemap Publicationsâ\u20ACŽ > â\u20ACŽInternational Journal Publicationsâ\u20ACŽ > â\u20ACŽ Inference of Regular Expressions for Text Extraction from Examples posted Mar 21, 2016, 4:48 AM by Eric Medvet Â  [ updated May 26, 2016, 2:57 AM ] IEEE Transactions on Knowledge and Data Engineering (TKDE), 2016 Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, Fabiano Tarlao Google Scholar, DOI:Â 10.1109\/TKDE.2016.2515587, Publisher version A large class of entity extraction tasks from text that is either semistructured or fully unstructured may be addressed by regular expressions, because in many practical cases the relevant entities follow an underlying syntactical pattern and this pattern may be described by a regular expression. In this work we consider the long-standing problem of synthesizing such expressions automatically, based solely on examples of the desired behavior. We present the design and implementation of a system capable of addressing extraction tasks of realistic complexity. Our system is based on an evolutionary procedure carefully tailored to the specific needs of regular expression generation by examples. The procedure executes a search driven by a multiobjective optimization strategy aimed at simultaneously improving multiple performance indexes of candidate solutions while at the same time ensuring an adequate exploration of the huge solution space. We assess our proposal experimentally in great depth, on a number of challenging datasets. The accuracy of the obtained solutions seems to be adequate for practical usage and improves over earlier proposals significantly. Most importantly, our results are highly competitive even with respect to human operators. A prototype is available as a web application at http:\/\/regex.inginf.units.it. ÄŒ Updating... ÄŠ 2016-TKDE-RegexInference.pdf (879k) Eric Medvet, Mar 21, 2016, 4:48 AM v.1 Ä� Ï\u20AC Sign in|Report Abuse|Powered By Google Sites ","flair":"three\tResearch"}
{"author":"shahinrostami","created":"Tue Nov 15 15:50:10 EST 2016","text":"3:06 - To skip to the main contentAn introduction to the topic of Evolutionary Computation, with a simple example of an Evolutionary Algorithm.This introduction is intended for everyone, specially those who are interested in learning about something new. No pre-existing knowledge of the subject or any scientific background is expected.My website:http:\/\/shahinrostami.comTwitter:@shahinrostamiFurther playing:http:\/\/boxcar2d.com","flair":"one\tDiscusssion"}
{"author":"The_Man_of_Science","created":"Tue Sep 27 00:24:09 EDT 2016","text":"Please use amodern browser with JavaScript enabled to use Coursera. 请下载现代的浏览器（IE10或Google Chrome）来使用Coursera。 تحميل Lädt... Chargement... Loading... Cargando... Carregando... Загрузка... Yükleniyor... 载入中 Please use amodern browser with JavaScript enabled to use Coursera. ","flair":"null\tnull"}
{"author":"darkconfidantislife","created":"Wed Oct 26 00:46:00 EDT 2016","text":"Hi guys, I'm having a bit of trouble understanding the difference between transposed convolution (deconvolution) and dilated convolution. Could someone please explain it to me? From what I understand, they both seem to be the same thing, you \"expand\" the matrix by interpolating with 0s to make the dimensions bigger and then convolve as normal.\n\nThanks in advance :)","flair":"one\tDiscussion"}
{"author":"downtownslim","created":"Tue Oct 04 15:28:05 EDT 2016","text":"We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight- Networks, the filters are approximated with binary values resulting in 32× memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations (in terms of number of the high precision operations) and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and out-perform these methods by large margins on ImageNet, more than 16% in top-1 accuracy. Less","flair":"null\tnull"}
{"author":"zergling103","created":"Thu Sep 29 21:23:44 EDT 2016","text":"Tl;dr - has this idea about neural network architecture, similar to autoencoders, been developed already? \n\nCorrect me if I'm wrong about anything, but this is my understanding so far:\n\nAutoencoders purposefully have an information bottleneck in the middle, and this bottleneck is what forces the network to learn high level representations of the input data. Otherwise, without the bottleneck, the network may discover that the optimal connection is one that is roughly equivalent to directly mapping each input to its corresponding output.\n\nHowever, because that bottleneck exists, it is very unlikely that an autoencoder could perform a perfect reconstruction of the input.\n\nContrast this with other encoding, like DCT as used by JPEG images. Each 8x8 block of values (pixel intensity)gets transformed into another block of 8x8 values (coefficients) - there is no bottleneck, and one could losslessly reconstruct the input pixel intensities from the coefficients. However, despite lacking a bottleneck, this representation is higher level than raw pixel intensities. Further, these coefficients are ordered in a way such that, generally, coefficients earlier in the array are likely to contribute more to reducing reconstruction error than those further down.\n\nThis had me thinking: one could build a network that could theoretically perfectly reconstruct the input data (like DCT), while retaining the ability to represent the input data with high-level abstractions as exhibited by autoencoders.\n\nSay we wanted to build an autoencoder on 32x32 pixel face images: One could imagine an autoencoder where the bottleneck has only 1 neuron. When training, the network would attempt to encode as much information as possible in that neuron. Obviously, reconstruction would be imperfect as it would likely only interpolate between two different images. However, because we are trying to minimize reconstruction error, the network might come up with two archetypal faces (perhaps male\/female, or, dark skinnes\/light skinned) that form a sort of line or curve of best fit to the training data.\n\nNow imagine that we added another neuron to the bottleneck layer. The second neuron would attempt to minimize the residual reconstruction error left over by the first neuron. Meanwhile, the first neuron still attempts to reconstruct the input data as though it had no help from other neurons in the bottleneck layer.\n\nNow imagine that this pattern repeats for the third neuron in this layer, the fourth, and so on: each neuron would be trained to minimize the residual reconstruction error left over by the neurons above it. The reconstruction error decreases exponentially as it gets passed down further. Neurons at the top of the chain in this bottleneck layer contribute the most to minimizing reconstruction error, but these contributions are very general. Further down the chain, neurons contribute much subtler reconstruction improvements. Neurons at the very bottom of the chain may contribute imperceptible improvements. In the end the \"bottleneck\" could have just as many neurons as the input layer, thus not being a bottleneck anymore, while still providing a high level representation of the input data.\n\nHas a network like this been developed already?","flair":"null\tnull"}
{"author":"jstaker7","created":"Tue Oct 04 00:41:02 EDT 2016","text":"So I've been thinking about this for a while. If we use a deep CNN to compressing an image into a 1D vector (which can be used for some arbitrary downstream application, like classification) how do I size the very last layer before the FC layers?\n\nFor example, our images may now only be 2x2 in size from all the downsampling that has happened from previous pooling layers. Do we just convolve with a 2x2 kernel and then pool again so we have a 1D vector? It seems like that last pooling layer would throw away so much information since each pixel now represents a quarter of our entire original image, yet my model seems to be performing OK. Is there a better way, such as flatten the matrix rather than pool on the last layer, or does it make much of a difference?","flair":"null\tnull"}
{"author":"Eridrus","created":"Thu Sep 29 17:34:55 EDT 2016","text":"Earlier this year at GCP NEXT, we introduced new Cloud Machine Learning products with the intention to change the way businesses operate and create new customer experiences, while deepening the insights derived from data.Today, we want to share how Google aims to help more businesses benefit from the advancements in machine learning, while making it easier for them to use it. Google Cloud Machine Learning is now publicly available in beta and can empower all businesses to easily train quality machine learning models at a faster rate. With its powerful distributed training capability, you can train models on terabytes of data within hours, instead of waiting for days. Integrated with Google Cloud Platform (GCP), Cloud Machine Learning is a fully-managed service that can scale and creates a rich environment across TensorFlow and cloud computing tools such as Google Cloud Dataflow, BigQuery, Cloud Storage and Cloud Datalab. We're also introducing a new feature, HyperTune, that automatically improves predictive accuracy. It allows data scientists to build better performing models faster by automatically tuning hyperparameters, instead of manually discovering values that work for their model. Machine learning can unlock new possibilities for businesses \u2014 from improving customer service to more accurately streamlining operations to creating new applications and experiences. One of our customers, Airbus Defense and Space, tested the use of Google Cloud Machine Learning to automate the process of detecting and correcting satellite images that contain imperfections such as the presence of cloud formations. Historically, this process was time consuming, prone-to-error and was unable to scale as satellite technology improved the quality and amount of images available.Satellite images are often used by industries such as agriculture and civil engineering. Common use-cases are precision farming for predicting crop yields and identifying the crops\u2019 health, environmental groups for monitoring forestry, and city governments for land management. The ability to detect patterns in satellite images \u2014 such as the difference between snow and clouds \u2014 is critical to Airbus Defense and Space\u2019s users who depend on highly precise, up-to-date and reliable information. We're also excited to announce a dedicated machine learning practice within our Professional Services team. Our practice helps immerse businesses into the full capabilities of machine learning to determine how machine learning can help to solve individual needs. For more information on these services and the rest of the Professional Services offered, please contact your Google Cloud Sales Representative or visit our website. And last, but certainly not the least, we\u2019re rolling out a certification programto help bring machine learning to more businesses than ever. Although real-world, hands-on experience is the best preparation for the certification, we recommend our data engineer training to help you strengthen your skill set. The Google Data Engineer Certification and training are aimed at businesses, partners, and data scientists who want to design, train, and deploy accurate machine learning models to gain insights previously out of reach. We recognize that machine learning has traditionally required specialized training and expertise. By opening these programs, more users can learn to create new machine learning applications. Ready to try out Cloud Machine Learning? Tell us what you think, and we\u2019ll do our best to address your feedback and make data science and analysis even simpler for you.","flair":"null\tnull"}
{"author":"lioru","created":"Mon Nov 07 05:03:51 EST 2016","text":"Hi all, \n\nI read in http:\/\/openreview.net\/pdf?id=Hyvw0L9el about an architecture that used to have NxN convolutions (PixelCNN), but in this paper they split it to 1xN followed by Nx1 (see Figure 3) .\n\nWhat can be the reason for this split?","flair":"one\tDiscussion"}
{"author":"perceptron01","created":"Sat Oct 15 14:09:14 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.02920 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Generative Adversarial Nets from a Density Ratio Estimation Perspective Authors: Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo (Submitted on 10 Oct 2016 (v1), last revised 9 Nov 2016 (this version, v2)) Abstract: Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. Comments: Add contents especially theoretical things for ICLR 2017 Subjects: Machine Learning (stat.ML) Cite as: arXiv:1610.02920 [stat.ML]   (or arXiv:1610.02920v2 [stat.ML] for this version) Submission history From: Masatoshi Uehara [view email] [v1] Mon, 10 Oct 2016 14:02:30 GMT (952kb,D) [v2] Wed, 9 Nov 2016 15:22:28 GMT (958kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"spreisel","created":"Fri Nov 18 08:57:12 EST 2016","text":"RecNet is a easy to use framework for recurrent neural networks. It implements a deep uni\/bidirectional Conventional\/LSTM\/GRU architecture in Python with use of the Theano library. The intension is a easy handling, light weight implementation with the opportunity to check out new ideas and to implement current research. Example of use: In case of error try to update pip\/setuptools. 1. Please provide your data in form of two lists and storage it in a klepto file. One list contains sequences of features and another the corresponding targets. Each element of the list should be a matrix with shape . Please find a full list of possible parameters below. 3. Use the provided function for generating mini batches, training, validation or usage. Please find complete training and usage scripts in the provided examples.","flair":"four\tProject"}
{"author":"dharma-1","created":"Tue Nov 08 08:58:37 EST 2016","text":"As video becomes an even more popular way for people to communicate, we want to give everyone state-of-the art creative tools to help you express yourself. We recently began testing a new creative-effect camera in the Facebook app that helps people turn videos into works of art in the moment. That technique is called \u201Cstyle transfer.\u201D It takes the artistic qualities of one image style, like the way Van Gogh paintings look, and applies it to other images and videos. It's a technically difficult trick to pull off, normally requiring the content to be sent off to data centers for processing on big-compute servers \u2014 until now. We've developed a new deep learning platform on mobile so it can \u2014 for the first time \u2014 capture, analyze, and process pixels in real time, putting state-of-the-art technology in the palm of your hand. This is a full-fledged deep learning system called Caffe2Go, and the framework is now embedded into our mobile apps. By condensing the size of the AI model used to process images and videos by 100x, we're able to run various deep neural networks with high efficiency on both iOS and Android. Ultimately, we were able to provide AI inference on some mobile phones at less than 1\/20th of a second, essentially 50 ms \u2014 a human eye blink happens at 1\/3rd of a second or 300 ms. The style-transfer tool in the camera is the result of a marriage between two technologies: the Caffe2go runtime and style-transfer models. Because our AI teams deal with both algorithms and large-scale systems, they were well suited to develop new models for both pursuits, making the style-transfer experience high-quality and fast. It took both technologies to make it possible for you to feel like you have Van Gogh's paintbrush in your hand when you pick up your phone to shoot a video. We started the work three months ago, setting out to do something nobody else had done: ship AI-based style transfer as a creative tool, but have it run live, in real time, on people's devices. A unique set of people across product, technology, and research groups jumped in on the project. Justin Johnson of the Facebook AI Research group was the author of one of the foundational research papers describing the technique, building off previous research in the field. Our Applied Machine Learning group had been working toward building an AI engine that would run on mobile devices. The Camera team had a clear understanding of the user needs. Along with the contribution of many others, these teams produced a best-in-class solution that runs highly optimized neural networks live on mobile devices. We'll explain how we thought about and developed the applicable technologies, starting with Caffe2go. Artificial intelligence has made a significant impact on computer science, but it's been mostly limited to big data centers that are miles or maybe hundreds of miles away from people using AI-powered services. So any AI processing something in \u201Creal time\u201D still suffered from the latency impact of having to travel to a data center to run on a GPU. Since we didn't think it would be practical to ask people to walk around with a supercomputer, we wanted to figure out a way to make AI work on the CPU on the most ubiquitous device out there \u2014 the smart phone. The phone can see, talk, and comprehend in real time without having to connect to remote servers, but they also have limitations. While they have improved significantly in computation power in recent years and are capable of carrying out billions of arithmetic computations every second, they also have various resource constraints like power, memory, and compute capability that require smart software design. As a result, mobile presents both an opportunity and challenge for machine learning systems. Our solution to this challenge was to design a particularly lightweight and modular framework. To this end, we built on top of the open-source Caffe2 project, using the Unix philosophy. This explicitly ensures the core framework, which declares and connects the components, is very light and able to have multiple modules attached \u2014 including mobile-specific optimizations. We kept a lean algorithm framework that allows engineers to describe the abstract computation as a directed acyclic graph (DAG), but make sure that no constraints are imposed on what input and output such nodes in the graph can carry out. This allows our engineering teams to implement and optimize modules on different platforms, while being able to connect such modules with ease. When the graph is actually run, it instantiates itself with the various hardware features to achieve maximum speed. As speed is at the core of computation-intensive mobile applications, especially images and videos, the lightweight design of the framework allows us to perform platform-specific optimizations for defined operators. A notable example is a library called NNPack that Caffe2 integrates in our mobile runtime. By utilizing a mobile CPU feature called NEON, we are able to significantly improve the mobile computation speed. On iOS devices, we have also embarked on integrating acceleration features such as the Metal language. All these are done with a modular design, with no need to change the general model definition. As a result, the algorithm side and the runtime side can safely rely on each other, and do not need to worry about any potential incompatibilities. Caffe2 is also our first industrial-strength deep learning platform that ships at full speed on four platforms with the same set of code: server CPU, GPU, iOS, and Android. Because of the modular design, the framework can speak the same language but be optimized for each platform. It's an implementation detail that is hidden from the developer; for example, the framework chooses between NNPack for mobile (iOS and Android) or CUDNN for server GPUs. As a result, the algorithm developer can focus on the algorithm work and not on how to run the convolution. Developers also benefit from a fast deployment design. From a developer's perspective, debugging mobile runtime can be a challenge, as the mobile toolchain is not as advanced as desktop and server counterparts. We addressed this by abstracting the neural network math from the hardware \u2014 a serialized network in Caffe2go can be carried out on both mobile phones and servers with the same numerical output. As a result, we can move a majority of work to a server environment \u2014 model training, performance inspection, user experience study \u2014 and after things look good, have a one-button deployment to the mobile environments. The idea of style transfer is not a new one. It was initially introduced in by researchers in a seminal paper, \u201CA Neural Algorithm for Artistic Style,\u201D that was published in August 2015. However, the technique was slow and required powerful servers. Over the next few months the research community advanced these techniques and increased the speed by a couple orders of magnitude, but was still using massive amounts of compute power on servers. Caffe2go made the AI processing fast and put it in the palm of your hand. But the style transfer models needed to be optimized as well, ensuring the experience was real time, while maintaining high-quality, high-resolution image. Models for traditional style-transfer work (even the feedforward variants) are big (in terms of the number of parameters) and slow. Our goal in creating the style-transfer application was to run new, smaller, more efficient models to deliver a high-quality video running at 20 FPS on iPhone6s or above that avoids dropping frames. We applied three major approaches for reducing the model size. We optimized the number of convolution layers (the most time-consuming part of processing) and the width of each layer, and we adjusted the spatial resolution during processing. The number of convolutional layers and their width can be used as separate levers for adjusting processing time, by adjusting how many aspects of the image are getting processed, or adjusting the number of times a separate processing action has taken place. For spatial resolution, we can adjust the actual size of what is being processed in the middle layers. By using early pooling (downscaling the size of the image being processed) and late deconvolution (upscaling the image after processes), we can speed the processing time up because the system is not processing as much information. We also found with these techniques that we could aggressively cut down on the width and depth of the network while maintaining reasonable quality. Image quality is subjective and it's very difficult to measure \u2014 especially for things like style transfer. Therefore we built visualization tools, including A\/B tests, and trained different models to ensure we were getting the best quality image results. Our large GPU cluster, powered by FBLearner Flow, allowed us to do this, as we could rapidly sweep over a large range of hyperparameters, such as model architecture, content\/style weight, and downsampling, to find a well-trained feedforward style that hit our performance goals while keeping and improving the quality. There are many other tricks that improve the quality \u2014 for example, applying instance normalization instead of the commonly used batch normalization helps in many styles, as do avoiding zero padding in the convolution layers reduce artifacts and applying different pre- and post-processing filters on style or content images. But in our testing we found that these approaches tend to work better for some styles and not as well on others. With the speed and quality optimizations made in the style-transfer technology, run on the Caffe 2 framework, a real-time image processing system is possible on the mobile experience. Caffe2go is core to the machine learning products at Facebook, together with research toolchains such as Torch. Because of its size, speed, and flexibility, we're rolling Caffe2go out across Facebook's stack. We are also committed to sharing our software and designs with the community so we can learn to better utilize the characteristics of multiple hardware platforms and algorithm designs, which is particularly important in cross-platform machine learning systems. We will be looking to open source parts of this AI framework over the coming months. As we move forward, you can imagine how having on-device AI running in real time could help make the world more open and connected for people in areas like accessibility, education, or others. The smart devices in our hands are going to continue disrupting the way we think about intelligence. With a swift, lightweight machine learning system like Caffe2go, we are committed to bringing you more awesome AI and AR experiences like getting access to Van Gogh's brush when you shoot a video.","flair":"two\tNews"}
{"author":"cvikasreddy","created":"Wed Nov 02 17:11:20 EDT 2016","text":"I am training a GAN on mnist dataset and when doing so, just in 5 steps(5 batches, batch_size=128), the discriminator loss go down to 0.0 and there is no further improvement in generator.\n\nHow do I solve this problem?","flair":"one\tDiscussion"}
{"author":"markovtsev","created":"Fri Nov 25 09:01:51 EST 2016","text":"While doing topic modeling of GitHub repositories, to which we dedicated the recent article Topic Modeling of GitHub Repositories, we gradually realized that our dataset should be filtered to achieve better results. Particularly, we should throw away \u201Cextreme forks\u201D, the Large Unidentified Flying Copy-Pasted Codebases. The typical example of such LUCPCs are web site engines, e.g. many people copy Wordpress into their repository and build their open-source blogs. Numerous github.io sites are the same copy-pasted examples. Folks like to learn web programming from books or online manuals copy-pasting the sample boilerplates. We are not saying it is wrong; sometimes it is even inevitable, sometimes git submodules seem a bigger evil in spite of all GitHub\u2019s efforts to support them. Such duplicates are bad for specifically topic modeling since a lot of \u201Cgarbage\u201D bags are introduced. We are working with repositories using the bag-of-names model, which treats each codebase as a sparse vector. The number of dimensions equals to the number of unique names occurring in all the source code we\u2019ve got, the values are equal to the corresponding TF-IDF-s (the derivatives from frequencies). Names are extracted using the source highlight library pygments, and additionally refined with several tricky heuristics. For example, consider the following Python snippet: It is transformed into Then we apply frequency filtering, TF-IDF, stemming, etc. We expect the resulting sparse vectors for duplicate repositories to be nearly the same. Of course, some files may contain minor edits and we cannot seek for the exact same bag-of-names; we have to make up something smarter. The problem which we\u2019ve just stated is by no means unique: many Natural Language Processing tasks involve the same challenges. Let\u2019s revise the common approach: thresholding by Jaccard Similarity. Imagine that we have two sets (in mathematical sense) - \\(A\\) and \\(B\\). The Jaccard Similarity Measure reflects the extent to which our two sets are similar to each other. \\[J=\\frac{\\left|A \\cap B\\right|}{\\left|A \\cup B\\right|}\\] That is, the power of the set intersection divided by the power of set union. The idea is to calculate pairwise \\(J\\) matrix for all our samples, treat it as a mutual similarity graph\u2019s incidence matrix and then pick the cliques which have each edges weight above the fixed threshold. There is the problem: we\u2019ve got too many samples, more than 10 million and we cannot calculate a 10Mx10M matrix in any observable future. Luckily for us there is a nice solution to that problem: MinHash algorithm. There are several modifications of MinHash-ing, they are basically equivalent to each other. Let\u2019s consider the easiest one. We randomly permute the elements of the union of all sets, and enumerate them. We pick the minimum index throughout contained elements for every set. We repeat this procedure multiple times, and take the average of the indices, \\(h\\). We call \\(h\\) \u201CMinHash\u201D, it is indeed a hash function, but unlike other hashes, it is consistent: similar items tend to yield near hash values. The difference between two MinHash values can be proved to approximate the Jaccard Similarity. There is a good blog post which explains how to apply the MinHash algorithm to find duplicates: On removing duplicates from a set of documents. Basically, we sort all the hash values and scan them using the window of the size which is specially tailored for the tolerated level of false-positives. MinHash has an awesome property: there are always no false-negatives, so we end up with the same result which we would end up in 10Mx10M brute force. Things change when you\u2019ve got a dictionary instead of a set: the keys are still unique and values are non-negative \u201Cweights\u201D of the corresponding keys. The Jaccard Similarity between dictionaries \\(A=\\{i: a_ i\\}, i\\in I\\) and \\(B=\\{j: b_ j\\}, j\\in J\\) is defined as \\[J=\\frac{\\sum_ {k\\in K}\\limits \\min(a_ k, b_ k)}{\\sum_ {k\\in K}\\limits \\max(a_k, b_k)}, K=I\\cup J\\] where \\(a_k = 0, k\\notin I\\) and \\(b_k = 0, k\\notin J\\). If the weights are binary, this formula is equivalent to the common Jaccard Similarity definition. The corresponding hashing method for the Weighted Jaccard Similarity is named \u201CWeighted MinHash\u201D and described in detail in Sergey Ioffe\u2019s article. His proposed algorithm depends on the parameter \\(K\\) which adjusts the resulting hash length. Thus given \\(K\\) and supposing that the integers are 32-bit we obtain the hash with size \\(8K\\) bytes. \\(Gamma(2, 1)\\) distribution can be efficiently calculated as \\(r = −\\ln(u_ 1 u_ 2)\\) where \\(u_ 1, u_ 2 ∼ Uniform(0, 1)\\). Having calculated all the hashes in the dataset, we can then conduct local sensitive hashing, an algorithm from Chapter 3, Mining of Massive Datasets. Briefly, we define several hash tables, each for it\u2019s own subhash, depending on the target level of false positives. Same elements will appear in the same bucket; union of the bucket sets across all the hash tables for a specific sample yields all the similar samples. If we\u2019d like to determine the sets of mutually similar samples a.k.a. cliques, we should consider the set intersection instead. The LSH algorithm is actually pretty fast by design, does not require much memory and works perfectly, even when implemented in Python (datasketch), whereas calculation of the hashes themselves is resource consuming. We\u2019ve developed an efficient Weighted MinHash calculation library MinHashCuda. It allows to offload the heavy-lifting to the GPU(s). MinHashCuda has two interfaces, C and Python, much like kmcuda, and actually borrowed quite a lot of boilerplate code from kmcuda. I will use the Python API throughout this section. Suppose that we\u2019ve got out dataset in the compressed sparse row format, particularly scipy.sparse.csr_matrix. In order to feed it to MinHashCuda, first you must initialize the hasher object: Internally, it will leverage cuRAND, NVIDIA\u2019s stock random generation library, to generate \\(r\\), \\(c\\) and \\(\\beta\\). If the precious GPU memory fits those arrays, then you may move along to the hash calculation, otherwise some dimensionality reduction is needed. For example, the Titan X 2016 has 12GB GDDR5X, so the theoretical limit is \\(\\frac{12*10^9}{3\\cdot sizeof(float)} = 10^9\\) dimensions. Of course, there must be some room for the samples, so practically the maximum dimensionality is about 500 million. Next, we calculate the hashes. Depending on whether the whole matrix fits into the memory, there are two options: In the latter case, we avoid using because it creates the new matrix and introduces much overhead. Finally, we free all the allocated resources: As usual, CUDA kernel performance dramatically varies depending on the chosen parallelization scheme, occupancy and the benevolence of the evil overlords. I had to rewrite it several times until I reached a satisfying result. The naive method of parallelization would be to assign rows to threads on the equivalent basis: each thread takes a row, enters the argmin loop and finally writes the whole hash. There are several problems with this approach: The solution to (1) is to split the incoming matrix into row intervals with roughly the same number of elements, not rows. (2) is solved by caching the partial minimum \\(a_ k\\) and \\((k^*, t_ {k^*})\\) into the shared memory, so that the row\u2019s elements are read only once. There is no ability to accurately cache the random values as well because different rows refer to arbitrary different column indices. Apart from solving (1) and (2), I had the special pleasure to implement multiple GPUs support. (1) appeared to be harder than it seems. The point is, (2) sets the limit on the maximum amount of shared memory per block, so the maximum number of elements consumed by each thread is upper bounded. We end up with a discrete optimization problem. It resembles bin packing, but instead of minimizing the number of bins, it focuses on the overall equal load and grouping the bins by CUDA blocks. Here is my solution: The overall complexity is thus \\(O(R(\\log R + \\log T) + T\\log T) = O(R \\log R)\\) since \\(T ∼ R\\). I hit the problems with the low kernel occupancy. The used number of registers was too high. I managed to dramatically reduce the register pressure applying the volatile trick. We successfully applied MinHashCuda to find duplicate repositories on GitHub. The size of our dataset was initially 13.6 million but later was filtered down to 9.6 million. To be precise, the matrix was 9624276 x 2422260 with the sparsity 0.00014, which is roughly equivalent to 9624276 x 340. We ran the hash calculation on two Titan Xs (Maxwell), it took 40 minutes. According to my estimation, the achieved speed is 600 times faster than the implementation in using 12 CPU cores with Intel\u2019s MKL. The result after LSH with a similarity threshold of 0.9 was about 467,000 duplicate groups with 1.66 million repositories. 110,000 repositories appeared to be *.github.io. Here are the examples: The complete dataset is published on data.world.","flair":"four\tProject"}
{"author":"phunter_lau","created":"Tue Nov 22 14:13:19 EST 2016","text":"Machine learning is playing an increasingly important role in many areas of our businesses and our lives and is being employed in a range of computing tasks where programming explicit algorithms is infeasible. At Amazon, machine learning has been key to many of our business processes, from recommendations to fraud detection, from inventory levels to book classification to abusive review detection. And there are many more application areas where we use machine learning extensively: search, autonomous drones, robotics in fulfillment centers, text and speech recognitions, etc. Among machine learning algorithms, a class of algorithms called deep learning hascome to represent those algorithms that can absorb huge volumes of data and learn elegant and useful patterns within that data: faces inside photos, the meaning of a text, or the intent of a spoken word. A set of programming models has emerged to help developers define and train AI models with deep learning; along with open source frameworks that put deep learning in the hands of mere mortals. Some examples of popular deep learning frameworks that we support on AWS include Caffe, CNTK, MXNet, TensorFlow, Theano, and Torch. Among all these popular frameworks, we have concluded that MXNet is the most scalable framework. We believe that the AI community would benefit from putting more effort behind MXNet. Today, we are announcing that MXNet will be our deep learning framework of choice. AWS will contribute code and improved documentation as well as invest in the ecosystem around MXNet. We will partner with other organizations to further advance MXNet. At AWS, we believe in giving choice to our customers. Our goal is to support our customers with tools, systems, and software of their choice by providing the right set of instances, software (AMIs), and managed services. Just like in Amazon RDS\u2015where we support multiple open source engines like MySQL, PostgreSQL, and MariaDB, in the area of deep learning frameworks, we will support all popular deep learning frameworks by providing the best set of EC2 instances and appropriate software tools for them. Amazon EC2, with its broad set of instance types and GPUs with large amounts of memory, has become the center of gravity for deep learning training. To that end, we recently made a set of tools available to make it as easy as possible to get started: a Deep Learning AMI, which comes pre-installed with the popular open source deep learning frameworks mentioned earlier; GPU-acceleration through CUDA drivers which are already installed, pre-configured, and ready to rock; and supporting tools such as Anaconda and Jupyter. Developers can also use the distributed Deep Learning CloudFormation template to spin up a scale-out, elastic cluster of P2 instances using this AMI for even larger training runs. As Amazon and AWS continue to invest in several technologies powered by deep learning, we will continue to improve all of these frameworks in terms of usability, scalability, and features. However, we plan to contribute significantly to one in particular, MXNet. Developers, data scientists, and researchers consider three major factors when selecting a deep learning framework: The same three things are important to developers at AWS and many of our customers. After a thorough evaluation, we have selected MXNet as our deep learning framework of choice , where we plan to use it broadly in existing and upcoming new services. As part of that commitment, we will be actively promoting and supporting open source development through code contributions (we've made quite a few already), improving the developer experience and documentation online and on AWS, and investing in supporting tools for visualization, development, and migration from other frameworks. MXNet is a fully featured, flexibly programmable, and ultra-scalable deep learning framework supporting state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). MXNet has its roots in academia and came about through the collaboration and contributions of researchers at several top universities. Founding institutions include the University of Washington and Carnegie Mellon University. \"MXNet, born and bred here at CMU, is the most scalable framework for deep learning I have seen, and is a great example of what makes this area of computer science so beautiful - that you have different disciplines which all work so well together: imaginative linear algebra working in a novel way with massive distributed computation leading to a whole new ball game for deep learning. We're excited about Amazon's investment in MXNet, and can't wait to see MXNet go from strength to strength\" Andrew Moore \u2013 Dean of Computer Science at Carnegie Mellon University. The efficiency by which a deep learning framework scales out across multiple cores is one of its defining features. More efficient scaling allows you to significantly increase the rate at which you can train new models, or dramatically increase the sophistication of your model for the same amount of training time. This is an area where MXNet shines: we trained a popular image analysis algorithm, Inception v3 (implemented in MXNet and running on P2 instances), using an increasing number of GPUs. Not only did MXNet have the fastest throughput of any library we evaluated (as measured by the number of images trained per second), but the throughput rose by almost the same rate as the number of GPUs used for training (with a scaling efficiency of 85%). In addition to scalability, MXNet offers the ability to both mix programming models (imperative and declarative), and code in a wide number of programming languages, including Python, C++, R, Scala, Julia, Matlab, and JavaScript. Computational efficiency is important (and goes hand in hand with scalability) but nearly as important is the memory footprint. MXNet can consume as little as 4 GB of memory when serving deep networks with as many as 1000 layers. It is also portable across platforms, and the core library (with all dependencies) fits into a single C++ source file and can be compiled for both Android and iOS. You can even run it in your browser using the JavaScript extensions! We're excited about MXNet. If you would like to learn more, you can check out the MXNet home page, or GitHub repository for more information, and can get started right now, using the Deep Learning AMI, or on your own machine. We'll also be hosting a Machine Learning \"State of the Union\" and a series of breakout sessions and workshops on using MXNet at AWS re:Invent on November 30th at the Mirage Hotel in Las Vegas. It's still day one for this new era of machine intelligence; in fact, we probably haven't even woken up and had our first cup of coffee yet. With tools like MXNet (and the other deep learning frameworks), and services such as EC2, it's going to be an exciting time.","flair":"two\tNews"}
{"author":"Prooffread3r","created":"Tue Oct 11 13:47:59 EDT 2016","text":" Toggle navigation prooffreader plus too nerdy or not nerdy enough for prooffreader.com, not whimsical enough for prooffreaderswhimsy. prooffreader.com prooffreaderplus prooffreaderswhimsy Tuesday, October 11, 2016 By  David Taylor10:22 AMdeep.learning, learning, machine.learning, neural.networks, pdfs, tutorial, videos5 comments Hugo Larochelle's neural network & deep learning tutorial videos, subtitled & screengrabbed Share This:    Facebook Twitter Google+ Stumble Digg Like a lot of data scientists (I consider myself more of a data spelunker, but I aspire to data science), I try my best to keep up with the latest discoveries in a very fast-changing field; and probably nothing has been as game-changing as the advent of deep learning. Deep Learning, explained to a five-year-old (okay, maybe fifteen-year-old): Data science been really good for a while now at data that can be explained in Excel spreadsheets, i.e. columns and rows: one row per observation, one column per variable. This is called structured data. Deep Learning allows us to create rows of column variables that describe a representation of unstructured data, like images or text. It's as if you had an automatic algorithm that could look through all your images, and create one column based on the likelihood the image contains a cat, another the likelihood it contains a shovel -- without having to tell the algorithm what a cat or shovel is, or what they look like, or determine that there are cats and shovels at all before running the algorithm. Deep Learning is rather math-intensive, and involves neural networks, a family of algorithms that's been around for a long time but has now come into its own. Unlike some skills, you can't learn it as a black box and then slowly come to understand it as you use it. There are foundations you need to acquire; tutorials you need to absorb. I live in Montreal, which recently hosted its annual Deep Learning Summer School; I couldn`t attend, but I heard great things about the lecture by Université de Sherbrooke's Hugo Larochelle. There's just one thing; I hate listening to videos. It's why I don't take Coursera classes now that I only have a short commute to work every day. I need to learn at my own pace. And I prefer to read. So when I realized Larochelle's lecture was based on a series of 92 videos on his YouTube channel, I wrote a Python script to add a black bar beneath them, burn subtitles into it, and take screenshots of every subtitle slide and make a pdf out of it so I can read them. I like to read. Here's an example screenshot: I'm sharing the fruits of my labor with you here: Videos with subtitles, pdfs of subtitled screenshots, and Python code I used to make them. Hugo Larochelle neural network lecture videos & pdfs with subtitles These are zip files of subtitled videos and pdfs of screenshots made from Hugo Larochelle's (University of Sherbrooke) YouTube playlist of 92 videos in 10 parts on neural networks. Videos with subtitles: Subtitled MP4s for Part 01, Feedforward neural networks [zip, 108.5 MB] Subtitled MP4s for Part 02, Training neural networks [zip, 238.6 MB] Subtitled MP4s for Part 03, Conditional random fields [zip, 250.2 MB] Subtitled MP4s for Part 04, Training CRFs [zip, 106.6 MB] Subtitled MP4s for Part 05, Restricted Boltzmann Machine [zip, 169.1 MB] Subtitled MP4s for Part 06, Autoencoder [zip, 136.8 MB] Subtitled MP4s for Part 07, Deep Learning [zip, 226.8 MB] Subtitled MP4s for Part 08, Sparse coding [zip, 152.8 MB] Subtitled MP4s for Part 09, Computer vision [zip, 191.4 MB] Subtitled MP4s for Part 10, Natural Language Processing [zip, 289.5 MB] PDFs of screenshots: PDFs for Part 01, Feedforward neural networks [zip, 120.9 MB] PDFs for Part 02, Training neural networks [zip, 287.0 MB] PDFs for Part 03, Conditional random fields [zip, 284.2 MB] PDFs for Part 04, Training CRFs [zip, 132.7 MB] PDFs for Part 05, Restricted Boltzmann Machine [zip, 189.8 MB] PDFs for Part 06, Autoencoder [zip, 161.1 MB] PDFs for Part 07, Deep Learning [zip, 307.9 MB] PDFs for Part 08, Sparse coding [zip, 200.9 MB] PDFs for Part 09, Computer vision [zip, 239.8 MB] PDFs for Part 10, Natural Language Processing [zip, 371.6 MB] Methodology: In a Python script (which you can see here: Part 1, Part 2), I: used requests and BeautifulSoup to parse the YouTube playlist; used youtube-dl to download the videos and WEBVTT subtitles; used pycaption to convert subtitles to SRT format; used ffmpeg (from a subprocess call) to add a black letterbox below each video, burn the subtitles into that box and then save png screenshots wherever there was a new subtitle line; used imagemagick to bundle pngs into pdfs; used zipfile to zip similar files together and deleted the originals. I'm David Taylor, aka prooffreader. About me Email ThisBlogThis!Share to TwitterShare to Facebook \u2022 \u2022 \u2022 ← Newer Post Older Post → Home 5 comments: yinaOctober 11, 2016 at 1:46 PM Wow thank you this is fantastic ReplyDelete yinaOctober 11, 2016 at 1:46 PM Wow thank you this is fantastic ReplyDelete GodestinyOctober 12, 2016 at 3:05 AM Thanks a lot for this initiative!! ReplyDelete yibo chenOctober 13, 2016 at 2:50 AM great work ReplyDelete Xavier PrudentNovember 3, 2016 at 8:37 AM Thanks for sharing! ReplyDelete Add comment Load more... Popular Posts Top 10 Python idioms I wish I'd learned earlier I've been programming all my life, but never been a programmer. Most of my work was done in Visual Basic because it's what ... How to quickly turn an IPython notebook into a blog post IPython notebooks are great for many things, but they're a little awkward to embed in blog post platforms like Blogger, Wordpress, etc. ... Hugo Larochelle's neural network & deep learning tutorial videos, subtitled & screengrabbed Like a lot of data scientists (I consider myself more of a data spelunker, but I aspire to data science), I try my best to keep up with the ... One of my favourite memories: The good, the bad and the ugly Last year, a new postdoc (that's short for postdoctoral fellow; she's a woman, but in science we call women fellows) showed me somet... Additional info for Comparison of letter position in words for eight languages Link to this week's blog,  Comparison of letter position in words for eight languages Link to May 27 blog,  Graphing the distribution o... Dataset: Single word frequencies per decade from Google Books I have crunched a public English language dataset in order to remove information that is least likely to be of interest to users, and I offe... Python scripts to shorten column names, or to fetch Google Ngrams data I've made a couple new GitHub repos: google_ngram_py , which allows you to look up one- to five-word phrases in Google Ngrams Viewer ... GIF version of Animated maps of earthquakes near China and Japan. 1970-2013 For those who don't cotton to HTML5 video: You can see the HTML5 video (which you can pause and use a slider to select individual fr... The mystery of Emma In this week's prooffreader.com post , I note that the U.S. Social Security baby names database has an unusual amount of boys named Emm... A simple progress 'bar' for IPython Notebooks Doing data science, I often start loop functions without a clear idea of how long they'll take. When working with exceptionally huge dat... Blog Archive ▼  2016 (3) ►  November (1) ▼  October (1) Hugo Larochelle's neural network & deep learning t... ►  March (1) ►  2015 (8) ►  October (1) ►  September (1) ►  August (2) ►  May (3) ►  April (1) ►  2014 (18) ►  December (1) ►  November (3) ►  October (1) ►  September (1) ►  July (2) ►  June (2) ►  May (3) ►  April (1) ►  March (1) ►  February (2) ►  January (1) About Me David Taylor A Renaissance man (or at least late medieval) with a working knowledge of classical music, linguistics, classics, history, chemistry, biochemistry, literature, design, computer science and cognition. Both geek and nerd, yet never dork. View my complete profile Search for: Copyright © prooffreader plus | Powered by Blogger Design by WpMultiverse | Blogger Theme by Lasantha - PremiumBloggerTemplates.com | BTheme.net | Distributed By Gooyaabi Templates Facebook Twitter GooglePlus Instagram Youtube Pinterest Linkedin Dribbble Tumblr Feed ","flair":"one\tDiscussion"}
{"author":"perceptron01","created":"Sat Nov 12 18:47:35 EST 2016","text":"Couldn't find any information online about the reasons for the change, but I saw this on their website: \"Having been acquired by Google in 2014, we are now part of the Alphabet group.\" after it was mentioned something about this [on here](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5c77r4\/r161102554_the_neural_noisy_channel\/#d9ull3m)","flair":"one\tDiscussion"}
{"author":"edkeens","created":"Tue Nov 08 18:39:57 EST 2016","text":"This repo contains basic algorithms\/agents used for reinforcement learning. More specifically, you can find here: I tested agents on OpenAI gym, CartPole-v0 environment, measuring how long does it take to solve environment (average reward of at least 195 for 100 consecutive episodes). Maximum number of episodes was 1000 and each learning procedure was run 100 times.","flair":"four\tProject"}
{"author":"yacob_uk","created":"Fri Oct 07 14:10:13 EDT 2016","text":"I'm not a ML person. But I do spend far too much time thinking about how I might apply ML to work problems.  \n\nIf I had examples of loc subject headings. Texts. And texts with loc subject headings assigned by human operators could I build a machine that learns from those human decisions and results in machine asserted subject headings. \n\nI have plenty of data sources, but minimal human operators for verifying machine processed decisions. \n\nAs a novice data wrangler with some python, is this something I could explore solo, or would I get better outcomes by joining up with a researcher.  \n\nI appreciate the vague openness of the question, and the dislike of non domain-expert questions in this sub. The root of this problem is a genuine contemporary information Science challenge and I'm extremely focused on trying to find ways of pulling us out of a very problematic situation.","flair":"null\tnull"}
{"author":"rmltestaccount","created":"Sat Oct 08 12:27:13 EDT 2016","text":" Interpretable ML for Complex Systems NIPS 2016 Workshop About Call for Papers Invited Talks Organizers Program Committee Schedule About Call for Papers Invited Talks Organizers Papers Program Committee Schedule Sitemap About Interpreting the structure and predictions of complex models Complex machine learning models, such as deep neural networks, have recently achieved great predictive successes for visual object recognition, speech perception, language modeling, and information retrieval. These predictive successes are enabled by automatically learning expressive features from the data. Typically, these learned features are a priori unknown, difficult to engineer by hand, and hard to interpret. This workshop is about interpreting the structure and predictions of these complex models. Interpreting the learned features and the outputs of complex systems allows us to more fundamentally understand our data and predictions, and to build more effective models. For example, we may build a complex model to predict long range crime activity. But by interpreting the learned structure of the model, we can gain new insights into the processes driving crime events, enabling us to develop more effective public policy. Moreover, if we learn, for example, that the model is making good predictions by discovering how the geometry of clusters of crime events affect future activity, we can use this knowledge to design even more successful predictive models. This 1 day workshop is focused on interpretable methods for machine learning, with an emphasis on the ability to learn structure which provides new fundamental insights into the data, in addition to accurate predictions. We wish to carefully review and enumerate modern approaches to the challenges of interpretability, share insights into the underlying properties of popular machine learning algorithms, and discuss future directions. Note the new date: Friday, December 9 We are grateful for support from our sponsors. Key Dates Workshop: 9 Dec 2016 Location: Barcelona, Spain Submission Deadline: 20 Oct 2016 Travel Award Deadline: 20 Oct 2016 Acceptance Notification: 1 Nov 2016 Sign in|Report Abuse|Print Page|Powered By Google Sites ","flair":"three\tResearch"}
{"author":"RefurbishedMac","created":"Fri Nov 25 09:10:57 EST 2016","text":"I'm a novice to the field, I've heard that neural networks can read CT\/MRI scans fairly accurate but I'm not sure how accurate. maybe someone can enlighten me in what ML algorithms can achieve in a medical context?","flair":"one\tDiscussion"}
{"author":"Residrew","created":"Mon Nov 14 17:22:42 EST 2016","text":"*[Asked this](https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/5cnrph\/time_series_classification\/) first in \/r\/learnmachinelearning with essentially no replies so I'm forwarding my questions here (if there's another sub more suitable please let me know).*\n\n***\n\nHi, fairly new to ML. I've had trouble grasping\/remembering concepts through just reading, so I've made an effort to jump onto a project with a problem I'm interested in.\n\nI have data of raw positional tracking data (25 fps) from NBA games (10 players and the ball), and I'm trying to detect when an on ball screen occurs. [Found a paper](http:\/\/www.sloansportsconference.com\/wp-content\/uploads\/2014\/02\/2014_SSAC_Recognizing-on-Ball-Screens.pdf) that did this, which I plan on loosely follow the steps in the paper. I've also found a good source to watch the games associated to the data I have for labelling. \n\nTo start i've implemented the segmentation of the data into actions [*2 Data Segmentation*], and watched one game and labelled all on ball screens to verify the sensitivity of detecting screens.\n\nRight now I'm stuck with a number of questions...\n\n***\n\n**Feature Extraction**\n\nFeature extraction is one thing I'm general pretty clueless about. In the paper [3.2 Feature Extraction], they take all pairwise distances between the ball-handler, his defender, the screener, and the basket (6 distance-time series) and defined the moment that the ball handler and the screen are closest. From what it looks like the features extracted were the time where min occurs, and the mean and average speeds for before and after the defined moment.\n\nWhy are those features here chosen, and in what is the general procedure for extracting features?\n\nAfter that they \"discretize each continuous feature into five binary features based on quintiles\". What does that mean? What is the benefit of doing it?  \n\nI'm pretty interested in general on feature engineering and want to read more about feature extraction and feature selection. Does anyone have a solid resource(s) on the topic?\n\n***\n\n**Small Dataset**\n\nAs one person there's no way I can watch all the games and label them (~600 games). In the paper their training\/test set was only 252\/234 time series respectively. From my understanding having a smaller dataset means it's much harder to avoid overfitting. In what ways can that be achieved and how large should be the dataset be? \n\n***\n\nLastly what are the best methods \/ approaches for time series classification? The paper uses a SVM but from some initial research, Hidden Markov Models, Dynamic Time Warping, and LSTMs are what I see commonly mentioned.\n","flair":"one\tDiscussion"}
{"author":"torvoraptor","created":"Fri Sep 30 00:15:30 EDT 2016","text":"Amazon will sponsor 12 university teams to compete in the 2016-2017 Alexa Prize. This year\u2019s inaugural competition focuses on the grand challenge of building a socialbot that can converse coherently and engagingly with humans on popular topics for 20 minutes.  The sponsored teams will receive a $100,000 stipend, Alexa-enabled devices, free Amazon Web Services (AWS) services to support their development efforts, and support from the Alexa Skills Kit (ASK) team. Amazon received over one hundred applications from leading universities across 22 countries. Additional teams invited to participate without sponsorship will be announced on December 12, 2016. All applications were reviewed and evaluated based on the following criteria: the potential scientific contribution to the field, the technical merit of the approach, the novelty of the idea, and the team\u2019s ability to execute against their plan.","flair":"null\tnull"}
{"author":"vernik911","created":"Fri Nov 25 08:05:38 EST 2016","text":" Home Moments Search query Search Twitter Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Language: English Bahasa Indonesia Bahasa Melayu Català Čeština Dansk Deutsch English UK Español Filipino Français Hrvatski Italiano Magyar Nederlands Norsk Polski Português Română Slovenčina Suomi Svenska Tiếng Việt Türkçe Ελληνικά Български език Русский Српски Українська мова עִבְרִית العربية فارسی मराठी हिन्दी বাংলা ગુજરાતી தமிழ் ಕನ್ನಡ ภาษาไทย 한국어 日本語 简体中文 繁體中文 Have an account? Log in Have an account? Remember me · Forgot password? New to Twitter? Sign up Mushroom AI @mushroomaibot Tweets 59 Following 50 Followers 38 Likes 5   More Likes Unmute @mushroomaibot Mute @mushroomaibot Follow Following Unfollow Blocked Unblock Pending Cancel Mushroom AI @mushroomaibot Mushroom AI recognizes different mushroom species by photo using deep convolutional neural network. mushroomai.ml Joined November 2016 49 Photos and videos Photos and videos Tweets Tweets Tweets & replies Media @mushroomaibot is blocked Are you sure you want to view these Tweets? Viewing Tweets won't unblock @mushroomaibot. View Tweets Close Mushroom AI followed Pinned Tweet Mushroom AI \u200F@mushroomaibot Nov 25 Mode \"alpha\". I can recognize only selected mushroom speciespic.twitter.com\/z4K4UQyCMS 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 12h12 hours ago Paxillus involutus 73.1% Agaricus bisporus 18.4% Cantharellus cibarius 5.3% @ilgar_al #mushroom #AI #MLpic.twitter.com\/rmoBQCir2y 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Lycoperdon perlatum 48.2% Amanita phalloides 32.3% Paxillus involutus 9.9% @athst #mushroom #AI #MLpic.twitter.com\/zByRiBdqkC 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Cantharellus cibarius 82.1% Paxillus involutus 12.3% Lycoperdon perlatum 4.7% @athst #mushroom #AI #MLpic.twitter.com\/WTj0vaBDML 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Suillus luteus 88.9% Leccinum scabrum 3.5% Agaricus bisporus 2.6% @ilgar_al #mushroom #AI #MLpic.twitter.com\/pwQZUEoZG9 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Boletus edulis 85.2% Agaricus campestris 8.5% Suillus luteus 2.5% @ilgar_al #mushroom #AI #MLpic.twitter.com\/V2nH2kzTWV 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 14h14 hours ago Paxillus involutus 32.4% Agaricus campestris 21.6% Suillus luteus 17.7% @ilgar_al #mushroom #AI #MLpic.twitter.com\/IYyGickFjl 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita phalloides 90.3% Agaricus campestris 3.4% Agaricus bisporus 3.3% @ilgar_al #mushroom #AI #MLpic.twitter.com\/LhrPURqCyD 0 replies 4 retweets 1 like Reply Retweet 4 Retweeted 4 Like 1 Liked 1 More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita phalloides 77.1% Leccinum scabrum 6.7% Agaricus campestris 6.2% @santichapela #mushroom #AI #MLpic.twitter.com\/WZ35tpsEfO 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Cantharellus cibarius 98.1% Lycoperdon perlatum 0.6% Agaricus bisporus 0.6% @santichapela #mushroom #AI #MLpic.twitter.com\/oLMjx5nJ7o 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Paxillus involutus 61.1% Cantharellus cibarius 18.9% Lycoperdon perlatum 10.1% @santichapela #mushroom #AI #MLpic.twitter.com\/mBAQVFX0aC 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Cantharellus cibarius 76.3% Lycoperdon perlatum 7.7% Paxillus involutus 5.2% @santichapela #mushroom #AI #MLpic.twitter.com\/bX4aQyrecs 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Leccinum scabrum 95.0% Boletus edulis 2.5% Suillus luteus 1.6% @santichapela #mushroom #AI #MLpic.twitter.com\/uKwv8vPCCF 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Amanita muscaria 99.8% Lycoperdon perlatum 0.1% Amanita phalloides 0.0% @santichapela #mushroom #AI #MLpic.twitter.com\/DPB4Es0xJP 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Leccinum scabrum 55.5% Suillus luteus 23.8% Amanita phalloides 14.7% @santichapela #mushroom #AI #MLpic.twitter.com\/m6KWRnfz9c 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 15h15 hours ago Paxillus involutus 46.8% Amanita phalloides 20.8% Leccinum scabrum 9.6% @santichapela #mushroom #AI #MLpic.twitter.com\/Ed7pEy7roB 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 20h20 hours ago Lycoperdon perlatum 51.1% Agaricus campestris 25.1% Amanita phalloides 13.2% @voodoohop #mushroom #AI #MLpic.twitter.com\/nVV2H4SKzD 0 replies 1 retweet 0 likes Reply Retweet 1 Retweeted 1 Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot 21h21 hours ago Lycoperdon perlatum 38.8% Cantharellus cibarius 27.9% Paxillus involutus 27.4% @hedonismo_sp #mushroom #AI #MLpic.twitter.com\/Hy7A2GbV7l 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot Nov 26 Amanita phalloides 76.3% Agaricus bisporus 7.9% Agaricus campestris 7.5% @gesamoje #mushroom #AI #MLpic.twitter.com\/vCVcYTgiVl 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet Mushroom AI \u200F@mushroomaibot Nov 26 Lycoperdon perlatum 61.2% Paxillus involutus 35.6% Cantharellus cibarius 2.2% @Antonkuznetsov5 #mushroom #AI #MLpic.twitter.com\/NjwMGT45kF 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet Mushroom AI Retweeted Peter Skomoroch \u200F@peteskomoroch Nov 26 San Francisco, CA Peter Skomoroch Retweeted Evgeny Vlasenko Interesting thread - it might not be obvious, but this is a simple case where a bad deep learning prediction could get someone killedhttps:\/\/twitter.com\/mahnunchik\/status\/802183480264232961 \u2026 Peter Skomoroch added, Evgeny Vlasenko @mahnunchik [Project] Mushroom AI - twitter bot which can recognize different mushroom species by photo https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5etha4\/project_mushroom_ai_twitter_bot_which_can\/?ref=share&ref_source=twitter \u2026 via @reddit 0 replies 3 retweets 14 likes Reply Retweet 3 Retweeted 3 Like 14 Liked 14 More Copy link to Tweet Embed Tweet @mushroomaibot hasn't tweeted yet. Back to top ↑ Loading seems to be taking a while. Twitter may be over capacity or experiencing a momentary hiccup. Try again or visit Twitter Status for more information. New to Twitter? Sign up now to get your own personalized timeline! Sign up You may also like · Refresh © 2016 Twitter About Help Terms Privacy Cookies Ads info Close Choose a trend location Dismiss Close Previous Next Close Go to a person's profile Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Close Retweet this to your followers? Optional comment for Retweet   Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @   140 Retweet Tweet Close Are you sure you want to delete this Tweet? Cancel Delete Close Promote this Tweet Close Block Cancel Block Add a location to your Tweets When you tweet with a location, Twitter stores that location. You can switch location on\/off before each Tweet and always have the option to delete your location history. Learn more Turn location on Not now Close Profile summary Close Your lists Close Create a new list List name Description Under 100 characters, optional Privacy Public · Anyone can follow this list Private · Only you can access this list Save list Close Close Copy link to Tweet Here's the URL for this Tweet. Copy it to easily share with friends. Close Embed this Tweet Embed this Video Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? Include parent Tweet Include media Preview Close Log in to Twitter Remember me · Forgot password? Don't have an account? Sign up » Close Sign up for Twitter Not on Twitter? Sign up, tune into the things you care about, and get updates as they happen. Sign up Have an account? Log in » Close Two-way (sending and receiving) short codes: Country Code For customers of United States 40404 (any) Canada 21212 (any) United Kingdom 86444 Vodafone, Orange, 3, O2 Brazil 40404 Nextel, TIM Haiti 40404 Digicel, Voila Ireland 51210 Vodafone, O2 India 53000 Bharti Airtel, Videocon, Reliance Indonesia 89887 AXIS, 3, Telkomsel, Indosat, XL Axiata Italy 4880804 Wind 3424486444 Vodafone » See SMS short codes for other countries Close Confirmation Close   Close Close Buy Now Close Buy Now Hmm... Something went wrong. Please try again. Skip all Welcome home! This timeline is where you\u2019ll spend most of your time, getting instant updates about what matters to you. Tweets not working for you? Hover over the profile pic and click the Following button to unfollow any account. Say a lot with a little When you see a Tweet you love, tap the heart \u2014 it lets the person who wrote it know you shared the love. Spread the word The fastest way to share someone else\u2019s Tweet with your followers is with a Retweet. Tap the icon to send it instantly. Join the conversation Add your thoughts about any Tweet with a Reply. Find a topic you\u2019re passionate about, and jump right in. Learn the latest Get instant insight into what people are talking about now. Get more of what you love Follow more accounts to get instant updates about topics you care about. Find what's happening See the latest conversations about any topic instantly. Never miss a Moment Catch up instantly on the best stories happening as they unfold. Back Next Next Tweet from user ","flair":"four\tProject"}
{"author":"NicolasGuacamole","created":"Thu Nov 03 06:22:25 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1606.02492v3 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1606 Change to browse by: cs cs.LG cs.NE References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Shreyas Saxena Jakob Verbeek Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Convolutional Neural Fabrics Authors: Shreyas Saxena, Jakob Verbeek (Submitted on 8 Jun 2016 (v1), last revised 28 Oct 2016 (this version, v3)) Abstract: Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a \"fabric\" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset. Comments: Added Final version (To appear at NIPS16 ) Subjects: Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1606.02492 [cs.CV]   (or arXiv:1606.02492v3 [cs.CV] for this version) Submission history From: Shreyas Saxena [view email] [v1] Wed, 8 Jun 2016 10:17:51 GMT (655kb,D) [v2] Thu, 9 Jun 2016 16:21:57 GMT (663kb,D) [v3] Fri, 28 Oct 2016 13:10:05 GMT (1786kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"prajit","created":"Tue Oct 18 13:09:45 EDT 2016","text":"Sorry, Periscope requires JavaScript for full functionality. Please enable JavaScript in your browser and reload the page. Get the App Log in | Sign up Ended last month Replay Unavailable Talk by Laurent Dinh - Density estimation using Real NVP Hugo Larochelle@hugo_larochelle Get the App iOSAndroid ","flair":"three\tResearch"}
{"author":"olBaa","created":"Tue Oct 04 05:50:42 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.00324 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Accelerating Deep Convolutional Networks using low-precision and sparsity Authors: Ganesh Venkatesh, Eriko Nurvitadhi, Debbie Marr (Submitted on 2 Oct 2016) Abstract: We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6% Top-1\/93% Top-5 on the Imagenet object classification challenge with low-precision network\\footnote{github release of the source code coming soon} while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP\/mm^2 equivalent for single-precision floating-point operations (~2 TFLOP\/mm^2 for half-precision). Subjects: Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.00324 [cs.LG]   (or arXiv:1610.00324v1 [cs.LG] for this version) Submission history From: Ganesh Venkatesh [view email] [v1] Sun, 2 Oct 2016 17:59:31 GMT (297kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"siddharth-agrawal","created":"Sat Oct 29 08:44:58 EDT 2016","text":"I need classes for the ImageNet 32x32 dataset present here: http:\/\/image-net.org\/small\/download.php. The tar file has a folder with all the images, but does not have any file linking the images to their respective classes. Is there an efficient way to get the classes for these images?","flair":"one\tDiscussion"}
{"author":"speechMachine","created":"Tue Nov 08 14:50:10 EST 2016","text":"Hello,\n\nThis question is with reference to a great paper on Bayesian Changepoint detection by Adams and McKay http:\/\/hips.seas.harvard.edu\/files\/adams-changepoint-tr-2007.pdf and its associated implementation http:\/\/www.inference.phy.cam.ac.uk\/rpa23\/cp\/gaussdemo.m.\n\nThe question is with regard to the predictive probability that appears in Step 3 of Algorithm 1 of the paper. If I am working with a scalar datum x_t, the predictive probability should be a scalar quantity with univariate distribution parameters right? Why is predprobs in gaussdemo.m a t x 1 vector with the studentpdf being evaluated with distribution parameters being estimated at everytime step. It looks like an evaluation of the predictive probabilities over multiple univariate distributions with the stored parameters for all previous time steps\/run lengths. Why is this necessary?\n","flair":"one\tDiscussion"}
{"author":"FR_STARMER","created":"Sun Oct 09 16:40:24 EDT 2016","text":"So I've found that mean-squared error really halts learning after a certain amount of time because it's slope decreases exponentially towards zero. So it's really good for tuning in to about where it needs to be, but not good at really tweaking itself to 0.\n\nSo I added MSE to MAE in this function: x^2 + abs(x), so when the parabola starts to bottom out, it switched to the abs function. Neat! But the problem is that MAE has a super slow learning rate, and if your using data that produces all sorts of different losses, it's not really good. So I developed a new function: (x^2 + abs(x))^0.8, so it's a parabolic slope at large values, linear slope at smaller values, and an exponentially increasing slope for very small values. Does this type of function aid in gradient decent?\n\nWhat should one be aiming for when choosing a loss function or writing their own? Other thoughts on loss functions in general?","flair":"one\tDiscussion"}
{"author":"KarlKastor","created":"Tue Nov 15 16:51:47 EST 2016","text":"A.I. ExperimentsA.I. Experiments Chrome Experiments Android Experiments Arts & Culture Experiments Experiments About Submit your own Introducing A.I. Experiments. Explore machine learning by playing with pictures, language, music, code, and more. Giorgio Cam Quick, Draw! The Infinite Drum Machine Thing Translator Bird Sounds A.I. Duet Visualizing High-Dimensional Space What Neural Networks See Made something you'd like to share? Submit it here. Privacy & Terms Submitting... Saving... ","flair":"four\tProject"}
{"author":"huyhcmut","created":"Tue Nov 08 10:52:08 EST 2016","text":"How can I train a GO chess engine using reinforcement learning when the size of the board is smaller than the standard size?\nThe normal way is using database chess game( when training a board with standard size). I want to know what is the best way to train if the size of my board is different(smaller) from standard GO board size? \n","flair":"one\tDiscussion"}
{"author":"pplonski","created":"Fri Nov 11 06:08:06 EST 2016","text":"MLJAR is a platform for rapid prototyping, development and deploying pattern recognition algorithms. In this tutorial the process of building binary classifiers on dataset from Numer.ai competition is presented.Please subscribe for beta at https:\/\/mljar.com","flair":"four\tProject"}
{"author":"DrPharael","created":"Wed Nov 09 05:25:40 EST 2016","text":"I am currently writing a paper on a regression method in which I am using a standard convolutional neural network (conv-relu-pool, ..., fully connected).\nFrom what I seeing in most recent papers from the machine learning community, people (righteously, I believe) assume that the reader will know what this is and don't even include a general reference to conv nets. \n\nIn my case though, the target audience is not really machine-learning researchers (but more an application-specific community) so I think it would make sense to include a reference to a generic description of conv nets (especially since no one has applied deep learning to the problem that I am trying to solve yet).\n\nDo you have any thoughts on which paper(s) I should cite when I mention conv nets ? I am a bit overwhelmed by the number of existing papers and the \"I was the first !\"-fights.","flair":"one\tDiscussion"}
{"author":"jayjaymz","created":"Tue Oct 25 20:49:44 EDT 2016","text":"Hello there. I'm tasked with using Recurrent Neural Netwroks to generate source code from a chunk of javascript projects.\n\nFirstly, I implemented Karpathy's lstm-char-rnn in keras. It provided some fun results. After that I'm looking for a different way to approach the problem.\n\nAs an idea, I wanted to label the characters extracted from the dataset (based on their usage in the code: string, identifier, numeric and so on), thus creating 2dimensional input. I would also like the output to be 2dimensional. I googled around and found no information as to how to implement it. Is there any known approach to having 2 dimensional input &amp; output of sequences with RNNs? Is there some other topology that could help me map sequences to sequences?\n\nThank you.","flair":"one\tDiscussion"}
{"author":"pmigdal","created":"Sat Oct 15 13:07:58 EDT 2016","text":"This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist. Winner of the 2014 Eric Ziegel award from Technometrics. For a more advanced treatment of these topics: The Elements of Statistical Learning. Slides and videos for Statistical Learning MOOC by Hastie and Tibshirani available separately here.Slides and video tutorials related to this book by Abass Al Sharifcan be downloaded here. \"An Introduction to Statistical Learning (ISL)\" by James, Witten, Hastie and Tibshirani is the \"how to'' manual for statistical learning. Inspired by \"The Elements of Statistical Learning'' (Hastie, Tibshirani and Friedman), this book provides clear and intuitive guidance on how to implement cutting edge statistical and machine learning methods. ISL makes modern methods accessible to a wide audience without requiring a background in Statistics or Computer Science. The authors give precise, practical explanations of what methods are available, and when to use them, including explicit R code. Anyone who wants to intelligently analyze complex data should own this book.              Larry Wasserman, Professor, Department of Statistics and Department of Machine Learning, CMU. As a textbook for an introduction to data science through machine learning, there is much to like about ISLR. It\u2019s thorough, lively, written at level appropriate for undergraduates and usable by nonexperts. It\u2019s chock full of interesting examples of how modern predictive machine learning algorithms work (and don\u2019t work) in a variety of settings.\"Matthew Richey, The American Mathematical Monthly, Vol. 123, No. 7 (August-September 2016).","flair":"four\tProject"}
{"author":"NIPS2016attendee","created":"Mon Oct 31 15:07:13 EDT 2016","text":"Just got an email regarding one of my rejected papers:\n\n&gt; as we had announced in our notification email for NIPS 2016, we would like to give authors of rejected NIPS submissions the opportunity to advertise their paper on an official NIPS webpage, in the spirit of the exhibition of rejects (see our source of inspiration https:\/\/en.wikipedia.org\/wiki\/Salon_des_Refus%C3%A9s).\n\nI know they cite an event that used to happen over 150 years ago as inspiration, but is there really a point now that arXiv exists? Was there some background discussion by the organizers? I feel like creating something like this undermines the review process for academia.","flair":"one\tDiscussion"}
{"author":"downtownslim","created":"Tue Nov 08 17:23:51 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.02200v1 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Unsupervised Cross-Domain Image Generation Authors: Yaniv Taigman, Adam Polyak, Lior Wolf (Submitted on 7 Nov 2016) Abstract: We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1611.02200 [cs.CV]   (or arXiv:1611.02200v1 [cs.CV] for this version) Submission history From: Yaniv Taigman [view email] [v1] Mon, 7 Nov 2016 18:14:57 GMT (8638kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"xavigiro","created":"Mon Nov 14 05:59:07 EST 2016","text":"We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis. We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions. Our workshop paper is available on arXiv. Please cite with the following Bibtex code: You may also want to refer to our publication with the more human-friendly Chicago style: Miriam Bellver, Xavier Giro-i-Nieto, Ferran Marques, and Jordi Torres. \"Hierarchical Object Detection with Deep Reinforcement Learning.\" In Deep Reinforcement Learning Workshop (NIPS). 2016. This python code enables to both train and test each of the two models proposed in the paper. The image zooms model extracts features for each region visited, whereas the pool45 crops model extracts features just once and then ROI-pools features for each subregion. In this section we are going to describe how to use the code. The code uses Keras framework library. If you are using a virtual environment, you can use the requirements.txt provided. First it is important to notice that this code is already an extension of the code used for the paper. During the training stage, we are not only considering one object per image, we are also training for other objects by covering the already found objects with the mean of VGG-16, inspired by what Caicedo et al. did on Active Object Localization with Deep Reinforcement Learning. First of all the weights of VGG-16 should be downloaded from the following link VGG-16 weights. If you want to use some pre-trained models for the Deep Q-network, they can be downloaded in the following links Image Zooms model and Pool45 Crops model. Notice that these models could lead to different results compared to the ones provided in the paper, due that these models are already trained to find more than one instance of planes in the image. You should also create two folders in the root of the project, called models_image_zooms and models_pool45_crops, and store inside them the corresponding weights. We will follow as example how to train the Image Zooms model, that is the one that achieves better results. The instructions are equal for training the Pool45 Crops model. The script is image_zooms_training.py, and first the path to the database should be configured. The default paths are the following: But you can change them to point to your own locations. The training of the models enables checkpointing, so you should indicate which epoch you are going to train when running the script. If you are training it from scratch, then the training command should be: There are many options that can be changed to test different configurations: class_object: for which class you want to train the models. We have trained it for planes, and all the experiments of the paper are run on this class, but you can test other categories of pascal, also changing appropiately the training databases. number_of_steps: For how many steps you want your agent to search for an object in an image. scale_subregion: The scale of the subregions in the hierarchy, compared to its ancestor. Default value is 3\/4, that denoted good results in our experiments, but it can easily be set. Take into consideration that the subregion scale and the number of steps is very correlated, if the subregion scale is high, then you will probably require more steps to find objects. bool_draw: This is a boolean, that if it is set to 1, it stores visualizations of the sequences for image searches. At each epoch the models will be saved in the models_image_zooms folder. To test the models, you should use the script image_zooms_testing.py. You should also configure the paths to indicate which weights you want to use, in the same manner as in the training stage. In this case, you should only run the command python image_zooms_testing.py. It is recommended that for testing you put bool_draw = 1, so you can observe the visualizations of the object search sequences. There is the option to just search for a single object in each image, to reproduce the same results of our paper, by just setting the boolean only_first_object to 1. We would like to especially thank Albert Gil Moreno and Josep Pujal from our technical support team at the Image Processing Group at the UPC. We also would like to thank Carlos Tripiana from the technical support team at the Barcelona Supercomputing center (BSC). If you have any general doubt about our work or code which may be of interest for other researchers, please use the public issues section on this github repo. Alternatively, drop us an e-mail at miriam.bellver@bsc.es and xavier.giro@upc.edu.","flair":"three\tResearch"}
{"author":"quit_daedalus","created":"Fri Oct 14 12:25:03 EDT 2016","text":"Hello there! \n\nI'm an EE undergrad who really loves to learn more about Machine Learning and has the \"Operations Research\" course this semester. \n\nOur course project is to find a real world problem and solve and optimize it using Linear Programming, as Machine Learning is one of my favorite topics I want to choose a Machine Learning Problem, but I have little idea about what I can choose and came here asking for help. Thanks!","flair":"one\tDiscussion"}
{"author":"Laser_Plasma","created":"Tue Sep 27 13:58:42 EDT 2016","text":"So, I'm kind of a newbie in deep learning, but trying to improve. I have a project to do using ENet (https:\/\/arxiv.org\/abs\/1606.02147), but I don't quite understand everything from the article. I have two main questions, probably stupid, but oh well:\n\n1. In Figure 2b, what exactly is the plus sign at the bottom? Is this a concatenation or just plain simple addition (doubt it, that would be weird), or something else?\n\n2. How exactly does Table 1 work? Are bottlenecks 2.x parallel and then concatenated at the end? Or are they executed consecutively? If it's the latter, then why doesn't the size change after the downsampling layer?\n\nI'll be grateful for any hints anyone may have for me, as well as any other resources for working with ENet etc. (in case it matters, I'm working in TensorFlow\/Python 3)","flair":"null\tnull"}
{"author":"bihaqo","created":"Wed Oct 05 15:57:18 EDT 2016","text":"I can't wrap my head around the solution to the blind spot problem proposed in the [PixelCNN paper](https:\/\/arxiv.org\/abs\/1606.05328).\n\nCan anyone explain what do they actually do? Do they use (1 x N) and (N x 1) convolutions? What are the inputs and the masks?","flair":"null\tnull"}
{"author":"acanai","created":"Tue Nov 15 09:31:20 EST 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose Bayesian Optimization for Probabilistic Programs (NIPS 2016 Spotlight) Tom Rainforth SubscribeSubscribedUnsubscribe55 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 941 views 11 Like this video? Sign in to make your opinion count. Sign in 12 0 Don't like this video? Sign in to make your opinion count. Sign in 1 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 11, 2016 Spotlight video for 30th Conference on Advances in Neural Information Processing Systems (2016). Paper is available here http:\/\/www.robots.ox.ac.uk\/~twgr\/asse.... Other links: Code - https:\/\/bitbucket.org\/twgr\/bopp\/ - http:\/\/www.robots.ox.ac.uk\/~fwood\/ang... Author's websites - http:\/\/www.robots.ox.ac.uk\/~twgr\/ - http:\/\/www.tuananhle.co.uk\/ - http:\/\/www.ccs.neu.edu\/home\/jwvdm\/ - http:\/\/www.robots.ox.ac.uk\/~mosb\/ - http:\/\/www.robots.ox.ac.uk\/~fwood\/ Category People & Blogs License Standard YouTube License Show more Show less Loading... Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next \"An Overview of Probabilistic Programming\" by Vikash K. Mansinghka - Duration: 1:02:55. Strange Loop 6,286 views 1:02:55 NIPS 2016 Spotlight: Full-Capacity Unitary Recurrent Neural Networks - Duration: 2:58. Thomas Powers 32 views 2:58 Thomas Wiecki - Probabilistic Programming in Python - Duration: 45:35. EuroPython Conference 878 views 45:35 NIPS 2016 Synthesis of MCMC and Belief Propagation spotlight presentation - Duration: 3:04. ì\u2022ˆì\u201E±ìˆ˜ 22 views 3:04 103 videos Play all Studio Ghibli OSTìµœí�¬ì£¼ Interpretable Nonlinear Dynamic Modeling of Neural Trajectories [NIPS 2016 spotlight] - Duration: 3:04. Brain is (not) a computer 69 views 3:04 NIPS 2016 Spotlight: Optimal Binary Classifier Aggregation for General Losses - Duration: 2:52. Akshay Balsubramani 9 views 2:52 Data Programming NIPS 2016 Spotlight Video - Duration: 3:24. HazyResearch 220 views 3:24 Coresets for Bayesian Logistic Regression - NIPS 2016 spotlight video - Duration: 3:11. Jonathan Huggins 334 views 3:11 NIPS 2016 spotlight video - CMICOT - Duration: 3:00. Kate Gladkikh 249 views 3:00 Probabilistic Programming in Quantitative Finance by Thomas Wiecki, PhD - Duration: 49:18. Quantopian 3,770 views 49:18 NIPS 2016: Stochastic Structured Prediction under Bandit Feedback - Duration: 2:34. statnlp-hdu 345 views 2:34 NIPS 2016 Spotlight: Multimodal Residual Learning for Visual QA - Duration: 3:01. Jin-Hwa Kim 219 views 3:01 Tue Herlau NIPS 2016 spotlight - Duration: 2:59. tueherlau 10 views 2:59 MIT researcher Vikash Mansinghka on probabilistic programming - Duration: 1:09:02. Columbia Data Science 1,028 views 1:09:02 Collaborative Recurrent Autoencoder for Recommender Systems - NIPS 2016 spotlight video - Duration: 2:56. Wang Hao 15 views 2:56 NIPS 2015 Workshop (Ghahramani) 15537 Bayesian Optimization: Scalability and Flexibility - Duration: 48:24. NIPS 201 views 48:24 Nando de Freitas: Bayesian Optimization - Duration: 1:03:32. Open Data Science Initiative 1,223 views 1:03:32 3. Bayesian Optimization of Hyper Parameters - Duration: 13:30. Artificial Intelligence Courses 2,167 views 13:30 Avi Pfeffer - Practical Probabilistic Programming with Figaro - MLconf SEA 2016 - Duration: 24:00. MLconf 521 views 24:00 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"three\tResearch"}
{"author":"gwern","created":"Sat Nov 19 17:56:15 EST 2016","text":" Under review as a conference paper at ICLR 2017 LEARNING TO SUPEROPTIMIZE PROGRAMS Rudy Bunel, Alban Desmaison, M. Pawan Kumar & Philip H.S. Torr Department of Engineering Science University of Oxford Oxford, UK {rudy,alban,pawan}@robots.ox.ac.uk, philip.torr@eng.ox.ac.uk Pushmeet Kohli Microsoft Research Redmond, WA 98052, USA pkohli@microsoft.com ABSTRACT Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compil- ers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sam- pling of modifications to the program from a proposal distribution, which are ac- cepted or rejected based on whether they preserve correctness and the improve- ment they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Mo- tivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal dis- tribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as ex- isting (\u201CHacker\u2019s Delight\u201D) programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization. 1 INTRODUCTION Considering the importance of computing to human society, it is not surprising that a very large body of research has gone into the study of the syntax and semantics of programs and programming languages. Code super-optimization is an extremely important problem in this context. Given a program or a snippet of source-code, super-optimization is the task of transforming it to a version that has the same input-output behaviour but can be executed on a target compute architecture more efficiently. In some sense, it is the natural analogue of the paraphrase problem in natural language processing where we want to change syntax without changing semantics. Decades of research has been done on the problem of code optimization resulting in the development of sophisticated rule-based transformation strategies that are used in compilers to allow them to perform code optimization. While modern compilers implement a large set of rewrite rules and are able to achieve impressive speed-ups, they fail to offer any guarantee of optimality, thus leaving room for further improvement. An alternative approach is to search over the space of all possible programs that are equivalent to the compiler output, and select the one that is the most efficient. If the search is carried out in a brute-force manner, we are guaranteed to achieve super-optimization. However, this approach quickly becomes computationally infeasible as the number of instructions and the length of the program grows. 1 Under review as a conference paper at ICLR 2017 In order to efficiently perform super-optimization, recent approaches have started to use a stochas- tic search procedure, inspired by Markov Chain Monte Carlo (MCMC) sampling (Schkufza et al., 2013). Briefly, the search starts at an initial program, such as the compiler output. It iteratively sug- gests modifications to the program, where the probability of a modification is encoded in a proposal distribution. The modification is either accepted or rejected with a probability that is dependent on the improvement achieved. Under certain conditions on the proposal distribution, the above proce- dure can be shown, in the limit, to sample from a distribution over programs, where the probability of a program is related to its quality. In other words, the more efficient a program, the more times it is encountered, thereby enabling super-optimization. Using this approach, high-quality implemen- tations of real programs such as the Montgomery multiplication kernel from the OpenSSL library were discovered. These implementations outperformed the output of the gcc compiler and even expert-handwritten assembly code. One of the main factors that governs the efficiency of the above stochastic search is the choice of the proposal distribution. Surprisingly, the state of the art method, Stoke (Schkufza et al., 2013), employs a proposal distribution that is neither learnt from past behaviour nor does it depend on the syntax or semantics of the program under consideration. We argue that this choice fails to fully exploit the power of stochastic search. For example, consider the case where we are interested in performing bitwise operations, as indicated by the compiler output. In this case, it is more likely that the optimal program will contain bitshifts than floating point opcodes. Yet, Stoke will assign an equal probability of use to both types of opcodes. In order to alleviate the aforementioned deficiency of Stoke, we build a reinforcement learning framework to estimate the proposal distribution for optimizing the source code under consideration. The score of the distribution is measured as the expected quality of the program obtained via stochas- tic search. Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm (Williams, 1992). We demonstrate the efficacy of our approach on two datasets. The first is composed of programs from \u201CHacker\u2019s Delight\u201D (Warren, 2002). Due to the limited diversity of the training samples, we show that it is possible to learn a prior distribution (un- conditioned on the input program) that outperforms the state of the art. The second dataset contains automatically generated programs that introduce diversity in the training samples. We show that, in this more challenging setting, we can learn a conditional distribution given the initial program that significantly outperforms Stoke. 2 RELATED WORKS Super-optimization The earliest approaches for super-optimization relied on brute-force search. By sequentially enumerating all programs in increasing length orders (Granlund & Kenner, 1992; Massalin, 1987), the shortest program meeting the specification is guaranteed to be found. As ex- pected, this approach scales poorly to longer programs or to large instruction sets. The longest reported synthesized program was 12 instructions long, on a restricted instruction set (Massalin, 1987). Trading off completeness for efficiency, stochastic methods (Schkufza et al., 2013) reduced the number of programs to test by guiding the exploration of the space, using the observed quality of programs encountered as hints. However, the reliance of stochastic methods on a generic unspecific exploratory policy made the optimization blind to the problem at hand. We propose to tackle this problem by learning the proposal distribution. Neural Computing Similar work was done in the restricted case of finding efficient implemen- tation of computation of value of degree k polynomials (Zaremba et al., 2014). Programs were generated from a grammar, using a learnt policy to prioritise exploration. This particular approach of guided search looks promising to us, and is in spirit similar to our proposal, although applied on a very restricted case. Another approach to guide the exploration of the space of programs was to make use of the gradients of differentiable relaxation of programs. Bunel et al. (2016) attempted this by simulating program execution using Recurrent Neural Networks. However, this provided no guarantee that the network parameters were going to correspond to real programs. Additionally, this method only had the possibility of performing local, greedy moves, limiting the scope of possible transformations. On 2 Under review as a conference paper at ICLR 2017 the contrary, our proposed approach operates directly on actual programs and is capable of accepting short-term detrimental moves. Learning to optimize Outside of program optimization, applying learning algorithms to improve optimization procedures, either in terms of results achieved or runtime, is a well studied subject. Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201CLearning to search\u201D framework. While this is similar in spirit to stochastic search, our setting differs in the crucial aspect of having a valid cost function instead of searching for one. More relevant is the recent literature on learning to optimize. Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks. Our work is similar, as we aim to improve the optimization process. However, as opposed to the gradient descent that they learn on a continuous unconstrained space, our initial algorithm is an MCMC sampler on a discrete domain. Similarly, training a proposal distribution parameterized by a Neural Network was also proposed by Paige & Wood (2016) to accelerate inference in graphical models. Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000). 3 LEARNING STOCHASTIC SUPER-OPTIMIZATION 3.1 STOCHASTIC SEARCH AS A PROGRAM OPTIMIZATION PROCEDURE Stoke (Schkufza et al., 2013) performs black-box optimization of a cost function on the space of programs, represented as a series of instructions. Each instruction is composed of an opcode, speci- fying what to execute, and some operands, specifying the corresponding registers. Each given input program T defines a cost function. For a candidate programR called rewrite, the goal is to optimize the following cost function: cost (R, T ) = ωe × eq(R, T ) + ωp × perf(R) (1) The term eq(R; T ) measures how well the outputs of the rewrite match the outputs of the reference program. This can be obtained either exacly by running a symbolic validator or approximately by running test cases. The term perf(R) is a measure of the efficiency of the program. In this paper, we consider runtime to be the measure of this efficiency. It can be approximated by the sum of the latency of all the instructions in the program. Alternatively, runtime of the program on some test cases can be used. To find the optimum of this cost function, Stoke runs an MCMC sampler using the Metropo- lis (Metropolis et al., 1953) algorithm. This allows us to sample from the probability distribution induced by the cost function: p(R; T ) = 1 Z exp(−cost (R, T ))). (2) The sampling is done by proposing random moves from a different proposal distribution: R\u2032 ∼ q( · |R). (3) The cost of the new modified program is evaluated and an acceptance criterion is computed. This acceptance criterion α(R, T ) = min ( 1, p(R\u2032; T ) p(R; T ) ) , (4) is then used as the parameter of a Bernoulli distribution from which an accept\/reject decision is sampled. If the move is accepted, the state of the optimizer is updated to R\u2032. Otherwise, it remains inR. While the above procedure is only guaranteed to sample from the distribution p( · ; T ) in the limit if the proposal distribution q is symmetric (q(R\u2032|R) = q(R|R\u2032) for all R,R\u2032), it still allows us to perform efficient hill-climbing for non-symmetric proposal distributions. Moves leading to an improvement are always going to be accepted, while detrimental moves can still be accepted in order to avoid getting stuck in local minima. 3 Under review as a conference paper at ICLR 2017 3.2 LEARNING TO SEARCH We now describe our approach to improve stochastic search by learning the proposal distribution. We begin our description by defining the learning objective (section 3.2.1), followed by a parameter- ization of the proposal distribution (section 3.2.2), and finally the reinforcement learning framework to estimate the parameters of the proposal distribution (section 3.2.3). 3.2.1 OBJECTIVE FUNCTION Our goal is to optimize the cost function defined in equation (1). Given a fixed computational budget of T iterations to perform program super-optimization, we want to make moves that lead us to the lowest possible cost. As different programs have different runtimes and therefore different associated costs, we need to perform normalization. As normalized loss function, we use the ratio between the best rewrite found and the cost of the initial unoptimized program R0. Formally, the loss for a set of rewrites {Rt}t=0..T is defined as follows: r({Rt}t=0..T ) = ( mint=0..T cost (Rt, T ) cost (R0, T ) ) . (5) Recall that our goal is to learn a proposal distribution. Given that our optimization procedure is stochastic, we will need to consider the expected cost as our loss. This expected loss is a function of the parameters θ of our parametric proposal distribution qθ: L(θ) = E{Rt}∼qθ [r({Rt}t=0..T )] . (6) 3.2.2 PARAMETERIZATION OF THE MOVE PROPOSAL DISTRIBUTION The proposal distribution (3) originally used in (Schkufza et al., 2013) takes the form of a hierar- chical model. The type of the move is initially sampled from a probability distribution. Additional samples are drawn to specify, for example, the affected location in the programs ,the new operands or opcode to use. Which of these probability distribution get sampled depends on the type of move that was first sampled. The detailed structure of the proposal probability distribution can be found in Appendix A. Stoke uses uniform distributions for each of the elementary probability distributions the model sam- ples from. This corresponds to a specific instantiation of the general stochastic search paradigm. In this work, we propose to learn those probability distributions so as to maximize the probability of reaching the best programs. Our chosen parameterization of q is to keep the hierarchical structure of the original work of Schkufza et al. (2013), as detailed in Appendix A, and parameterize all the elementary proba- bility distributions (over the positions in the programs, the instructions to propose or the arguments) independently. The set θ of parameters for qθ will thus contain a set of parameters for each ele- mentary probability distributions. A fixed proposal distribution is kept through the optimization of a given program. The stochastic computation graph corresponding to a run of the Metropolis algo- rithm is given in Figure 1. We have assumed the operation of evaluating the cost of a program to be a deterministic function, as we will not model the randomness of measuring performance. 3.2.3 LEARNING THE PROPOSAL DISTRIBUTION In order to learn the proposal distribution, we will use stochastic gradient descent on our loss func- tion (6). We obtain the first order derivatives with regards to our proposal distribution parame- ters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estima- tor (Glynn, 1990) or the score function estimator (Fu, 2006). This estimator relies on a rewriting of the gradient of the expectation. For an expectation with regards to a probability distribution x ∼ fθ, the REINFORCE estimator is: ∇θ ∑ x f(x; θ)r(x) = ∑ x r(x)∇θf(x; θ) = ∑ x f(x; θ)r(x)∇θ log(f(x; θ)), (7) and provides an unbiased estimate of the gradient. 4 Under review as a conference paper at ICLR 2017 Feature of original program Proposal Distribution Neural Network (a) BackPropagation Move Categorical Sample (b) REINFORCE Program Candidate Rewrite Candidate score (c) Score Acceptance criterion (d) (d) New rewrite Bernoulli (e) (g) Cost (f) Figure 1: Stochastic computation graph of the Metropolis algorithm used for program super- optimization. Round nodes are stochastic nodes and square ones are deterministic. Red arrows corresponds to computation done in the forward pass that needs to be learned while green arrows correspond to the backward pass. Full arrows represent deterministic computation and dashed ar- rows represent stochastic ones. The different steps of the forward pass are: (a) Based on features of the reference program, the proposal distribution q is computed. (b) A random move is sampled from the proposal distribution. (c) The score of the proposed rewrite is experimentally measured. (d) The acceptance criterion (4) for the move is computed. (e) The move is accepted with a probability equal to the acceptance criterion. (f) The cost is observed, corresponding to the best program obtained during the search. (g) Moves b to f are repeated T times. A helpful way to derive the gradients is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs (Schulman et al., 2015). We introduce one \u201Ccost node\u201D in the computation graphs at the end of each iteration of the sampler. The associated cost corresponds to the normalized difference between the best rewrite so far and the current rewrite after this step: ct = min ( 0, ( cost (Rt, T )−mini=0..t−1 cost (Ri, T ) cost (R0, T ) )) . (8) The sum of all the cost nodes corresponds to the sum of all the improvements made when a new lowest cost was achieved. It can be shown that up to a constant term, this is equivalent to our objective function (5). As opposed to considering only a final cost node at the end of the T iterations, this has the advantage that moves which were not responsible for the improvements would not get assigned any credit. 5 Under review as a conference paper at ICLR 2017 For each round of MCMC, the gradient with regards to the proposal distribution is computed using the REINFORCE estimator which is equal to ∇̂θ,iL(θ) = (∇θ log qθ(Ri|Ri−1)) ∑ t>i ct. (9) As our proposal distribution remains fixed for the duration of a program optimization, these gradients needs to be summed over all the iterations to obtain the total contribution to the proposal distribution. Once this gradient is estimated, it becomes possible to run standard back-propagation with regards to the features on which the proposal distribution is based on, so as to learn the appropriate feature representation. 4 EXPERIMENTS 4.1 SETUP Implementation Our system is built on top of the Stoke super-optimizer from Schkufza et al. (2013). We instrumented the implementation of the Metropolis algorithm to allow sampling from parameterized proposal distribution and to keep track of the traces through the stochastic graph. Using those traces, we can compute the estimator of our gradients, implemented using the Torch framework (Collobert et al., 2011). Datasets We validate the feasibility of our learning approach on two experiments. The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bit-manipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013). Those are short programs, all performing similar types of tasks. Some examples include identifying whether an integer is a power of two from its binary representation, counting the number of bits turned on in a register or computing the maximum of two integers. An exhaustive description of the tasks is given in Appendix B. Our second corpus of programs is automatically generated and is more diverse. Models The models that we are learning are a set of simple elementary probabilities for the cat- egorical distribution over the instructions and over the type of moves to perform. We learn the parameters of each separate distribution jointly, using a Softmax transformation to enforce that they are proper probability distributions. For the types of move where opcodes are chosen from a specific subset, the probabilities of each instruction are appropriately renormalized. For each experiment, we train two types of models. Our first model, henceforth denoted the bias, is not conditioned on any property of the programs to optimize. By learning this simple proposal distribution, it is only possible to capture a bias in the dataset. This can be understood as an optimal proposal distribution that Stoke should default to. The second model is a probability distribution conditioned on the input program to optimize. For each input program, we generate a Bag-of-Words representation based on the opcodes of the pro- gram. This is then embedded through a three hidden layer Multi Layer Perceptron, with ReLU activation unit. The proposal distribution over the instructions and over the type of moves are each the result of passing the outputs of this embedding through a linear transformation followed by a Softmax. All of our models are trained using the Adam (Kingma & Ba, 2015) optimizer, with its default hyper-parameters β1 = 0.9, β2 = 0.999, � = 10−8. We use minibatches of size 32. The bias is trained with a learning rate of 10, while the Multi layer perceptron uses a learning rate of 0.1. Those learning rates are divided by the size of the minibatches. For each estimate of the gradient, we draw 100 samples for our estimator. 4.2 EXISTING PROGRAMS In order to have a larger corpus than the twenty-five programs initially present in \u201CHacker\u2019s De- light\u201D, we generate various starting points for each optimization. This is accomplished by running Stoke with a cost function where ωp = 0 in (1), and keeping only the correct programs. Duplicate 6 Under review as a conference paper at ICLR 2017 programs are filtered out. This allows us to create a larger dataset from which to learn. Examples of these programs at different level of optimization can be found in Appendix C. We divide this augmented Hacker\u2019s Delight dataset into two sets. All the programs corresponding to even-numbered tasks are assigned to the first set, which we use for training. The programs corre- sponding to odd-numbered tasks are kept for separate evaluation, so as to evaluate the generalisation of our learnt proposal distribution. The optimization process is visible in Figure 2, which shows a clear decrease of the training loss and testing loss for both models. While simply using stochastic super-optimization allows to dis- cover programs 40% more efficient on average, using a tuned proposal distribution yield even larger improvements, bringing the improvements up to 60%, as can be seen in Table1. Due to the similar- ity between the different tasks, conditioning on the program features does not bring any significant improvements. (a) Bias (b) Multi-layer Perceptron Figure 2: Proposal distribution training. Both models learn to improve the performance of the stochastic optimization. Despite a higher performance on the training dataset, gains mostly gener- alise to unseen tasks. Model Training Test Uniform 57.38% 60.90 % Bias 34.93 % 40.16% MLP 33.72% 38.89% Table 1: Final average relative score on the Hacker\u2019s Delight benchmark. While both models im- prove with regards to the initial proposal distribution based on uniform sampling, note that the MLP reaches better performances, indicating that conditioning on the input features is useful. In addition, to clearly demonstrate the practical consequences of our learning, we present in Figure 3 a superposition of score traces, sampled from the optimization of a program of the test set. Figure 3a corresponds to our initialisation, an uniform distribution as was used in the work of Schkufza et al. (2013). Figure 3b corresponds to our optimized version. It can be observed that, while the uni- form proposal distribution was successfully decreasing the cost of the program, our learnt proposal distribution manages to achieve lower scores in a more robust manner and in less iterations. 4.3 AUTOMATICALLY GENERATED PROGRAMS While the previous experiments shows promising results on a set of programs of interest, the limited diversity of programs might have made the task too simple, as evidenced by the good performance of a blind model. Indeed, despite the data augmentation, only 25 different tasks were present, all variations of the same programs task having the same optimum. To evaluate our performance on a more challenging problem, we automatically synthesize a larger dataset of programs. Our methods to do so consists in running Stoke repeatedly with a constant 7 Under review as a conference paper at ICLR 2017 (a) Before Training (b) After Training Figure 3: Superposition of the ratio between the cost of the best program found and the starting point, using an uniform proposal distribution, on the lef, as opposed to a learnt one, conditioned on the input program, on the right. cost function, for a large number of iterations. This leads to a fully random walk as every proposed programs will have the same cost, leading to a 50% chance of acceptance. We generate 600 of these programs, 300 that we use as a training set for the optimizer to learn over and 300 that we keep as a test set. The performance achieved on this more complex dataset is shown in Figure 4 and Table 2. (a) Bias (b) Multi-layer Perceptron Figure 4: Training of the proposal distribution on the automatically generated benchmark. (4a) cor- responds to the unconditioned proposal distribution, while (4b) shows the evolution of the training when making use of the features of the program. Model Training Test Uniform 84.91% 85.13 % Bias 66.11% 69.48% MLP 64.97% 67.66% Table 2: Final average relative score. Even on the more diverse automatically generated dataset, the Multi Layer Perceptron performs better. 8 Under review as a conference paper at ICLR 2017 5 CONCLUSION Within this paper, we have formulated the problem of optimizing the performance of a stochas- tic super-optimizer as a Machine Learning problem. We demonstrated that learning the proposal distribution of a MCMC sampler was feasible and lead to faster and higher quality improvements. It is interesting to compare our method to the synthesis-style approaches that have been appearing recently in the Deep Learning community (Graves et al., 2014) that aim at learning algorithms directly using differentiable representations of programs. We find that the stochastic search-based approach yields a significant advantage compared to those types of approaches, as the resulting program can be run independently from the Neural Network that was used to discover them. Several improvements are possible to the presented methods. In mature domains such as Com- puter Vision, the representations of objects of interests have been widely studied and as a result are successful at capturing the information of each sample. In the domains of programs, obtaining infor- mative representations remains a challenge. Our proposed approach ignores part of the structure of the program, notably temporal. Significant performant boosts could be achieved with richer models, provided the associated scalability challenges were addressed. Finally, gathering a larger dataset of frequently used programs so as to measure more accurately the practical performance of those methods seems the evident next step for the task of program synthesis. 9 Under review as a conference paper at ICLR 2017 REFERENCES Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In NIPS, 2016. Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip HS Torr, and M Pawan Kumar. Adaptive neural com- pilation. In NIPS. 2016. Berkeley Churchll, Eric Schkufza, and Stefan Heule. Stoke. https:\/\/github.com\/StanfordPL\/ stoke, 2016. Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment for machine learning. In NIPS, 2011. Janardhan Rao Doppa, Alan Fern, and Prasad Tadepalli. Hc-search: A learning framework for search-based structured prediction. JAIR, 2014. Michael C. Fu. Gradient estimation. Handbooks in Operations Research and Management Science. 2006. Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 1990. Torbjörn Granlund and Richard Kenner. Eliminating branches using a superoptimizer and the GNU C compiler. ACM SIGPLAN Notices, 1992. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014. Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of loop-free programs. In PLDI, 2011. Varun Jampani, Sebastian Nowozin, Matthew Loper, and Peter V Gehler. The informed sampler: A discrim- inative approach to bayesian inference in generative computer vision models. Computer Vision and Image Understanding, 2015. Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. Oracle-guided component-based program synthesis. In International Conference on Software Engineering, 2010. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A probabilistic programming language for scene perception. In CVPR, 2015. Ke Li and Jitendra Malik. Learning to optimize. CoRR, 2016. Henry Massalin. Superoptimizer: A look at the smallest program. In ACM SIGPLAN Notices, 1987. Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 1953. Brookes Paige and Frank Wood. Inference networks for sequential Monte Carlo in graphical models. In ICML, 2016. Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. SIGPLAN, 2013. John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Henry S Warren. Hacker\u2019s delight. 2002. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. Wojciech Zaremba, Karol Kurach, and Rob Fergus. Learning to discover efficient mathematical identities. In NIPS. 2014. Song-Chun Zhu, Rong Zhang, and Zhuowen Tu. Integrating bottom-up\/top-down for object recognition by data driven markov chain monte carlo. In CVPR, 2000. 10 https:\/\/github.com\/StanfordPL\/stoke https:\/\/github.com\/StanfordPL\/stoke Under review as a conference paper at ICLR 2017 A STRUCTURE OF THE PROPOSAL DISTRIBUTION The sampling process of a move is a hierarchy of sampling step. The easiest way to represent it is as a generative model for the program transformations. Depending on what type of move is sampled, different series of sampling steps have to be performed. For a given move, all the probabilities are sampled independently so the probability of proposing the move is the product of the probability of picking each of the sampling steps. The generative model is defined in Figure 5. It is going to be parameterized by the the parameters of each specific probability distribution it samples from. The default Stoke version uses uniform probabilities over all of those elementary distributions. 11 Under review as a conference paper at ICLR 2017 1 def proposal(current_program): 2 move_type = sample(categorical(all_move_type)) 3 if move_type == 1: % Add empty Instruction 4 pos = sample(categorical(all_positions(current_program))) 5 return (ADD_NOP, pos) 6 7 if move_type == 2: % Delete an Instruction 8 pos = sample(categorical(all_positions(current_program))) 9 return (DELETE, pos) 10 11 if move_type == 3: % Instruction Transform 12 pos = sample(categorical(all_positions(current_program))) 13 instr = sample(categorical(set_of_all_instructions)) 14 arity = nb_args(instr) 15 for i = 1, arity: 16 possible_args = possible_arguments(instr, i) 17 % get one of the arguments that can be used as i-th 18 % argument for the instruction \u2019instr\u2019. 19 operands[i] = sample(categorical(possible_args)) 20 return (TRANSFORM, pos, instr, operands) 21 22 if move_type == 4: % Opcode Transform 23 pos = sample(categorical(all_positions(current_program))) 24 args = arguments_at(current_program, pos) 25 instr = sample(categorical(possible_instruction(args))) 26 % get an instruction compatible with the arguments 27 % that are in the program at line pos. 28 return(OPCODE_TRANSFORM, pos, instr) 29 30 if move_type == 5: % Opcode Width Transform 31 pos = sample(categorical(all_positions(current_program)) 32 curr_instr = instruction_at(current_program, pos) 33 instr = sample(categorical(same_memonic_instr(curr_instr)) 34 % get one instruction with the same memonic that the 35 % instruction \u2019curr_instr\u2019. 36 return (OPCODE_TRANSFORM, pos, instr) 37 38 if move_type == 6: % Operand transform 39 pos = sample(categorical(all_positions(current-program)) 40 curr_instr = instruction_at(current_program, pos) 41 arg_to_mod = sample(categorical(args(curr_instr))) 42 possible_args = possible_arguments(curr_instr, arg_to_mod) 43 new_operand = sample(categorical(possible_args)) 44 return (OPERAND_TRANSFORM, pos, arg_to_mod, new_operand) 45 46 if move_type == 7: % Local swap transform 47 block_idx = sample(categorical(all_blocks(current_program))) 48 possible_pos = pos_in_block(current_program, block_idx) 49 pos_1 = sample(categorical(possible_pos)) 50 pos_2 = sample(categorical(possible_pos)) 51 return (SWAP, pos_1, pos_2) 52 53 if move_type == 8: % Global swap transform 54 pos_1 = sample(categorical(all_positions(current_program))) 55 pos_2 = sample(categorical(all_positions(current_program))) 56 return (SWAP, pos_1, pos_2) 57 58 if move_type == 9: % Rotate transform 59 pos_1 = sample(categorical(all_positions(current_program))) 60 pos_2 = sample(categorical(all_positions(current_program))) 61 return (ROTATE, pos_1, pos_2) Figure 5: Generative Model of a Transformation. 12 Under review as a conference paper at ICLR 2017 B HACKER\u2019S DELIGHT TASKS The 25 tasks of the Hacker\u2019s delight Warren (2002) datasets are the following: 1. Turn off the right-most one bit 2. Test whether an unsigned integer is of the form 2(n− 1) 3. Isolate the right-most one bit 4. Form a mask that identifies right-most one bit and trailing zeros 5. Right propagate right-most one bit 6. Turn on the right-most zero bit in a word 7. Isolate the right-most zero bit 8. Form a mask that identifies trailing zeros 9. Absolute value function 10. Test if the number of leading zeros of two words are the same 11. Test if the number of leading zeros of a word is strictly less than of another work 12. Test if the number of leading zeros of a word is less than of another work 13. Sign Function 14. Floor of average of two integers without overflowing 15. Ceil of average of two integers without overflowing 16. Compute max of two integers 17. Turn off the right-most contiguous string of one bits 18. Determine if an integer is a power of two 19. Exchanging two fields of the same integer according to some input 20. Next higher unsigned number with same number of one bits 21. Cycling through 3 values 22. Compute parity 23. Counting number of bits 24. Round up to next highest power of two 25. Compute higher order half of product of x and y Reference implementation of those programs were obtained from the examples directory of the stoke repository (Churchll et al., 2016). 13 Under review as a conference paper at ICLR 2017 C EXAMPLES OF HACKER\u2019S DELIGHT OPTIMISATION The first task of the Hacker\u2019s Delight corpus consists in turning off the right-most one bit of a register. When compiling the code in Listing 6a, llvm generates the code shown in Listing 6b. A typical example of an equivalent version of the same program obtained by the data-augmentation procedure is shown in Listing 6c. Listing 6d contains the optimal version of this program. Note that such optimization are already feasible using the stoke system of Schkufza et al. (2013). 1 # i n c l u d e < s t d i n t . h> 2 3 i n t 3 2 t p01 ( i n t 3 2 t x ) { 4 i n t 3 2 t o1 = x − 1 ; 5 re turn x & o1 ; 6 } (a) Source. 1 pushq %rbp 2 movq %rsp , %rbp 3 movl %edi , −0x4(% rbp ) 4 movl −0x4(% rbp ) , %e d i 5 s u b l $0x1 , %e d i 6 movl %edi , −0x8(% rbp ) 7 movl −0x4(% rbp ) , %e d i 8 a n d l −0x8(% rbp ) , %e d i 9 movl %edi , %eax 10 popq %rbp 11 r e t q 12 nop 13 nop 14 nop (b) Optimization starting point. 1 b l s r l %edi , %e s i 2 s e t s %ch 3 xorq %rax , %r a x 4 s a r b $0x2 , %ch 5 rorw $0x1 , %di 6 subb $0x3 , %d i l 7 mul l %ebp 8 subb %ch , %dh 9 r c r b $0x1 , %d i l 10 cmovbel %e s i , %eax 11 r e t q (c) Alternative equivalent program. 1 b l s r l %edi , %eax 2 r e t q (d) Optimal solution. Figure 6: Program at different stage of the optimization. 14 Introduction Related Works Learning Stochastic Super-optimization Stochastic search as a program optimization procedure Learning to search Objective function Parameterization of the Move Proposal Distribution Learning the Proposal Distribution Experiments Setup Existing Programs Automatically Generated Programs Conclusion Structure of the proposal distribution Hacker's Delight Tasks Examples of Hacker's delight optimisation ","flair":"three\tResearch"}
{"author":"cmon_krunn","created":"Sat Nov 19 22:00:05 EST 2016","text":"This is my Keras implementation of the paper with the same name (https:\/\/arxiv.org\/pdf\/1611.02049v1.pdf) The dataset was obtained from https:\/\/archive.ics.uci.edu\/ml\/datasets\/UJIIndoorLoc Huge shoutout to Aqib Saeed (https:\/\/github.com\/aqibsaeed\/) for their initial TensorFlow implementation (https:\/\/github.com\/aqibsaeed\/Place-Recognition-using-Autoencoders-and-NN)!","flair":"four\tProject"}
{"author":"luminerius","created":"Mon Oct 17 11:09:16 EDT 2016","text":"Hi ML! We have a ton of tutorials and discussions about ML hardware systems\/providers and libraries\/frameworks here, but I feel we don't talk nearly enough about the workflows we use to utilize those technologies, which are often just as important to success. \n\nSo let's have a quick chat about ML workflows! How do you spend your time? What tools do you use on a typical project, and in what order? How did you arrive at your current workflow, and what do you wish you could improve with it?","flair":"one\tDiscussion"}
{"author":"Guanoco","created":"Mon Oct 10 12:22:51 EDT 2016","text":"Hello all,\n\nIt seems like there isn't a week in which deep learning doesn't come up as achieving some kind of remarkable task.\nI understand that one of the powers of deep learning is that it is capable of learning the features. \nThis capacity seems totally decoupled from the underlaying problem. So basically I read this as \"no matter what problem you have... You can use deep learning\". \n\nNow.. I know there must be a caveat. I just don't know which.\nWhat kind of problems are not applicable for deep learning?","flair":"one\tDiscussion"}
{"author":"3eyedravens","created":"Sat Oct 08 10:17:12 EDT 2016","text":" Learning to Describe E-Commerce Images from Noisy Online Data Takuya Yashima, Naoaki Okazaki, Kentaro Inui, Kota Yamaguchi, and Takayuki Okatani Tohoku University, Sendai, Japan Abstract. Recent study shows successful results in generating a proper language description for the given image, where the focus is on detecting and describing the contextual relationship in the image, such as the kind of object, relationship between two objects, or the action. In this paper, we turn our attention to more subjective components of descriptions that contain rich expressions to modify objects \u2013 namely attribute ex- pressions. We start by collecting a large amount of product images from the online market site Etsy, and consider learning a language generation model using a popular combination of a convolutional neural network (CNN) and a recurrent neural network (RNN). Our Etsy dataset contains unique noise characteristics often arising in the online market. We first apply natural language processing techniques to extract high-quality, learnable examples in the real-world noisy data. We learn a generation model from product images with associated title descriptions, and ex- amine how e-commerce specific meta-data and fine-tuning improve the generated expression. The experimental results suggest that we are able to learn from the noisy online data and produce a product description that is closer to a man-made description with possibly subjective at- tribute expressions. 1 Introduction Imagine you are a shop owner and trying to sell a handmade miniature doll. How would you advertise your product? Probably giving a good description is one of the effective strategies. For example, stating Enchanting and unique fairy doll with walking stick, the perfect gift for children would sound more appealing to customers than just stating miniature doll for sale. In this paper, we consider automatically generating good natural language descriptions for product images which have rich and appealing expressions. Natural language generation has become a popular topic as the vision com- munity makes a significant progress in deep models to generate a word sequence given an image [27,12]. Existing generation attempts focus mostly on detecting and describing the contextual relationship in the image [18], such as a kind of object in the scene (e.g., a man in the beach) or the action of the subject given a scene (e.g., a man is holding a surfboard). In this paper, we turn our atten- tion to generating proper descriptions for product images with rich attribute expressions. 2 Takuya Yashima et al. Attributes have been extensively studied in the community [3,23,14]. Typical assumption is that there are visually recognizable attributes, and we can build a supervised dataset for recognition. However, as we deal with open-world vo- cabulary on the web, we often face much complex concepts consisting of phrases rather than a single word. The plausible approach would be to model attributes in terms of a language sequence instead of individual words. The challenge is that attribute expressions can be subjective and ambiguous. Attribute-rich expres- sions, such as antique copper flower decoration, or enchanting and unique fairy doll, require higher-level judgement on the concept out of lower-level appearance cues. Even humans do not always agree on the meaning of abstract concepts, such as coolness or cuteness. The concept ambiguity brings a major challenge in building a large-scale corpus of conceptually obscure attributes [29,20]. We attempt to learn attribute expressions using large-scale e-commerce data. Product images in e-commerce websites typically depict a single object without much consideration to the contextual relationship within an image, in contrast to natural images [21,18,25]. Product descriptions must convey, specific color, shape, pattern, material, or even subjective and abstract concepts out of the single image with a short title, or with a longer description in the item detail for those interested in buying the product, e.g., Beautiful hand felted and heathered purple & fuschia wool bowl. Although e-commerce data look appealing in terms of data availability and scale, descriptions and meta-data such as tags contain web-specific noise due to the nature of online markets, such as fragmented texts for search optimization or imbalance of distribution arising from shop owners. Naively learning a generation model results in poor product description, e.g., made to order. In this paper, we apply natural language processing to extract images and texts suitable for learning a generation model. Our language generation model is based on the popular image-to-sequence architecture consisting of a convolutional neural network (CNN) and a recurrent neural network (RNN) [27,12]. We learn a generation model using the prod- uct images and associated titles from the pre-processed dataset, and show that we are able to generate a reasonable description to the given product image. We also examine how e-commerce meta-data (product category) and optimiza- tion to the dataset (fine-tuning) affect the generation process. We annotate a handful of images using crowdsourcing and compare the quality of generated at- tribute expressions using machine translation metrics. The results suggest that e-commerce meta-data together with fine-tuning generate a product description closer to human. We summarize our contribution in the following. \u2013 We propose to learn attributes in the form of natural language expression, to deal with the exponentially many combination of open-world modifier vocabulary. \u2013 We collect a large-scale dataset of product images from online market Etsy, as well as human annotation of product descriptions using crowdsourcing for evaluation purpose. We release data to the academic community1. 1 http:\/\/vision.is.tohoku.ac.jp\/~kyamagu\/research\/etsy-dataset http:\/\/vision.is.tohoku.ac.jp\/~kyamagu\/research\/etsy-dataset Learning to Describe E-Commerce Images from Noisy Online Data 3 \u2013 We propose a simple yet effective data cleansing approach to transform e- commerce data into a corpus suitable for learning. \u2013 Our empirical study shows that our model can generate a description with attribute expressions using noisy e-commerce data. The study also suggests utilizing e-commerce meta-data can further improve the description quality. 2 Related Work Language generation Generating a natural language description from the given image has been an active topic of research in the vision community [21,11,27,12,6,28]. Early attempts have used retrieval-based approach to generate a sentence [21,11], and recently a deep-learning approach becomes a popular choice. For example, Vinyals et al. [27] shows that they can generate a high-quality language descrip- tion of the scene image using a combination of a CNN and a RNN. Karpathy et al. [12] also shows that their model can generate partial descriptions of given image regions, as well as a whole image. Antol et al. [1] studies a model which is able to generate sentences in answer to various questions about given images. In this paper, we are interested in generating a natural language expression that is rich in attribute. Previous work mostly focuses on natural images where the major goal is to understand the scene semantics and spatial arrangement, and produce an objective description. The closest to our motivation is perhaps the work by Mathews et al. [20] that studies a model to generate more expressive description with sentiment. They build a new dataset by asking crowd workers to re-write description of images contained in MS-COCO, and report successful generation with sentiment, for instance, beautiful, happy, or great. We take a different approach of utilizing e-commerce data to build an attribute-rich corpus of descriptions. Attribute recognition Semantic or expressive attributes have been actively studied in the community as a means of ontological entity [16] or localizable visual elements [3], expecting that these semantic information can be useful for many applications. In this work, we consider attribute expressions as a natural language description that modifies an object (specifically, a product) and conveys details possibly with abstract words. The attribute expressions are from open- world vocabulary in the real-world e-commerce data. In that sense, we have a similar spirit to weakly supervised learning [5,9,24]. We propose to use a sequence generation model rather than attempting to learn a classifier from exponentially many combinations of attribute expressions. Vision in e-commerce Several attempts have been made to apply computer vision in e-commerce applications [19,14,8,7,13], perhaps for the usefulness in a specific scenario such as better user experience in retrieval or product recommen- dation. The earlier work by Berg et al. [3] propose a method of automatic visual attribute discovery using web data, specifically product images from shopping 4 Takuya Yashima et al. Fig. 1: Our language generation model combining CNN and RNN. websites. Our work has the same motivation that we wish to learn language de- scription of attributes from the e-commerce data, though we use variety of prod- ucts and try to capture abstract attributes using language generation model. Very recently, Zakrewsky et al. [30] reports an attempt of popularity prediction of products offered in Etsy. The results suggest the potential usefulness of image feature for selling strategies, such as advertisement. 3 Language generation model Our language generation is based on the combination of convolutional neural networks (CNN) to obtain image representation and recurrent neural networks (RNN), using LSTM cells to translate the representation into a sequence of words [27,12,32]. In addition to the image input, we also consider inserting e- commerce meta-data to the RNN. In this paper, we utilize the category of prod- uct as extra information available in the e-commerce scenario, and feed into the RNN as a one-hot vector. Note that each product could have more than one category, such as a main category and sub categories, but in this paper we use only the main category for simplicity. Figure 1 illustrates our generation model. Let us denote the input product image I\u2019s feature by zv = CNN (I), the one- hot vector of the product category in meta-data by zc, and the one-hot vector of the currently generated word at description position t by xt. Our sequence generator is then expressed by: Hin = { [1;Whi [zv; zc] ;0] (t = 1) [1;Whxxt;ht−1] (otherwise) (1) (i, f, o, g) = WLSTMHin, (2) ct = f � ct−1 + i� g, (3) ht = o� tanh(ct), (4) yt = softmax(Wohht + bo), (5) where Whi,Whx,WLSTM ,Woh, bo are weights and biases of the network. We learn these parameters from the dataset. Gates i, f, o, g are controlling whether each input or output is used or not, allowing the model to deal with the vanishing Learning to Describe E-Commerce Images from Noisy Online Data 5 Fig. 2: Product data in Etsy dataset. gradient problem. We feed the image and category input only at the beginning. The output yt represents an unnormalized probability of each word, and has the length equal to the vocabulary size + 1 to represent a special END token to indicate the end of a description. To learn the network, we use product image, title and category information. The learning procedure starts by setting h0 = 0, y1 to the desired word in the description (yt = y1 indicates the first word in the sequence), and x1 to a special START symbol to indicate the beginning of the sequence. We feed the rest of the words from the ground truth until we reach the special END token at the end. We learn the model to maximize the log probability in the dataset. At test time, we first set h0 = 0, x1 to the START token, and feed the image representation zv with the category vector zc. Once we get an output, we draw a word according to yt and set the word to xt, the word predicted at the previous step (so when t = 2, each xt is yt−1). We repeat the process until we observe the END token. When training, we use Stochastic Gradient Descent, set the initial learning rate to 1.0e-3, and lower as the process iterates. In this paper, we do not back- propagate the gradient to CNN and separately train CNN and RNN. We evaluate how different CNN models perform in Section 5. 4 Building corpus from e-commerce data We collect and build the image-text corpus from the online market site Etsy. We prepare pairs of a product image and title as well as product meta-data suitable for learning attribute expressions. The challenge here is how to choose good descriptions for learning. In this section, we briefly describe the e-commerce data and our approach to extract useful data using syntactic analysis and clustering. 6 Takuya Yashima et al. 4.1 E-commerce data Etsy is an online market for hand-made products [31]. Figure 2 shows some examples of product images from the website. Each listing contains various in- formation, such as image, title, detailed description, tags, materials, shop owner, price, etc. We crawled product listings from the website and downloaded over two million product images. We apply various pre-processing steps to transform the crawled raw data into a useful corpus to learn attribute expressions. Note that this semi-automated approach to build a corpus is distinct from the previous language generation efforts where the approach is to start from supervised dataset with clean annno- tations [18]. Our corpus is from the real-world market, and as common in any Web data [21,25], the raw listing data from Etsy contain a lot of useless data for learning, due to a huge amount of near-duplicates and inappropriate language use for search engine optimization. For example, we observed the following titles: \u2013 Army of two Airsoft Paintball BB Softair Gun Prop Helmet Salem Costume Cosplay Goggle Mask Maske Masque jason MA102 et \u2013 teacher notepad set - bird on apple \/ teacher personalized stationary \/ per- sonalized stationery \/ teacher notepad \/ teacher gift \/ notepad Using such raw data to learn a generation model results in poor language quality in the output. 4.2 Syntactic analysis One common observation in Etsy is that there are fragments of noun phrases, often considered as a list of keywords targeting at search engine optimization. Although generating search-optimized description could be useful in some ap- plication, we are not aiming at learning keyword fragments in this paper. We attempt to identify such fragmented description by syntactic analysis. We first apply Stanford Parser [4] and estimate part-of-speech (POS) tags, such as noun or verb, for each word in the title. In this paper, we define mal- formed descriptions by the following criteria: \u2013 more than 5 noun phrases in a row, or \u2013 more than 5 special symbols such as slash, dash, and comma. Figure 3 shows a few accepted and rejected examples from Etsy data. Note that due to the discrepancy between our Etsy titles and the corpus Stanford Parser is trained on, we found even the syntactic parser frequently failed to assign correct POS tags for each word. We did not apply any special pre-processing for such cases since most of the failed POS tagging resulted in the sequence of nouns, which in turn leads to reject. Learning to Describe E-Commerce Images from Noisy Online Data 7 peppermint，tangerine， and hemp lip balms coffee bag backpack green jadeite jade beaded， natural green white color beads size 8mm charm necklace (a) Syntactically accepted chunky pink and purple butterfly cuddle critter cape set newborn to 3 months photo prop travel journal diary notebook sketch book - keep calm and go to canada - ivory texas license plate bird house (b) Syntactically rejected Fig. 3: Accepted and rejected examples after syntactic analysis. Some products have grammatically invalid title due to the optimization to search engine. 4.3 Near-duplicate removal Another e-commerce specific issue is that there is a huge amount of near-duplicates. Near-duplicates are commonly occurring phenomena in web data. Here, we de- fine near-duplicate items as products whose titles are similar and differ only in a small part of the descriptions such as shown in Table 1. Those near-duplicates add a strong bias towards specific phrasing and affect the quality of the gener- ated description. Without a precaution, the trained model always generates a similar phrase for any kind of images. In Etsy, we observe near-duplicates among the products offered by the same shop and listed in the same kind of categories, such as a series of products under the same category, as shown in Table 1. We find that such textual near-duplicates also exhibit visually similar appearance. Note that near-duplicates can happen for visually similar items but with differ- ent description, such as items under the same category but from a different shop. However, we find that such cases are considerably rare compared to the textual near-duplicates in Etsy, perhaps due to the nature of a hand-made market where majority of products are one-of-a-kind. 8 Takuya Yashima et al. Table 1: Examples of near-duplicate products. CUSTOM iPad SLEEVE 1, 2, New, 3 Black Chevron Lime Green Personalized Monogram CUSTOM iPad SLEEVE 1, 2, New, 3 Light Pink Chevron Gray Fancy Script PERSONALIZED Monogram CUSTOM iPad SLEEVE 1, 2, New, 3 Black Damask Hot Pink Personalized Monogram CUSTOM iPad SLEEVE 1, 2, New, 3 Dark Blue Lattice Lime Green PERSONALIZED Mono- gram CUSTOM iPad SLEEVE 1, 2, New, 3 Blue Diagonal Green PERSONALIZED Monogram CUSTOM iPad SLEEVE 1, 2, New, 3 Light Blue Orange Floral Pattern Teal PERSONALIZED Monogram We automatically identify near-duplicates using shop identity and title de- scription. We apply the following procedure to sub-sample product images from the raw online data. 1. Group products if they are sold by the same shop and belonging to the same category. 2. For each group, measure the similarity of the descriptions between all pairs within the group. 3. Depending on the pairwise similarity, divide the group into sub-groups by DBSCAN clustering. 4. Randomly sub-sample pre-defined number of product images from each sub- group. Our approach is based on the observation that the same shop tend to sell near- duplicates. We divide products into sub-groups to diversify the variation of de- scriptions. We divide the group into sub-groups based on thresholding on the Jaccard similarity: JG = S1 ∩ S2 · · · ∩ Sn S1 ∪ S2 · · · ∪ Sn , (6) where Si represents a set of words in the title description. Low-similarity within a cluster indicates the group contains variety of descriptions, and consists of subtly different products. We apply Density-Based Spatial Clustering of Applications with Noise (DBSCAN) [10] implemented in sklearn to obtain sub-clusters. Fig- ure 4 shows an example of groups. The purpose of the sub-sampling approach is to trim excessive amount of similar products while keeping variety. After clustering, we randomly pick a certain number of products from each cluster. We determine the number of samples per cluster KG using the following Learning to Describe E-Commerce Images from Noisy Online Data 9 Fig. 4: Near-duplicates clustering. Our clustering only uses meta-data and tex- tual descriptions to identify near-duplicates, but the resulting clusters tend to be visually coherent. threshold: KG = 1 N m∑ k=1 nk + σ. (7) Here, N is the total number of the groups, nk is the number of the products in the group k and σ is the standard deviation of the distribution of the number of products in the whole groups. We leave out some of the products if the number of products in a certain group or a subgroup is far above the average. After the sub-sampling process, all kinds of products should have been equally distributed in the corpus. Out of over 2 million products from the raw Etsy products, we first selected 400k image-text pairs by syntactic analysis, and applied near-duplicate removal. We obtained approximately 340k product images for our corpus. We take 75% of the images for training and reserved the rest (25%) for testing in our experiment. We picked up 100 images from testing set for human annotation, and did not use for quantitative evaluation due to the difficulty in obtaining ground truth. 5 Experiment We evaluate our learned generation model by measuring how close the generated expressions are to human. For that purpose, we collect human annotations to a small number of testing images and measure the performance using machine- translation metrics. 5.1 Human annotation We use crowdsourcing to collect human description of the product images. We designed a crowdsourcing task to describe the given product image. Figure 5 depicts our annotation interface. We asked workers to come up with a descriptive and appealing title to sell the given product. During the annotation task, we 10 Takuya Yashima et al. Fig. 5: Crowdsourcing task to collect human description. provide workers the original title and the category information to make sure they understand what kind of products they are trying to sell. We used Amazon Mechanical Turk and asked 5 workers per image to collect reference descriptions for 100 test images. As seen in Figure 5, our images are quite different from natural scene images. Often we observed phrases rather than a complete sentence in the human annotation. This observation suggests that the essence of attribute expression is indeed in the modifiers to the object rather than the recognition of subject-verb relationships. 5.2 Evaluation metrics We use seven metrics to evaluate the quality of our generated descriptions: BLEU-1 [22], BLEU-2, BLEU-3, BLEU-4, ROUGE [17], METEOR [2], and CIDEr [26]. These metrics have been widely used in natural language process- ing, such as machine translation, automatic text summarization, and recently in language generation. Although our goal is to produce attribute-aware phrases but not necessarily sentences, these metrics can be directly used to evaluate our model using the reference human description. BLEU-N evaluates descrip- tions based on precision on N-grams, ROUGE is also based on N-grams but intended for recall, METEOR is designed for image descriptions, and CIDEr is also proposed for image descriptions and using N-gram based method. We use the coco-caption implementation [26] to calculate the above evaluation metrics. 5.3 Experimental conditions We use AlexNet [15] for the CNN architecture of our generation model, and extract a 4,096 dimensional representation from fc7 layer given an input image Learning to Describe E-Commerce Images from Noisy Online Data 11 Table 2: Evaluation results. Method Bleu1 Bleu2 Bleu3 Bleu4 Rouge Meteor CIDEr Category+Fine-tune 15.1 6.55 2.58 × 10−5 5.56 × 10−8 12.9 4.69 11.2 Category+Pre-train 9.43 3.72 1.65 × 10−5 3.74 × 10−8 9.74 3.70 8.01 Fine-tune 8.95 3.94 2.03× 100 3.06× 10−4 4.98 2.24 1.88 Pre-train 8.77 3.26 1.50 × 10−5 3.50 × 10−8 9.32 3.36 6.87 MS-COCO 1.01 2.13 8.30 × 10−6 1.70 × 10−8 8.31 2.40 2.79 and its product category. In order to see how domain-knowledge affects the quality of language generation, we compare a CNN model trained on ImageNet, and a model fine-tuned to predict 32 product categories in Etsy dataset. Our recurrent neural network consists of a single hidden layer with 4,096 dimensional image feature and a 32 dimensional one-hot category indicator for an input. We use LSTM implementation of [12]. We compare the following models in the experiment. \u2013 Category+Fine-tune: Our proposed model with a fine-tuned CNN and a category vector for RNN input. \u2013 Category+Pre-train: Our proposed model with a pre-trained CNN and a category vector for RNN input. \u2013 Fine-tune: A fine-tuned CNN with RNN without a category vector. \u2013 Pre-train: A pre-trained CNN with RNN without a category vector. \u2013 MS-COCO: A reference CNN+RNN model trained on MS-COCO dataset [18] without any training in our corpus. We include MS-COCO model to evaluate how domain-transfer affects the quality of generated description. Note that MS-COCO dataset contains more objective descriptions for explaining objects, actions, and scene in the given image. 5.4 Quantitative results We summarize the performance evaluation in Table 2. Note that all scores are in percentage. Our Category+Fine-tune model achieves the best performance, except for BLEU-3 and BLEU-4, in which Fine-tune model achieves the best. We suspect overfitting happened in the Fine-tune only case where the model learned to predict certain 3- or 4-word phrases such as Made to order, some happened to be unexpectedly correct, and resulted in the sudden increase BLEU increase. However, we did not observe a similar improvement in other scores, such as ROUGE or CIDEr. We conjecture this possibly-outlier result could be attributed to BLEU\u2019s evaluation method. From the result, we observe that introducing the category vector has the largest impact on the description quality. We assume that category information supplements semantic knowledge in the image feature even if the category is not apparent from the product appearance, and that results in stabler language 12 Takuya Yashima et al. Table 3: Comparison of generated descriptions. Image Original Zombie Chicken - Needle Felted Wool Sculpture - MADE TO ORDER Rooted Fairy Walking sticks Category + Fine-tune needle felted animal art print primitive doll Category + Pre-train needle felted cat custom made to order hand knit scarf Fine-tune custom made to order hand made hand painted ric rac trims 11mm wide custom made to order Pre-train needle felted penguin wedding invitation black and white striped leggings MS-COCO a white teddy bear sit- ting on top of a green field a close up of a pair of scissors a man sitting on a bench with a bunch of bananas Human Made-to-Order Felted Wool Zombie Chicken A beautiful abstract illustration of a star, rooted to the Earth, surrounded by rabbits. Enchanting and unique fairy doll with walking stick, the perfect gift for children generation for difficult images. Note that in the e-commerce scenario, meta-data are often available for free without expensive manual annotation. The difference between Pre-train and Fine-tune models explains how domain- specific image feature contributes to better learning and helps the model to gen- erate high-quality descriptions. The result indicates that a pre-trained CNN is not sufficient to capture the visual patterns in Etsy dataset. MS-COCO baseline is performing significantly worse than other models, indicating that the general description generated by MS-COCO is far from attribute-centric description in product images. There is a significant difference between descriptions in the MS- COCO dataset and our Etsy corpus. The former tends to be complete, gram- matically correct descriptions focusing on the relationship between entities in the scene, whereas Etsy product titles tend to omit a verb and often do not require recognizing spatial arrangement of multiple entities. A product description can be a fragment of phrases as seen in the actual data, and a long description can look rather unnatural. 5.5 Qualitative results Table 3 shows examples of generated descriptions by different models as well as the original product titles. Fine-tune+category model seems to have generated better expressions while other methods sometimes fail to generate meaningful description (e.g., custom made to order). MS-COCO model is generating signifi- cantly different descriptions, and always tries to generate a description explaining types of objects and the relationship among them. Learning to Describe E-Commerce Images from Noisy Online Data 13 reclaimed wood coffee ta- ble handmade journal note- book watercolor painting of moai statues at sunset crochet barefoot sandals with flower hand painted ceramic mug I\u2019m going to be a big brother t-shirt Fig. 6: Examples of generated descriptions. Our model correctly identifies at- tribute expressions (left 2 columns). The rightmost column shows failure cases due to corpus issues. Our model generates somewhat attribute-centric expressions such as needle felted or primitive. Especially the latter expression is relatively abstract. These examples confirms that at least we are able to automatically learn attribute ex- pressions from noisy online data. The descriptions tend to be noun phrases. This tendency is probably due to the characteristics of e-commerce data containing phrases rather than a long, grammatically complete sentences. Our generation results correctly reflect this characteristics. Figure 6 shows some examples of generated descriptions by our model (category + fine-tune). Our model predicts attribute expressions such as reclaimed wood or hand-painted ceramic. We ob- serve failure due to corpus quality in some categories. For example, the painting or the printed t-shirts in Figure 6 suffer from bias towards specific types of products in the category. Sometimes our model gives a better description than the original title. For example, The middle in Table 3 shows a product entitled Rooted, but it is almost impossible to guess the kind of product from the name, or maybe even from the appearance. Our model produces art print for this ex- ample, which seems to be much easier to understand the product kind and closer to our intuition, even if the result is not accurate. 6 Discussion In this paper, we used a product image, a title and category information to gen- erate a description. However, there is other useful information in the e-commerce 14 Takuya Yashima et al. data, such as tags, materials, or popularity metrics [31]. Especially, a product description is likely to have more detailed information about the product, with many attribute-like expressions having plenty of abstract or subjective words. E-commerce dataset looks promising in this respect since sellers are trying to attract more customers by \u201Cgood\u201D phrases which have a ring to it. If we are able to find attribute expressions in the longer product description, we can expand our image-text corpus to a considerably larger scale. The chal- lenge here is that we then need to identify which description is relevant to the given image, because product descriptions contain irrelevant information also. For example, in Etsy, a product often contains textual description on shipping information. For a preliminary study, we applied a syntactic parser on Etsy product descriptions, but often observed an error in a parse tree, due to gram- matically broken descriptions in item listings. Identifying which description is relevant or irrelevant seems like an interesting vision-language problem in the e-commerce scenario. Finally, in this paper we left tags and material information in the item list- ings in Etsy dataset. These meta-data could be useful to learn a conventional attribute or material classifier given an image, or to identify attribute-specific expressions in the long product description. 7 Conclusion and future work We studied the natural language generation from product images. In order to learn a generation model, we collected product images from the online market Etsy, and built a corpus to learn a generation model by applying dataset cleans- ing procedure based on syntactic analysis and near-duplicate removal. We also collected human descriptions for evaluation of the generated descriptions. The empirical results suggest that our generation model fine-tuned on Etsy data with categorical input successfully learns from noisy online data, and produces the best language expression for the given product image. The result also indicates a huge gap between the task nature of attribute-centric language generation and a general scene description. In the future, we wish to improve our automatic corpus construction from noisy online data. We have left potentially-useful product meta-data in this study. We hope to incorporate additional information such as product descrip- tion or tags to improve language learning process, as well as to realize a new application such as automatic title and keywords suggestion to shop owners. Also, we are interested in improving the deep learning architecture to the gen- erate attribute expressions. Acknowledgement This work was supported by JSPS KAKENHI Grant Numbers JP15H05919 and JP15H05318. Learning to Describe E-Commerce Images from Noisy Online Data 15 References 1. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Ba- tra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015. 2. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evalua- tion with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and\/or summarization, volume 29, pages 65\u201372, 2005. 3. Tamara L Berg, Alexander C Berg, and Jonathan Shih. Automatic attribute discovery and characterization from noisy web data. In ECCV, pages 663\u2013676. Springer, 2010. 4. Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural networks. In EMNLP, pages 740\u2013750, 2014. 5. Xinlei Chen, A. Shrivastava, and A. Gupta. Neil: Extracting visual knowledge from web data. In ICCV, pages 1409\u20131416, Dec 2013. 6. Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. Language Models for Image Captioning : The Quirks and What Works. Association for Computational Linguistics (ACL), pages 100\u2013105, 2015. 7. Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu, and Eliza- beth Churchill. When relevance is not enough: Promoting visual attractiveness for fashion e-commerce. arXiv preprint arXiv:1406.3561, 2014. 8. Wei Di, Neel Sundaresan, Robinson Piramuthu, and Anurag Bhardwaj. Is a picture really worth a thousand words?:-on the role of images in e-commerce. In Proceed- ings of the 7th ACM international conference on Web search and data mining, pages 633\u2013642. ACM, 2014. 9. Santosh Divvala, Ali Farhadi, and Carlos Guestrin. Learning everything about anything: Webly-supervised visual concept learning. In CVPR, 2014. 10. Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, 1996. 11. Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899, 2013. 12. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137, 2015. 13. M Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C Berg, and Tamara L Berg. Where to buy it: Matching street clothing photos in online shops. In ICCV, 2015. 14. Adriana Kovashka, Devi Parikh, and Kristen Grauman. Whittlesearch: Image search with relative attribute feedback. In Computer Vision and Pattern Recogni- tion (CVPR), 2012 IEEE Conference on, pages 2973\u20132980. IEEE, 2012. 15. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information pro- cessing systems, pages 1097\u20131105, 2012. 16. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. Pattern Analysis and Ma- chine Intelligence, IEEE Transactions on, 36(3):453\u2013465, 2014. 16 Takuya Yashima et al. 17. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8, 2004. 18. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer, 2014. 19. Si Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan. Street-to-shop: Cross-scenario clothing retrieval via parts alignment and aux- iliary set. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Con- ference on, pages 3330\u20133337. IEEE, 2012. 20. Alexander Patrick Mathews, Lexing Xie, and Xuming He. Senticap: Generating image descriptions with sentiments. CoRR, abs\/1510.01431, 2015. 21. Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing im- ages using 1 million captioned photographs. In Advances in Neural Information Processing Systems, pages 1143\u20131151, 2011. 22. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002. 23. Devi Parikh and Kristen Grauman. Relative attributes. In Dimitris N. Metaxas, Long Quan, Alberto Sanfeliu, and Luc J. Van Gool, editors, ICCV, pages 503\u2013510. IEEE Computer Society, 2011. 24. Chen Sun, Chuang Gan, and Ram Nevatia. Automatic concept discovery from parallel text and visual corpora. In ICCV, pages 2596\u20132604, 2015. 25. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817, 2015. 26. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus- based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575, 2015. 27. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164, 2015. 28. Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image cap- tion generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. 29. Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Robust image sentiment analysis using progressively trained and domain transferred deep networks. arXiv preprint arXiv:1509.06041, 2015. 30. S. Zakrewsky, K. Aryafar, and A. Shokoufandeh. Item Popularity Prediction in E-commerce Using Image Quality Feature Vectors. ArXiv e-prints, May 2016. 31. Stephen Zakrewsky, Kamelia Aryafar, and Ali Shokoufandeh. Item popularity prediction in e-commerce using image quality feature vectors. arXiv preprint arXiv:1605.03663, 2016. 32. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. CoRR, abs\/1409.2329, 2014. Learning to Describe E-Commerce Images from Noisy Online Data ","flair":"null\tnull"}
{"author":"clbam8","created":"Wed Nov 16 16:30:36 EST 2016","text":"Here at AYLIEN we have a team of researchers who like to keep abreast of, and regularly contribute to, the latest developments in the field of Natural Language Processing. Recently, one of our research scientists, Sebastian Ruder, attended EMNLP 2016 in Austin, Texas. In this post, Sebastian has highlighted some of the stand-out papers and trends from the conference. I spent the past week in Austin, Texas atEMNLP 2016, the Conference on Empirical Methods in Natural Language Processing. There were a lot of papers at the conference (179 long papers, 87 short papers, and 9 TACL papers all in all) \u2014 too many to read each single one. The entire program can be foundhere. In the following, I will highlight some trends and papers that caught my eye: One thing that stood out was that RL seems to be slowly finding its footing in NLP, with more and more people using it to solve complex problems: Dialogue was a focus of the conference with all of the three keynote speakers dealing with different aspects of dialogue: Christopher Potts talked about pragmatics and how to reason about the intentions of the conversation partner; Stefanie Tellex concentrated on how to use dialogue for human-robot collaboration; finally, Andreas Stolcke focused on the problem of addressee detection in his talk. Among the papers, a few that dealt with dialogue stood out: Seq2seq models were again front and center. It is not common for a method to have its own session two years after its introduction (Sutskever et al., 2014). While in the past years, many papers employed seq2seq e.g. for Neural Machine Translation, some papers this year focused on improving the seq2seq framework: Since seq2seq\u2019s use for dialogue modelling was popularised byVinyals and Le, it is harder to get it to work with goal-oriented tasks that require an intermediate representation on which to act. Semantic parsing is used to convert a message into a more meaningful representation that can be used by another component of the system. As this technique is useful for sophisticated dialogue systems, it is great to see progress in this area: While mapping from text-to-text with the seq2seq paradigm is still prevalent, EMNLP featured some cool papers on natural language generation from other inputs: Parsing and syntax are a mainstay of every NLP conference and the community seems to particularly appreciate innovative models that push the state-of-the-art in parsing: The ACL \u201916 outstanding paper byAndor et al.introduced a globally normalized model for parsing, while the best EMNLP \u201816 paper byLee et al.combines a global parsing model with a local search over subtrees. There were still papers on word embeddings, but it felt less overwhelming than at the past EMNLP or ACL, with most methods trying to fix a particular flaw rather than training embeddings for embeddings\u2019 sake.Pilevhar and Collierde-conflate senses in word embeddings, whileWieting et al.achieve state-of-the-art results for character-based embeddings. Sentiment analysis has been popular in recent years (as attested by the introductions of many recent papers on sentiment analysis). Sadly, many of the conference papers on sentiment analysis reduce to leveraging the latest deep neural network for the task to beat the previous state-of-the-art without providing additional insights. There are, however, some that break the mold:Teng et al.find an effective way to incorporate sentiment lexicons into a neural network, whileHu et al.incorporate structured knowledge into their sentiment analysis model. By now, it is clear to everyone: Deep Learning is here to stay. In fact, deep learning and neural networks claimed the two top spots of keywords that were used to describe the submitted papers. The majority of papers used at least an LSTM; using no neural network seems almost contrarian now and is something that needs to be justified. However, there are still many things that need to be improved \u2014 which leads us to\u2026 While making incremental progress is important to secure grants and publish papers, we should not lose track of the long-term goals. In this spirit, one of the best workshops that I\u2019ve attended was the Uphill Battles in Language Processing workshop, which featured 12 talks and not one, but four all-star panels on text understanding, natural language generation, dialogue and speech, and grounded language. Summaries of the panel discussions should be available soon at theworkshop website. This was my brief review of some of the trends of EMNLP 2016. I hope it was helpful.","flair":"two\tNews"}
{"author":"liviu-","created":"Sat Nov 19 15:42:24 EST 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 11 Star 233 Fork 42 liviu-\/notebooks Code Issues 0 Pull requests 0 Projects 0 Pulse Graphs Permalink Branch: master Switch branches\/tags Branches Tags master Nothing to show Nothing to show Find file Copy path notebooks\/bayesian_linear_regression.ipynb 992a908 Nov 22, 2016 liviu- Link to proof showing that A^T A is invertible 1 contributor Users who have contributed to this file liviu- Download History 1.12 MB Sorry, something went wrong. Reload? Sorry, we cannot display this file. Sorry, this file is invalid so it cannot be displayed. Viewer requires iframe. Jump to Line Go Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"TheDuke57","created":"Tue Nov 15 22:20:31 EST 2016","text":"I am curious about using CNNs to aid in fluid flow analysis. I looked around on google and arxiv but only found one paper titled \"Convolutional Neural Networks for Steady Flow Approximation\" which is surprisingly from Autodesk. They seem to have produced some pretty interesting and impressive results, but that is the only reference I have found. Most papers are in reference to mapping CFD to structural models or predicting rates of reactions in fluid flows. \n\nhttp:\/\/www.kdd.org\/kdd2016\/papers\/files\/adp1175-guoA.pdf\n\nBasically, they created input 2D and 3D 'images' as distances from the surface of a submerged object. Then ran a series of convolution, fully connected, and de-convolution layers to predict the 2D and 3D velocity fields. The result was a 2-4 order-of-magnitude speed up on predicting the velocity field compared to the LBM method and sacrificed of 1-3% error. There are some drawbacks with regards to input velocity and control of the boundary conditions, but it is very interesting. \n\nDoes anyone know of similar research or similar papers? What are your thoughts on the idea? ","flair":"one\tDiscussion"}
{"author":"sprintletecity","created":"Sat Oct 22 19:19:09 EDT 2016","text":"There is a part in the paper (https:\/\/arxiv.org\/pdf\/1511.06391v4.pdf) that discusses the process block - how does it make sense to concatenate the read-out to the query vector q_t? Shouldn't this be an add operation, since concatenation will change the dimension of q_t? I just don't understand how this can be evolved using the lstm.","flair":"one\tDiscussion"}
{"author":"hardmaru","created":"Wed Nov 02 13:23:21 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.09615 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Compressed Learning: A Deep Neural Network Approach Authors: Amir Adler, Michael Elad, Michael Zibulevsky (Submitted on 30 Oct 2016) Abstract: Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1610.09615 [cs.CV]   (or arXiv:1610.09615v1 [cs.CV] for this version) Submission history From: Amir Adler [view email] [v1] Sun, 30 Oct 2016 07:54:19 GMT (85kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"ill-logical","created":"Sat Oct 15 20:15:30 EDT 2016","text":"For the Q&amp;A task, Graves *et al* report 16.7% mean error and 3.8% best error (over 20 runs).\n\nGiven such a big variance of the results, does it really make sense to compare them between different architectures (*e.g.* 4.2% and 7.5% MemN2N error)?\n\nI'm glad that Graves *et al* are reporting the mean error too. It was apparently considered OK to just report the best error over many runs and an unspecified, but potentially astronomical number of hyperparameter grid search trials on this dataset.","flair":"three\tResearch"}
{"author":"crowsonkb","created":"Tue Oct 25 04:01:06 EDT 2016","text":"[Original Python code for image synthesis](https:\/\/gist.github.com\/crowsonkb\/550fe172cd277bb2f057574f2e75aff4) - lots of image-specific things here.\n\n[Torch code that I tried training neural networks with](https:\/\/gist.github.com\/crowsonkb\/8da6cc4bfc5e99565ea7f897700a0bc0) - adapted from the Python code with help from [optim\/lbfgs.lua](https:\/\/github.com\/torch\/optim\/blob\/master\/lbfgs.lua).\n\nIt was originally for stylized image synthesis by inverting CNNs ([neural style](https:\/\/github.com\/crowsonkb\/style_transfer) i.e. Gatys et al.). The usual way of doing this is to start from a white noise image and apply gradient descent or L-BFGS to it, using gradients from the backward pass of the CNN. It isn't a stochastic problem and ordinary L-BFGS works well on it.\n\nSince momentum helps so much with gradient descent on this problem, I got ideas about incorporating momentum into L-BFGS (Nesterov ofc.). I already had custom L-BFGS code that I wrote to have a line search capable of dealing with the weird way I was normalizing the gradients. I made some fortuitous discoveries along the way. I ended up applying damping as well (forming the L-BFGS y vectors as a linear combination of the original y and the step), and learning how to modify the y vectors to produce a Hessian estimation that scaled the learning rates per-parameter. For stability's sake, and because I needed learning rate decay anyway, I scaled the learning rates with the Adagrad scaling matrix (per-parameter L2 norm of gradients seen so far). To my surprise the result behaved in a stable manner without a line search and actually worked better than Adam or L-BFGS on my image synthesis problem. Since I had modified the L-BFGS quasi-Newton algorithm with damping, momentum, and scaling, I started calling the result DMSQN.\n\nI then reimplemented it in Torch just to see what would happen when I tried this weird thing on neural network training instead of the problem it was originally adapted for. To my great surprise, it actually performed well and didn't blow up numerically. There doesn't seem a reason to recommend its use over SGD or Adam, but it at least wasn't worse. The lack of a line search, along with the previously-mentioned damping, momentum, and scaling, probably account for why it worked in the stochastic regime even though it wasn't originally designed for it. (oLBFGS for instance does not use a line search and uses a similar form of damping; adaQN repurposes the Adagrad scaling matrix but IMO in not as stability-promoting a way as I used it)\n\nHas anyone experimented with incorporating momentum into L-BFGS or other second-order methods, at all? There's a lot of prior work on damping for instance.","flair":"four\tProject"}
{"author":"vighneshbirodkar","created":"Fri Oct 07 18:05:28 EDT 2016","text":"My data is 64 x 64 images. I have tried the obvious approach of training the whole autoencoder end-to-end, but I could not get good reproductions as I decreased the feature vector length.\n\nIs layer-wise training helpful ? I found [this](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/54tdww\/autoencoders_using_residual_networks\/) paper which suggests that training one encoder layer at a time with the its corresponding decoder layer.  Are there any better approaches than this ?\n\nFrom what I understand Layer wise pre-training works as follows:\nEncoder = E1, E2, E3, E4\n\nDecoder = D1, D2, D3, D4\n\nTrain E1 -  D1\n\nTrain E1 - E2 - D2 - D1 ( E1, D1, fixed)\n\nTrain E1- E2 - E3 - D3 - D2 - D1 (E1, E2, D1, D2 fixed)\n\n.........and so on \n\nI am assuming that the loss function is MSE.\n\nBefore I proceed I would like to know if my understanding is correct or if there are better approaches out there. I might try Denoising\/Variatinal Autoencoders later, but I thought it'd be better to get a simple model working first.","flair":"null\tnull"}
{"author":"badhri","created":"Tue Nov 15 17:03:02 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01462 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.CL stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Authors: Hakan Inan, Khashayar Khosravi, Richard Socher (Submitted on 4 Nov 2016) Abstract: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5. Comments: 10 pages, 2 figures, 3 tables Subjects: Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML) Cite as: arXiv:1611.01462 [cs.LG]   (or arXiv:1611.01462v1 [cs.LG] for this version) Submission history From: Hakan Inan [view email] [v1] Fri, 4 Nov 2016 17:36:20 GMT (303kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"evc123","created":"Mon Oct 31 06:24:15 EDT 2016","text":" Under review as a conference paper at ICLR 2017 LEARNING GRAPHICAL STATE TRANSITIONS Daniel D. Johnson Department of Computer Science Harvey Mudd College 301 Platt Boulevard ddjohnson@hmc.edu ABSTRACT Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that can transform graph-structured inputs into output sequences. In this work I introduce a set of graph-based transformations, which I combine to construct a versatile extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and mod- ify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a fam- ily of Turing machines. 1 INTRODUCTION Many different types of data can be formulated using a graph structure. One form of data that lends itself to a graphical representation is data involving relationships (edges) between entities (nodes). Abstract maps of places and paths between them also have a natural graph representation, where places are nodes and paths are edges. In addition, many data structures can be expressed in graphical form, including linked lists and binary trees. Substantial research has been done on producing output when given graph-structured input (Kashima et al., 2003; Shervashidze et al., 2011; Perozzi et al., 2014; Bruna et al., 2013; Duvenaud et al., 2015). Of particular relevance to this work are Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), or GNNs, which extends recursive neural networks by assigning states to each node in a graph based on the states of adjacent nodes. Recently Li et al. (2016) have modified GNNs to use gated state updates and to produce output sequences. The resulting networks, called GG-NNs and GGS-NNs, are successful at solving a variety of tasks with graph-structured input. This work further builds upon GG-NNs and GGS-NNs by allowing graph-structured intermediate representations, as well as graph-structured outputs. This is accomplished using a more flexible graph definition, along with a set of graph transformations which take a graph and other information as input and produce a modified version of the graph. Combining these transformations with a recurrent input model yields the Gated Graph Transformer Neural Network model (GGT-NN), which incrementally constructs a graph given natural language input, and can either produce a final graph representing its current state, or use the graph to produce a natural language output. Extending GG-NNs in this way opens up a wide variety of applications. Since many types of data can be naturally expressed as a graph, it is possible to train a GGT-NN model to manipulate a meaningful graphical internal state. In this paper I demonstrate the GGT-NN model on the bAbI task dataset, which contains a set of stories about the state of the world. By encoding this state as a graph, a GGT-NN can learn to update the world state based on the input sentences and answer questions based on its internal graph. I also demonstrate that this architecture can learn complex update rules by training it to model a simple 1D cellular automaton and arbitrary 4-state Turing machines. This requires the network to learn how to transform its internal state based on the rules of each task. 1 Under review as a conference paper at ICLR 2017 1.1 GRU Gated Recurrent Units (GRU) are a type of recurrent network cell introduced by Cho et al. (2014). Each unit uses a reset gate r and an update gate z, and updates according to r(t) = σ ( Wrx (t) + Urh (t−1) + br ) z(t) = σ ( Wzx (t) + Uzh (t−1) + bz ) h̃(t) = φ ( Wx + U ( r(t) � h(t−1) ) + b ) h(t) = z� h(t−1) + (1− z)� h̃(t) where σ is the logistic sigmoid function, φ is an activation function (here tanh is used), x(t) is the input vector at timestep t, h(t) is the hidden output vector at timestep t, and W,U,Wr,Ur,Wz, Uz , b, br and bz are learned weights and biases. Note that � denotes elementwise multiplication. 1.2 GG-NN AND GGS-NN Gated Graph Neural Networks (GG-NN) are a form of graphical neural network models described by Li et al. (2016). In a GG-NN, a graph G = (V, E) consists of a set V of nodes v with unique values 1, . . . , |V| and a set E of directed edges e = (v, v\u2032) ∈ V × V oriented from v to v\u2032. Each node has an annotation xv ∈ RN and a hidden state hv ∈ RD. Additionally, each edge has a type ye ∈ {1, · · · ,M}. Initially, h(1)v is set to the annotation xv padded with zeros. Then nodes exchange information for some fixed number of timesteps T according to the propagation model h (1) v = [x>v ,0] > a (t) v = A>v:[h (t−1)> 1 · · ·h (t−1)> |V| ] > (1) z (t) v = σ(Wza (t) v + Uh (t−1) v ) r (t) v = σ(Wra (t) v + Urh (t−1) v ) h̃ (t) v = tanh(Wa (t) v + U(r (t) v � h(t−1)v )) h(t)v = (1− z(t)v )� h(t−1)v + z(t)v � h̃(t)v . Here a(t)v represents the information received by each node from its neighbors in the graph, and the matrix A ∈ RD|V|×2D|V| has a specific structure that determines how nodes communicate. The first half of A, denoted A(out) ∈ RD|V|×D|V|, corresponds to outgoing edges, whereas the second half A(in) ∈ RD|V|×D|V| corresponds to incoming edges. Each edge type y corresponds to specific forward and backward propagation matrices Py,P\u2032y ∈ RD×D which determine how to propagate information across an edge of that type in each direction. The D × D-sized submatrix of A(out) in position i, j contains Py if an edge of type y connects nodes ni to nj , or 0 if no such edge connects in that direction. Similarly, theD×D-sized submatrix of the matrix A(in) in position i, j contains P\u2032y if an edge of type y connects nodes nj to ni, or 0 if no such edge connects in that direction. Av: ∈ RD×2D|V| is the submatrix of A corresponding to node v. Thus, multiplication by Av: in 1 is equivalent to taking the following sum: a(t)v = ∑ v\u2032∈V ( M∑ y=1 sedge(v, v \u2032, y)�Py + sedge(v\u2032, v, y)�P\u2032y ) h (t−1) v\u2032 (2) where sedge(v, v\u2032, y) is 1 if e = (v, v\u2032) ∈ E and ye = y, and 0 otherwise. The output from a GG-NN is flexible depending on the task. For node selection tasks, a node score ov = g(h (T ) v ,xv) is given for each node, and then a softmax operation is applied. Graph-level outputs are obtained by combining an attention mechanism i and a node representation function j, both implemented as neural networks, to produce the output representation hG = tanh (∑ v∈V σ(i(h (T ) v ,xv))� tanh(j(h(T )v ,xv)) ) (3) Gated Graph Sequence Neural Networks (GGS-NN) are an extension of GG-NNs to sequential output o(1), . . . ,o(K). At each output step k, the annotation matrix X is given by X (k) = [x (k) 1 , . . . ,x (k) |V|] > ∈ R|V|×LV . A GG-NN Fo is trained to predict an output sequence o(k) from X (k), and another GG-NN FX is trained to predict X (k+1) from X (k). Prediction of the output at each step is performed as in a normal GG-NN, and prediction of X (k+1) from the set of all final hidden statesH(k,T ) (after T propagation steps of FX) occurs according to the equation x (k+1) v = σ ( j(h (k,T ) v ,x (k) v ) ) . 2 Under review as a conference paper at ICLR 2017 2 DIFFERENTIABLE GRAPH TRANSFORMATIONS In this section, I describe some modifications to the graph structure to make it fully differentiable, and then propose a set of transformations which can be applied to a graph structure in order to transform it. In particular, I redefine a graph G = (V, C) ∈ Γ as a set V of nodes v with unique values 1, . . . , |V|, and a connectivity matrix C ∈ R|V|×|V|×Y , where Y is the number of possible edge types. As before, each node has an annotation xv ∈ RN and a hidden state hv ∈ RD. However, there is an additional constraint that ∑N j=1 xv,j = 1. One can then interpret xv,j as the level of belief that v should have type j out of N possible node types. Each node also has a strength sv ∈ [0, 1]. This represents the level of belief that node v should exist, where sv = 1 means the node exists, and sv = 0 indicates that the node should not exist and thus should be ignored. Similarly, elements of C are constrained to the range [0, 1], and thus one can interpret Cv,v\u2032,y as the level of belief that there should be a directed edge of type y from v to v\u2032. (Note that it is possible for there to be edges of multiple types between the same two nodes v and v\u2032, i.e. it is possible for Cv,v\u2032,y = Cv,v\u2032,y\u2032 = 1 where y 6= y\u2032.) 2.1 NODE ADDITION The node addition transformation Tadd : Γ × Rα → Γ takes as input a graph G and an input vector a ∈ Rα, and produces a graph G\u2032 with additional nodes. The annotation and strength of each new node is determined by a function fadd : Rα × Rβ → R × RN × Rβ , where α is the length of the input vector, β is the length of the internal state vector, and as before N is the number of node types. The new nodes are then produced according to (s|VG |+i,x|VG |+i,hi) = fadd(a,hi−1), (4) starting with h0 initialized to some learned initial state, and recurrently computing sv and xv for each new node, up to some maximum number of nodes. Based on initial experiments, I found that implementing fadd as a GRU layer followed by 2 hidden tanh layers was effective, although other recurrent networks would likely be similarly effective. The node hidden states hv are initialized to zero. The recurrence should be computed as many times as the maximum number of nodes that might be produced. The recurrent function fadd can learn to output sv = 0 for some nodes to create fewer nodes, if necessary. 2.2 NODE STATE UPDATE The node state update transformation Th : Γ×Rα → Γ takes as input a graph G and an input vector a ∈ Rα, and produces a graph G\u2032 with updated node states. This is accomplished by performing a GRU-style update for each node, where the input is a concatenation of a and that node\u2019s annotation vector xv and the state is the node\u2019s hidden state, according to rv = σ (Wr[a xv] + Urhv + br) , zv = σ (Wz[a xv] + Uzhv + bz) , h̃\u2032v = tanh (W[a xv] + U (r� hv) + b) , h\u2032v = zv � h\u2032v + (1− zv)� h̃\u2032v 2.2.1 DIRECT REFERENCE UPDATE For some tasks, performance can be improved by providing information to nodes of a particular type only. For instance, if the input is a sentence, and one word of that sentence directly refers to a node type (e.g., if nodes of type 1 represent Mary, and Mary appears in the sentence), it can be helpful to allow all nodes of type 1 to perform an update using this information. To accomplish this, Th can be modified to take node types into account. (This modification is denoted Th,direct.) Instead of a single vector a ∈ Rα, the direct-reference transformation takes in A ∈ RN×α, where An ∈ Rα is the input vector for nodes with type n. The update equations then become av = xvA rv = σ (Wr[av xv] + Urhv + br) , zv = σ (Wz[av xv] + Uzhv + bz) , h̃\u2032v = tanh (W[av xv] + U (r� hv) + b) , h\u2032v = zv � h\u2032v + (1− zv)� h̃\u2032v 3 Under review as a conference paper at ICLR 2017 2.3 EDGE UPDATE The edge update transformation TC : Γ×Rα → Γ takes a graph G and an input vector a ∈ Rα, and produces a graph G\u2032 with updated edges. For each pair of nodes (v, v\u2032), the update equations are cv,v\u2032 = fset(a,xv,hv,xv\u2032 ,hv\u2032) rv,v\u2032 = freset(a,xv,hv,xv\u2032 ,hv\u2032) C\u2032v,v\u2032 = (1− Cv,v\u2032)� cv,v\u2032 + Cv,v\u2032 � (1− rv,v\u2032). The functions fset, freset : Rα×2N×2D → [0, 1]Y are implemented as neural networks. (In my experiments, I used a simple 2-layer fully connected network.) cv,v\u2032,y gives the level of belief in [0, 1] that an edge from v to v\u2032 of type y should be created if it does not exist, and rv,v\u2032,y gives the level of belief in [0, 1] that an edge from v to v\u2032 of type y should be removed if it does. Setting both to zero results in no change for that edge, and setting both to 1 toggles the edge state. 2.4 PROPAGATION The propagation transformation Tprop : Γ → Γ takes a graph G = G(0) and runs a series of T propagation steps (as in GG-NN), returning the resulting graph G\u2032 = G(T ). The GG-NN propagation step is extended to handle node and edge strengths, as well as to allow more processing to occur to the information transferred across edges. The full propagation equations for step t are a(t)v = ∑ v\u2032∈V sv\u2032 M∑ y=1 Cv,v\u2032,y � f fwdy (xv\u2032 ,h (t−1) v\u2032 ) + Cv\u2032,v,y � f bwd y (xv\u2032 ,h (t−1) v\u2032 ) (5) z(t)v = σ(Wz[a (t) v xv] + Uh (t−1) v + bz) (6) r(t)v = σ(Wr[a (t) v xv] + Urh (t−1) v + br) (7) h̃ (t) v = tanh(W[a (t) v xv] + U(r (t) v � h(t−1)v ) + bh) (8) h(t)v = (1− z(t)v )� h(t−1)v + z(t)v � h̃ (t) v . (9) Equation 5 has been adjusted in the most significant manner (relative to 2). In particular, sv\u2032 restricts propagation so that nodes with low strength send less information to adjacent nodes, sedge has been replaced with C to allow edges with fractional strength, and the propagation matrices Py,P\u2032y have been replaced with arbitrary functions f fwdy , f bwd y : RN × RD → Rα, where α is the length of the vector a. I used a fully connected layer to implement each function in my experiments. Equations 6, 7, and 8 have also been modified slightly to add a bias term. 2.5 AGGREGATION The aggregation transformation Trepr : Γ → Rα produces a graph-level representation vector from a graph. It functions very similarly to the output representation of a GG-NN, given in equation 3, but is modified slightly to take into account node strengths. As in GG-NN, both i and j are neural networks, and in practice a single fully conected layer appears to be adequate for both. hG = tanh (∑ v∈V svσ(i(h (T ) v ,xv))� tanh(j(h(T )v ,xv)) ) . 3 GATED GRAPH TRANSFORMER NEURAL NETWORK (GGT-NN) Combining a series of these transformations yields a Gated Graph Transformer Neural Network (GGT-NN). Depending on the configuration of the transformations, a GGT-NN can take textual or graph-structured input, and produce textual or graph-structured output. Here I describe one partic- ular GGT-NN configuration, designed to build and modify a graph based on a sequence of input sentences, and then produce an answer to a query. For each sentence k, each word is converted to a one-hot vector w(k)l , and the sequence of words (of length L) is passed through a GRU layer to produce a sequence of partial-sentence represen- tation vectors p(k)l . The full sentence representation vector i (k) is initialized to the last partial 4 Under review as a conference paper at ICLR 2017 Algorithm 1 Graph Transformation Pseudocode 1: G ← ∅ 2: for k from 1 to K do 3: G ← Th(G, i(k)) 4: if direct reference enabled then 5: G ← Th,direct(G,D(k)) 6: end if 7: if intermediate propagation enabled then 8: G ← Tprop(G) 9: end if 10: haddG ← Trepr(G) 11: G ← Tadd(G, [i(k) haddG ]) 12: G ← TC(G, i(k)) 13: end for 14: G ← T queryh (G, i query) 15: if direct reference enabled then 16: G ← T queryh,direct(G,D query) 17: end if 18: G ← T queryprop (G) 19: hanswerG ← T query repr (G) 20: return foutput(hanswerG ) representation vector p(k)L . Furthermore, a direct-reference input matrix D (k) is set to the sum of partial representation vectors corresponding to the words that directly reference a node type, i.e. D (k) n = ∑ l∈Rn p (k) l where Rn is the set of words in sentence k that directly refer to node type n. Next, a series of graph transformations are applied, as depicted in Algorithm 1. Depending on the task, direct reference updates and per-sentence propagation can be enabled or disabled. The output function foutput will depend on the specific type of answer desired. If the answer is a single word, foutput can be a multilayer perceptron followed by a softmax operation. If the answer is a sequence of words, foutput can use a recurrent network (such as a GRU) to produce a sequence of outputs. Note that transformations with different superscripts (Th and T queryh , for instance) refer to similar transformations with different learned weights. 3.1 SUPERVISION As with many supervised models, one can evaluate the loss based on the likelihood of producing an incorrect answer, and then minimize the loss by backpropagation. However, based on initial experiments, the model appeared to require additional supervision to extract meaningful graph- structured data. To provide this additional supervision, I found it beneficial to provide the correct graph at each timestep and train the network to produce that graph. This occurs in two stages, first when new nodes are proposed, and then when edges are adjusted. For the edge adjustment, the edge loss between a correct edge matrix C∗ and the computed edge matrix C is given by Ledge = ∑ C∗ � ln(C) + (1− C∗)� ln(1− C). The node adjustment is slightly more complex. Multiple nodes are added in each timestep, and those nodes are added in some order, but the order of the nodes is arbitrary. The order in which the nodes are created does not matter, only the existence of the nodes is important. Thus it should be possible for the network to determine the optimal ordering of the nodes. In fact, this is important because there is no guarantee that the nodes will be ordered consistently in the training data. Vinyals et al. (2016) demonstrate a simple method for training a network to output unordered sets: the network produces a sequence of outputs, and these outputs are compared with the closest order- ing of the training data, i.e., the ordering of the training data which would produce the smallest loss when compared with the network output. Vinyals et al. show that when using this method, the net- work arbitrarily chooses an ordering which may not be the optimal ordering for the task. However, in this case any ordering should be sufficient, and I found the arbitrary orderings selected in this way to work well in practice. In particular, letting s∗π(v) and x ∗ π(v) denote the correct strength and annotations of node v under ordering π, the loss becomes Lnode = max π |Vnew|∑ v=|Vold|+1 s∗π(v) ln(sv) + (1− s ∗ π(v)) ln(1− sv) + x ∗ π(v) ln(xv). At this point the correct values C∗, s∗π(v) and x ∗ π(v) are substituted into the graph for further process- ing. Note that only the edges and the new nodes are replaced by the supervision. The hidden states of all existing nodes are propagated without adjustment. 5 Under review as a conference paper at ICLR 2017 Direct reference No direct reference Task Accuracy No. ex. req. ≥ 95% Accuracy No. ex. req. ≥ 95% 1 - Single Supporting Fact 100% 100 99.3% 1000 2 - Two Supporting Facts 100% 250 94.3% FAIL 3 - Three Supporting Facts 98.7% 1000 88.0% FAIL 4 - Two Arg. Relations 98.8% 1000 97.8% 1000 5 - Three Arg. Relations 87.2% FAIL 80.2% FAIL 6 - Yes\/No Questions 100% 100 92.3% FAIL 7 - Counting 100% 250 94.4% FAIL 8 - Lists\/Sets 100% 250 96.7% 1000 9 - Simple Negation 100% 250 88.4% FAIL 10 - Indefinite Knowledge 96.6% 1000 71.4% FAIL 11 - Basic Coreference 100% 100 99.8% 1000 12 - Conjunction 99.9% 500 99.3% 1000 13 - Compound Coref. 100% 100 99.2% 1000 14 - Time Reasoning 97.8% 1000 44.9% 1000 15 - Basic Deduction 99.1% 500 100% 500 16 - Basic Induction 100% 100 100% 500 17 - Positional Reasoning 88.9% FAIL 51.3% FAIL 18 - Size Reasoning 97.9% 1000 89.4% FAIL 19 - Path Finding 100% 500 29.4% FAIL 20 - Agent\u2019s Motivations 100% 250 99.0% 250 Table 1: Performance of GGT-NN on the bAbI tasks. \u201CNo. ex. req. ≥ 95%\u201D refers to the number of training examples required before the network was able to reach 95% accuracy or better on the task. 4 EXPERIMENTS 4.1 BABI TASKS I evaluated the GGT-NN model on the bAbI tasks, a set of simple natural-language tasks, where each task is structured as a sequence of sentences followed by a query (Weston et al., 2016). The gener- ation procedure for the bAbI tasks includes a \u201CKnowledge\u201D object after each sentence, representing the current state of knowledge after that sentence. I exposed this knowledge object in graph format, and used this to train a GGT-NN in supervised mode. The knowledge object provides names for each node type, and direct reference was performed based on these names: if a word in the sentence matched a node type name, it was parsed as a direct reference to all nodes of that type. For details on this graphical format, see Appendix A. 4.1.1 RESULTS I trained two versions of the GGT-NN model for each task: one with and one without direct refer- ence. Tasks 3 and 5, which involve a complex temporal component, were trained with intermediate propagation, whereas all of the other tasks were not because the structure of the tasks made such complexity unnecessary. Most task models were configured to output a single word, but task 19 was configured using a GRU to output multiple words, and task 8 (the listing task) was configured to output a strength for each possible word to allow multiple words to be selected without having to consider ordering. Results are shown in Table 1. The GGT-NN model with direct reference performed very well on the majority of the tasks, reaching accuracies of at least 95% in all but two tasks, and reaching 100% accuracy in the majority of the tasks. Additionally, for many of the tasks, the model was able to reach 95% accuracy using 500 or fewer of the 1000 training examples. The two exceptions were task 5 (Three Arg. Relations) and task 17 (Positional Reasoning), for which the model was not able to attain a high accuracy. Task 5 involves sophisticated temporal reasoning, and thus requires a complex graphical structure to model accurately. Task 17 has a larger number of possible entities than the other tasks: each entity consists of a color (chosen from five options) and a shape (chosen from four shapes), for a total of 20 unique entities that must be represented separately. It is likely that these additional complexities caused the network performance to suffer. 6 Under review as a conference paper at ICLR 2017 Of particular interest is the performance of the GGT-NN model with direct reference on task 19, the pathfinding task. Previous models, such as the end-to-end memory networks described by Sukhbaatar et al. (2015), have struggled to learn this task. On the other hand, GGS-NN models were able to successfully learn the pathfinding task, but required the input to be preprocessed into graphical form even during testing (Li et al., 2016). The current results demonstrate that the pro- posed GGT-NN model is able to solve the pathfinding task when given textual input. In general, the GGT-NN model with direct reference performs better than the model without it (see Table 1). Although the model without direct reference reaches 95% accuracy on more than half of the tasks, it fails to reach 95% accuracy on multiple other tasks. Additionally, when compared to the direct-reference model, it requires more training examples in order to reach the accuracy threshold. This indicates that, although the model can be used without direct reference, adding direct reference greatly improves the training of the model. 4.2 RULE DISCOVERY To demonstrate the power of GGT-NN to model a wide variety of graph-based problems, I applied the GGT-NN to two additional tasks. In each task, a sequence of data structures were transformed into a graphical format, and the GGT-NN was tasked with predicting the data for the next timestep based on the current timestep. No additional information was provided as textual input; instead, the network was tasked with learning the rules governing the evolution of the graph structure over time. 4.2.1 CELLULAR AUTOMATON TASK The first task used was a 1-dimensional cellular automaton, specifically the binary cellular automa- ton known as Rule 30 (Wolfram, 2002). Rule 30 acts on an infinite set of cells, each with a binary state (either 0 or 1). At each timestep, each cell deterministically changes state based on its previous state and the states of its neighbors. In particular, the update rules are Current neighborhood 111 110 101 100 011 010 001 000 Next value 0 0 0 1 1 1 1 0 Cell states can be converted into graphical format by treating the cells as a linked list. Each of the cells is represented by a node with edges connecting it to the cell\u2019s neighbors, and a value edge is used to indicate whether the cell is 0 or 1. This format is described in more detail in Appendix A. 4.2.2 TURING MACHINES The second task was simulating an arbitrary 2-symbol 4-state Turing machine. A Turing machine operates on an infinite tape of cells, each containing a symbol from a finite set of possible symbols. It has a head, which points at a particular cell and can read and write the symbol at that cell. It also has an internal state, from a finite set of states. At each timestep, based on the current state and the contents of the cell at the head, the machine writes a new symbol, changes the internal state, and can move the head left or right or leave it in place. The action of the machine depends on a finite set of rules, which specify the actions to take for each state-symbol combination. Note that the version of Turing machine used here has only 2 symbols, and requires that the initial contents of the tape be all 0 (the first symbol) except for finitely many 1s (the second symbol). When converting a Turing machine to graphical format, the tape of the machine is modeled as a linked list of cells. Additionally, each state of the machine is denoted by a state node, and edges between these nodes encode the transition rules. There is also a head node, which connects both to the current cell and to the current state of the machine. See Appendix A for more details. 4.2.3 ANALYSIS The GGT-NN model was trained on 1000 examples of the Rule 30 automaton with different ini- tial states, each of which simulated 7 timesteps of the automaton, and 20,000 examples of Turing machines with different rules and initial tape contents, each of which simulated 6 timesteps of the Turing machine. Performance was then evaluated on 1000 new examples generated with the same format. The models were evaluated by picking the most likely graph generated by the model, and 7 Under review as a conference paper at ICLR 2017 Original Task Generalization: 20 Generalization: 30 Automaton 100.0% 87.0% 69.5% Turing 99.9% 90.4% 80.4% Table 2: Accuracy of GGT-NN on the Rule 30 Automaton and Turing Machine tasks. 1000 iterations 2000 iterations 3000 iterations 7000 iterations Ground truth Figure 1: Visualization of network performance on the Rule 30 Automaton task. Top node (purple) represents zero, bottom node (blue) represents 1, and middle nodes (green, orange, and red) repre- sent individual cells. Blue edges indicate adjacent cells, and gold edges indicate the value of each cell. Three timesteps occur between each row. comparing it with the correct graph. The percent accuracy denotes the fraction of the examples for which these two graphs were identical at all timesteps. In addition to evaluating the performance on identical tasks, the generalization ability of the models was also assessed. The same trained models were evaluated on versions of the task with 20 and 30 timesteps of simulation. Results are shown in Table 2. The models successfully learned the assigned tasks, reaching high levels of accuracy for both tasks. Additionally, the models show the ability to generalize to large inputs, giving a perfect output in the majority of extended tasks. For visualization purposes, Figure 1 shows the model at various stages of training when evaluated starting with a single 1 cell. 5 CONCLUSION The results presented here show that GGT-NNs are able to successfully model a wide variety of tasks using graph-structured states and potentially could be useful in solving many other types of problems. The specific GGT-NN model described here can be used as-is for tasks consisting of a sequence of input sentences and graphs, optionally followed by a query. In addition, due to the modular nature of GGT-NNs, it is possible to reconfigure the order of the transformations to produce a model suitable for a different task. As one example, Appendix B describes a version of the model that uses the full sequence of sentence graphs while computing the answer to the query, instead of basing the answer on the final graph only. One downside of the current model is that the time and space required to train the model increase very quickly as the complexity of the task increases, which limits the model\u2019s applicability. It would be very advantageous to develop optimizations that would allow the model to train faster and with smaller space requirements. There are exciting potential uses for the GGT-NN model. One particularly interesting application would be using GGT-NNs to extract graph-structured information from unstructured textual de- scriptions. More generally, the graph transformations provided here may allow machine learning to interoperate more flexibly with other data sources and processes with structured inputs and outputs. ACKNOWLEDGMENTS I would like to thank Harvey Mudd College for computing resources. I would also like to thank the developers of the Theano library, which I used to run my experiments. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1053575. 8 Under review as a conference paper at ICLR 2017 REFERENCES Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol- ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, pp. 2224\u20132232, 2015. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729\u2013734. IEEE, 2005. Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs. In ICML, volume 3, pp. 321\u2013328, 2003. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. ICLR, 2016. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre- sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701\u2013710. ACM, 2014. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009. Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg- wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539\u2013 2561, 2011. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440\u20132448, 2015. Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs\/1605.02688, May 2016. URL http:\/\/arxiv.org\/abs\/ 1605.02688. John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D Peterson, et al. XSEDE: accelerating scientific discovery. Computing in Science & Engineering, 16(5):62\u201374, 2014. Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. ICLR, 2016. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. ICLR, 2016. Stephen Wolfram. A new kind of science, volume 5. Wolfram media Champaign, 2002. 9 http:\/\/arxiv.org\/abs\/1605.02688 http:\/\/arxiv.org\/abs\/1605.02688 Under review as a conference paper at ICLR 2017 1. John grabbed the milk. 2. John travelled to the bedroom. 3. Sandra took the football. 4. John went to the garden. 5. John let go of the milk. 6. Sandra let go of the football. 7. John got the football. 8. John grabbed the milk. Where is the milk? actor_is_in_location Milk John Bedroom Football Sandra Garden gettable_is_in_location gettable_is_in_actor Figure 2: Diagram of one sample story from the bAbI dataset (Task 2), along with a graphical representation of the knowledge state after the italicized sentence. APPENDIX A GRAPH FORMAT DETAILS A.1 BABI TASKS The knowledge graph object used during generation of the bAbI tasks is structured as a dictionary relating entities to each other with specific relationship types. Entities are identified based on their names, and include people (John, Mary, Sandra), locations (bedroom, kitchen, garden), objects (football, apple, suitcase), animals (mouse, wolf, cat), and colors (white, yellow, green), depending on the particular task. Relationships between entities are also expressed as strings, and are directed: if John is holding the milk there is an \u201Cis in\u201D relationship from \u201Cmilk\u201D to \u201CJohn\u201D; if Sandra is in the bedroom there is an \u201Cis in\u201D relationship from \u201CSandra\u201D to \u201Cbedroom\u201D; if Lily is green there is a \u201Chas color\u201D relationship from \u201CLily\u201D to \u201Cgreen\u201D, etc. The transformation from the knowledge object to a graph is straighforward: each entity used is assigned to a new node type, and relationships between entities are represented as edges between the correpsonding nodes. To avoid confusion from overloaded relationships (such as \u201Cis in\u201D being used to represent an object being held by a person as well as a person being in a room), relation names are given a distinct edge type depending on the usage context. For instance, when a person is carrying an object, the generic \u201Cis in\u201D relationship becomes an edge of type \u201Cgettable is in actor\u201D. Some of the graph representations had to be modified in order to ensure that they contained all of the necessary information. For instance, task 3 requires the network to remember where items were in the past, but the knowledge object only contained references to their current locations. In these cases, a linked list structure was added to the knowledge object to allow the history information to be represented in the graph. In particular, each time an item changed locations, a new \u201Crecord\u201D node was added, with a \u201Cprevious\u201D edge to the previous history node and a \u201Cvalue\u201D edge to the current location of the item. Each item then connected to the most recent history node using a \u201Chistory-head\u201D edge. This ensures that the history of each node is present in the graph. In a few of the tasks, specific entities had multi-word representations. While this works for normal input, it makes it difficult to do direct reference, since direct reference is checked on an individual word level. These tasks were modified slightly so that the entities are referred to with single words (e.g. \u201Cred square\u201D instead of \u201Cred square\u201D). An example of a graph produced from the bAbI tasks is given in Figure 2. A.2 CELLULAR AUTOMATON The cellular automaton task was mapped to graphical format as follows: Nodes have 5 types: zero, one, init-cell, left-cell, and right-cell. Edges have 2 types: value, and next-r. There is always exactly one \u201Czero\u201D node and one \u201Cone\u201D node, and all of the cell nodes form a linked list, with a \u201Cvalue\u201D edge 10 Under review as a conference paper at ICLR 2017 1. init 1 2. init 1 3. init 1 4. init 1 5. init 1 6. init 0 7. init 0 8. init 0 9. init 1 10. init 1 11. init 1 12. init 1 13. init 0 14. simulate 15. simulate 16. simulate 17. simulate 18. simulate 19. simulate 20. simulate Zero One New cells (left) New cells (right) Initial cells Value edges Neighbor edges Figure 3: Diagram of one example from the automaton task, along with a graphical representation of the automaton state after the fourth simulate command (italicized). 1. rule state 3 0 0 state 0 L 2. rule state 1 0 1 state 0 R 3. rule state 2 1 1 state 2 L 4. rule state 3 1 0 state 3 L 5. rule state 0 1 0 state 0 R 6. rule state 0 0 1 state 2 N 7. rule state 2 0 0 state 2 R 8. rule state 1 1 1 state 0 N 9. start state 1 10. input symbol 0 head 11. input symbol 0 12. input symbol 0 13. input symbol 1 14. run 15. run 16. run 17. run 18. run 19. run Current state States and rules Head Cells Zero One Current cell Figure 4: Diagram of an example from the Turing machine task, with a graphical representation of the machine state after the second run command (italicized). connecting to either zero or one, and a \u201Cnext-r\u201D edge pointing to the next cell to the right (or no edge for the rightmost cell). At the start of each training example, there are 13 timesteps with input of the form \u201Cinit X\u201D where X is 0 or 1. These timesteps indicate the first 13 initial cells. Afterward, there are 7 \u201Csimulate\u201D inputs. At each of these timesteps, one new left-cell node is added on the left, one new right-cell node is added on the right, and then all cells update their value according to the Rule 30 update rules. An example of the graphical format for the cellular automaton task is given in Figure 3. A.3 TURING MACHINE For the Turing machine task, nodes were assigned to 8 types: state-A, state-B, state-C, state-D, head, cell, 0, and 1. Edges have 16 types: head-cell, next-left, head-state, value, and 12 types of the form rule-R-W-D, where R is the symbol read (0 or 1), W is the symbol written (0 or 1), and D is the direction to move afterward (Left, Right, or None). State nodes are connected with rule edges, which together specify the rules governing the Turing machine. Cell nodes are connected to adjacent cells with next-left edges, and to the symbol on the tape with value edges. Finally, the head node is connected to the current state with a head-state edge, and to the current cell of the head with a head-cell edge. At the start of each training example, each of the rules for the Turing machine are given, in the form \u201Crule state-X R W state-Y D\u201D. Next, the initial state is given in the format \u201Cstart state-X\u201D, and the initial contents of the tape (of length 4) are given sequentially in the format \u201Cinput symbol-X\u201D, with the position for the head to start marked by \u201Cinput symbol-X head\u201D. Finally, there are 6 \u201Crun\u201D inputs, after each of which the head node updates its edges and the cell at the head updates its value according to the rules of the Turing machine. If the head leaves the left or right of the tape, a new node is introduced there. An example of the graphical format for the Turing machine task is given in Figure 4. 11 Under review as a conference paper at ICLR 2017 Algorithm 2 Sequence-Extended Pseudocode G0 ← ∅ . Initialize G to an empty graph for k from 1 to K do . Process each sentence Gk ← Th(Gk−1, i(k)) if direct reference enabled then Gk ← T directh (Gk,D(k)) end if if intermediate propagation enabled then Gk ← Tprop(Gk) end if haddGk ← Trepr(Gk) Gk ← Tadd(Gk, [i(k) haddGk ]) Gk ← TC(Gk, i(k)) end for hanswersummary ← 0 . Initialize hanswersummary to the zero vector for k from 1 to K do . Process the query for each graph Gk ← T queryh (Gk, i query) if direct reference enabled then Gk ← T query,directh (Gk,D query) end if Gk ← T queryprop (Gk) hanswerGk ← T query repr (Gk) hanswersummary ← fsummarize(hanswerGk ,h answer summary) end for return foutput(hanswersummary) APPENDIX B GRAPH SEQUENCE INPUT The model described in Section 3 conditions the output of the model on the final graph produced by the network. This is ideal when the graph represents all of the necessary knowledge for solving the task. However, it may also be desirable for each graph to represent a subset of knowledge corre- sponding to a particular time, and for the output to be based on the sequence of graphs produced. For instance, in the third bAbI task (which requires reasoning about the temporal sequence of events) each graph could represent the state of the word at that particular time, instead of representing the full sequence of events prior to that time. In Appendix A, section A.1, I describe a transformation to the tasks which allows all information to be contained in the graph. But this adds complexity to the graphical structure. If it were possible for the model to take into account the full sequence of graphs, instead of just the final one, we could maintain the simplicity of the graph transformation. To this end, I present an extension of the GGT-NN model that can produce output using the full graphical sequence. In the extended model, the graphical output of the network after each input sentence is saved for later use. Then, when processing the query, the same set of query transfor- mations are applied to every intermediate graph, producing a sequence of representation vectors hanswer1 , . . . ,h answer K . These are then combined into a final summary representation vector h answer summary using a recurrent network such as a GRU layer, from which the output can be produced. The modi- fied pseudocode for this is shown in Algorithm 2. I evaluated the extended model on bAbI tasks 3 and 5, the two tasks which asked questions about a sequence of events. (Note that although Task 14 also involves a sequence of events, it uses a set of discrete named time periods and so is not applicable to this modification.) The model was trained on each of these tasks, without the extra record and history nodes used to store the sequence, instead simply using the sequence of graphs to encode the relevant information. Due to the simpler graphs produced, intermediate propagation was also disabled. 12 Under review as a conference paper at ICLR 2017 Direct reference No direct reference Task Accuracy Accuracy 3 - Three Supporting Facts 90.3% 65.4% 5 - Three Arg. Relations 89.8% 74.2% Table 3: Performance of the sequence-extended GGT-NN on the two bAbI tasks with a temporal component. Results from training the model are shown in Table 3. The accuracy of the extended model appears to be slightly inferior to the original model in general, although the extended direct-reference model of task 5 performs slightly better than its original counterpart. One possible explanation for the inferiority of the extended model is that the increased amount of query processing made the model more likely to overfit on the training data. Even so, the extended model shows promise, and could be advantageous for modeling complex tasks for which preprocessing the graph would be impractical. 13 Introduction GRU GG-NN and GGS-NN Differentiable Graph Transformations Node Addition Node State Update Direct Reference Update Edge Update Propagation Aggregation Gated Graph Transformer Neural Network (GGT-NN) Supervision Experiments bAbI Tasks Results Rule Discovery Cellular Automaton Task Turing Machines Analysis Conclusion Graph Format Details bAbI Tasks Cellular Automaton Turing Machine Graph Sequence Input ","flair":"three\tResearch"}
{"author":"hardmaru","created":"Thu Nov 03 23:15:56 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.01144 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Categorical Reparameterization with Gumbel-Softmax Authors: Eric Jang, Shixiang Gu, Ben Poole (Submitted on 3 Nov 2016 (v1), last revised 22 Nov 2016 (this version, v2)) Abstract: Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1611.01144 [stat.ML]   (or arXiv:1611.01144v2 [stat.ML] for this version) Submission history From: Eric Jang [view email] [v1] Thu, 3 Nov 2016 19:48:08 GMT (996kb,D) [v2] Tue, 22 Nov 2016 23:18:13 GMT (856kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"spruceabtuse","created":"Wed Nov 16 19:32:00 EST 2016","text":" Tech #\u200B{{::display_hashtag}} Tech #\u200B{{::display_hashtag}} {{article.article.pretty_date}} {{article.metrics.page_views}} views Edit Post {{circ_link.title}} Google Taps AMD For Accelerating Machine Learning In The Cloud {{article.article.images.featured.caption}} Most Read {{contrib_block.display_advoice_brand}} {{contrib_block.display_voice_type}} {{contrib_block.display_author.name}}Voice Aaron Tilley ,   {{contrib_block.display_author.type}} Contributor {{contrib_block.display_author.type}} Contributor Opinions expressed by Forbes Contributors are their own. Full Bio Recent Posts Popular Posts Full profile →'\"> The author is a Forbes contributor. The opinions expressed are those of the writer. Loading ... Loading ... This story appears in the {{article.article.magazine.pretty_date}} issue of {{article.article.magazine.pubName}}. Subscribe AMD is finally making its first moves into deep learning. Google will start equipping its sprawling data center infrastructure with AMD\u2019s graphics processing units (or GPUs) to accelerate deep learning applications, AMD announced on Tuesday at a supercomputing conference in Salt Lake City, Utah. GPUs, typically used for generating the latest in gaming graphics, have been booming in deep learning, which is a flavor of artificial intelligence where the computer teaches itself how to do certain tasks. GPUs have caught on because of their capabilities in \u201Cparallel computing,\u201D a technique that involves multiple calculations happening simultaneously. That makes GPUs much faster at running deep learning neural nets than more generalized processors. AMD\u2019s rival, Nvidia, has been the primary chip provider here. A Google data center in Changhua, central Taiwan (Photo credit: SAM YEH\/AFP\/Getty Images) The hardware, under AMD\u2019s Radeon brand, will be the FirePro S9300 x2 GPUs. Google plans to start rolling out the AMD hardware in 2017 into its cloud products, the Google Compute Engine and the Google Cloud Machine Learning service. This is a big customer announcement for AMD, who has been struggling to bring its data center business back to life. AMD has other data center deals in the works. In October, it announced a GPU deal for Chinese e-commerce giant Alibaba\u2019s data centers. \u201CGoogle is building up its GPU-based infrastructure, and they want to ensure they offer AMD\u2019s architecture,\u201D said Raja Koduri, senior vice president and chief architect at AMD, in an interview. \u201CNobody has heard what AMD is doing in deep learning. This is a major first step for us.\u201D AMD\u2019s GPU rival, Nvidia, is way out ahead of the company in the data center and deep learning market. The company started out in this market several years ago and its early bets have started paying off. In Nvidia\u2019s third quarter earnings, published last week, it reported that the data center revenue grew 193% year-over-year to $240 million. Nvidia is already supplying loads of its chips to large tech players like Google. It\u2019s likely Google wants to second source its data center GPUs to give it more power in price negotiations with Nvidia. AMD thinks it has a good chance to compete in the market. \u201COur dual GPU server products have more floating-point compute operations than Nvidia and more memory bandwidth,\u201D said Koduri. \u201CWe can run deep neural network algorithms much faster.\u201D (The number of floating-point operations a computer can calculate per second is used as a performance metric.) \u201CThe appetite for more compute and bandwidth is huge,\u201D he said. \u201CCustomers are just lining up.\u201D To help AMD compete in the deep learning market, the company also announced an updated set of software tools for developing deep learning applications on AMD\u2019s GPUs called the Radeon Open Compute Platform. It contains software support for programming on GPU hardware and math libraries. But again, Nvidia is way ahead of AMD here. It\u2019s been providing software tools for deep learning to developers for years now. AMD has a lot of catching up to do. Gallery Stunning Photos Of Google's Massive Data Centers Launch Gallery 16 images Start Gallery Page {{article.article.page + 1}} \/ {{article.article.pages.length}} Continue {{article.article.calledOutCommentCount||''}} Comment on this story Print Report Corrections Reprints & Permissions Trending on Forbes {{channel_name}} false ","flair":"two\tNews"}
{"author":"rushter_","created":"Sun Nov 13 10:25:48 EST 2016","text":"A collection of minimal and clean implementations of machine learning algorithms. This project is targeting people who want to learn internals of ml algorithms or implement them from scratch. The code is much easier to follow than the optimized libraries and easier to play with. All algorithms are implemented in Python, using numpy, scipy and autograd. Your contributions are always welcome!","flair":"four\tProject"}
{"author":"jesuslop","created":"Mon Oct 03 22:01:10 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.00633 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.RO < prev | next > new | recent | 1610 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Robotics Title: Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates Authors: Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine (Submitted on 3 Oct 2016 (v1), last revised 23 Nov 2016 (this version, v2)) Abstract: Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations. Subjects: Robotics (cs.RO); Artificial Intelligence (cs.AI); Learning (cs.LG) Cite as: arXiv:1610.00633 [cs.RO]   (or arXiv:1610.00633v2 [cs.RO] for this version) Submission history From: Shixiang Gu [view email] [v1] Mon, 3 Oct 2016 16:52:10 GMT (2248kb,D) [v2] Wed, 23 Nov 2016 10:23:25 GMT (2268kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"Frozen_Turtle","created":"Wed Nov 16 09:05:01 EST 2016","text":"This past week, I have been working on the assignments from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition. In particular, I spent a few hours deriving a correct expression to backpropagate the batchnorm regularization (Assigment 2 - Batch Normalization) . While this post is mainly for me not to forget about what insights I have gained in solving this problem, I hope it could be useful to others that are struggling with back propagation. Batch normalization is a recent idea introduced by Ioffe et al, 2015 to ease the training of large neural networks. The idea behind it is that neural networks tend to learn better when their input features are uncorrelated with zero mean and unit variance. As each layer within a neural network see the activations of the previous layer as inputs, the same idea could be apply to each layer. Batch normalization does exactly that by normalizing the activations over the current batch in each hidden layer, generally right before the non-linearity. To be more specific, for a given input batch $x$ of size $(N,D)$ going through a hidden layer of size $H$, some weights $w$ of size $(D,H)$ and a bias $b$ of size $(H)$, the common layer structure with batch norm looks like Implementing the forward pass of the batch norm transformation is straightforward The tricky part comes with the backward pass. As the assignment proposes, there are two strategies to implement it. The 2nd step made me realize I did not fully understand backprogation before this assignment. Backpropation, an abbreviation for \u201Cbackward propagation of errors\u201D, calculates the gradient of a loss function $\\mathcal{L}$ with respect to all the parameters of the network. In our case, we need to calculate the gradient with respect to $\\gamma$, $\\beta$ and the input $h$. where each gradient with respect to a quantity contains a vector of size equal to the quantity itself. For me, the aha-moment came when I decided to properly write the expression for these gradients. For instance, the gradient with respect to the input $h$ literally reads To derive a close form expression for this expression, we first have to recall that the main idea behind backpropagation is chain rule. Indeed, thanks to the previous backward pass, i.e. into ReLu in our example, we already know We can therefore chain the gradient of the loss with respect to the input by the gradient of the loss with respect to the outputs which reads which we can also chain by the gradient with respect to the centred input $\\hat{h}_{kl}$ to break down the problem a little more The second term in the sum simply reads . All the fun part actually comes when looking at the third term in the sum. Instead of jumping right into the full derivation, let\u2019s focus on just the translation for one moment. Assuming the batch norm as just being a translation, we have where the expression of $\\mu_l$ is given above. In that case, we have if $i=j$ and $0$ otherwise. Therefore, the first term is $1$ only if $k=i$ and $l=j$ and the second term is $1\/N$ only when $l=j$. Indeed, the gradient of $\\hat{h}$ with respect to the $j$ input of the $i$ batch, which is precisely what the left hand term means, is non-zero only for terms in the $j$ dimension. I think if you get this one, you are good to backprop whatever function you encounter so make sure you understand it before going further. This is just the case of translation though. What if we consider the real batch normalization transformation ? In that case, the transformation considers both translation and rescaling and reads Therefore, the gradient of the centred input with respect to the input reads As the gradient of the standard deviation $\\sigma_l^2$ with respect to the input $h_{ij}$ reads Wrapping everything together, we finally find that the gradient of the loss function $\\mathcal{L}$ with respect to the layer inputs finally reads The gradients of the loss with respect to $\\gamma$ and $\\beta$ is much more straightforward and should not pose any problem if you understood the previous derivation. They read After the hard work derivation are done, you can simply just drop these expressions into python for the calculation. The implementation of the batch norm backward pass looks like and with that, you good to go ! In this post, I focus on deriving an analytical expression for the backward pass to implement batch-norm in a fully connected neural networks. Indeed, trying to get an expression by just looking at the centered inputs and trying to match the dimensions to get $d\\gamma$, $d\\beta$ and $dh$ simply do not work this time. In contrast, working the derivative on papers nicely leads to the solution ;) To finish, I\u2019d like to thank all the team from the CS231 Stanford class who do a fantastic work in vulgarizing the knowledge behind neural networks. For those who want to take a look to my full implementation of batch normalization for a fully-connected neural networks, you can found it here.","flair":"one\tDiscussion"}
{"author":"reworksophie","created":"Fri Oct 28 05:45:43 EDT 2016","text":"Neil Lawrence is Senior Principal Scientist at , and Professor of Machine Learning at the . His main technical research interest is machine learning through probabilistic models, with focuses on both the algorithmic side of these models as well as their application. He has a particular interest on applications in personalized health and the developing world. At the 2016 Deep Learning Summit in London , Neil presented . In this talk, Neil shares expertise on fundamental changes in the deep learning field, data efficiency, privacy, and the challenges we face moving forward. View the presentation with slides below.","flair":"null\tnull"}
{"author":"bionicscrotum","created":"Sun Oct 16 08:26:05 EDT 2016","text":"Hey everyone,\n\nI'm currently doing a Master's focusing on ML and deep learning in particular. The more I got into DL, the more I realized that neither my laptop, nor the (CPU-only) compute cluster from my university are enough to train any nontrivial models.\n\nBecause of this, I've decided to build my own (student-budget, but still as beefy as possible) rig. I plan on using it to quickly iterate on models in may areas, ranging from NLP to medical image processing (e.g. 3D CNNs over MRI data). This would be my first PC build. I plan on using it for gaming less than 5% of the time.\n\nAfter doing a fair bit of research, I learned that the best card I could get at the moment would (obviously) be the Pascal Titan X, but that the 1080 has a much better price-quality ratio. This ratio is even better for the 1070, but I believe that the increased memory bandwidth of the 1080 makes it worth my money.\n\nI aimed for a beefy but not over-the-top CPU, and a fast but small SSD to keep my OS and 1-2 small datasets on which I would be working. I don't plan on working with huge datasets (&gt;100-200Gb ) in the foreseeable future. I got 32 gigs of ram in a 2x16 kit, so that I can upgrade to 64 if necessary (this is likely not a bottleneck given the 8Gb video memory).\n\nI would be *very* grateful for some feedback\/hints on my current specs. For example:\n\n  1. Is the CPU too much? Would a plain-old i5 also work?\n  2. Should I aim for a different GPU manufacturer (e.g. MSI), or is Zotac fine?\n  3. Is there anything (apart from monitor, keyboard, and mouse) that it missing?\n  4. Any blatant incompatibilities\/weird stuff?\n\nThank you very much in advance! Here's the build:\n\n[PCPartPicker part list](http:\/\/pcpartpicker.com\/list\/gRv7wV) \/ [Price breakdown by merchant](http:\/\/pcpartpicker.com\/list\/gRv7wV\/by_merchant\/)\n\nType|Item|Price\n:----|:----|:----\n**CPU** | [Intel Core i7-6700 3.4GHz Quad-Core Processor](http:\/\/pcpartpicker.com\/product\/7V7CmG\/intel-cpu-bx80662i76700) | $294.68 @ OutletPC \n**CPU Cooler** | [be quiet! SHADOW ROCK LP 51.4 CFM CPU Cooler](http:\/\/pcpartpicker.com\/product\/9FGj4D\/be-quiet-cpu-cooler-bk002) | $29.90 @ Newegg \n**Motherboard** | [Asus Z170-E ATX LGA1151 Motherboard](http:\/\/pcpartpicker.com\/product\/k4dFf7\/asus-motherboard-z170e) | $130.95 @ Amazon \n**Memory** | [Corsair Vengeance LPX 32GB (2 x 16GB) DDR4-2666 Memory](http:\/\/pcpartpicker.com\/product\/nDx9TW\/corsair-memory-cmk32gx4m2a2666c16r) | $159.99 @ Newegg \n**Storage** | [Samsung SM951 256GB M.2-2280 Solid State Drive](http:\/\/pcpartpicker.com\/product\/Tr7CmG\/samsung-internal-hard-drive-mzhpv256hdgl00000) | $160.00 \n**Storage** | [Western Digital Caviar Black 1TB 3.5\" 7200RPM Internal Hard Drive](http:\/\/pcpartpicker.com\/product\/gV8Zxr\/western-digital-internal-hard-drive-wd1002faex) | $74.00 @ Amazon \n**Video Card** | [Zotac GeForce GTX 1080 8GB AMP! Edition Video Card](http:\/\/pcpartpicker.com\/product\/KC648d\/zotac-geforce-gtx-1080-8gb-amp-edition-video-card-zt-p10800c-10p) | $604.00 @ B&amp;H \n**Case** | [Fractal Design Define R5 w\/Window (Black) ATX Mid Tower Case](http:\/\/pcpartpicker.com\/product\/9XL7YJ\/fractal-design-case-fdcadefr5bkw) | $119.99 @ NCIX US \n**Power Supply** | [Corsair RMx 650W 80+ Gold Certified Fully-Modular ATX Power Supply](http:\/\/pcpartpicker.com\/product\/Rp8H99\/corsair-power-supply-cp9020091na) | $102.50 @ Amazon \n | *Prices include shipping, taxes, rebates, and discounts* |\n | **Total** | **$1676.01**\n | Generated by [PCPartPicker](http:\/\/pcpartpicker.com) 2016-10-16 08:08 EDT-0400 |","flair":"one\tDiscussion"}
{"author":"improbabble","created":"Wed Oct 05 14:46:41 EDT 2016","text":"MRPT is a library for approximate nearest neighbor search written in C++11. According to our experiments MRPT is currently the fastest alternative to reach high recall levels in common benchmark data sets. In the offline phase of the algorithm MRPT indexes the data with a collection of random projection trees. In the online phase the index structure allows us to answer queries in superior time. More technical details can be found in the pre-print version of our paper.","flair":"null\tnull"}
{"author":"JacobResch","created":"Sat Oct 22 18:03:26 EDT 2016","text":"Today, we shall marvel at an excellent work on image editing that is using generative adversarial networks.The paper \"Generative Visual Manipulation on the Natural Image Manifold\" is available here:https:\/\/people.eecs.berkeley.edu\/~jun...The source code is available here: https:\/\/github.com\/junyanz\/iGAN_____________________Siraj's YouTube channel is available here:https:\/\/www.youtube.com\/channel\/UCWN3...WE WOULD LIKE TO THANK OUR GENEROUS PATREON SUPPORTERS WHO MAKE TWO MINUTE PAPERS POSSIBLE:Sunil Kim, Julian Josephs, Daniel John Benton, Dave Rushton-Smith, Benjamin Kang.https:\/\/www.patreon.com\/TwoMinutePapersSubscribe if you would like to see more of these! - http:\/\/www.youtube.com\/subscription_c...Image credits:Thumbnail background image - https:\/\/pixabay.com\/photo-1081808\/Muscle photoshop disaster - http:\/\/buzzive.com\/photoshop-muscle-f...Music: Dat Groove by Audionautix is licensed under a Creative Commons Attribution license (https:\/\/creativecommons.org\/licenses\/...)Artist: http:\/\/audionautix.com\/Splash screen\/thumbnail design: Felícia Fehér - http:\/\/felicia.huKároly Zsolnai-Fehér's links:Facebook → https:\/\/www.facebook.com\/TwoMinutePap...Twitter → https:\/\/twitter.com\/karoly_zsolnaiWeb → https:\/\/cg.tuwien.ac.at\/~zsolnai\/","flair":"three\tResearch"}
{"author":"tvetus","created":"Wed Nov 09 12:31:43 EST 2016","text":"I'm having a hard time finding good information on this. How quickly and how accurately can information be extracted? To quantify it from an information theoretic perspective, how many bits per minute (or words per minute) would should I expect for a task like Morse code. Is it good enough to control a wheel chair in real time, safely? Response time?","flair":"one\tDiscussion"}
{"author":"drsxr","created":"Wed Nov 23 13:05:21 EST 2016","text":" Seeing it all: Convolutional network layers map the function of the human visual system Michael Eickenberg, Alexandre Gramfort, Gaël Varoquaux, Bertrand Thirion To cite this version: Michael Eickenberg, Alexandre Gramfort, Gaël Varoquaux, Bertrand Thirion. Seeing it all: Convolutional network layers map the function of the human visual system. NeuroImage, Elsevier, 2016, <10.1016\/j.neuroimage.2016.10.001>. <hal-01389809> HAL Id: hal-01389809 https:\/\/hal.inria.fr\/hal-01389809 Submitted on 29 Oct 2016 HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci- entific research documents, whether they are pub- lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d\u2019enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. https:\/\/hal.archives-ouvertes.fr https:\/\/hal.inria.fr\/hal-01389809 1 Seeing it all: Convolutional network layers map the function of the human visual system Michael Eickenberg1,3∗, Alexandre Gramfort2,3, Gaël Varoquaux1,3, Bertrand Thirion1,3 1 Inria Parietal Team, Inria Saclay, France 2 CNRS LTCI, Télécom ParisTech, Université Paris-Saclay 3 Neurospin, I2BM, DSV, CEA Saclay ∗ E-mail: michael.eickenberg@inria.fr Abstract Convolutional networks used for computer vision represent candidate models for the computations performed in mammalian visual systems. We use them as a detailed model of human brain activity during the viewing of natural images by constructing predictive models based on their different layers and BOLD fMRI activations. Analyzing the predictive performance across layers yields characteristic fingerprints for each visual brain region: early visual areas are better described by lower level convolutional net layers and later visual areas by higher level net layers, exhibiting a progression across ventral and dorsal streams. Our predictive model generalizes beyond brain responses to natural images. We illustrate this on two experiments, namely retinotopy and face-place oppositions, by synthesizing brain activity and performing classical brain mapping upon it. The synthesis recovers the activations observed in the corresponding fMRI studies, showing that this deep encoding model captures representations of brain function that are universal across experimental paradigms. 2 1 Introduction Human and primate visual systems are highly performant in recognizing objects and scenes, providing the basis of an excellent understanding of the ambient 3D world. The visual cortex is hierarchically organized, which means that many functional modules have feedforward and feedback connections compatible with a global ordering from lower levels to higher levels [1]. The concept of visual \u201Cpathways\u201D or \u201Cstreams\u201D [2, 3] is an established pattern which identifies principal directions of information flow for specific tasks, namely object representation in the \u201Cventral stream\u201D (from occipital cortex into temporal cortex) and localization and spatial computations in the \u201Cdorsal stream\u201D (from occipital cortex into parietal cortex). They share much processing in the occipital early visual areas and less oustide of them. The ventral visual stream encompasses visual areas V1, V2, V3, V4 and several inferotemporal (IT) regions. Pure feedforward pathways from V1 to IT (via other areas) exist, and probably account for rapid object recognition [4, 5]. Many parts of the human and primate visual cortices exhibit retinotopic organization in so-called visual field maps: The image presented to the retina is kept topographically intact in the next processing steps on the cortical surface [6]. This results in a one-to-one correspondence between a point on the retina and the \u201Ccenters of processing\u201D for that point in the visual field maps, such that neighboring points on the retina are processed nearby in the visual field maps as well. The seminal work of [7] showed that cat and other mammal V1 neurons selectively respond to edges with a certain location and orientation in the visual field. This discovery inspired a long line of research investigating the nature of the computa- tions performed in other visual regions and how they are implemented. As an example, certain monkey V2 neurons were found to react to combinations of orientations, such as 3 corners [8]. Recently, it has been put forward that V2 may be an efficient encoder of expected natural image statistics arising from interactions of first-order edges [9]. V4 is reported to respond to more complex geometric shapes, color, and a large number of other stimulus characteristics. Recently it has been posited that V4 performs mid-level feature extraction towards the goal of bottom-up and top-down figure-ground segmentation [10]. Further down the ventral pathway, neurons in the IT cortex have been shown to be selective to parts of objects, objects and faces [11,12]. Taken together, these findings indicate an increasing trend in abstractness of the representations formed along the ventral stream. FMRI has been used very successfully to identify and delineate the aforementioned visual field maps as well as brain regions that seem to specialize to certain tasks in the sense that their responses are particularly strong for specific types of stimuli. This type of result has typically been derived using statistical contrast maps opposing various visual stimuli. The contributions [13\u201315], for instance, use this technique to localize specialized regions: areas for faces, body parts, places. Finer models, known as \u201Cencoding\u201D models or forward modeling techniques [16], have been used to study the brain response to stimuli in greater detail [17\u201319]. This setting usually relies on richer models, going beyond binary contrasts, towards a more open description of the link between stimulus and activation. The validity of the corresponding stimulus representation is then established by testing how well it predicts brain activity, often with a linear model, by using cross-validation on held-out data. For example, in [17], almost 2000 naturalistic images were used as stimuli and the BOLD signal responses were then fit using a predictive model based on Gabor filterbank responses of the images shown. Primary visual cortex was very well modeled, but also extrastriate areas such as visual area V4 were well explained by the Gabor filter model. In this contribution, we make use of the hierarchical organization of modern convolu- 4 tional networks for object recognition to model human brain activity. We create encoding models [16] from the processing layers of the convolutional network OverFeat [20], which each represent feature maps at different levels of complexity. We train a linear predictive model of brain activity for each of the layers on the datasets of [17] and [21] and compare their ability to describe brain activity for every voxel by evaluating the predictive score on held-out data. The scores of the different layers outline continuous progression profiles that are distinct in each visual area. We demonstrate that the model captures the cognitive architecture of the visual system by investigating its generalization capacity to visual-neuroscience paradigms beyond natural-image viewing. To do so we use stimuli unseen by our model, of which some come from totally different experiments and follow vastly different pixel statistics. Our predictive model, which can be seen as data-driven forward model to generate fMRI activations, is used to synthesize putative brain activation maps corresponding to these novel stimuli. This methodology enables our model to reproduce classical experiments in the extensive literature of paradigm-driven fMRI research. We consider two of these experiments: retinotopic mapping, i.e. the capturing of spatial information to sufficient accuracy for the generation of visual field maps, and a faces\/places contrast to capture high-level information. Previous work has used convolutional networks with fMRI data [23, 24]. However it focused on specific experiments. Showing that results generalize across datasets and paradigms brings an important novel step to the use of convolutional networks for the study of human vision. First, we show the validity of the approach on a new dataset with videos rather than still images. Second, we synthesize plausible brain activity to new images from completely different experiments that rely on hand-crafted, well controlled stimuli. These results demonstrate that convolutional networks capture universal representations 5 Figure 1: The experimental setup. Top left: 16 Examples of stimulus images (similar in content to the original stimuli presented to the subjects, and identical in masking) which are input to the convolutional network. Top middle: Selected features of first layer (top left of panel) and image patches activating these features (other eight panels). Top right: Image space gradients of selected feature maps from layer 5 (left panel) and example patches driving these feature maps. The gradients show which change in the image would lead to a stronger activation of the feature map (see [22]). Middle: Depicts convolutional net layers. Every layer is evaluated for its predictive capacity of all the voxels. For each layer, the corresponding predictive model is depicted by an arrow pointing downward from the convolutional net. It yields a score for each voxel, giving rise to a map of the brain, depicted below the arrow. Bottom: The close-up views are intended to highlight different areas that are well modeled: The first layer models best medial occipital regions close to the Calcarine fissure, the last layer explains more variance in lateral and inferior occipital regions. The middle layer shows an intermediate score map between the two extremes. 6 of the stimuli that linearly map to and separate cognitive processes, such that this link generalizes to unseen experimental paradigms. 2 Biological relevance of multi-layer vision models The Gabor filter pyramid employed in the original work of [17] can be seen as an instance of a biologically inspired computer vision model. Indeed, all of modern computer vision, in its roots, has been inspired by biological vision. The basic filter extraction techniques at the beginning of the most successful computer vision pipelines are based on local image gradients or laplacians [25, 26], which are operations that have been found in V1 as edge detection and in the LGN as center-surround features. The HMAX model was constructed to incorporate the idea of hierarchies of layers [27]. HMAX models are layered architectures that typically begin with edge detection using oriented filters, followed by a spatial and across-channel max-pooling. Subsequent layers implement other forms of localized (convolutional) processing, such as linear template matching. Using a supervised classifier at the end of this processing, it reached near state-of-the-art object recognition capacities in [28]. The natural question to ask in the context of predictive modeling of BOLD fMRI in visual areas is \u201CWhat comes after the Gabor filter pyramid?\u201D. The scattering transform model [29,30] provided only one supplementary layer of which one cannot state much more than the existence of brain voxels which it models well [31]. The scattering transform is a cascade of complex wavelets and complex moduli, which has good mathematical stability properties and yields rich representations. The layers C1 and C2 of HMAX as used in [28] were obtained using random templates taken from the preceding pooling layer activation. They were not geared optimally towards object recognition. This made the difference 7 between layers difficult to evaluate (see e.g. [32]). Although quite similar in architecture, deep artificial neural networks are of much greater interest here. Indeed, they optimize intermediate layers towards increasing overall performance in object detection, which is known to be performed also in IT cortex in humans and primates (see [33], [32]). Artificial neural networks for computer vision attain state-of-the-art results with optimized feature hierarchies in a layered architecture composed of stacked layers with units that compute a linear transformation of the activations of previous layers followed by a simple pointwise nonlinearity. For instance, the first linear transformations are typically similar to Gabor filters and the corresponding non-linearities perform edge detection. Recent breakthroughs in the field of artificial neural networks have led to a series of unprecedented improvements in a variety of tasks, all achieved with the same family of architectures. Notably in domains previously considered to be the strongholds of human superiority over machines, such as object and speech recognition, these algorithms have gained ground, and, under certain metrics, have surpassed human performance [34]. Bridging to neuroscience, [33] and [35], using electrophysiological data, have shown that IT neuron activity is predictive of object category in a similar way as the penultimate layer of a deep convolutional network which was not trained on the stimuli. Even more striking: a deep convolutional network can predict the activity of IT neurons much better than either lower-level computer vision models or object category predictors. Furthermore, deep convolutional networks trained on object categories and linked to neural activity with simple linear models predict this neural activity as well as the same network trained directly on neural data, suggesting that the encoding of object categories in the network is a good proxy for the representation of neural activity. These two works inspired us to investigate the link between computer-vision convolutional networks and brain activity with fMRI in order to obtain a global view of the system. Indeed, fMRI is much more 8 noisy and indirect than electrophysiological data, but it brings a wide coverage of the visual system. Inspection of the first layer of a convolutional net reveals that it is composed of filters strongly resembling Gabor filters, as well as color boundaries, and color blob filters (shown at the top of Fig. 1). These features are similar in nature V1 receptive fields. To understand the other end of the hierarchy, close to the output of a convolutional network, we apply the successive transformations of such a network to a natural image representing object categories. This most often yields a correct object identification (classification rates have risen from around 80% to around 96% over the 1000 object categories of imagenet in the last 3 years [36, 37]). Since this classification is a linear transformation of the penultimate layer representation space from which one can also predict IT neural activity linearly, there must be a correspondence in representation. Indeed, there exist high-level visual areas tuned to specific object categories: body parts, faces, places. We have thus established that there are similarities between the computations of convolutional networks and cognitive vision at the beginning and at the end of the ventral stream object-recognition process. Evaluating its intermediate layers with respect to how well they explain activity in visual areas of the brain is a stepping stone towards a bigger picture of the correspondence. 3 Methods 3.1 Datasets We consider two different datasets of BOLD fMRI responses to visual stimulation of very different nature: still images and videos. The still images dataset [38] originates from [17] and [18]. 1750 gray scale natural images in a circular frame of visual angle 20 degrees were 9 presented at an interstimulus interval of 4s for the duration of 1s in three flashes \u201CON-OFF- ON-OFF-ON\u201D, each \u201CON\u201D and \u201COFF\u201D phase of 0.2s duration. The content of the photos included animals, buildings, food, humans, indoor scenes, manmade objects, outdoor scenes, and textures, taken from the Corel Stock Photo Libraries from Corel Corporation, Ontario, Canada, the Berkeley Segmentation Dataset, and the authors personal collections. Each image was shown twice in total, in the same scanner run, to subjects fixating a central cross. Every eighth stimulus presentation were uniform gray empty events. 25 scanner runs divided into 5 scanner sessions we acquired 1. 120 validation images were presented during 10 scanner runs divided into the same 5 scanner sessions. Each validation image was presented 13 times overall. BOLD signal acquisition was performed using a surface coil in 18 coronal slices of matrix size 64x64, slice thickness 2.5mm, in-slice sampling 2mm x 2mm, repetition time TR=1s. Reconstruction was performed using ReconTools2. A phase correction was applied to reduce Nyquist ghosting and image distortion. Slice timing correction was done via sinc interpolation. After motion correction and manual realignment of scanner runs, the multiple responses to each image were averaged into one activation map using a GLM model with an individual hemodynamic response function per voxel, estimated using alternate optimization and a low-frequency Fourier basis for the HRF function. We work with the obtained activation maps and the stimuli. ROI boundaries were obtained by standard retinotopic mapping. Data from two healthy subjects with normal or corrected-to-normal vision are available in [38]. The video stimulus was first presented in [19] and used also in [21]. It consists of movie trailers and wildlife documentaries cut into blocks of 5-15 seconds and randomly shuffled. 1A scanner session is the full amount of time spent in the scanner from entrance to exit and usually lasts around 90 minutes. A scanner run is a continuous sequence of measurements. Several runs can take place during a session, separated by short breaks. 2https:\/\/github.com\/matthew-brett\/recon-tools 10 A train set of two hours duration and no repetition was separated from a test set in which around 10 minutes of unique stimulus were cut into blocks of around 3 minutes and repeated in random order 10 times each. Subjects fixated a central cross while passively viewing these stimuli. 30 axial slices of 4mm thickness were acquired, covering the full brain. In-slice sampling was 2mm x 2mm. The acquired data were motion corrected and manually realigned. The ten runs of the validation set were averaged to reduce noise. This dataset comprises one subject. Both datasets provide functionally localized regions of interest. Visual areas V1, V2, V3, V4, V3A, V3B and LOC were determined using phase-coded retinotopic mapping. All surface projections were computed and flatmap diagrams were created using the pycortex software [39]. ROI boundaries were outlined according to localized maps, provided as volume maps in the dataset of [17] and as outlines for the data from [21]. Volume ROIs were projected to the surface using a nearest neighbor projection and outlines drawn along the borders of the projections. 3.2 The encoding pipeline We chose the \u201Clarge\u201D version of the deep convolutional net \u201COverFeat\u201D [20] to run our analyses. It features six convolutional layers and three fully connected ones. Details can be found in [20]. Here, we are interested in convolutional networks not to classify images, but as a means to transform them into successive intermediate representations: from Gabor-like features to abstract shapes (see Fig. 1). Using the sklearn-theano3 software, the network was applied to all stimulus images and the outputs of all neural network layers kept. Since the intermediate representations are rather large (e.g. ∼ 106 features on the first layer), each channel of each layer was spatially smoothed and subsampled to achieve a number of 3http:\/\/sklearn-theano.github.io 11 features of around 25000 per layer. This was achieved by determining the smallest integer spatial subsampling necessary to obtain 25000 features or less: for instance, the first layer having 96× 113× 113 = 1225824 features, a spatial subsampling of factor 8 per axis is necessary to bring the number of features down to 1225824\/(8×8) ≈ 19154. The smoothing parameter for the Gaussian is set to 0.35× d, where d is the downsampling factor (here 8). For the video data, sampled at 15Hz at an acquisition TR of 2 s, temporal downsampling was additionally performed by calculating the temporal mean across 30 frames at a time. A compressive non-linearity, log(1 + x) was applied pointwise, similarly to the procedure described in [16]. Using only the stimuli from the training set, `2-penalized linear regression (ridge regression) was used to fit a forward model for the outputs of each layer for each brain voxel. The choice of Ridge regression is due to practical considerations such as computation speed and simplicity. Better model selection could be attempted with a penalty enforcing exact zeros, such as the `1-norm or group-structured norms grouping features located in one place. However, for the given data shape this is prohibitive in computational resources. Contrary to [33], we employ a linear kernel instead of a Gaussian one. In addition to the isotropic `2-penalty, a Gaussian kernel has a hyperparameter controling the kernel width, providing a continuous ensemble of models ranging from nearest-neighbor- to linear-projection-based predictions. [33] studied the full hyperparameter path while predicting from the last layer. Nearest-neighbor type decisions, unlike linear decisions, indicate a complicated decision boundary and thus do not reveal as simple representation of brain activity. Here we work only with linear decision boundaries in order to be able to compare the complexities of the convolutional network layers on as equal footing as possible. For the video data, temporally lagged copies of the outputs at t-4, t-6 and t-8 seconds were used in order to take into account the hemodynamic lag. 12 We proceed by evaluating how well the activity of each brain voxel can be modeled by each of the OverFeat layers separately. The fitted model was evaluated in a K-Fold cross-validation scheme with bagging. The training data were themselves divided into train\/test splits (in accordance with scanner sessions: \u201Cleave one session out\u201D, K=5 for images, K=3 for videos) and a model trained on an inner train split was evaluated on the corresponding test split to select an optimal penalty. Model scores were obtained using predictive r2 score for the dataset of [17]. This means that for a voxel v the activation yvtest for the test set images was compared to the prediction by our model yvpred as follows: r 2 v = 1 − \u2016yvtest−yvpred\u2016 2 \u2016yvtest−mean(yvtest)\u20162 , where mean(yvtest) is the mean activation of voxel v on the test set. Video predictions were evaluated using correlation score rv = 〈yvpred−mean(y v pred),y v test−mean(yvtest)〉 \u2016yvpred−mean(y v pred)\u2016\u2016y v test−mean(yvtest)\u2016 . The optimal models for each train\/test split of the train data were averaged in order to gain stability of predictions. Mean scores over folds for the optimal penalty were kept as a quantitative measure of goodness of fit. For further analysis we keep all voxels up to a false discovery rate (FDR) [40] of 0.01. In order to obtain a selection criterion we choose the maximal score over all layers as a statistic. This choice is necessary due to the fact that we cannot know a priori which layer will describe the voxel\u2019s activity well. The null distribution of these maximum layer score values was obtained by a permutation test (100,000 permutations) on 14 different voxels distributed across the brain volume. Comparison of the histograms of the obtained distributions showed that they are essentially identical and can be used as a global null hypothesis for all brain voxels. The FDR was evaluated using the p-values for every voxel calculated from an empirical distribution obtained by concatenating all permutations over the 14 voxels. A schematic4 of the encoding model is provided in Fig. 1. The lowest level layer is 4All artificial neural network layers are depicted as being convolutional, although the last three are what is generally known as \u201Cfully connected\u201D layers. However, all fully connected layers can be reformulated as 13 depicted on the left and the highest level layer on the right. The surface images below each layer show an r2 score map for the predictive model learnt on this layer. The scores are normalized per voxel such that the sum of scores across layers is 1. This accounts for differences in signal-to-noise ratio across brain regions and highlights the comparison of layers. In the results section (4), we use these voxel-level prediction scores with a per-ROI analysis of the cross-layer profile of reponses and a more systematic mapping of layer preferences across all voxels that are well-explained by the model. 3.3 Synthesis of visual experiments Using the predictive models learnt on each convolutional network layer, we build a simple summary model by averaging all layer model predictions for each voxel. We validate the predictive capacity of this averaged model by using it as a forward model able to synthesize brain activation maps: Using the ridge-regression coefficients, our model predicts full brain activation maps (\u201Cbeta maps\u201D) from new stimuli. These activation maps can be understood using the standard-analysis framework for brain mapping, in which one evaluates a general linear model with relatively few condition regressors, e.g. contrasting the activation maps between two different experimental conditions. We propose to revisit two classic fMRI vision experiments, retinotopy and the faces versus places contrast, by generating them with our forward model. Since these are known experiments, they can be compared and interpreted in context. At the same time, they test different levels of complexity of our model. Retinotopy is purely bound to receptive field location which captures global coarse-grain organization of the images, while the convolutions and [20] takes advantage of this to perform detection and localization. 14 distinction of faces necessitates higher-level features that are closer to semantic meaning. Note that retinotopic mapping was also used in the original study [17] to validate the forward model estimated using Gabor filters. In contrast to our setting, retinotopy was estimated by localizing receptive field maxima for each voxel instead of using the predictive model as a data synthesis pipeline. 3.3.1 Retinotopy We created \u201Cnatural retinotopy\u201D stimuli (see [41]) by masking natural images with wedge- shaped masks. The wedges were 30◦ wide and placed at 15◦ steps, yielding 24 wedges in total. After creation of exact binary masks, they were slightly blurred with a Gaussian kernel of standard deviation amounting to 2% of the image width. We chose 25 random images from the validation set of [17] and masked each one with every wedge mask. The thus obtained set of 600 retinotopy stimuli were fed through the encoding pipeline to obtain brain images for each one of them. These brain images were then used for a subsequent retinotopy analysis. The design matrix for this analysis contains the cosine and the sine of the wedge angle of each stimulus and a constant offset. The retinotopic angle is calculated from the arising beta maps by computing the arctangent of the beta map values for the sine and cosine regressors. Responsiveness of the model to retinotopy was quantified by the F-statistic of the analysis. In order to obtain an easily interpretable retinotopic map, the beta maps were smoothed with a Gaussian kernel of standard deviation 1 voxel before the angle was calculated. Display threshold is set at F > 1. 3.3.2 Synthesizing a \u201CFaces versus Places\u201D contrast Discriminating faces from places involves-higher level feature extraction. While, with certain stimulus sets, the distinction can also be done based on low-level features such as 15 edge detectors, this is almost certainly untrue for the mechanism by which mammalian brains process faces due to the strong invariance and selectivity properties with respect to nontrivial transformations that they can undergo (see [42] for a discussion). In this sense, being able to replicate a \u201Cfaces versus places\u201D contrast with the proposed brain activity synthesis is a test for the ability to reproduce a higher-level mechanism. We compute a ground-truth contrast against which we test our syntheses by selecting 45 close-up images of faces and 48 images of scenes (outdoor landscapes as well as exteriors and interiors of buildings from the dataset of [17]). Examples similar to the original stimulus and identical in masking are depicted in Fig. 6 (A). The 45 face images used were the only close-up images of human faces in the dataset. All other photos with faces were either taken at a distance, of several persons at once, or of human statues or animal faces. All images are unique in content. No slightly modified (e.g. shifted) copies of any image exists in the dataset. Using a standard GLM, we compute a contrast map for \u201Cface > place\u201D and \u201Cplace > face\u201D, which are shown in Fig. 6 (C), thresholded at t = 3.0 in red tones and blue tones respectively. Our first experiment is to synthesize brain activity using precisely the 93 images which produced the ground truth contrast. We trained our predictive model on the remaining 1657 training set images of [17] after removal of the 93 selected face and place stimuli. As stated above, these remaining images do not contain any images simply related to faces. After computing the synthesized activation images for the latter, we proceeded to analyze them using the same standard GLM procedure as above for the ground truth. Due to the fact that the noise structure of the synthetic model is different, the threshold of the generated contrast must be chosen in a different manner. We use a precision-recall approach that can be described in the following way: Having fixed the threshold of the ground truth contrast at t = 3.0, we define the support of the map as all the voxels 16 that pass threshold. For a given threshold t on the synthesized map we define recall as the percentage of the support voxels from the ground truth contrast that are active in the thresholded synthesized map and precision as the percentage of active voxels in the thresholded synthesized map that are in the support of the ground truth map. We define the synthesized map threshold tR50 as the threshold guaranteeing a minimum of 50% recall while maximizing precision. Our second experiment tests the generalization capacity of our model in a more extreme situation: In order to make sure that our feedforward model is not working with particularities of the stimulus set other than the features relevant to faces and scenes, we also used our model to generate a faces-versus-places opposition using stimuli from [43]. These stimuli were originally used to show distributed and overlapping representations of different classes of objects in ventral visual areas. Among the stimuli are 48 pictures of faces and 48 pictures of houses. These stimuli are notably different in appearance from the ones used to train our model: they are centered and scaled, and tightly segmented on a light gray background, while the images used to train the model are natural images, with objects of varying size and position on a busy background (see figure 6(A) and (B)). We applied the same feedforward pipeline to synthesize activation maps from each of these images and the same GLM analysis and thresholding procedure as described in the preceding experiment. 4 Experimental results All experimental results were obtained on volume data. For visualization purposes they were subsequently projected to surface maps. In the case of the images dataset, this projection is slightly distorted in areas distant from the occipital pole. Furthermore, the 17 field of view of the acquisition is restricted to the occipital lobe. On inspection of the three zoomed panels from Fig. 1 one observes that the score maps are different across layers. On the left, the model based on the first layer explains medial occipital regions well with respect to the others. It includes the calcarine sulcus, where V1 is situated, as well as its surroundings, which encompass ventral and dorsal V2 and V3. This contrasts to the score map on the right, which represents the highest level model. The aforementioned medial occipital regions are relatively less well explained, but lateral occipital, ventral occipital and dorsal occipital regions exhibit comparatively higher scores. 4.1 Quantifying layer preference For each voxel, we call the set of scores associated with the prediction of its activity from each layer the score fingerprint of that voxel. Given the fact that layer outputs are correlated (across layers) and each voxel contains many neurons, we do not expect sharp peaks in the score fingerprint for a specific \u201Cbest\u201D layer. Rather we expect a progression of scores over layers indicating a global trend towards simple, intermediate or more high-level representations. Using the ROI definitions provided by the datasets, we can study the mean score fingerprints per region of interest. The average score fingerprint per ROI was obtained using the 25% best predicted voxels within the region. For each region of interest, the mean score fingerprint was normalized by its maximum value. The resulting normalized progressions are shown in Fig. 2. We observe that for both subjects, the score fingerprint for V1 peaks at the first layer. It then decreases in relative accuracy as the layer index increases. For the mean fingerprint of V2, the peak lies on the second layer and the subsequent decrease is a little slower than that of the V1 fingerprint. This indicates that V2 is selective for a mix of higher-level features less present in V1. The V3 mean score fingerprint also peaks at 18 1 2 3 4 5 First convolutional layers 20% 40% 60% 80% 90% 95% 100% Pe rc en to fm ax im um sc or e pe rr eg io n Subject 1 V1 V2 V3 V3A V3B V4 LatOcc 1 2 3 4 5 First convolutional layers 20% 40% 60% 80% 90% 95% 100% Pe rc en to fm ax im um sc or e pe rr eg io n Subject 2 V1 V2 V3 V3A V3B V4 LatOcc Figure 2: Normalized average score fingerprints over ROIs. Score progressions for two subjects averaged over regions of interest provided by the dataset. For each ROI, the score progression was normalized by its maximally predictive layer score. For V1 we observe peak score in layer 1 and a downward trend towards higher level layers. The V2 fingerprint peaks in the second layer and then decreases slightly slower than the V1 fingerprint. V3 fingerprint also peaks in layer 2 but decreases more slowly than V1\/V2 fingerprints. V4 fingerprint peaks much later than the ones of V1\/V2\/V3 but is not much worse described by lower level layers. Fingerprints of V3A\/B and LOC show a strong increase across layers. layer 2 and decreases less fast than the V2 fingerprint, indicating a selectivity mix of again slightly higher levels of representation than present in V2. The mean V4 fingerprint peaks significantly later than the first three, around layers 4 and 5. V4 is, however, well explained by the complete hierarchy of features: the score fingerprint is constantly above 70% of its maximum score. In contrast, the dorsal areas V3A and V3B are much less well modeled by lower level layers than by higher level layers. Similarly, the lateral occipital complex (LOC) shows a strong increase in relative score with increasing representation layer number. In Fig. 3 we show a winner-takes-all (\u201Cargmax\u201D) map over spatially smoothed scores (σ = 1 voxel). It is obtained by smoothing each score map and then associating each voxel with the layer which best fitted its activity. This marker provides compelling outlines of the organization of the visual system: the map of which layer of the convolutional 19 Figure 3: Best model per voxel. Among the voxels which are modeled by at least one of the convolutional network layers, we show which network layer models which region best. This is achieved by smoothing the layer score maps (σ = 1 voxel) and assigning each voxel to the layer of maximal score. One observes that the area around the Calcarine sulcus, where V1 lies, is best fit using the first layer. Further one observes a progression in layer selectivity in ventral and dorsal directions, as well as very strong hemispheric symmetry. network explains best brain activity segments the organization of the visual system well. One observes that medial occipital regions are mostly in correspondence with the first layer, that there is a progression in layers along the ventral and dorsal directions, which is symmetric, and that there is a global symmetry across hemispheres. In order to better show the layer selectivity of each voxel as represented by its score fingerprint in a brain volume, we derived a summary statistic based on the following observation. As can be seen in Fig. 2, the average fingerprints of each region of interest have either an upward or a downward trend. It turns out that the first principal component of all score fingerprints over significantly well predicted voxels is a linear trend. Moreover, it explains over 80% of the variance of all fingerprints. The projection onto it can therefore be used as a summary of the voxel fingerprint. Here we use a fixed trend going from -1 at layer 1 to 1 at layer 9 in steps of 0.25. Projecting the score fingerprints onto this 20 Figure 4: Fingerprint summaries as brain map. We compute a summary statistic for voxel fingerprints by evaluating their inner product with an ascending linear trend from -1 to 1 in nine steps of 0.25. This yields low values for low layer preference and high values for late layer preference. Observe the preference for low-level models in earlier visual areas V1 and V2. With increasingly higher layer selectivity for V3, V4 and ulterior visual areas, a trend from low level to high level representation across the ventral and dorsal visual pathways becomes apparent. ascending trend, which amounts to evaluating the global slope, yields a summary of the voxel fingerprint. It is shown for subject 1 in Fig. 4 on the left. We observe that V1 fingerprints project almost entirely to the low level range of models, indicated by blue hues. V2 shows more presence of green, indicating intermediate level models. This trend continues in V3. V4 shows a clear preference for mid-level models. Subsequent regions show a tendency towards even higher level representations. This progression is mirrored exactly on the second panel of Fig. 4. Applying an identical visualization technique to the score fingerprints obtained from modeling the video experiment, we observe a very similar progression of model selectivity across the early visual areas. As above, the fingerprint summary indicates lower level layer preference in V1 and V2, intermediate layers in V3 and V4 and high level layers in parts of lateral occipital 21 and specialized areas such as the extrastriate body area (EBA, [14]) and the transverse occipital sulcus (TOS, [44]). Recall that the latter data were acquired in a completely different experiment, with videos instead of images. It is to be noted that the convolutional network was applied directly to the individual frames of the video, followed by a temporal aggregation (temporal averaging by blocks) in order to reach the temporal scale of the fMRI acquisition. No explicit motion processing or other video-specific processing was incorporated. The fact that the same underlying model obtains similar results is a strong demonstration of the reproducibility of our findings. 4.2 Synthesis of visual experiments 4.2.1 Retinotopy The angular-preference maps obtained by synthesizing fMRI activation from virtual wedge- shaped stimuli can be seen in Fig. 5. Comparison to existing literature shows that the model indeed captures the transitions of known retinotopic regions. For instance, one can observe the sign inversions of the gradient of the angle map at the transitions from ventral V1 to ventral V2 and ventral V3 to ventral V4. These transitions are very clear and in perfect correspondence with the outlines of the volume-based retinotopic regions of interest provided with the dataset \u2013also shown on the figure. The transitions in dorsal primary visual areas are apparent but slightly less well delineated. We suspect that the decreased performance in dorsal areas is due to surface projection difficulties, arising from distortion between available anatomical and functional images in anterior-posterior direction. These projection errors probably also explain the absence of signal in the occipital pole surrounding the fovea. In sum, the synthesized angle-preference map is 22 Figure 5: Retinotopic map for subject 1. Synthesizing the responses to retinotopic wedge stimuli and performing a classic phase-coding GLM analysis, we show the retinotopic angle map at display threshold F = 1. As can be seen in the ventral part of the brain map (lower half), the retinotopic mapping indicates visual angle inversions exactly at the locations previously identified by a localizer, aligning perfectly with the visual map borders traced on the surface. Dorsal areas (upper half) exhibit the same tendencies in a less pronounced manner. consistent with respect to the subject-specific delineations of reference structures in the visual system (see [41] and [6]). 4.2.2 Replicating the \u201CFaces versus Places\u201D contrast We first synthesize the brain activity corresponding to the images used to define the ground-truth contrast (but left out during model training). The synthesized contrast for the 93 held-out stimuli from [17] are shown in Fig. 6 (D). The contrast generated from 23 stimuli from the study of [43] can be seen in Fig. 6 (E). The similarity of both simulated contrasts to the ground truth contrast in Fig. 6 (C) is striking. The areas that respond to faces are lateral occipital and inferior occipital. The Lateral Occipital Complex is known to have face-selective subparts [45] and the inferior occipital Occipital Face Area is also known to be involved in face processing. It is possible that some more generally body part selective areas are active as well since the stimuli used to obtain the ground truth contrast may also contain a view on e.g. part of the torso [14,46]. Note that both the fusiform face area and the fusiform body area are outside the field of view of the acquisition and thus invisible to the ground truth contrast and the synthesized contrast. The areas responsive to places are mainly dorsal in the given field of view. We observe activation in regions that are most likely to be transverse occipital sulcus (TOS) and inferior intraparietal sulcus (IPS). Since these regions are typically close together anatomically and as no localizer for them was performed on the given brain, it is difficult to tell them apart. However, [44] shows that TOS is strongly scene selective whereas inferior IPS may be more concerned with object individuation and localization. Note that the habitually mentioned place-selective Parahippocampal Place Area [15] is also not within the field of view of the acquisition. In conclusion, the simulated face\/place contrasts using stimuli from [17] and from the very different stimulus set of [43] both create an activation contrast very close to the ground-truth contrast, which highlights regions well-known in the existing literature. We perform an additional experiment to show that this synthesis of face\/place opposition is driven by the high-level features. We attempt to generate such a contrast using only the first layer from the model. As can be seen on Fig. 6 (F), the regions previously identified can no longer be distinguished from the strong noise in the surroundings. Fig. 6 (G) 24 Figure 6: Synthesizing Face versus Place contrast. (A) Examples of the stimuli similar to those of [17] containing close up photos of faces (45 total) and places (48 total), removed from the train set of the synthesis model. (B) Examples of the stimuli from [43] for faces and places (48 for each in total). (C) Contrast of BOLD activity from a GLM model of the held-out face and place stimuli. Referred to as ground truth in view of the synthetic data. (D) Predicted contrast for the 93 held out face and place stimuli from the training set of [17]. Thresholded at best precision given minimum recall of 50% of ground truth activation support. (E) Predicted contrast for the 96 face and house stimuli from [43]. Thresholded as in D. (F) Predicted contrast for the 96 face and house stimuli from [43] using only layer 1, i.e. a first order, edge-detector type feature map. Thresholded at 50% recall of ground truth as in D. Note the strong noise component in the map compared to D and E. (G) Precision-recall curve for support recovery of ground truth map when predicting on face\/house stimuli from [43].: For varying thresholds, precision is the percentage of active voxels which are also active in ground truth; recall is the percentage of ground truth voxels recovered. Full lines correspond to average over layers, dashed lines correspond to prediction using only layer 1. Red represents the \u201Cfaces > places\u201D contrast and blue represents the \u201Cplaces > faces\u201D contrast. Note that the field of view is restricted to occipital areas. Ventral temporal areas such as FFA and PPA are invisible to this analysis. 25 depicts the precision-recall curves for face and place selective areas for the averaged model and for the layer 1 model. Studying the high precision range at the left of the diagram, it becomes clear that the proposed average synthesis model shares its strongest activations exactly with the ground truth contrast, leading to 100% precision. There is no threshold for which this is the case for the model obtained from layer 1. 5 Discussion The study of the mammalian visual system has historically been led by crafting stimuli designed to selectively trigger neural activation in various sub-systems of the visual cortex, from edges [7], to abstract shapes and faces [11\u201313,47,48]. However, an observed response of the visual system is conditional to the types of stimuli that were tested. Elicited neural responses from parametrically varied synthetic stimuli may be strongly related to the chosen stimulus ensemble, making generalizations difficult. Naturalistic stimuli provide experimental settings that are closer to real-life ecological settings, and evoke different responses [49]. They contain a rich sampling of the visual challenges that the human brain tackles. While most detailed understanding about neural computation has been pushed forward using electrophysiological experiments, the non-invasive methodology of fMRI offers the benefit of full-brain coverage. Many fMRI studies investigate binary hypotheses by crafting stimuli specific to a question, whether they be naturalistic or not. In contrast, the dataset on which we rely [38], is an investigation of the BOLD fMRI responses to a large number of not specifically chosen natural stimulus images, showing that it is possible to identify the stimulus among thousands of candidate images. Departing from studies based on manual crafting of specific stimuli and corresponding restrictive hypotheses, we propose to model brain responses due to pure natural image statistics. 26 Indeed, capturing the rich statistics in images of the world that surrounds us must be a driving principle of the structure of visual cortex, as suggested by [50] for the primary visual areas. Here, we rely on a powerful computational model capturing these statistics: a deep convolutional network with enough representational capacity to approach human-level core object recognition [33]. Based on the convolutional network OverFeat, we have built a feedforward model explaining brain activity elicited by visual stimulation from the image representations in the various layers of the convolutional network. We fitted a separate model for each layer to full brain activity and obtained prediction scores for each one of them. These prediction scores were analyzed in order to establish a comparison between the convolutional network feature hierarchy and brain regions. In an ROI analysis we show that early visual areas are better modeled with lower-level layers from the convolutional network but that progressing ventrally and dorsally from the calcarine sulcus there is a clear increase in selectivity for complex representations. Furthermore, score fingerprint summaries obtained by mapping this ascending trend show a clear spatial gradient in affinity to higher level representations: Starting at V1 we observe a clear dominance of low-level layers in the score fingerprint. Across subsequent extrastriate visual areas we observe a gradual and continuous increase in relative predictive power of the complex representations. The same result was obtained for a representation of score fingerprints due to a visual movie experiment. This yields a second indicator of the existence of a gradient in complexity coming from a completely different dataset. Finding the same overall structure on such different stimuli is a strong confirmation that the uncovered structure is not spurious or due to experiment design. 27 5.1 Related work Prior studies have linked brain activation to convolutional networks of computer vision. In [24] the authors evaluate a large number of computer vision models, including a convolutional network. They assess their representational capacity with respect to brain activity while subjects viewed images of objects. They find among other results that the last layers of the network exhibit similar representational similarities as IT neurons in the macaque as well as fMRI activation in humans. Recent proof of concept work [23] uses a convolutional network (different from the one used here, see [36]), enabling the layer-wise analysis of voxel scores across layers. These results also reveal a gradient in complexity of representation. Here we show that the mapping goes beyond a specific experimental paradigm by reproducing our analysis on a video-viewing experiment. Finally, we show that beyond the gradient, the convolutional network can define a full mapping, with successive areas, of the visual cortex. Also concurrent with the present work is [51], in which different computer vision algorithms and all layers of the convolutional network introduced in [36] are compared to the BOLD activity on the data of [17]. The analysis is mostly restricted to representational similarity analysis, but a form of \u201Cremixing\u201D features with the weights of a predictive ridge regression is introduced. A score progression across layers and regions of interest is also shown. This functional characterization does rely to some extent on the structural similarity between the functional organization of the visual cortex and that of the computational model. In a convolutional network, the linear transformation is restricted to the form of a convolution, which forces the replication of the same linear transformation at different positions in the preceding layer image. This forces similarity of processing across the 2D extent of the image and constrains the receptive fields of the units to be localized and 28 spatially organized. This spatial sparsity saves computational resources and entails a strong inductive bias on the optimization by encoding locality and translation covariance. It is however important to note that biological visual systems generally do not exhibit linear translation covariance. The retinotopic correspondence map allocates much more cortical surface to foveal regions than to peripheral regions. This is called cortical magnification (see e.g. [52] for details). A limitation of our treatment of video data is that it is necessarily restricted to a frame- by-frame analysis. While visual neurons generally perform spatiotemporal operations, our best approximation is marginal in space and time. Even in this setting, the increasingly linear representations of invariances with layer depth leads to a slower temporal change in signal at higher layers. While spatiotemporal features do obtain an increase in performance for low-level features even for BOLD measure [19], the spatiotemporally separated setting is nevertheless an acceptable approximation, which improves with layer abstraction level. Future work should address the predictive capacity of spatiotemporally informed video analysis networks. Departing from prior work, which bases the neuroscientific validation on mostly descriptive arguments, we introduce a new method for validating rich encoding models of brain activity. We generated synthetic brain activation for known, standard fMRI experiments and analyzed them in the task-fMRI standard analysis framework. We chose two experiments at different levels of complexity: Retinotopy, a low-level spatial organization property of the visual system, and the faces versus places contrast, an experiment necessitating high-level recognition capacity and complex representations. The results show that both experiments are well replicated. Angle gradient sign inversion lines indicating the bounds of visual areas are correctly identified. Face and place selective voxels as defined by a previously calculated contrast on true BOLD signal are correctly 29 identified in the synthesized contrast in the sense that the voxels responding strongest to the simulated contrast are those that are the strongest in the BOLD contrast. This notion is visualized in a rigorous manner by presenting the synthetic maps at a threshold that recovers at least 50% of the supra-threshold area t ≥ 3.0 of the original activation map. Both for left-out face and place stimuli from the original experiment and the stimuli of faces and houses used from [43], the model had never seen these images at training time. It had seen the same type of image as the held out set in the sense that they were taken from the same photo base, had the same round frame and the same mean intensity. The type of image coming from [43] was segmented differently \u2013tightly around the object\u2013 making the framing very different in addition to very different mean intensities and pixel dynamics. Our synthesis model for brain activation was robust to these differences and yielded very similar contrasts to the ground truth. Similarly, the retinotopy stimuli were constructed from previously unseen images, and the geometry of the retinopy wedges was entirely new to the system as well. Generalizing to such images, with different statistics from those of the experiment used to build the model, is clear evidence that our model captures the brain representations of high-level invariants and concepts in the images. We have thus built a data-driven forward model able to synthesize visual cortex brain activity from an experiment involving natural images. This model transcends experimental paradigms and recovers neuroscientific results which would typically require the design of a specific paradigm and a full fMRI acquisition. In the current setting, any passive viewing task with central fixation can be simulated using this mechanism. After a validation of correspondence on many contrasts for which one has BOLD fMRI ground truth, one could use it in explorative mode to test new visual experimental paradigms. Discrepancies, i.e. the inability of the model to describe the response to a new stimulus adequately, would provide cues to refine this quantitative model of the visual cortex activity. Importantly, 30 these synthetic experiments are a non-trivial step forward for the experimental process: They provide a new way of leveraging open forward-modeling techniques. Indeed, having an underlying forward model that is able to capture experimental results which until now had to be obtained in specific, dedicated experimental paradigms, once sufficiently validated on known contrasts, will provide a new tool for investigation of the stimuli-driven fMRI measures. For instance, predicting activity for new stimuli of interest can be used for experiment design. 5.2 Perspectives Several paths of research open up from this point. First and foremost, the forward modeling pipeline suffers from high dimensionality, strong correlations at all layers and lack of data to disambiguate them. These issues need to be addressed in order to be able to draw more clear-cut conclusions. 5.2.1 Reproduce more contrasts One step forward in this direction is to continue testing known fMRI experiments using convolutional networks as a black-box model basis for brain image synthesis. As soon as one runs into a discrepancy between predicted contrast and ground truth, several reasons can be imagined: 1) The neural network employed simply does not have the capacity to provide a rich enough representation for this particular type of brain activity. 2) The neural network has sufficient capacity, but did not see enough examples in order to create a differentiated representation of the images at hand. 3) The neural network has a sufficient representation to explain brain activity, but there do not exist enough image\/brain image pairs to be able to train a predictive model that generates appropriate brain images. These points can be tested in a sequential manner and measures can be taken to appropriately 31 adjust the forward model. 5.2.2 Exploit cortico-cortical connections While a certain number of works have already focused on fine-grained study of connectivity in visual areas [53, 54], both after retinotopically localized stimulation and through co- activations at rest, using a fine-grained forward model such as the one presented opens the door to a new form of connectivity modeling. Instead of using the BOLD signal from one visual area to predict activity upstream in the hierarchy, which can lead to artificially high predictive scores due to spatially structured noise, it is now possible to predict ulterior areas using the voxel predictions obtained for preceding areas. By evaluating predictions obtained from previous layers against direct prediction from the convolutional net representation, one can assess the degree of information loss incurred by the measurement modality. 6 Acknowledgments This work was funded by the European Union Seventh Framework Programme (FP7\/2007- 2013) under grant agreement no. 604102 (\u201CHuman Brain Project\u201D) and through the France-Berkeley Fund 2012 edition. Our thanks go out to Alexander Huth, Natalia Bilenko and Jack Gallant for providing the video data, Danilo Bzdok for great comments on the manuscript and Kyle Kastner and Olivier Grisel for very helpful exchanges on the methods. 32 References 1. Felleman D, Van Essen DV (1991) Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex . 2. Mishkin M, Ungerleider LG (1982) Contribution of striate inputs to the visuospatial functions of parieto-preoccipital cortex in monkeys. Behavioural Brain Research 6: 57\u201377. 3. Goodale M, Milner D (1992) Separate visual pathways for perception and action. Trends in Neurosciences 15: 20\u201325. 4. Thorpe S, Fize D, Marlot C (1996). Speed of processing in the human visual system. doi:10.1038\/381520a0. 5. Fabre-Thorpe M, Delorme A, Marlot C, Thorpe S (2001) A limit to the speed of processing in ultra-rapid visual categorization of novel natural scenes. Journal of cognitive neuroscience 13: 171\u2013180. 6. Wandell B, Dumoulin SO, Brewer A (2007) Visual field maps in human cortex. Neuron 56: 366\u2013383. 7. Hubel DH, Wiesel TN (1959) Receptive fields of single neurones in the cat\u2019s striate cortex. The Journal of physiology 148: 574\u2013591. 8. Anzai A, Peng X, Van Essen DC (2007) Neurons in monkey visual area V2 encode combinations of orientations. Nature neuroscience 10: 1313\u20131321. 9. Freeman J, Ziemba CM, Heeger DJ, Simoncelli EP, Movshon JA (2013) A functional and perceptual signature of the second visual area in primates. Nature neuroscience 16: 974\u201381. 33 10. Roe AW, Chelazzi L, Connor CE, Conway BR, Fujita I, et al. (2012) Toward a Unified Theory of Visual Area V4. Neuron 74: 12\u201329. 11. Desimone R, Albright T, Gross C, Bruce C (1984) Stimulus-selective Properties of Inferior Temporal Neurons in the Macaque. Journal of Neuroscience 4: 2051\u20132062. 12. Logothetis NK, Pauls J, Poggio T (1995) Shape representation in the inferior temporal cortex of monkeys. Current biology : CB 5: 552\u2013563. 13. Kanwisher N, Mcdermott J, Chun MM (1997) The Fusiform Face Area : A Module in Human Extrastriate Cortex Specialized for Face Perception. The Journal of Neuroscience 17: 4302\u20134311. 14. Downing PE, Jiang Y, Shuman M, Kanwisher N (2001) A cortical area selective for visual processing of the human body. Science (New York, NY) 293: 2470\u20132473. 15. Epstein R, Kanwisher N (1998) A cortical representation of the local visual environ- ment. Nature 392: 598\u2013601. 16. Naselaris T, Kay KN, Nishimoto S, Gallant JL (2011) Encoding and decoding in fMRI. NeuroImage 56: 400\u2013410. 17. Kay KN, Naselaris T, Prenger RJ, Gallant JL (2008) Identifying natural images from human brain activity. Nature 452: 352\u2013355. 18. Naselaris T, Prenger RJ, Kay KN, Oliver M, Gallant JL (2009) Bayesian Recon- struction of Natural Images from Human Brain Activity. Neuron 63: 902\u2013915. 19. Nishimoto S, Vu AT, Naselaris T, Benjamini Y, Yu B, et al. (2011) Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology 21: 1641\u20131646. 34 20. Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, et al. (2013) OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks. arXiv preprint arXiv:13126229 : 1\u201315. 21. Huth AG, Nishimoto S, Vu AT, Gallant JL (2012) A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain. Neuron 76: 1210\u20131224. 22. Simonyan K, Vedaldi A, Zisserman A (2013) Deep Inside Convolutional Net- works: Visualising Image Classification Models and Saliency Maps. arXiv preprint arXiv:13126034 : 1\u20138. 23. Guclu U, van Gerven MaJ (2015) Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream. Journal of Neuroscience 35: 10005\u201310014. 24. Khaligh-Razavi SM, Kriegeskorte N (2014) Deep Supervised , but Not Unsupervised , Models May Explain IT Cortical Representation. PLoS Computational Biology 10. 25. Canny J (1986) A Computational Approach to Edge Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8. 26. Simoncelli EP, Freeman WT (1995) The Steerable Pyramid: A Flexible Multi- Scale Derivative Computation. Conference, Ieee International Processing, Image (Rochester, NY) III: 444\u2013447. 27. Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nature neuroscience 2: 1019\u20131025. 35 28. Serre T, Wolf L, Bileschi S, Riesenhuber M, Poggio T (2007) Robust object recog- nition with cortex-like mechanisms. IEEE Transactions on Pattern Analysis and Machine Intelligence 29: 411\u2013426. 29. Mallat S (2012) Group Invariant Scattering. Communications on Pure and Applied Mathematics 65: 1331\u20131398. 30. Bruna J, Mallat S (2013) Invariant scattering convolution networks. IEEE Transac- tions on Pattern Analysis and Machine Intelligence 35: 1872\u20131886. 31. Eickenberg M, Pedregosa F, Senoussi M, Gramfort A, Thirion B (2013) Second order scattering descriptors predict fMRI activity due to visual textures. In: Pattern Recognition in NeuroImaging, IEEE International Workshop on. pp. 5\u20138. 32. Kriegeskorte N, Mur M, Ruff DA, Kiani R, Bodurka J, et al. (2008) Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60: 1126\u20131141. 33. Cadieu CF, Hong H, Yamins DLK, Pinto N, Ardila D, et al. (2014) Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition. Arxiv 10: 35. 34. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521: 436\u2013444. 35. Yamins DLK, Hong H, Cadieu CF, Solomon Ea, Seibert D, et al. (2014) Performance- optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences of the United States of America 111: 8619\u201324. 36 36. Krizhevsky a, Sutskever I, Hinton G (2012) Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems : 1097\u20131105. 37. He K, Zhang X, Ren S, Sun J (2015) Delving Deep into Rectifiers: Surpassing Human- Level Performance on ImageNet Classification. arXiv preprint arXiv:150201852 . 38. Kay KN, Naselaris T, Gallant J (2011). fmri of human visual areas in response to natural images. crcns.org. doi:http:\/\/dx.doi.org\/10.6080\/K0QN64NG. 39. Gao JS, Huth AG, Lescroart MD, Gallant JL (2015) Pycortex: an interactive surface visualizer for fmri. Frontiers in neuroinformatics 9. 40. Benjamini Y, Hochberg Y (1995). Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. doi:10.2307\/2346101. URL http: \/\/www.jstor.org\/stable\/2346101. 95\/57289. 41. Sereno M, Dale A, Reppas J (1995) Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging. Science . 42. Pinto N, Cox DD, DiCarlo JJ (2008) Why is real-world visual object recognition hard? PLoS Computational Biology 4: 0151\u20130156. 43. Haxby JV, Gobbini MI, Furey ML, Ishai A, Schouten JL, et al. (2001) Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science (New York, NY) 293: 2425\u20132430. http:\/\/www.jstor.org\/stable\/2346101 http:\/\/www.jstor.org\/stable\/2346101 95\/57289 37 44. Bettencourt KC, Xu Y (2013) The role of transverse occipital sulcus in scene perception and its relationship to object individuation in inferior intraparietal sulcus. Journal of cognitive neuroscience 25: 1711\u201322. 45. Grill-Spector K, Kourtzi Z, Kanwisher N (2001) The lateral occipital complex and its role in object recognition. Vision Research 41: 1409\u20131422. 46. Taylor JC, Wiggett AJ, Downing PE (2007) Functional MRI analysis of body and body part representations in the extrastriate and fusiform body areas. Journal of neurophysiology 98: 1626\u20131633. 47. Gallant JL, Connor CE, Rakshit S, Lewis JW, Van Essen DC (1996) Neural responses to polar, hyperbolic, and Cartesian gratings in area V4 of the macaque monkey. Journal of neurophysiology 76: 2718\u20132739. 48. Bentin S, Allison T, Puce A, Perez E, McCarthy G (1996) Electrophysiological studies of face perception in humans. Journal of cognitive neuroscience 8: 551. 49. Gallant JL, Connor CE, Van Essen DC (1998) Neural activity in areas V1, V2 and V4 during free viewing of natural scenes compared to controlled viewing. Neuroreport 9: 2153\u20132158. 50. Olshausen BA, Field DJ (1996) Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381: 607. 51. Khaligh-Razavi SM, Henriksson L, Kay K, Kriegeskorte N (2014) Explaining the hierarchy of visual representational geometries by remixing of features from many computational vision models. bioRxiv . 38 52. Schira MM, Wade AR, Tyler CW (2007) Two-dimensional mapping of the central and parafoveal visual field to human visual cortex. Journal of neurophysiology 97: 4284\u20134295. 53. Heinzle J, Kahnt T, Haynes JD (2011) Topographically specific functional connec- tivity between visual field maps in the human brain. NeuroImage 56: 1426\u20131436. 54. Haak KV, Winawer J, Harvey BM, Renken R, Dumoulin SO, et al. (2013) NeuroIm- age Connective fi eld modeling 66: 376\u2013384. Introduction Biological relevance of multi-layer vision models Methods Datasets The encoding pipeline Synthesis of visual experiments Retinotopy Synthesizing a ``Faces versus Places'' contrast Experimental results Quantifying layer preference Synthesis of visual experiments Retinotopy Replicating the ``Faces versus Places'' contrast Discussion Related work Perspectives Reproduce more contrasts Exploit cortico-cortical connections Acknowledgments ","flair":"three\tResearch"}
{"author":"antiprior","created":"Sun Nov 06 12:59:38 EST 2016","text":" Under review as a conference paper at ICLR 2017 LEARNING A STATIC ANALYZER: A CASE STUDY ON A TOY LANGUAGE Manzil Zaheer Carnegie-Mellon University manzil.zaheer@cmu.edu Jean-Baptiste Tristan & Michael Wick & Guy L. Steele Jr. Oracle Labs jean.baptiste.tristan@oracle.com ABSTRACT Static analyzers are meta-programs that analyze programs to detect potential er- rors or collect information. For example, they are used as security tools to detect potential buffer overflows. Also, they are used by compilers to verify that a pro- gram is well-formed and collect information to generate better code. In this paper, we address the following question: can a static analyzer be learned from data? More specifically, can we use deep learning to learn a static analyzer without the need for complicated feature engineering? We show that long short-term mem- ory networks are able to learn a basic static analyzer for a simple toy language. However, pre-existing approaches based on feature engineering, hidden Markov models, or basic recurrent neural networks fail on such a simple problem. Finally, we show how to make such a tool usable by employing a language model to help the programmer detect where the reported errors are located. 1 INTRODUCTION Can programming language tools, such as static analyzers, be learned from data using deep learning? While research projects trying to use machine learning to design better programming language tools are burgeoning, they all rely on feature engineering (Brun & Ernst, 2004; Kolter & Maloof, 2006; Yamaguchi et al., 2012; Tripp et al., 2014; Raychev et al., 2015; Allamanis et al., 2015; Nguyen & Nguyen, 2015; Gvero & Kuncak, 2015; Long & Rinard, 2016). Unfortunately, feature engineering for programs is difficult and indeed the features often seem ad-hoc and superficial. This raises the question of whether it could be possible to approach a complicated problem such as static analysis \u2013 the automated detection of program properties \u2013 from almost raw features. In this paper, our goal is to present a very simple experiment that clearly shows that not only feature engineering can completely fail for even the simplest static analysis task, but that deep learning with neural networks can indeed be successful. The task in which we are interested is simple: we want to ensure that program variables are defined before they are used. We design a toy language to focus on the problem, and indeed our language is so simple that if it satisfies the aforementioned property, then it is semantically valid. Since programs are sequences of tokens, we experiment with different types of sequence learning methods (Xing et al., 2010). We try feature-based methods in which we extract features from the sequence and then use a classifier to decide whether or not the program is semantically valid. We show that they all fail, including methods that compute a sequence embedding. Then, we try different model- based methods (Lipton, 2015): hidden Markov models (HMM), recurrent neural networks (RNN), and long short-term memory networks (LSTM). Our results show that HMM and RNN do poorly (albeit better than random), while an LSTM is almost perfectly accurate. This finding is somewhat surprising as static analysis is essentially a document classification problem and LSTMs are known to perform poorly on related tasks, such as sentiment analysis (Dai & Le, 2015). The obvious question about such an experiment is: why would we want to learn a static analyzer for a problem that we know of a perfectly fine engineered solution? The answer is that we want to initiate investigation into the use of deep-learning for program analysis, and our broader hopes are two-fold. First, static analyzers are very complicated and often limited by the amount of false positive and false negatives they generate. In cases where false negatives are unacceptable, a learned static analyzer may not be the right approach. But when the goal is rather to find a good balance 1 Under review as a conference paper at ICLR 2017 between false positives and false negatives, learned static analyzers might be more flexible. Second, as we will briefly show in the paper, learned static analyzers have a resilience to small errors that might lead to more robust tools. Indeed, even though our goal is to detect errors in syntactically valid programs, our tool work despite the presence of small syntactic errors, such as the omission of the semicolon. This resilience to errors is in our opinion a very promising aspect of learned methods for the analysis of programs. Another key problem with static analysis programs is that to be useful, they need to help the pro- grammer understand what is the cause of the error. In that case, models based on recurrent neural networks really shine because they can be trained to provide such information. Indeed, in our ex- periment, we show how to use a language model to locate the position of erroneous variables in examples classified by the static analyzer as being wrong. This is very important for practical static analysis since a tool that merely reports the existence of an error in a large code file is not useful. The paper is organized as follows. In section 2 we introduce the programming language of study and the corresponding static analysis task. In section 3, we review how we created the dataset used to learn the static analyzer and the methods that we have tried. In section 4, we explain how we learn to report error messages to help the programmer understand how to fix an error. 2 A STATIC ANALYSIS TASK Our goal is to study the following static analysis problem: given a program, is every variable defined before it is being used? Because this problem is undecidable for a Turing complete language, pro- gramming languages such as Java impose constraints on what is a correct variable initialization. For example, a variable may not in general be defined within only one branch of an if-then-else statement and used afterward since it can be impossible to guarantee which branch will be executed for every run. In order to better understand whether this is feasible and which methods work, we design a toy lan- guage. As an example, in this language, we can write a program that computes the 42th Fibonnacci number as follows. 1 v0 = 1 ; v1 = 1 ; 2 v2 = 0 ; 3 w h i l e ( v2 < 42) { 4 v3 = v1 ; 5 v1 = v0 + v1 ; 6 v0 = v3 ; 7 v2 = v2 + 1 ; 8 } 9 r e t u r n v1 ; If we were to invert lines 4 and 6, then not only would the program be incorrect, but it would be semantically invalid since in the first execution of the loop, variable v3 has not yet been defined. In order to precisely explain what the task is, we now briefly present the syntax and semantics of our experimental programming language. 2.1 THE LANGUAGE We present the syntax of the language in Backus-Naur form in figure 1. The symbols delimited by 〈〉 are non-terminals while the symbols delimited by \u2018\u2019 are terminals. Symbol 〈program〉 is the starting non-terminal. A program is composed of an optional statement followed by an expression. The statement can be a list of statements, control-flow statements like conditionals or iterations, or the binding of an expression to a variable. The expressions are simple arithmetic expressions. For simplicity, the test expressions used in conditional statements are distinct from the other expressions, which is a simple syntactic way to enforce basic type safety. The integers are simple integer values of the form [0− 9]+ while the identifiers are of the form v[0− 9]+. The semantics of our experimental programming language is presented as a big-step operational semantics in figure 2. For simplicity, we only present a subset of the rules. It is composed of 2 Under review as a conference paper at ICLR 2017 〈program〉 ::= \u2018return\u2019 〈expression〉 \u2018;\u2019 | 〈statement〉 \u2018return\u2019 〈expression〉 \u2018;\u2019 〈statements〉 ::= 〈statement〉 | 〈statement〉 〈statements〉 〈statement〉 ::= 〈statements〉 | 〈identifier〉 \u2018=\u2019 〈expression〉 \u2018;\u2019 | \u2018if\u2019 \u2018(\u2019 〈test〉 \u2018)\u2019 \u2018{\u2019 〈statement〉 \u2018}\u2019 \u2018else\u2019 \u2018{\u2019 〈statement〉 \u2018}\u2019 | \u2018if\u2019 \u2018(\u2019 〈test〉 \u2018)\u2019 \u2018{\u2019 〈statement〉 \u2018}\u2019 | \u2018while\u2019 \u2018(\u2019 〈test〉 \u2018)\u2019 \u2018{\u2019 〈statement〉 \u2018}\u2019 〈test〉 ::= 〈expression〉 \u2018=\u2019 〈expression〉 | 〈expression〉 \u2018<=\u2019 〈expression〉 〈expression〉 ::= 〈multiplicative〉 | 〈expression〉 \u2018+\u2019 〈multiplicative〉 | 〈expression〉 \u2018-\u2019 〈multiplicative〉 〈multiplicative〉 ::= 〈unary〉 | 〈multiplicative〉 \u2018*\u2019 〈unary〉 | 〈multiplicative〉 \u2018\/\u2019 〈unary〉 〈unary〉 ::= 〈atomic〉 | \u2018+\u2019 〈unary〉 | \u2018-\u2019 〈unary〉 〈atomic〉 ::= 〈integer〉 | 〈identifier〉 | \u2018(\u2019 〈expression〉 \u2018)\u2019 Figure 1: Syntax of the language, presented in Backus-Naur form. The symbols delimited by 〈〉 are non-terminals while the symbols delimited by \u2018\u2019 are terminals. 〈program〉 is the starting non- terminal. Γ ` e1 ⇒ v1 Γ ` e2 ⇒ v2 Γ ` e1 + e2 ⇒ v1 + v2 ADD Γ ` i⇒ i INT x ∈ Γ Γ ` x⇒ Γ(x) LOOKUP Γ ` e1 ⇒ v1 Γ ` e2 ⇒ v2 v1 = v2 Γ ` e1 = e2 ⇒ T TEST1 Γ ` e1 ⇒ v1 Γ ` e2 ⇒ v2 v1 6= v2 Γ ` e1 = e2 ⇒ F TEST2 Γ, s ∗→ Γ\u2032 Γ, s→ Γ\u2032 CLOSURE Γ ` e⇒ v Γ, x = e→ (x, v) :: Γ INTRO Γ ` t⇒ T Γ, s→ Γ\u2032 Γ\u2032, while (t) s→ Γ\u2032\u2032 Γ, while (t) s→ Γ\u2032\u2032 WHILE1 Γ ` t⇒ F Γ, while (t) s→ Γ WHILE2 ∅, s→ Γ Γ ` e⇒ v Js; return eK = v PROGRAM Figure 2: Semantics of the language, presented as inference rules. The semantics is defined as four predicates formalizing the evaluation of expressions (Γ ` e⇒ v), single statement step (Γ, s⇒ Γ), the reflexive and transitive closure of statements (Γ, s ∗→ Γ), and the evaluation of the program overall (JpK = v). 3 Under review as a conference paper at ICLR 2017 Method Accuracy (%) Unigram features + logistic regression .5 Unigram features + multilayer perceptron .5 Bigram features + logistic regression .5 Bigram features + multilayer perceptron .5 Embedding features + logistic regression .5 Embedding features + multilayer perceptron .5 Hidden Markov model .57 Recurrent neural network .62 Long short-term memory network .99 Table 1: Accuracy of different learning algorithms on the static analysis task. LR stands for logistic regression, MLP stands for multilayer perceptron, HMM stands for hidden Markov Model, RNN stands for recurrent neural network, LSTM stands for Long Short-Term Memory. four predicates. The predicate Γ ` e ⇒ v denotes the value v resulting from evaluating e in the environment Γ. The environment is simply a list of bindings from variables to their values. We present four rules that define this predicate, ADD, INT, LOOKUP, TEST1, and TEST2. The most important is the LOOKUP rule which states that the value of a variable is the value associated to it in the environment. Note that this is only well-defined if the variable actually is in the environment, otherwise the semantics is undefined. The goal of our static analyzer is to ensure this can never happen. The predicate Γ, s ⇒ Γ denotes the execution of a statement that transforms the environment by adding variable bindings to it. For example, the INTRO rule shows that a variable assignment adds a variable binding to the environment, The CLOSURE rule states that a possible transition is the reflexive and transitive execution of a single statement Γ, s ∗→ Γ. The rules WHILE1 and WHILE2 formalize the execution of a while loop. Finally, the predicate JpK = v denotes the evaluation of a complete program into a resulting value. 2.2 THE TASK Now that we have presented the language, we can state more precisely the goal of the static analysis. A program such as \u201Cv1 = 4; return v1 + v2;\u201D, while syntactically valid, is not well-defined since variable v2 has not been defined. A static analyzer is a function that takes such a program as an input and returns a Boolean value. analyze : token sequence 7−→ Boolean Function analyze should return true only if every variable is defined before it is used. We chose the input to be the sequence of tokens of the program rather than the raw characters for simplicity. It is easy to define such a function directly, but our goal is to see whether we can learn it from examples. Note that unlike previous work combining static analysis and machine learning, we are not trying to improve a static analyzer using machine learning, but rather learning the static analyzer completely from data. 3 LEARNING A STATIC ANALYZER To learn the static analyzer, we compile a balanced set of examples in which programs are labeled with a single Boolean value indicating whether the program should be accepted or not. The dataset contains 200,000 examples, half of which are valid programs and half of which are invalid programs. The invalid programs are of two forms. Half of them contain variables that have not been defined at all, the other half contains programs where the order of statements has been swapped and a variable use appears before its definition. Note that this swapping of statements results in documents that have the exact same bag-of-words, but different labels. Of the 200,000 examples, we use 150,000 as the training set and 50,000 as the test set while making sure to respect a perfect balance between valid and invalid programs. 4 Under review as a conference paper at ICLR 2017 To create this dataset, we have built our own compiler and example generator for our language. The example generator only produces syntactically valid programs. The programs are generated using a variety of random decisions: for example, when trying to generate a statement, we must decide with what probability we want to choose a variable assignment versus a while loop or another type of statement. We vary the probability to try to avoid producing a dataset with a spurious signal, but this is a very delicate issue. We also try our classifiers on hand-written programs. We apply several different machine learning methods, including LSTM to the problem (described below) and present results in Table 1. N-grams and classification We attempt to learn the static analyzer using a classic approach of feature engineering followed by classification. We try both unigram and bigrams features and clas- sify the examples using either a linear logistic regression or a non-linear multilayer perceptron. We expect this approach to fail since n-gram features fail to capture statement ordering, and this serves as a test to make sure our dataset does not contain any spurious signal. Indeed, these methods do not perform better than random. Sequence embedding and classification We also attempt to use an LSTM for our feature engi- neering. In this case, we first train an LSTM as language model. Then, for classification, we first execute our language model on the example program and use the last hidden state as an embedding. This embedding is used as an input to both a logistic regression and a multilayer perceptron. This approach fails as well and does not perform better than random. It is important to note that we might also consider using an RNN encoder-decoder to produce the embedding but we leave this for future work. Sequence classication Finally, we tried three model-based approaches to sequence classification. First, we tried to use an HMM trained using the Baum-Welch algorithm. Second, we tried to train a vanilla RNN with a cross-entropy loss using stochastic gradient descent (SGD). Third, we tried to train an LSTM with cross-entropy loss and SGD. More precisely, we use the variant of SGD known as RMSProp. In both cases we used the Keras framework with the Theano backend. These sequence classification approaches perform better than the other approaches. However, the HMM and the RNN still perform poorly. Interestingly, the LSTM can achieve an accuracy of 98.9%. The training of the LSTM is very robust, we did not need to do any complicated parameter search to obtain these results. The false positive rate (i.e. the program is correct but predicted as faulty) is 0.9% and the false negative rate (i.e. the program is faulty but classified as correct) is 0.2%. In conclusion, an out-of-the-box LSTM achieves a promising accuracy on this task. Interestingly, none of the other approaches including HMM or RNN, could deliver satisfactory results. It should be noted that it might be possible to improve the accuracy of the LSTM by extending it with a differentiable data structure (Graves et al., 2014) (one that would implement a set interface) but we leave this for future work. 4 REPORTING USEFUL ERROR MESSAGES While the above experiment demonstrates that it is possible to learn an accurate static analyzer; practically, such an analyzer is somewhat useless unless we can also help the programmer locate the potential errors. That is, imagine if a tool reported that there is a potential buffer overflow in your code base without any indication of where the problem is: it would not be of much use. Therefore we train a second LSTM as a language model over true instances of our programming language. That is, we train the LSTM to predict the next character in the sequence, and for every character in the sequence, the model provides the probability of observing this specific character. The idea is that we want to look at all the variable-use in the program and if the probability of this variable use is below a certain threshold, then we report the use as a potential source of error. We present several such examples in Figure 3. We color a variable-use in blue if its probability is above the threshold and in purple if it is below the threshold and therefore potentially the source of the error. 5 Under review as a conference paper at ICLR 2017 1. v1 = 37; v2 = (v 1 + 20) ; 2. v1 = 37; v1 = (v 2 + 20) ; 3. v2 = 37; v1 = (v 2 + 20) ; 4. v2 = 37; v2 = (v 2 + 20) ; 5. v2 = 37; v2 = (v 2 + 20) ; v3 = (v 2 + 40) ; 6. v2 = 37; v2 = (v 2 + 20) ; v2 = (v 3 + 40) ; 7. v2 = 37; v2 = (v 2 + 20) ; v3 = (v 1 + v 2 ) ; 8. v2 = 37; v1 = (v 2 + 20) ; v3 = (v 1 + v 2 ) ; 9. v1 = 37; v2 = (v 2 + 20) ; v3 = (v 1 + v 2 ) ; 10. v1 = 37; v3 = (v 2 + 20) ; v5 = (v 3 + v 4 ) ; 11. v1 = 37; v3 = (v 2 + 20) ; v5 = (v 3 + v 2 ) ; 12. v1 = 37 v2 = (v 1 + 20) ; 13. v1 = 37 v1 = (v 2 + 20) ; Figure 3: Example of programs annotated with variable usage. The use colored in blue are consid- ered to have been properly defined while the use in purple are considered to be faulty. This tool is run when the classifier detects a program error to help the programmer understand what the problem is. 6 Under review as a conference paper at ICLR 2017 As we can see from the examples, the method works well. The first four examples show simple cases with only two variables. Note that from the perspective of a bag-of-words classifier, these two programs are identical. Yet the LSTM language model, which takes into account the \u201Cword\u201D order is able to model them differently. Examples 5-11 are more complicated in that the variables are used or defined several times. In Example 9, the language model accurately reports the first use of v2 as incorrect and the second use of v2 as correct. This is a somewhat interesting example as the incorrect use of v2 is in the definition of v2 itself. In example 10, we can see that the language model can handle multiple incorrect variable uses; this success crucially depends on the ability of the language model to recover from the error and still accurately model the remainder of the program. Finally, examples 12 and 13 demonstrate robustness. Despite the fact that these two examples are syntactically incorrect, the language model correctly reports the semantic errors. The resilience of the learned tools to small errors is part of what makes them so promising for program analysis. 5 RELATED WORK There is a growing body of work in employing machine learning to improve programming language tools. In such works, machine learning is used to complement the traditional static analysis methods; further, they rely on extensive feature engineering. In Brun & Ernst (2004), dynamic analysis is used to extract features that are used to detect latent code errors. In Kolter & Maloof (2006), n- gram features are used to detect viruses in binary code. In Yamaguchi et al. (2012), parts of the abstract syntax tree of a function is embedded into a vector space to help detect functions similar to a known faulty one. In Tripp et al. (2014), various lexical and quantitative features about a program is used to improve an information analysis and reduce the number of false alarms reported by the tool. In Raychev et al. (2015), dependency networks are used with a conditional random field to de-obfuscate and type Javascript code. In Allamanis et al. (2015), the structure of the code is used to suggest method names. In Nguyen & Nguyen (2015), n-grams are used to improve code completion tools. In Gvero & Kuncak (2015), program syntax is used to learn to tool that can generate Java expressions from free-form queries. In Long & Rinard (2016), a feature extraction algorithm is designed to improve automatic patch generation. 6 CONCLUSION We have shown that it is possible to learn a static analyzer from data. Even though the problem we address is particularly simple and on a toy language, it is interesting to note that in our experiments, only LSTM networks provided a reasonable enough solution. We have also shown that it is possible to make the static analyzer useful by using a language model to help the programmer understand where to look in the program to find the error. Of course, this experiment is very far from any practical tool. First, dealing with more complicated programs involving memory, functions, and modularity should be vastly more complex. Also, our solution is very brittle. For example, in our language, the space of variable names is very restricted, it might be much more difficult to deal with normal variable names where a specific variable name could not appear at all in the training dataset. Finally, a fundamental issue are false positives, that is, programs that are wrongly classified as being without error. This is a serious problem that may make such a tool risky to use. However, note that there are useful programming language tools that indeed generate false positive. For instance, a tool that report buffer overflows might not catch every error, but it is still useful if it catches some. Another possibility is to consider approaches were a result is verified by an external tools. For example, in the field of certified compilation, Tristan & Leroy (2008) have shown that it can be acceptable to use an untrusted, potentially bogus, program transformation as long as each use can be formally checked. Also, as exemplified by Gulwani & Necula (2003; 2004; 2005) some static analysis algorithms do trade a small amount of unsoundness for much faster computation, which can be necessary when applying programming tools to very large code base. 7 Under review as a conference paper at ICLR 2017 REFERENCES Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class names. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC\/FSE 2015, pp. 38\u201349, New York, NY, USA, 2015. ACM. ISBN 978-1- 4503-3675-8. doi: 10.1145\/2786805.2786849. URL http:\/\/doi.acm.org\/10.1145\/ 2786805.2786849. Yuriy Brun and Michael D. Ernst. Finding latent code errors via machine learning over program executions. In Proceedings of the 26th International Conference on Software Engineering, ICSE \u201904, pp. 480\u2013490, Washington, DC, USA, 2004. IEEE Computer Society. ISBN 0-7695-2163-0. URL http:\/\/dl.acm.org\/citation.cfm?id=998675.999452. Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. CoRR, abs\/1511.01432, 2015. URL http:\/\/arxiv.org\/abs\/1511.01432. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs\/1410.5401, 2014. URL http:\/\/arxiv.org\/abs\/1410.5401. Sumit Gulwani and George C. Necula. Discovering affine equalities using random interpretation. In Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201903, pp. 74\u201384, New York, NY, USA, 2003. ACM. ISBN 1-58113-628-5. doi: 10.1145\/604131.604138. URL http:\/\/doi.acm.org\/10.1145\/604131.604138. Sumit Gulwani and George C. Necula. Global value numbering using random interpretation. In Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan- guages, POPL \u201904, pp. 342\u2013352, New York, NY, USA, 2004. ACM. ISBN 1-58113-729-X. doi: 10.1145\/964001.964030. URL http:\/\/doi.acm.org\/10.1145\/964001.964030. Sumit Gulwani and George C. Necula. Precise interprocedural analysis using random interpretation. In Proceedings of the 32Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201905, pp. 324\u2013337, New York, NY, USA, 2005. ACM. ISBN 1-58113-830- X. doi: 10.1145\/1040305.1040332. URL http:\/\/doi.acm.org\/10.1145\/1040305. 1040332. Tihomir Gvero and Viktor Kuncak. Synthesizing java expressions from free-form queries. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Pro- gramming, Systems, Languages, and Applications, OOPSLA 2015, pp. 416\u2013432, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-3689-5. doi: 10.1145\/2814270.2814295. URL http:\/\/doi.acm.org\/10.1145\/2814270.2814295. J. Zico Kolter and Marcus A. Maloof. Learning to detect and classify malicious executables in the wild. J. Mach. Learn. Res., 7:2721\u20132744, December 2006. ISSN 1532-4435. URL http: \/\/dl.acm.org\/citation.cfm?id=1248547.1248646. Zachary Chase Lipton. A critical review of recurrent neural networks for sequence learning. CoRR, abs\/1506.00019, 2015. URL http:\/\/arxiv.org\/abs\/1506.00019. Fan Long and Martin Rinard. Automatic patch generation by learning correct code. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan- guages, POPL \u201916, pp. 298\u2013312, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-3549- 2. doi: 10.1145\/2837614.2837617. URL http:\/\/doi.acm.org\/10.1145\/2837614. 2837617. Anh Tuan Nguyen and Tien N. Nguyen. Graph-based statistical language model for code. In Proceedings of the 37th International Conference on Software Engineering - Volume 1, ICSE \u201915, pp. 858\u2013868, Piscataway, NJ, USA, 2015. IEEE Press. ISBN 978-1-4799-1934-5. URL http:\/\/dl.acm.org\/citation.cfm?id=2818754.2818858. Veselin Raychev, Martin Vechev, and Andreas Krause. Predicting program properties from \u201Dbig code\u201D. In Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201915, pp. 111\u2013124, New York, NY, USA, 2015. ACM. ISBN 978- 1-4503-3300-9. doi: 10.1145\/2676726.2677009. URL http:\/\/doi.acm.org\/10.1145\/ 2676726.2677009. 8 http:\/\/doi.acm.org\/10.1145\/2786805.2786849 http:\/\/doi.acm.org\/10.1145\/2786805.2786849 http:\/\/dl.acm.org\/citation.cfm?id=998675.999452 http:\/\/arxiv.org\/abs\/1511.01432 http:\/\/arxiv.org\/abs\/1410.5401 http:\/\/doi.acm.org\/10.1145\/604131.604138 http:\/\/doi.acm.org\/10.1145\/964001.964030 http:\/\/doi.acm.org\/10.1145\/1040305.1040332 http:\/\/doi.acm.org\/10.1145\/1040305.1040332 http:\/\/doi.acm.org\/10.1145\/2814270.2814295 http:\/\/dl.acm.org\/citation.cfm?id=1248547.1248646 http:\/\/dl.acm.org\/citation.cfm?id=1248547.1248646 http:\/\/arxiv.org\/abs\/1506.00019 http:\/\/doi.acm.org\/10.1145\/2837614.2837617 http:\/\/doi.acm.org\/10.1145\/2837614.2837617 http:\/\/dl.acm.org\/citation.cfm?id=2818754.2818858 http:\/\/doi.acm.org\/10.1145\/2676726.2677009 http:\/\/doi.acm.org\/10.1145\/2676726.2677009 Under review as a conference paper at ICLR 2017 Omer Tripp, Salvatore Guarnieri, Marco Pistoia, and Aleksandr Aravkin. Aletheia: Improving the usability of static security analysis. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, CCS \u201914, pp. 762\u2013774, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2957-6. doi: 10.1145\/2660267.2660339. URL http:\/\/doi.acm. org\/10.1145\/2660267.2660339. Jean-Baptiste Tristan and Xavier Leroy. Formal verification of translation validators: A case study on instruction scheduling optimizations. In Proceedings of the 35th Annual ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages, POPL \u201908, pp. 17\u201327, New York, NY, USA, 2008. ACM. ISBN 978-1-59593-689-9. doi: 10.1145\/1328438.1328444. URL http: \/\/doi.acm.org\/10.1145\/1328438.1328444. Zhengzheng Xing, Jian Pei, and Eamonn Keogh. A brief survey on sequence classification. SIGKDD Explor. Newsl., 12(1):40\u201348, November 2010. ISSN 1931-0145. doi: 10.1145\/1882471.1882478. URL http:\/\/doi.acm.org\/10.1145\/1882471.1882478. Fabian Yamaguchi, Markus Lottmann, and Konrad Rieck. Generalized vulnerability extrapola- tion using abstract syntax trees. In Proceedings of the 28th Annual Computer Security Appli- cations Conference, ACSAC \u201912, pp. 359\u2013368, New York, NY, USA, 2012. ACM. ISBN 978- 1-4503-1312-4. doi: 10.1145\/2420950.2421003. URL http:\/\/doi.acm.org\/10.1145\/ 2420950.2421003. 9 http:\/\/doi.acm.org\/10.1145\/2660267.2660339 http:\/\/doi.acm.org\/10.1145\/2660267.2660339 http:\/\/doi.acm.org\/10.1145\/1328438.1328444 http:\/\/doi.acm.org\/10.1145\/1328438.1328444 http:\/\/doi.acm.org\/10.1145\/1882471.1882478 http:\/\/doi.acm.org\/10.1145\/2420950.2421003 http:\/\/doi.acm.org\/10.1145\/2420950.2421003 Introduction A Static Analysis Task The Language The Task Learning a Static Analyzer Reporting Useful Error Messages Related Work Conclusion ","flair":"three\tResearch"}
{"author":"adamnemecek","created":"Mon Oct 24 15:24:36 EDT 2016","text":"This project is still under active development and not guaranteed to have a stable API. This is especially true because the TensorFlow C API used by this project has not yet stabilized. If you only intend to use TensorFlow from within Rust, then you don't need to build TensorFlow manually and can follow the automatic steps. If you do need to use TensorFlow outside of Rust, the manual steps will provide you with a TensorFlow header file and shared library that can be used by other languages. Install SWIG and NumPy. The version from your distro's package manager should be fine for these two. Also install Bazel, which you may need to do from source. Then run . Since TensorFlow is built during this process, and the TensorFlow build is very memory intensive, we recommend using the flag which tells cargo to use only one task, which in turn tells TensorFlow to build with only on task. Of course, if you have a lot of RAM, you can use a higher value. To include the especially unstable API (which is currently the module), use . Install TensorFlow from source. The Python\/pip steps are not necessary, but building is. Copy $TENSORFLOW_SRC\/bazel-bin\/tensorflow\/libtensorflow_c.so to \/usr\/local\/lib. If this is not possible, add $TENSORFLOW_SRC\/bazel-bin\/tensorflow to LD_LIBRARY_PATH. If you are building TensorFlow version 0.9.0 or earlier, use $TENSORFLOW_SRC\/bazel-bin\/tensorflow\/libtensorflow.so instead. You may need to run to reset 's cache after copying libtensorflow.so. Now run as usual. To include the especially unstable API (which is currently the module), use . RFCs are issues tagged with RFC. Check them out and comment. Discussions are welcome. After all, thats what a Request For Comment is for! The especially unstable parts of the API (which is currently the modul) are feature-gated behind the feature to prevent accidental use. See http:\/\/doc.crates.io\/manifest.html#the-features-section. (We would prefer using an attribute, but that doesn't exist yet.) This project is not directly affiliated with the TensorFlow project, although we do intend to communicate and cooperate with them. Developers and users are welcome to join #tensorflow-rust on irc.mozilla.org. See CONTRIBUTING.md for information on how to contribute code. This is not an official Google product.","flair":"four\tProject"}
{"author":"rmltestaccount","created":"Sun Oct 09 11:16:32 EDT 2016","text":"The WaveNet neural network architecture directly generates a raw audio waveform, showing excellent results in text-to-speech and general audio generation (see the DeepMind blog post and paper for details). The network models the conditional probability to generate the next sample in the audio waveform, given all previous samples and possibly additional parameters. After an audio preprocessing step, the input waveform is quantized to a fixed integer range. The integer amplitudes are then one-hot encoded to produce a tensor of shape . A convolutional layer that only accesses the current and previous inputs then reduces the channel dimension. The core of the network is constructed as a stack of causal dilated layers, each of which is a dilated convolution (convolution with holes), which only accesses the current and past audio samples. The outputs of all layers are combined and extended back to the original number of channels by a series of dense postprocessing layers, followed by a softmax function to transform the outputs into a categorical distribution. The loss function is the cross-entropy between the output for each timestep and the input at the next timestep. In this repository, the network implementation can be found in model.py.","flair":"four\tProject"}
{"author":"pkmital","created":"Sun Oct 16 18:14:44 EDT 2016","text":"Students will be required to submit 5 courseworks completing the given iPython\/Jupyter notebooks and 1 open-ended final project. This course requires a total of approximately 50-60hours of work, including both lectures and independent student learning time. Plagiarism:We learn by doing our own work, and by collaborating with other students. Discussing course content and assignments with your peers is an important and helpful way to deepen your learning. However, encouraging others to copy your homework and submit it as their own is a form of cheating. So please don't post your completed assignments or correct answers to quizzes, tests, or other assessments to the discussion forums or in repositories outside of Kadenze.","flair":"two\tNews"}
{"author":"Franck_Dernoncourt","created":"Wed Oct 05 00:12:46 EDT 2016","text":"I am looking for benchmarks based on neural networks libraries (Theano\/TensorFlow\/Torch\/Caffe\/\u2026) to compare the performance between different GPUs.\n\nI am aware of:\n\n- https:\/\/github.com\/jcjohnson\/cnn-benchmarks (CNN in Torch)\n- https:\/\/github.com\/jcjohnson\/neural-style#speed (CNN in Torch)\n- https:\/\/github.com\/glample\/rnn-benchmarks (vanilla RNNs and LSTM in Theano, TensorFlow, and Torch)\n\nWhat are some other benchmark codes?","flair":"null\tnull"}
{"author":"scardax88","created":"Tue Oct 04 04:03:10 EDT 2016","text":"Piotr Mirowski is a Research Scientist at Google DeepMind, the research lab focused on \u201Csolving intelligence\u201D, and investigating new research directions such as deep reinforcement learning or systems neuroscience. Dr. Mirowski has a decade-long experience on machine learning at its highest level: after obtaining his Ph.D. in computer science (2011) at New York University under the supervision of Prof. Yann LeCun, he worked at Bell Labs and Microsoft Bing before joining DeepMind. Before that, he worked as a research engineer at Schlumberger Research. We interviewed him on his activities, the current rise of deep learning, and the new challenges to look forward in AI. Piotr Mirowski will hold a tutorial on \u201CLearning Sequences\u201D on the 24th October \u2013 do not miss it if you attend the conference! INNS BigData: Can you describe briefly your research interests and main activities? Piotr: The common thread running through my research interests is learning sequences. It started when, in 2002 and freshly out of engineering school, I was working at Schlumberger on geological logs, classifying rocks deposited through sedimentary processes using a combination of neural networks and hidden Markov models. It continues today when I train robotic agents using deep reinforcement learning at DeepMind. Recurrent neural networks and non-convex optimization are my everyday tools now, even though, in the past, I have worked on data-poor applied problems that required a different approach (e.g. learning gene regulation networks from sequences of mRNA or electric load forecasting from hourly transformer measurements). Today, in my work at DeepMind, I am not only interested in solving a time series prediction problem, but also in discovering insight about a sequence by learning its possible representations in the hidden units of an RNN, revisiting with fresh eyes some of the work I did during my PhD with Yann LeCun at NYU. INNS BigData: Since 2014, you work as a research scientist at Google DeepMind. What project(s) are you involved with? Piotr: I work within the deep learning research team, which focuses on fundamental research in various aspects of representation learning. We work primarily on image and sound data (there are other teams at DeepMind who work on text or other type of structured or unstructured data). This research can include training generative models of images building upon variational auto-encoders or deep reinforcement learning for learning to control agents playing games. Whilst I am still in the process of publishing my current work, I can mention some of the work published by my colleagues from the deep learning research team, which includes Pixel RNN, PixelCNN and WaveNet, which is their application to voice synthesis. Unsupervised Learning of 3D Structure from Images, Asynchronous Deep Reinforcement Learning which beats the previous Deep Q-Learning baseline for playing Atari games, or Progressive Neural Nets, which are a way to address the problem of continual learning across a variety of different tasks (without forgetting how to solve earlier tasks). INNS BigData: DeepMind is on a quest to \u201Csolve intelligence\u201D. After mastering deep learning, what is the next step according to your view? Piotr: I hope that not all of deep learning is considered as having been mastered yet! What may have been mastered are simple pattern recognition tasks with fully labelled supervision. The next steps, on which we are working at DeepMind, are to learn autonomous agents solving increasingly complex tasks in increasingly complex environments, where the only labelled supervision comes from very sparse rewards during reinforcement learning. The challenges now are to learn good representations about the environment, to make predictions about it, and to learn a fully differentiable memory to solve complex tasks in one-shot. INNS BigData: Before DeepMind, you worked at Microsoft and the Bell Laboratories. Have you found comparable research environments? Piotr: Bell Labs was defined, from the forties to the nineties, as the ideal corporate research environment. The management was very supportive of research carried out by individuals and had the courage of undertaking ambitious and risky projects. The funding model was simple and came directly from AT&T operations. For more details, I recommend reading The Idea Factory, a book that gives thrilling examples of achievements at Bell Labs: the first transatlantic cable, the transistor, the first telecommunication satellite Telstar or CCD cameras. What one could add to that book is the impact of research in statistical learning and pattern recognition at Bell Labs, which resulted in such methods as Support Vector Machines, Boosting and of course, Convolutional Networks and the beginning of Deep Learning. I worked at Bell Labs in Murray Hill, NJ, between 2011 and 2013, with Tin Kam Ho, and I tremendously benefitted from the research freedom and Tin\u2019s support, the academic focus, and the inspiration from Bell Labs\u2019 history. I then briefly worked in a product team at Microsoft Bing, and greatly enjoyed interacting with colleagues from Microsoft Research. I believe that today, several corporate research labs, including DeepMind, Microsoft Research and a few others have taken the relay of ambitious academic research in the tradition of Bell Labs. What makes DeepMind stand out are focus and collaboration as well as academic rigour. Researchers are not only encouraged to conduct independent research but also to participate in regular, lab-wide discussions. As a result, most of the papers come from the joint effort of different teams. We all work towards the same goals (solving artificial general intelligence), starting from different perspectives. INNS BigData: Back to deep learning, it has already revolutionized image and audio processing, and many agree that the next challenges await in the biomedical field. What should we expect in the near future? Piotr: The analysis of medical data is definitely a field that could benefit from advances in deep learning, and DeepMind Health has just begun exploring whether ML methods could reduce the amount of time it takes to plan radiotherapy treatment for head and neck cancers. Another interesting field of application of research in AI is on text understanding, as illustrated by the recent push into conversational bots. INNS BigData: More in general, which are going to be the biggest technological challenges in the future, according to your point of view? Training deep learning models is expensive in terms of energy and time, especially when one considers doing all the possible hyper-parameter sweeps. While a few of my colleagues at DeepMind have successfully used machine learning to reduce the energy footprint of data servers, the models and problems keep growing in size and complexity. A solution could come from new hardware or from machine learning algorithm themselves. INNS BigData: To close, what would you say to a student starting today his\/her PhD in machine learning? Piotr: The PhD will be your most intense and fulfilling educational experience! Learn more about the activities of Dr. Mirowski on his blog and his Twitter account! Simone Scardapane is a post-doc fellow in La Sapienza, Rome. He is enthusiast about machine learning technologies, and he is helping as a publicity chair for the conference.","flair":"null\tnull"}
{"author":"anantzoid","created":"Tue Nov 15 10:41:59 EST 2016","text":"Altify automizes the task of inserting alternative text attributes for image tags. Altify uses Microsoft Computer Vision API's deep learning algorithms to caption images in an HTML file and returns a new HTML file in which alt attributes are filled out with their corresponding captions. Notice: Altify will now ignore any image tag whose alt attribute has content or is just an empty string. (In compliance with standard web practices) A new HTML file called altify.html is created next to the HTML file you selected. (Or Desktop, when using older versions) A close up of a cat looking at the camera Humans are currently better at captioning images than machines. Use responsibly!","flair":"four\tProject"}
{"author":"Staturecrane","created":"Thu Oct 20 10:28:35 EDT 2016","text":"A combination of the DCGAN implementation by soumith and the variational autoencoder by Kaixhin. The model produces 64x64 images from inputs of any size via center cropping. You can modify the code relatively easily to produce different sized outputs (adding more convolutional layers, for instance), as well as to rescale images instead of cropping them. Images are randomly flipped horizontally to get better coverage on training data. I have added white noise to the original inputs that go through the discriminator after reading this post on stabilizing GANS. The noise level is annealed over time to help the generator and discriminator converge. To run, execute the script using where the input folder is expected to contain color images. The model resamples the training set after every epoch so as to fit on a GPU and still (eventually) sample all of the data. \"Output\" is for samples generated by the model, and \"reconstructions folder\" is to just save some reconstructions from the training set, to see how the VAE is doing (it's not going to do particularly well, but that's okay; it's there to assist the GAN).","flair":"four\tProject"}
{"author":"gimortz","created":"Sat Oct 08 03:53:25 EDT 2016","text":"See how useAIble has reimagined neural networks and generates better decisions in less time and with less effort than other artificial neural networks. In this first episode of the blog series \u201CRunning with Scissors\u201D, watch useAIble challenge Google TensorFlow to see which engine can learn to play the classic arcade game Lunar Landing faster. Then try it out for yourself at http:\/\/useaible.com\/machine-learning-...","flair":"null\tnull"}
{"author":"Neb519","created":"Thu Nov 03 17:44:13 EDT 2016","text":"Hey everyone,\n\nI recently published [this write-up](https:\/\/www.kaggle.com\/forums\/f\/15\/kaggle-forum\/t\/25058\/mltools-my-r-package-for-machine-learning-tools) on Kaggle describing my (non CRAN'ed) R package, [mltools](https:\/\/github.com\/ben519\/mltools), with various functions and techniques I use for machine learning.  I think it's a good read for anyone interested in machine learning and competing on Kaggle - not just R users.","flair":"four\tProject"}
{"author":"clbam8","created":"Tue Nov 15 22:32:53 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.02344 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: A Convolutional Encoder Model for Neural Machine Translation Authors: Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin (Submitted on 7 Nov 2016 (v1), last revised 17 Nov 2016 (this version, v2)) Abstract: The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. Comments: 13 pages Subjects: Computation and Language (cs.CL) Cite as: arXiv:1611.02344 [cs.CL]   (or arXiv:1611.02344v2 [cs.CL] for this version) Submission history From: Michael Auli [view email] [v1] Mon, 7 Nov 2016 23:46:45 GMT (175kb,D) [v2] Thu, 17 Nov 2016 01:45:37 GMT (175kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"slap_bet","created":"Wed Oct 12 12:06:50 EDT 2016","text":"I'm currently writing a couple of papers which use CNNs and one thing I really struggle with is making nice looking diagrams that are actually clear. I've seen figures in other papers (for example, picturing convolutional layers as cubes, stuff like that) that seem to be very clear but I haven't really figured out 1) what they're using to make those and 2) there is not really an agreed upon style for enumerating and illustrating network design. Does anyone have any light to shed here, is there a drawing tool (LaTeX compatible) that is good for this kind of thing or is it just lots and lots of tikz?","flair":"one\tDiscussion"}
{"author":"princealiiiii","created":"Wed Nov 23 01:17:08 EST 2016","text":"Over the last few years, researchers have applied deep learning to all sorts of datasets in biology and medicine. On almost every problem \u2014 from identifying protein-DNA interactions to diagnosing Alzheimer\u2019s disease from brain scans \u2014 deep learning techniques have performed remarkably well, leaving traditional machine learning in the dust. These results are somewhat surprising \u2014 deep learning was inspired by the architecture of the human nervous system, which explains why it works so well on conventional, \u201Chuman\u201D tasks like image recognition and natural language processing. But what about tasks like sifting through large amounts of DNA sequences and identifying sites that bind to proteins \u2014 why do neural networks perform so well? A convolutional neural network can take a look at this fMRI scan, and determine if it comes from a patient with Alzheimer\u2019s. In this post, I examine 4 fundamental deep learning architectures, and consider why they are suited to solving problems in medicine and functional genomics. But before that, let\u2019s start with machine learning basics. Machine learning is pattern recognition. Historically, the earliest patterns that computers were tasked to recognize were linear patterns that mapped input data, represented in the form of feature vectors, to output data, represented as discrete or continuous scalar labels. Linear models can only describe certain kinds of datasets, and so a long time ago, researchers found ways of transforming non-linear relationships into linear ones, which were then easily tackled by standard linear machine learning algorithms. However, these approaches required knowing (or guessing) the explicit non-linear relationships between the input data and output data, and \u201Chand-engineering\u201D features to remove these non-linearities. The focus of this guide is not on these traditional approaches, but on deep learning and its corresponding architecture, deep neural networks. The beauty and power of deep neural networks (or \u201Cnets\u201D) is that computers are made to guess the nonlinear relationships within the dataset on their own. The beauty is the simplicity: just as a typical machine learning method iteratively guesses the linear weights that maps inputs to outputs, neural networks iteratively guess the nonlinear function that best map the inputs to the outputs. The difference, of course, is that the set of nonlinear functions that could possibly map inputs to outputs is much larger than the set of linear functions. So how do neural networks do it? Can neural networks really really find any possible non-linear function that relates inputs and outputs? It turns out that the answer is yes, a standard neural network with just 1 hidden layer (more on that layer later) can approximate any continuous, non-linear function! This is a useful theoretical result, but practically speaking, it doesn\u2019t mean much, as the theorem says nothing about the computation time or memory required to perform this approximation for an arbitrary function. Yet it turns out that most \u201Creal-world\u201D non-linear datasets are characterized by certain properties that are well-suited to learning by deep neural nets, meaning that neural net-based algorithms are highly efficient in capturing patterns within those datasets. Although it is not necessary to know these properties to use neural nets, it is interesting to think about how these properties, such as locality and symmetry, connect to the fundamental physical processes that generate the data, as well as which properties are emphasized in each of the 4 architectures that we will discuss. Let\u2019s start by considering a classic problem in functional genomics. Transcription Factor Binding: during transcription (the process in which DNA sequences are read and converted to RNA), a variety of proteins bind to the DNA molecule being transcribed. These proteins, called transcription factors, have different effects: some may promote the process of transcription, while others will inhibit it. It is of great interest to researchers to know exactly where (i.e. what part of the DNA sequence) a particular transcription factor will bind. To learn this information, biologists can conduct experiments that provide \u201Crough\u201D answers: for example, a researcher might do an experiment with 500 DNA sequences, each consisting of 40 nucleotide bases (a typical 40-base sequence might be: where each of A, T, C, and G represents one base). We may learn through experiments that the transcription factor binds to the first 200 sequences very strongly, to the next 200 weakly, and not at all to the last 100 sequences. This gives us some information about DNA-protein binding, but doesn\u2019t completely tells us where the protein binds, because the protein only requires a particular sequence of 4 or 5 bases (this is known as a motif) to bind to, and some small variations in the motif may be OK (the protein will still bind to the sequences, but not as strongly) while other variations in the motif may disrupt binding altogether. In addition, certain global properties (such as the percentage of bases that are C and G, known as the GC-content, in the sequence) may affect binding as well. Given this dataset, how do we identify the motif (or motifs!) that the transcription factor binds most strongly to? As a first pass, you might decide to frame this as a linear regression problem: given a matrix of 500 columns, where each column encodes 1 input sequence, and a vector of labels of length 500, where each label encodes the affinity of a sequence to the protein, what weight vector maps the input matrix to the output labels most closely? Such an approach is illustrated below (you can think of each base being represented by an integer: A=0, T=1, C=2, G=3, although there is a better way to encode categorical features): The problem with this formulation is that does not take into account any non-linearities in the relationship between input sequences and binding affinity. In particular, we\u2019d like the algorithm to identify motifs, such as \u201CCCGGT\u201D that cause the transcription factor to bind. However, linear techniques So, how do we extend our model to include these nonlinear relationships? In the traditional machine learning framework, you would do \u201Cfeature engineering\u201D \u2014 select features that capture the nonlinear nature of the problem you are interested in solving. For example, in this case, it might make sense to make each feature the number of times every possible 5-nucleotide sequence (\u201C5-mer\u201D) appears in the entire 40-nucleotide sequence. However, this requires a lot of thought and domain expertise. In the framework of deep learning, we instead create a bunch of neurons, each of which takes as input a linear combination of the original features. Then, each of these neurons activate, meaning that they output a value that is a non-linear function of their input. (The particular choice of non-linear function can vary, and won\u2019t be discussed here). The output values can then be summed to produce a predicted label. This approach is shown here: A layer of neurons is known as a hidden layer, as it does not directly represent the input, nor the output, but is an intermediary between the two. You might ask \u2014 what if we were to add another layer of intermediary neurons? Deep neural networks take this concept to heart, and can have many hidden layers of neurons that feed into each other before outputting a prediction value. Like this: This allows them to capture more and more complex features. For example, the neurons in the first hidden layers might capture small motifs. The neurons in the second hidden layer might capture combinations of motifs, or global properties, like GC-content (although global properties could be captured in the first layer as well). And so on. As an aside, it is worth mentioning that the idea of having more than 1 layer only makes sense with a nonlinear activation function. If we restrict ourselves to dealing with linear activation functions, adding multiple layers offers no advantage, since a linear function of a linear combination is still a linear combination. However, linear combinations of non-linear functions can produce non-linear functions that are not part of the original set of functions. You\u2019ll notice something interesting in the 2-layer neural network that I illustrated above. The first hidden layer consists of 4 neurons, but the weights that feed into neurons 2, 3, and 4 follow a particular pattern. Each neuron has the same set of weights, but these weights are applied to different parts of the input sequence. It\u2019s not surprising that the neural network has learned this pattern of weights, since (if trained on a good dataset,) it should learn to identify the same motif, wherever it may be in the sequence. Therefore, we might expect one neuron to search for the motif at the beginning of the beginning of the DNA sequence, another to search in the middle, and another near the end of the sequence. If we already expect the weights to take this form, why not use a neural network architecture that requires all of our neurons in a given layer to learn the same weights, only translated across the input sequence? What\u2019s the advantage of doing this? Because this significantly reduces the number of free parameters in our model, which in turn cuts down the computational time and memory, as well as reduces the chance of overfitting our data. This kind of architecture is known as convolutional neural network, and a layer that enforces the constraint just described is known as a convolution layer. Again, the power of convolution layers is that they tend to pick the \u201Cright\u201D patterns \u2014 those that repeat throughout our input features \u2014 all while having very few parameters that need to be learned. Here\u2019s an example of a convolutional layer that tries to learn a 5-base motif anywhere in our input sequence. You\u2019ll notice that the number of free parameters in the convolutional layer is just 5, instead of the significantly bigger number in the first hidden layer of the fully-connected neural network above. Even the fully-connected layer that follows the convolutional layer usually has a simple structure (in our cases it is uniform, because it responds uniformly to matched motifs anywhere in the DNA sequence), so we can simplify it using a max pooling layer, which further reduces the number of parameters in the model. ConvNets are in fact the architecture used by a recent paper in Nature for predicting protein-DNA interactions. Now, let\u2019s say your biologist friend comes along and tells you a little bit more about functional genomics. Now, we\u2019re faced with a whole new type of problem, one that requires keeping track of what features have been identified in our sequence so far, and adjusting our prediction weights accordingly. In the jargon of natural language processing, there is now a grammar to the DNA-protein binding problem, and taking this grammar into account requires a different way of thinking about processing data, one that is sequential,instead of singular. A neural network architecture that is suited to sequential inputs and outputs is the recurrent neural network (RNN). A simple RNN is illustrated in the following graphic. There are two fundamental differences between this RNN and the feedforward neural nets we saw earlier. 1) We are training many DNA sequences, one after another, and there is a sequential relationship between each set of input features 2) a memory unit is introduced that stores the state of the RNN and which is updated at every step in the sequence. The architecture shown here is often described as a \u201Cvanilla RNN\u201D because it is probably the simplest way you could incorporate memory into a neural network. It has some shortcomings, such as the fact that the RNN does not retain any long-term memory (the memory vector only affects the very next output). To address this shortcoming, the long short-term memory (LSTM) architecture is often used to improve accuracy and stability. A recent paper in functional genomics used a combined CNN-RNN architecture with LSTM units to achieve breakthrough performance in protein-DNA binding prediction. Let\u2019s shift gears and look at a different problem: diagnosing autism based solely on brain scans. Here are two fMRIs, one taken from a subject with ASD and one without, taken from the ABIDE dataset. Classifying which fMRI belongs to which subject falls into the category of image recognition, and a ConvNet is well-suited to the task. However, there\u2019s a problem \u2013 each of these brain scans generally consists of over a million voxels. Yet, it hard to generate even a few hundred clean images to train our ConvNet. Because the number of features vastly outnumbers the number of training examples, we are likely to overfit our model, even with a convolutional neural network. To address this problem, note that many of the voxels in the fMRIs are either blank or are similar to neighboring voxels. So, we might consider first compressing our input images into a representation that consists of fewer features. In other words, we map our highly-dimensional images into a lower-dimensional space, which we can then classify more easily. How do we know if we have a good mapping into a lower dimension? There are plenty of lower-dimensional representations that lose too much information. For example, I could multiply all of the pixel values in my original image by zero to obtain a very sparse representation. But clearly, this representation is terrible. A good way to measure how much information is lost is to see how well we can reconstruct our original image from the compressed representation. With this in mind, we can construct an autoencoder neural network that tries to minimize the difference between the original input image and one that has been compressed and then reconstructed: Unlike other architectures above, this is an unsupervised neural network. Instead of trying to minimize the difference between predicted and actual labels, there are no labels at all, and instead, the difference between input images and reconstructed images. Once we have found a compressed representation, then we can feed that into a convolutional neural network to perform the classification between patients with autism and those without autism. When analyzing datasets in medicine and biology there is surprisingly often a neural network architecture that is well-suited to the application. Although neural networks are often toted as \u201Cblind machine learning\u201D \u2014 in that they can be applied without any domain expertise \u2014 we\u2019ve seen how knowing something about the biology underlying a problem can help find the right architecture.","flair":"one\tDiscussion"}
{"author":"bronzestick","created":"Tue Nov 22 11:32:44 EST 2016","text":"In several applications using LSTMs\/RNNs (such as char-rnn or Alex Graves handwriting generation), given the input at current time-step we output (or predict) the parameters governing the distribution of the output. At training time, at each time-step, we train the model to maximize the likelihood of the true output w.r.t the predicted distribution. \n\nBut at prediction time, at the last observed time-step we predict the output distribution and sample from it to get the input at next time-step. Given this predicted input, we again obtain an output distribution from the LSTM. But this output distribution is not truly indicative of the variability in the prediction because the uncertainty from previous time-step is not propagated correctly (only a single sample is taken). This discrepancy keeps increasing as we do multi-step prediction. \n\nIs there any work in the past that dealt with this problem? This problem is not just restricted to LSTMs (or RNNs) but any predictive model which doesn't propagate uncertainty through non-linear functions. ","flair":"one\tDiscussion"}
{"author":"tsendsuren","created":"Sat Oct 22 07:54:17 EDT 2016","text":"Need help cloning? Learn how to clone a repository. Atlassian SourceTree is a free Git and Mercurial client for Windows. Atlassian SourceTree is a free Git and Mercurial client for Mac.","flair":"four\tProject"}
{"author":"primaryobjects","created":"Mon Oct 24 09:35:23 EDT 2016","text":"Can computer software be designed to be more emotional? Imagine the idea of conversing with your computer, perhaps checking the weather. The weather appears to be cold and rainy for the early part of the day. This naturally brings a certain feeling of negativity, perhaps even dread, to most people. Typical computer programs of today will simply report the weather and prompt for the next query, without giving the user\u2019s disposition a single thought. This develops little to no empathy with the user, and could even result in a negative association with the software. This is of particular impact to conversational UI, which relies on successful and repeated human interaction in order to produce results. What if a computer program using artificial intelligence, instead, responded with a more positive outlook. Perhaps, the software could mention that, while it\u2019s currently raining outside, it will warm up with clear skies, later this afternoon. This could be done without the user specifically asking about the weather later in the day. Imagine the effect this could have on the individual using the software. Would this motivate the user to interact with the computer more? A computer that can form a more intimate connection with the user could very well end up stimulating increased usage of its software, ultimately resulting in higher demand and productivity, not to mention, increased profits for the company that develops it! In this article, we\u2019re going to build a proof-of-concept program, capable of simulating emotion in software, also known as affective computing. We\u2019ll explore the methods for sentiment analysis within human responses using artificial intelligence, measuring and reacting to conversational emotions such as love and hate, and developing longer relationships between human and computer. Does this seem far-fetched? Let\u2019s give it a try! The conversational user interface has long existed as a natural language exchange between humans and computers. It\u2019s been in use for almost as long as the computer terminal has existed. Allowing users to communicate with computer software by text chat or voice is often considered to be the ultimate level of connectivity between humans and machine. Early forms of terminal programs allowed interaction with the user through a simple command prompt console. These are still found today in many text-based interfaces, including the Windows, Mac, or Linux command prompt, as well as databases, and a variety of other software. These typically involve the user entering short and specific commands to execute code and receive text responses from the software. Communication is often cryptic and difficult to read. It is also common to require multiple commands, in a sequence, to complete a particular task. It\u2019s not too difficult to see how traditional command-line text interfaces could be improved with conversational user interfaces. What might normally take five sequences of commands and responses to complete a task, could be achieved with a single sentence, communicated to the software. The response could be far more human-friendly and even cater itself towards additional tasks that suit the purpose. Advancing from command-line text interfaces to voice-based software, makes it even easier to see the power behind conversational user interfaces. With access to thousands of APIs readily available over the Internet, software can easily tap into web services to achieve far more power than ever before. When this is combined with voice-based conversational UI and advanced accuracy levels of speech recognition, it brings an entirely new level of interaction between the computer and user. Pointing and clicking will develop into complete conversations, with a growing feeling of the computer actually working \u201Cwith\u201D the user, rather than \u201Cfor\u201D the user. Naturally, this lends itself more easily to adapting emotion into the conversations, in an effort to make the software interaction even more human-like. So, how exactly can we build emotional software? Let\u2019s take a look at what is required. Creating artificially intelligent emotion within software boils down to two core concepts. First, it requires recognizing the emotion of the user. Second, it requires responding with emotion to the user. A basic table expressing these tasks, along with potential solutions, is shown below. The first requirement, recognition, can be based upon a determination of sentiment analysis within the user\u2019s phrasing. If a phrase contains negative association keywords, it\u2019s a good guess that the user is expressing negative emotional sentiment or frustration. Likewise, positive keywords, could indicate a favorable disposition of the user. By making this determination, the software can trigger the associated state for positive or negative emotional interaction. This leads is to the second requirement, which is an emotional response. Once the sentiment of the user\u2019s phrase has been determined, the software needs to formulate a response, including emotional phrasing where applicable, or standard phrasing for other cases. Since there can be variety of acceptable computer responses to a particular query, at least two (neutral, positive, and likely more) responses would be required for a query in a given state of sentiment. A simple diagram of including emotion within a conversational UI is included below. Notice, how the software\u2019s conversation consists of an array of possible responses to a query. Based on sentiment, the software may alter its response by first, proactively fetching additional data, and then responding to the user if the data is deemed favorable. This effect is created by transitioning a state machine across neutral, positive, and negative sentiment. Each response to a user\u2019s query consists of an array of sentences, and even actions. These additional actions can trigger depending on sentiment states, with additional data being fetched on behalf of the user, and only announced back to the user if deemed favorable. Of course, the user can still specifically ask for this data, but the key point is having the software empathize with the user, offering a way to \u201Ccheer up\u201D the conversation with positive data points. Let\u2019s consider an example case of a user that has requested a stock chart for the day. The figures look disappointing. The user might speak the phrase, \u201CI\u2019m really unhappy with these financial figures.\u201D or \u201COh, just great.\u201D. The software can contain three possible responses for the current state, as shown below: User Query: \u201CI\u2019m really unhappy with these financial figures.\u201DEmotional Sentiment: NegativeSoftware State: Stock price has been supplied to user.Available Responses:\u201CPlease say a ticker symbol.\u201D\u201CAlthough numbers are down, the 30-day moving average is well within positive territory.\u201D\u201CThe numbers are indeed positive, should we review the volume as well?\u201D Considering the above example, the software has determined a negative sentiment from the user, as a result of a loss on a stock purchase. A typical response to a conversational UI might be to simply prompt the user to ask about another stock symbol. However, once sentiment has been determined, the software could proactively fetch the 30-day moving average for the stock, and if positive, let the user know about this favorable news. If the additional metric turns out to be negative, the software could search for yet another metric (60-day moving average, volume, YTD gain, etc.), until a positive metric is found or the search is exhausted. Likewise, if a user expresses overtly positive emotion to a stock price, perhaps the software could proactively ask to check the stock volume as well. Given the above, it\u2019s clear that sentiment plays an important part in formulating an emotional response. Let\u2019s take a look at this in detail. The first requirement for measuring emotion within a conversational user interface, is to detect the positivity or negativity in a given conversation. To be specific, we\u2019re looking at the conversation between a human and computer. Short and simple typed commands have been replaced with flowing sentences in this context. As such, the user is conversing with the software, while performing tasks for a certain goal. If the user speaks an inaudible or invalid phrase, it might be typical for the computer to respond with a \u201Ccatch-all\u201D phrase, asking the user to repeat the question or provide a list of options. This can be unnecessary and may even invoke frustration from the user. It might be more advantageous to detect the frustration ahead of time, and offer the user alternatives for completing the task. To detect frustration from the user, we can utilize sentiment analysis. Specifically, we can try to measure the degree of positivity within a sentence and track this level of emotion throughout the conversation. If the level falls below a certain threshold, the software could take corrective action to assist the user in a friendly manner, or at the very least, show empathy. Below is an example of a more traditional interaction with software. Notice how the user expresses frustration after the computer plays the wrong song. However, the computer doesn\u2019t recognize the command and simply fails to reply. In the above conversation, the computer ceases to reply to the negatively charged spoken phrase, simply because none of the terms match any of the program\u2019s key phrases. We\u2019re lucky, in this case, that the user tries again and re-issues a new query in a different form. For some users, we might not get so lucky, in which case the user simply quits the software and gives up. The computer recognizes the new phrase, advancing its internal state (to determine a specific song), and asks the user to name the song to play. Imagine if the conversation were changed, as follows: In the above conversation, we can see a much more flowing interaction. In fact, the computer is even showing empathy when the user first expresses frustration. By reacting in a more human-like fashion, the user could be put at ease, and perhaps even find it more enjoyable working with the software. In addition, notice how the computer has tracked the context of the conversation. After the user issues a request to play a song, the conversational UI sets its context to \u201Cplaying a song\u201D. Although, it\u2019s playing the wrong song (from Green Day instead of Britney Spears), the context of \u201Cplaying a song\u201D is still valid. When the user then issues the negatively charged response, \u201CNo, I said Britney Spears\u201D, the software understands that the user wants to play a song (that\u2019s our current state) and updates the context subject from \u201CGreen Day\u201D to \u201CBritney Spears\u201D. Now, the computer can ask which song, since it already knows the artist and task, allowing the user to simply state the song title. We can dig into the state machine transition for the example scenario above. The software state might appear, as follows: In the above state flow, notice how the internal variables for the software transition according to the triggering of sentiment. Initially, the internal state was set to playing a song by a particular artist. When the user indicates that the artist was interpreted incorrectly (through a failure of voice recognition), the negative sentiment is detected and results in a transition of the subject. In addition, the song value is set to null. The software now asks the user to provide a value for the song, at which point, the state transition may now be completed. Now that we\u2019ve seen how important sentiment analysis is, for determining the state of a conversation between human and computer, let\u2019s dig into the details of calculating sentiment. It\u2019s clear that there are certain keywords and phrases that evoke negativity or frustration. If we can key off of these phrases, we can detect a change in emotion from the user and respond more appropriately to help guide them towards completing a task. One method for measuring conversational emotion is through artificial intelligence and natural language processing, with machine learning and sentiment analysis. There are a variety of different methods for sentiment analysis, each with differing degrees of accuracy. Basic techniques can include simple keyword searches, such as using the AFINN word list or other dictionary-based algorithms. Artificial intelligence based techniques include trained sentiment analysis, and often result in higher accuracies than their non-AI counterparts. ∗ Note, we\u2019re about to get into some nitty-gritty data science stuff! Feel free to skip ahead to the demo, if you prefer! As an example of the difference between keyword-based sentiment analysis and AI machine learning models, we can take a look at some simple accuracy tests on a large Twitter dataset. The dataset contains 1,600,000 records of tweets that were recorded over a given time. Each tweet was automatically scored as having positive or negative emotion by detecting the happy :) or sad :( emoticons within the body of the tweet. Below is a table showing sentiment analysis accuracy measurements using differing algorithms. Accuracy in sentiment analysis can be difficult to get just right. Standard keyword-based methods produce lower accuracy results, as they only take into account a canned list of emotionally charged terms. In addition, non-machine learning based algorithms can not increase accuracy by learning additional traits from a training set. By contrast, machine learning algorithms are capable of identifying more features in larger datasets, increasing the accuracy over larger volumes of data. Let\u2019s see how the AFINN word list scores on accuracy for sentiment analysis. AFINN is an English word list that associates a score between -5 and +5 for differing emotional terms. The terms were manually labeled. We can use R to load the Twitter dataset and process a small subset of the data for accuracy scoring of sentiment analysis. First, we\u2019ll use the following R packages: Next, we can use the following code to read the Twitter dataset for sentiment analysis and accuracy measurements. When we run the above code, we find AFINN scoring 64.64% accuracy for sentiment analysis against the Twitter dataset. This is certainly better than random chance, but it can likely be improved upon. We can also check the sentiment analysis accuracy of the Twitter dataset using a machine learning AI approach. To do this, we\u2019ll first build a corpus of the terms within the dataset. This effectively becomes our dictionary of terms. We then strip down the terms by removing punctuation, stopwords, numbers, and use stemming to make variations of the same word common. Finally, we narrow down the dictionary, using sparse terms, so that we only keep terms that appear more frequently within all of the documents. This helps speed up the training time of the machine learning algorithms. Our end result is a document term matrix, consisting of columns for each term in our corpus, and rows for each document. The value in each cell corresponds to the frequency of the term appearing in the document. The document term matrix is a useful way to encode text for a variety of natural language processing tasks, such as trending topics, classifying documents, searching by keyword, or even detecting a hacked tweet. Below is an example of building a corpus. In the above code, you can see how we\u2019ve created the document term matrix from the Twitter dataset, effectively encoding the tweets into an array of numbers for each term. Since each document now exists as a row with the same number of columns (one for each term), our array is well-formed for processing by machine learning algorithms. We\u2019ve also setup a test document term matrix that uses the same word dictionary as the training set. This means the test array will contain the same number of columns as the training, and can thus be used in any trained machine learning models that we create. For our first sentiment analysis accuracy attempt, we can try a simple logistic regression model. We\u2019ll pass all columns (all terms) of the document term matrix into the algorithm and train against the \u201Cy\u201D value, which indicates positive sentiment. The code is shown below. Upon training the logistic regression model, we find an accuracy of 76% on the training set and 67% on the test set. This is an improvement over AFINN, although minimal. For our next sentiment analysis accuracy measurement, we can try using the XGBoost algorithm. Below is an example of the code. Upon training the model, we find an accuracy of 84% on the training set and 67% on the test set. This is slightly better than the logistic regression model With a support vector machine (SVM) model, we can boost the accuracy for sentiment analysis even further. With a larger training dataset, an accuracy of about 82% on a cross-validation set can be achieved. It\u2019s possible to edge the accuracy even further with larger datasets and longer training. Additional techniques include the usage of recurrent neural networks to take into account the placement of emotionally important terms within a sentence. Now that we\u2019ve addressed how to detect the emotion of a conversation, we can begin to address a solution for aiding the user. Sentiment analysis has given us an indicator of neutral, positive, or negative sentiment that can be used as a trigger mechanism for advancing the internal state of our conversational UI or artificial intelligence routine. We\u2019ve seen some examples above, of what a state machine like this might look like. Let\u2019s try implementing one! As a first attempt at a simple conversational UI for a book review app, we\u2019ll implement a basic chat bot interface. In order to keep the code concise and easily understandable, we\u2019ll use just a handful of hard-coded keywords and responses that our chatbot conversational UI will recognize and respond to. The following topics will evoke a response: We\u2019ll store the keywords, along with their associated responses, in a key\/value pair that we\u2019ll call our \u201Cbrain\u201D. We\u2019ll simply compare the user\u2019s text (or speech, if using speech recognition) to find any matching keyword within our bot\u2019s brain. If a match is found, the bot simply responds with the resulting value. Below is an example session of chatting with the Book Reviews conversational UI version 1.0. Notice, in the above conversation with the chatbot, the user begins by simply saying \u201Chello\u201D to greet the bot. The bot recognizes the keyword, as it matches one of the hard-coded terms within its brain, and outputs the associated response. Instructing the user on how to interact with the bot to obtain a book review. Next, the user asks about the book, \u201CAlice in Wonderland\u201D, and the chatbot plainly responds with the rating for the book. Next, comes the important part! When the user exclaims, \u201CI love Alice in Wonderland!\u201D, which clearly evokes an emotional connotation, the chatbot simply responds with the rating for the book. This is expected, as the software is operating exactly as was intended. It is simply matching a keyword and responding with a value, with no determination of further characteristics about the user\u2019s emotional disposition. Likewise, the second book review request contains an unfavorable rating. Again, the chatbot makes no distinct expression, and plainly responds with the rating of the book. No doubt, feeling dissatisfaction with the software, the user states how they feel and terminates the app. We\u2019re going to take a look at upgrading the Book Reviews app to a version 2.0 in just a bit. There, we\u2019ll add emotion! However, first let\u2019s take a quick look at the code for the original version. If you\u2019re curious, the code for the first book review app is shown below. You can see how simplistic the request and response model for the brain is. We simply store key\/value pairs and match against the user\u2019s input using the simple command. While this software may not win any awards, it gets the point across of implementing a simple conversational UI style app. You can find the full code and a demo for this chatbot at https:\/\/jsfiddle.net\/dbyzztxp\/1\/ We\u2019ve just seen a simple example of the Book Reviews app, utilizing a conversational UI for chatting with the user. The bot made no expression of emotion and simply responded to user queries in a plain fashion. No characteristics of the user\u2019s input or emotional disposition was measured, as the software only needed to match a keyword and respond. We\u2019re now going to give our bot an upgrade with some built-in emotion! Below is an example session of chatting with the Book Reviews conversational UI version 2.0, utilizing emotional responses. Notice, how much different, if not intimate, the conversation becomes. Just as before, the user initiates the conversation with the chatbot with a greeting of, \u201Chello\u201D. The chatbot, unable to detect any sentiment from the user, simply responds with a standard greeting. So far, this is no different than our first version. Next, the user comments about the poor functionality of the prior version of the software. Specifically, the user includes the term \u201Chello\u201D again (to initiate another greeting), but also expresses clear negative sentiment in the message by using the word \u201Cbad\u201D (which happens to be a keyword phrase in our bot\u2019s brain for measuring sentiment). The chatbot matches the term \u201Chello\u201D, but instead of simply responding with a default value, it makes a measurement of sentiment on the sentence. The sentiment is detected as negative and the bot responds in kind, by offering to help cheer the user up with a book review. Cool! Next, the user asks for a review about \u201CAlice in Wonderland\u201D. Again, the chatbot makes a measurement of sentiment, detects a neutral response, and responds with the associated value for that sentiment. This happens to be a standard neutral response, which is the same as the first version app. However, after hearing the good review of 4 stars, the user exclaims, \u201CI love Alice in Wonderland!\u201D (just as they did in the last example). This is where emotion really makes a difference! The artificial intelligence in the conversational UI makes a measurement of sentiment on the user\u2019s input sentence. Since the term \u201Clove\u201D corresponds to a positive sentiment, the chatbot responds with a positively charged result. In this case, the response is a recommendation for a similar book by the same author. Let\u2019s consider the ramifications of this. The chatbot has just successfully measured the sentiment of a user\u2019s response and determined it to be positive. This occurred, along with an association to having just retrieved a review for a particular book. The software can infer that the user probably likes this book (and the author too), and thus, recommend a similar book by the same author. The user did not explicitly request a similar book. Nor are we cluttering a web page user interface with potentially distracting information, such as lists of related books, similar books, and books that others have purchased. Instead, we\u2019re specifically targeting the positive emotional sentiment, expressed by the user, and keying off of this indicator to recommend another book by the same author. By tuning into the emotional disposition of the user, we may have just boosted the utilization value (not to mention, book sales!) of the software. You can follow through the rest of the chat-bot session conversation. The user asks about another book review, this time for \u201CPemberley\u201D (just as they did in the first version example). The chatbot detects neutral sentiment and responds with a plain rating for the book. However, this time, when the user expresses negative emotion about the review (as the book only holds a 2-star review), the chatbot is able to key off of the negative emotion detected, and respond accordingly. Having detected the negative sentiment, the conversational UI retrieves the negative-associated response for the topic, which is to offer the user a higher-rated, but similar, book by a different author. We\u2019re, again, tapping into the hidden value of the user\u2019s emotional sentiment to take advantage of potential opportunities. Where the user might simply exit the app at this point (negative sentiment has a way of causing this!), the bot attempts to re-context the conversation by offering another book. This could have the effect of increasing user retention within the app and boosting book sales via the recommendation of related (and emotionally on-topic) books! Concluding the conversation, the user states that they also dislike the recommended book (much to our dismay), and asks the app for help. Again, the chatbot is able to detect the negative sentiment in the request for help. Empathizing with the user and attempting to turn the user\u2019s disposition more positive, the bot reminds the user that retrieving additional book reviews can be easy. It follows with instructions for retrieving another book review. We\u2019ve just seen some very interesting results from the inclusion of emotion into our book reviews conversational UI chatbot. Let\u2019s see how this all works. Just as we had in the first version, there is a brain database containing key\/value pair responses. The difference this time, however, is that for each matching keyword we provide three different corresponding values. Each value is separated by sentiment. Below is an example of this simple data structure. Notice how each term has multiple responses, differentiated by detected sentiment. While the responses in this example are hard-coded for specific books, it\u2019s easy to see how we could dynamically query against an API and incorporate those results into the sentiment-specific responses. The main method is nearly identical as the first version, but includes a measurement of sentiment. The above method, while similar to the original version, now includes a line to determine the sentiment. It uses the resulting emotional value to retrieve the associated response with the matching keyword. In this way, multiple potential responses can be found for any single topic. The code for calculating the sentiment, is simply a hard-coded keyword list of emotionally charged terms (not too unlike the AFINN model for measuring sentiment, as discussed earlier). This is to keep the demonstration simple. However, a more accurate sentiment calculation can be done by using an artificial intelligence machine learning model, as we\u2019ve shown earlier. You can find the full code and a demo for this chatbot at https:\/\/jsfiddle.net\/z0rkyq4L\/2\/ It\u2019s clear that there is more to recognize in a conversational UI than simple keyword and phrase matching. After all, we\u2019ve just seen how powerful the effects of sentiment can be, when considering the emotional disposition of a user conversing with the software. While traditional conversational UI ignores contextual properties, such as sentiment, and tends to issue plain responses to user queries with a simple utterance match, sentiment can bring a chatbot conversation to a whole new level. Through the addition of sentiment detection, we were able to enhance a conversational UI and integrate more closely with the user, empathizing with their emotional state, and responding accordingly. Through the use of emotionally recognized responses, the conversational UI was able to tap into potentially missed opportunities, by recommending products, as inferred from the user\u2019s emotion regarding the topic. When positive emotion was detected, the chatbot selected a specific response type and product recommendation. Likewise, negative emotion helped steer the chatbot conversation in alternative directions, offering other product recommendations and help to the user. Considering the effects of sentiment analysis in a conversational UI, perhaps we could take into account even further traditionally hidden attributes. By recognizing the importance and opportunities of additional speech characteristics, including emotion, sentiment, intonation, loudness, gender, and a variety of other metrics, we can hope to bring a closer connection between computer and human interaction. The source code for this project is available on GitHub. This article was written by Kory Becker, software developer and architect, skilled in a range of technologies, including web application development, web services, machine learning, and data science.","flair":"three\tResearch"}
{"author":"gabrielgoh","created":"Fri Sep 30 04:52:59 EDT 2016","text":" Google Research Blog The latest news from Research at Google Image Compression with Neural Networks Thursday, September 29, 2016 Posted by Nick Johnston and David Minnen, Software Engineers Data compression is used nearly everywhere on the internet - the videos you watch online, the images you share, the music you listen to, even the blog you're reading right now. Compression techniques make sharing the content you want quick and efficient. Without data compression, the time and bandwidth costs for getting the information you need, when you need it, would be exorbitant! In \"Full Resolution Image Compression with Recurrent Neural Networks\", we expand on our previous research on data compression using neural networks, exploring whether machine learning can provide better results for image compression like it has for image recognition and text summarization. Furthermore, we are releasing our compression model via TensorFlow so you can experiment with compressing your own images with our network. We introduce an architecture that uses a new variant of the Gated Recurrent Unit (a type of RNN that allows units to save activations and process sequences) called Residual Gated Recurrent Unit (Residual GRU). Our Residual GRU combines existing GRUs with the residual connections introduced in \"Deep Residual Learning for Image Recognition\" to achieve significant image quality gains for a given compression rate. Instead of using a DCT to generate a new bit representation like many compression schemes in use today, we train two sets of neural networks - one to create the codes from the image (encoder) and another to create the image from the codes (decoder). Our system works by iteratively refining a reconstruction of the original image, with both the encoder and decoder using Residual GRU layers so that additional information can pass from one iteration to the next. Each iteration adds more bits to the encoding, which allows for a higher quality reconstruction. Conceptually, the network operates as follows: The initial residual, R[0], corresponds to the original image I: R[0] = I. Set i=1 for to the first iteration. Iteration[i] takes R[i-1] as input and runs the encoder and binarizer to compress the image into B[i]. Iteration[i] runs the decoder on B[i] to generate a reconstructed image P[i]. The residual for Iteration[i] is calculated: R[i] = I - P[i]. Set i=i+1 and go to Step 3 (up to the desired number of iterations). The residual image represents how different the current version of the compressed image is from the original. This image is then given as input to the network with the goal of removing the compression errors from the next version of the compressed image. The compressed image is now represented by the concatenation of B[1] through B[N]. For larger values of N, the decoder gets more information on how to reduce the errors and generate a higher quality reconstruction of the original image. To understand how this works, consider the following example of the first two iterations of the image compression network, shown in the figures below. We start with an image of a lighthouse. On the first pass through the network, the original image is given as an input (R[0] = I). P[1] is the reconstructed image. The difference between the original image and encoded image is the residual, R[1], which represents the error in the compression. Left: Original image, I = R[0]. Center: Reconstructed image, P[1]. Right: the residual, R[1], which represents the error introduced by compression. On the second pass through the network, R[1] is given as the network\u2019s input (see figure below). A higher quality image P[2] is then created. So how does the system recreate such a good image (P[2], center panel below) from the residual R[1]? Because the model uses recurrent nodes with memory, the network saves information from each iteration that it can use in the next one. It learned something about the original image in Iteration[1] that is used along with R[1] to generate a better P[2] from B[2]. Lastly, a new residual, R[2] (right), is generated by subtracting P[2] from the original image. This time the residual is smaller since there are fewer differences between the reconstructed image, and what we started with. The second pass through the network. Left: R[1] is given as input. Center: A higher quality reconstruction, P[2]. Right: A smaller residual R[2] is generated by subtracting P[2] from the original image. At each further iteration, the network gains more information about the errors introduced by compression (which is captured by the residual image). If it can use that information to predict the residuals even a little bit, the result is a better reconstruction. Our models are able to make use of the extra bits up to a point. We see diminishing returns, and at some point the representational power of the network is exhausted. To demonstrate file size and quality differences, we can take a photo of Vash, a Japanese Chin, and generate two compressed images, one JPEG and one Residual GRU. Both images target a perceptual similarity of 0.9 MS-SSIM, a perceptual quality metric that reaches 1.0 for identical images. The image generated by our learned model results in an file 25% smaller than JPEG. Left: Original image (1419 KB PNG) at ~1.0 MS-SSIM. Center: JPEG (33 KB) at ~0.9 MS-SSIM. Right: Residual GRU (24 KB) at ~0.9 MS-SSIM. This is 25% smaller for a comparable image quality Taking a look around his nose and mouth, we see that our method doesn\u2019t have the magenta blocks and noise in the middle of the image as seen in JPEG. This is due to the blocking artifacts produced by JPEG, whereas our compression network works on the entire image at once. However, there's a tradeoff -- in our model the details of the whiskers and texture are lost, but the system shows great promise in reducing artifacts. Left: Original. Center: JPEG. Right: Residual GRU. While today\u2019s commonly used codecs perform well, our work shows that using neural networks to compress images results in a compression scheme with higher quality and smaller file sizes. To learn more about the details of our research and a comparison of other recurrent architectures, check out our paper. Our future work will focus on even better compression quality and faster models, so stay tuned! Google Labels: Image Processing , Neural Networks , TensorFlow    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"hsk90","created":"Sat Nov 12 09:31:06 EST 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 26 Star 263 Fork 17 SKTBrain\/awesome-starcraftAI Code Issues 0 Pull requests 0 Projects 0 Pulse Graphs A curated list of resources dedicated to StarCraft AI. 41 commits 2 branches 0 releases 2 contributors MIT Clone or download Clone with HTTPS Use Git or checkout with SVN using the web URL. Download ZIP Find file Branch: master Switch branches\/tags Branches Tags gh-pages master Nothing to show Nothing to show New pull request Latest commit c4caf47 Nov 13, 2016 hshyunsookim committed on GitHub Merge pull request #2 from richoux\/master \u2026 Add the Sat and Optimization section, plus two surveys (and fix a mistake in one reference) Permalink Failed to load latest commit information. images add images Nov 11, 2016 LICENSE Initial commit Nov 11, 2016 README.md Fix Santi name Nov 12, 2016 README.md Awesome StarCraft AI A curated list of resources dedicated to StarCraft AI. We are looking for more contributors and maintainers! Table of Contents API\/Codes Replays Research Papers Surveys Benchmark Thesis Dataset Bayesian Approach Satisfaction and Optimization Planning Prediction Control Full Game Play Learning from Demonstration Reports Competitions Others Photo Credit: Google DeepMind [Link] Photo Credit: Gabriel Synnaeve [Link] API\/Codes The Brood War API (BWAPI). [GitHub] StarCraft II API - Technical Design, Blizzard. [Link] SparCraft, a combat simulator for StarCraft. [GitHub] BWTA, A terrain analyzer for BWAPI. [Link] BroodWar Replay Scrapper in Python, Gavriel Synnaeve. [GitHub] Replays StarCraft Brood War Data Mining (replays, analyzer, datasets), Alberto Uriarte. [Link] GosuGamers, 8K replays as of April 2013. [Link] TeamLiquid, professional community with tournament match replays. [Link] ICCUP, International Cyber Cup, professional community with tournament match replays. [Link] BWReplays, a compilation of replays including previous resources. [Link] Research Papers Surveys S. Ontañón, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill, M. Preuss, A survey of real-time strategy game ai research and competition in starcraft, IEEE TCIAIG, 2013. [Survey] S. Ontañón, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill, M. Preuss, RTA AI Problems and Techniques, Springer Encyclopedia of Computer Graphics and Games, 2015. [Survey] D. Churchill, M. Preuss, F. Richoux, G. Synnaeve, A. Uriarte, S. Ontañón, M. Certický, StarCraft Bots and Competitions, Springer Encyclopedia of Computer Graphics and Games, 2016. [Survey] R. Lara-Cabrera, C. Cotta, A. Fernandez-Leiva, A review of computational intelligence in RTS games, IEEE FOCI, 2013. [Survery] Benchmark A. Uriarte, S. Ontãñón, A Benchmark for StarCraft Intelligent Agents, AAAI AIIDE, 2015. [Paper] Thesis G. Synnaeve, Bayesian programming and learning for multi-player video games: application to RTS AI, Ph.D. Thesis, INPG, 2012. [Thesis] J. Hagelback, Multi-Agent Potential Field Based Architectures for Real-Time Strategy Game Bots, Ph.D. Thesis, BIT, 2012. [Thesis] D. Churchill, Heuristic Search Techniques for Real-Time Strategy Games, Ph.D. Thesis, U. Alberta, 2016. [Thesis] Dataset G. Synnaeve, P. Bessiere, A Dataset for StarCraft AI & an Example of Armies Clustering, arXiv, 2012. [Paper] G. Robertson, I. Watson, An Improved Dataset and Extraction Process for Starcraft AI, FLAIRS, 2014. [Paper] Bayesian Approach G. Synnaeve, P. Bessiere, A bayesian model for opening prediction in rts games with application to starcraft, IEEE CIG, 2011. [Paper] G. Synnaeve, P. Bessiere, A Bayesian model for RTS units control applied to StarCraft, IEEE CIG, 2011. [Paper] G. Synnaeve, P. Bessiere, Special tactics: A bayesian approach to tactical decision-making, IEEE CIG, 2012. [Paper] Satisfaction and Optimization M. Certický, Implementing a Wall-In Building Placement in StarCraft with Declarative Programming, arXiv, 2013. [Paper] J. Fradin, F. Richoux, Robustness and Flexibility of GHOST, AIIDE Third Workshop on Artificial Intelligence in Adversarial Real-Time Games, 2015. [Paper] F. Richoux, A. Uriarte, J.-F. Baffier, GHOST: A Combinatorial Optimization Framework for Real-Time Problems, IEEE TCIAIG, 2016. [Paper] F. Richoux, A. Uriarte, S. Ontañón, Walling in Strategy Games via Constraint Optimization, AAAI AIIDE, 2014. [Paper] Planning B. Weber. M. Mateas, Case-Based Reasoning for Build Order in Real-Time Strategy Games, AAAI AIIDE, 2009. [Paper] B. Weber, M. Mateas, A. Jhala, Applying Goal-Driven Autonomy to StarCraft, AAAI AIIDE, 2010. [Paper] D. Churchill, M. Buro, Build Order Optimization in StarCraft, AAAI AIIDE, 2011. [Paper] D. Churchill, M. Buro, Incorporating Search Algorithms into RTS Game Agents, AAAI AIIDE Workshop, 2012. [Paper] M. Stanescu, N. Barriga, M. Buro, Hierarchical Adversarial Search Applied to Real-Time Strategy Games, AAAI AIIDE, 2014. [Paper] Prediction B. Weber, M. Mateas, A Data mining approach to strategy prediction, IEEE CIG, 2009. [Paper] H. Park, H. Cho, K. Lee, K. Kim, Prediction of Early Stage Opponents Strategy for StarCraft AI using Scouting and Machine Learning, Workshop at SIGGRAPH Asia, 2012. [Paper] H. Cho, K. Kim, S. Cho, Replay-based Strategy Prediction and Build Order Adaptation for StarCraft AI Bots, IEEE CIG, 2013. [Paper] M. Stanescu, S. Hernandez, G. Erickson, R. Greiner, M. Buro, Predicting Army Combat Outcomes in StarCraft, AAAI AIIDE, 2013. [Paper] Control B. Weber, M. Mateas, A. Jhala, A Particle Model for State Estimation in Real-Time Strategy Games, AAAI AIIDE, 2011. [Paper] A. Shantia, E. Begue, M. Wiering, Connectionist Reinforcement Learning for Intelligent Unit Micro Management in StarCraft, IJCNN, 2011. [Paper] D. Churchill, A. Saffidine, M. Buro, Fast Heuristic Search for RTS Game Combat Scenarios, AAAI AIIDE, 2012. [Paper] D. Churchill, M. Buro, Incorporating Search Algorithms into RTS Game Agents, AAAI AIIDE Workshop, 2012. [Paper] S. Wender, I. Watson, Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft: Broodwar, IEEE CIG, 2012. [Paper] D. Churchill, M. Buro, Portfolio Greedy Search and Simulation for Large-Scale Combat in StarCraft, IEEE CIG, 2013. [Paper] K. Nguyen, Z. Wang, R. Thawonmas, Potential Flows for Controlling Scout Units in StarCraft, IEEE CIG, 2013. [Paper] Full Game Play B. Weber, M. Mateas, A. Jhala, Building Human-Level AI for Real-Time Strategy Games, AAAI ACS 2011. [Paper] J. Young, F. Smith, C. Atkinson, K. Poyner, T. Chothia, SCAIL: An integrated Starcrat AI system, IEEE CIG, 2012. [Paper] Learning from Demonstration B. Weber, S. Ontañón, Using Automated Replay Annotation for Case-Based Planning in Games, ICCBR-Games Workshop, 2010. [Paper] B. Weber, M. Mateas, A. Jhala, Learning from Demonstration for Goal-Driven Autonomy, AAAI, 2012. [Paper] Reports J. Lewis, P. Trinh, D. Kirsh, A Corpus Analysis of Strategy Video Game Play in Starcraft: Brood War, COGSCI, 2011. [Paper] M. Buro, D. Churchill, Real-Time Strategy Game Competitions, AI Magazine, 2012. [Report] G. Robertson, I. Watson, A Review of Real-time Strategy Game AI, AI Magazine, 2014. [Report] M. Kim, S. Kim, K. Kim, A. Dey, Evaluation of StarCraft Artificial Intelligence Competition Bots by Experienced Human Players, CHI, 2016. [Report] Competitions AAAI AIIDE StarCraft AI Competition, 2016. [Link] IEEE CIG StarCraft AI Competition, 2016. [Link] Highlights Video. [Link] SSCAIT, Student StarCraft AI Tournament [Link] Others The Berkeley Overmind Project [Project] StarCraft AI, Resource for Custom AIs. [Link] StarCraft AI Blog, by Jay Scott. [Blog] StarCraft II AI Blog, by Matt Fisher. [Blog] Game AI 101, SK T-Brain. [Link] Maintainer: Hyunsoo Kim Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"warmspringwinds","created":"Tue Nov 22 18:07:13 EST 2016","text":"I have written up a couple of posts about Classification and Segmentation.\nMostly as a progress report for myself in learning Tensorflow.\nMight be useful for someone :)\n\nhttp:\/\/warmspringwinds.github.io\/tensorflow\/tf-slim\/2016\/10\/30\/image-classification-and-segmentation-using-tensorflow-and-tf-slim\/\n\nhttp:\/\/warmspringwinds.github.io\/tensorflow\/tf-slim\/2016\/11\/22\/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim\/","flair":"four\tProject"}
{"author":"seann999","created":"Mon Oct 03 09:09:02 EDT 2016","text":"In deep supervised learning, you can overfit a small dataset as a sanity check: making sure your model is implemented correctly and can actually learn before going on to train on your real, big dataset. Are there similar strategies in reinforcement learning, where one can get results in a few minutes before moving on to spend a day training on Space Invaders or even Pong?","flair":"null\tnull"}
{"author":"adamnemecek","created":"Thu Oct 06 21:01:24 EDT 2016","text":" Journal of Machine Learning Research 16 (2015) 2081-2097 Submitted 8\/14; Revised 11\/14; Published 10\/15 Photonic Delay Systems as Machine Learning Implementations Michiel Hermans michiel.hermans@ulb.ac.be OPERA - Photonics Group Université Libre de Bruxelles Avenue F. Roosevelt 50, 1050 Brussels, Belgium Miguel C. Soriano miguel@ifisc.uib-csic.es Instituto de F́ısica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC) Campus Universitat de les Illes Balears E-07122 Palma de Mallorca, Spain Joni Dambre joni.dambre@ugent.be ELIS departement Ghent University Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium Peter Bienstman peter.bienstman@ugent.be INTEC departement Ghent University Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium Ingo Fischer ingo@ifisc.uib-csic.es Instituto de F́ısica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC) Campus Universitat de les Illes Balears E-07122 Palma de Mallorca, Spain Editor: Yoshua Bengio Abstract Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and poten- tially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encod- ing of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform signifi- cantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers. Keywords: recurrent neural networks, optical computing, machine learning models c©2015 Michiel Hermans, Miguel C. Soriano, Joni Dambre, Peter Bienstman, and Ingo Fischer. Hermans, Soriano, Dambre, Bienstman, and Fischer 1. Introduction Applied research in neural networks is currently strongly influenced by available computer architectures. Most strikingly, the increasing availability of general-purpose graphical pro- cessing unit (GPGPU) programming has sped up the computations required for training (deep) neural networks by an order of magnitude. This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on state- of-the-art performances on tasks such as computer vision (Krizhevsky et al., 2012; Cireşan et al., 2010). One class of neural models which has only seen limited effects of the boost in speed from GPUs are recurrent models. Recurrent neural networks (RNNs) are very interesting for processing time series, as they can take into account an arbitrarily long context of their in- put history. This has important implications in tasks such as natural language processing, where the desired output of the system may depend on context that has been presented to the network a relatively long time ago. In common feedforward networks such dependen- cies are very hard to include without scaling up the model to an impractically large size. Recurrent networks, however, can\u2013at least in principle\u2013carry along relevant context as they are being updated. In practice, recurrent models suffer from two important drawbacks. First of all, where feedforward networks fully benefit from massively parallel architectures in terms of scal- ability, recurrent networks, with their inherently sequential nature do not fit so well into this framework. Even though GPUs have been used to speed up training RNNs (Sutskever et al., 2011; Hermans and Schrauwen, 2013), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks. The second issue is that training RNNs is a notoriously slow process due to problems associated with fading gradients, which is especially cumbersome if the network needs to learn long-term dependencies within the input time series. Recent attempts to solve this problem using the Hessian-free approach have proved promising (Martens and Sutskever, 2011). Other attempts using stochastic gradient descent combined with more heuristic ideas have been described in Bengio et al. (2013). In this paper we will consider a radical alternative to common, digitally implemented RNNs. A steadily growing branch of research is concerned with Reservoir Computing (RC), a con- cept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series (Jaeger, 2001; Jaeger and Haas, 2004; Maass et al., 2002; Steil, 2004; Lukosevicius and Jaeger, 2009). Despite its simplicity, RC has several important advantages over traditional gradient descent training methods. First of all, the training process is extremely fast. Only output weights are trained, and this is performed by solving a single linear system of equations. Second, and of great importance, the RC concept is applicable to any non-linear dynamical system, as long as it exhibits consistent responses, a high-dimensional state space, and fading memory. This has opened lines of research that go beyond common digital implementations and into analog physical implementations. The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples (Fernando and Sojakka, 2003), mechanical constructs and tensegrity structures (Caluwaerts et al., 2013; Hauser et al., 2011), electro- 2082 Photonic Delay Systems as Machine Learning Implementations optical devices (Larger et al., 2012; Paquot et al., 2012), fully optical devices (Brunner et al., 2013) and nanophotonic circuits (Vandoorne et al., 2008, 2014). As opposed to digital im- plementations, physical systems can offer great speed-ups, inherent massive parallelism, and great reductions in power consumption. In this sense, physical dynamical systems as ma- chine learning implementation platforms may one day break important barriers in terms of scalability. In the near future, especially optical computing devices might find applications in several tasks where fast processing is essential, such as in optical header recognition, optical signal recovery, or fast control loops. The RC paradigm, despite its notable successes, still suffers from an important drawback. Its inherently unoptimized nature makes it relatively inefficient for many important machine learning problems. When the dimensionality of the input time series is low, the expansion into a high-dimensional nonlinear space offered by the reservoir will provide a sufficiently di- verse set of features to approximate the desired output. If the input dimensionality becomes larger, however, relying on random features becomes increasingly difficult as the space of possible features becomes so massive. Here, optimization with gradient descent still has an important edge over the RC concept: it can shape the necessary nonlinear features auto- matically from the data. In this paper we aim to integrate the concept of gradient descent in neural networks with physically implemented analog machine learning models. Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system (Larger et al., 2012; Paquot et al., 2012; Soriano et al., 2013). In order to use such a system as a reservoir, an input time series is encoded into a continuous time signal and subsequently used to drive the dynamics of the physical setup. The response of the device is recorded and converted to a high-dimensional feature set, which in turn is used with linear regression in the common RC setup. In this particular case, the randomness of RC is incorporated in the input encoding. This encoding is per- formed offline on a computer, but is usually completely random. Even though efforts have been performed to improve this encoding in a generic way (by ensuring a high diversity in the network\u2019s response, discussed in Rodan and Tino (2011) and Appeltant et al. (2014)), a way to create task-specific input encodings is still lacking. In Hermans et al. (2014b), the possibility to use backpropagation through time (BPTT) (Rumelhart et al., 1986) as a generic optimization tool for physical dynamical systems was addressed. It was found that BPTT can be used to find remarkably intricate solutions to complicated problems in dynamical system design. In Hermans et al. (2014a) simulated results of BPTT used as an optimization method for input encoding in the physical sys- tem described above were presented. In this paper we go beyond this work and show for the first time experimental evidence that model-based BPTT is a viable training strategy for physical dynamical systems. We choose two often-used high-dimensional data sets for validation, and we show that input encoding that is optimized using BPTT in a common machine learning approach, provides a significant boost in performance for these tasks when compared to random input encodings. This not only demonstrates that machine learning approaches are more broadly applicable than is generally assumed, but also that physical analog computers can in fact be considered as parametrizable machine learning models, and may play a significant role in the next generation of signal processing hardware. This paper is structured as follows: first of all we discuss the physical system and its corre- 2083 Hermans, Soriano, Dambre, Bienstman, and Fischer sponding model in detail. We explain how we convert the continuous-time dynamics of the system into a discrete-time update equation which we use as model in our simulation. Next, we present and analyze the results on the tasks we considered and compare experimental and simulated results. 2. Physical System In this section we will explain the details of the physical system. We will start by formally introducing its delay dynamics operating in continuous time. Next, we will explain how the feedback delay can be used for realizing a high-dimensional state space encoded in time, and we demonstrate that\u2013combined with special input and output encoding\u2013the setup can be seen as a special case of RNN. Finally we explain how we discretize the system\u2019s input and output encoding, which enables us to approximate the dynamics of the system by a discrete-time update equation. The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda- type dynamics (Larger et al., 2004; Weicker et al., 2012). We provide a schematic depiction of the physical setup in Figure 1. It consists of a laser source, a Mach-Zehnder modulator, a long optical fiber (≈ 4 km) which acts as a physical delay line, and an electronic circuit which transforms the optical beam intensity in the fiber into a voltage. This voltage is amplified and low-pass filtered and can be measured to serve as the system output. Moreover, it is added to an external input voltage signal, and then serves as the driving signal for the Mach-Zehnder modulator. The measured output signal is well described by the following differential equation (Larger et al., 2012): T ȧ(t) = −a(t) + β [ sin2(a(t−D) + z(t) + φ)− 1\/2 ] . (1) Here, the signal a(t) corresponds to a measured voltage signal (down to a constant scaling and bias factor). The factor T is the time scale of the low-pass filtering operation in the electronic circuit, equal to 0.241 µs, β is the total amplification in the loop, which in the experiments can be varied by changing the power of the laser source. D is the delay of the system, which has been chosen as 20.82 µs. z(t) is the external input signal, and φ is a constant offset phase (which can be controlled by setting a bias voltage), which we set at π\/4 for all results presented in this paper. For ease of notation we will call the system a delay-coupled Mach-Zehnder, which we abbreviate as DCMZ. Note that the parameters β and φ, together with the global scaling of the input signal z(t), control the global dynamical behavior of the system (Larger et al., 2012). Indeed, previous research in the RC context have identified the role of these parameters in connection with task performance. They found that good performance is usually found when the parameters put the system in an asymptotically stable regime. For instance, if we keep φ = π\/4, and β < 1, the system state will always fall back to zero in the absence of input. In the case of β > 1, the state of the system will spontaneously start to oscillate, which has a detrimental effect on task performance. In this paper we will simply use values for β and φ that were found to generally work well in the reservoir setup. 2084 Photonic Delay Systems as Machine Learning Implementations 2.1 Input and Output Encoding Delay-coupled systems have\u2013in principle\u2013an infinite-dimensional state space, as these sys- tems directly depend on their full history covering an interval of one delay time. This prop- erty has been the initial motivation for using delay-coupled systems in the RC paradigm in the past years. Suppose we have a multivariate input time series, which we will denote by si, for i ∈ {1, 2, · · · , S}, S being the total number of instances (the length of the input sequence). Each si is a column vector of size Nin× 1, with Nin the number of input dimen- sions. We wish to construct an accompanying output time series yi. We convert each data point si to a continuous-time segment zi(t) as follows: zi(t) = m0(t) + m T(t)si, where m0(t) and m(t) are masking signals, which are defined for t ∈ [0 · · ·P ], with P the masking period. The signal m0(t) is scalar, and constitutes a bias signal, and m(t) is a column vector of size Nin × 1. The total input signal z(t) is then constructed by time- concatenation of the segments zi(t): z(t) = zi(t mod P ) for t ∈ {(i− 1)P · · · iP}. Similarly, we define an output mask u(t). We divide the state variable time traces a(t) in segments ai(t) of duration P such that a(t) = ai(t mod P ) for t ∈ {(i− 1)P · · · iP}. The output time series yi is then defined as yi = y0 + ∫ P 0 dt ai(t)u(t). (2) It is possible to see the delay-coupled dynamical system combined with the masking prin- ciple as a special case of an infinite-dimensional discrete-time recurrent neural network, as illustrated in Figure 2. The recurrent weights, connecting the hidden states over time, are fixed, and manifested by the delayed feedback connection. The input and output weights correspond to the input and output masks. The role of the parameters D and P is important to consider. If they are equal to each other the recurrent network analogy, as shown in Figure 2b, reduces to a network where all nodes have self-connections, and interaction between different nodes between different tasking pe- riods is due to a combination of the low-pass filtering effect and the self-connection. If the difference between D and P is small, there will be direct time-interaction between different nodes. In fact, using a difference of one masking step between D and P has been the basis for opto-electronic systems that do not have a low-pass filter (Paquot et al., 2012). If the difference between D and P becomes significant it is difficult to anticipate how performance will be affected. If D � P , most interactions will happen within a single masking period, such that there will be little useful interaction between the nodes at different time steps. If D � P , the nodes interact over connections that bridge several time steps. We found that, for small differences of D and P , there is little to no noticeable effect on performance, such that we kept D = P , as was used in previous publications. 2085 Hermans, Soriano, Dambre, Bienstman, and Fischer Input signal Output record Mach-ZehnderLaser source Long fiber (delay line) photodiode amplifier\/filter Figure 1: Schematic depiction of a delay-coupled Mach-Zehnder interferometer. In practice, we cannot measure the state trajectory with infinite time resolution, nor can we produce signals with an arbitrary time dependency, as there will always be constraints that limit the maximum bandwidth of the generated signals. Therefore, we assume that m0(t), m(t) and u(t) all consist of piecewise constant signals 1, which are segmented in Nm parts, Nm being the number of masking steps: m0(t) = m0k for t ∈ {(k − 1)Pm · · · kPm}, m(t) = mk for t ∈ {(k − 1)Pm · · · kPm}, u(t) = uk for t ∈ {(k − 1)Pm · · · kPm}, (3) where the length of each step is given by Pm = P\/Nm. This means that we now have a finite number of parameters that fully determine m0(t), m(t) and u(t). Note that, due to our choice of P = D, Pm will by definition be an integer number of times the delay length D. This is convenient for the next section, where we will make a discrete-time approximation of the system, but it is not a necessary requirement of the system to perform well. 2.2 Converting the System to a Trainable Machine Learning Model In Hermans et al. (2014b) it was shown that BPTT can be applied to models of continuous- time dynamical systems. Indeed, it is perfectly possible to simulate the system using differ- ential equation solvers and consequently compute parameter gradients. One issue, however, is the significant computational cost. Note that, in a common discrete-time RNN, a single state update corresponds to a single matrix-vector multiplication and the application of a nonlinearity. In our case it involves the sequential computation of the full time trace of ai(t). This is considerably more costly to compute, especially given the fact that\u2013as in most gradient descent algorithms\u2013we may need to compute it on large amounts of data and this for multiple thousands of iterations. Due to the piecewise constant definition of u(t) we can make a good approximation of a(t). 1. Note that with a finite frequency bandwidth we cannot produce immediate jumps from one constant level to the next. Therefore, we make sure that the duration of each constant part is much longer than the transient in between, and we can safely ignore it. 2086 Photonic Delay Systems as Machine Learning Implementations Figure 2: Schematic representation of the masking principle. a: Depiction of the input time series si and the way it is converted into a continuous-time signal by means of the input masking signals m(t). The horizontal line in the middle shows the time evolution of the system state a(t). We have depicted two connection arrows at one point in time, which indicate that a(t) depends on its immediately preceding value (due to the low-pass filtering operation), and its delayed value. The state trajectories are divided into segments each of which are projected to an output instance yi. b: The same picture as in panel a, but now represented as a time- unfolded RNN. We have shown the connections between the states as light grey arrows, but note that there are in principle infinitely many connections.) First we combine Equations 2 and 3. This gives us: yi = Nm∑ k=1 ∫ kPm (k−1)Pm dt ukai(t) = Nm∑ k=1 ukāik, where āik = ∫ kPm (k−1)Pm dt ai(t). This means that we can represent ai(t) by a finite set of variables āik. To represent the full time trace of a(t) we adopt a simplified notation as follows2: āj = āik, where j = (i− 1)Nm + k. Now we make the following approximation: we assume that for the duration of a single masking step, we can replace the term a(t − D) by āi−Nm , that is, we consider it to be constant. With this assumption, we can solve Equation 1 for the duration of one masking step: a(t) = γi + (âi − γi) exp ( − t T ) for t ∈ {0 · · ·Pm}, (4) with γi = β [ sin2(āi−Nm + z(t) + φ)− 1\/2 ] , and âi the value of a(t) at the start of the interval. Integrating over the interval t = {0 · · ·Pm} we find: āi = (âi − γi)κ+ Pmγi, 2. Please do not confuse with the index i in ai(t). Here the index indicates single masking steps, rather than full mask periods. 2087 Hermans, Soriano, Dambre, Bienstman, and Fischer with κ = 1− e−Pm\/T . We can eliminate âi as follows. First we derive from Equation 4 that âi+1 = (âi − γi)e−Pm\/T + γi. If we combine this expression with the following two: āi = (âi − γi)κ+ Pmγi, āi+1 = (âi+1 − γi+1)κ+ Pmγi+1, we can eliminate âi, and we end up with the following update equation for āi: āi+1 = ρoāi + ρ1γi + ρ2γi+1, with ρ0 = e −Pm\/T , ρ1 = Tκ−Pme−Pm\/T , and ρ2 = Pm−Tκ. This leads to a relatively quick- to-compute update equation to simulate the system. BPTT can also be readily applied on this formula, as it is a simple update equation just like for a common RNN. This is the simulation model we used for training the input and output masks of the system. We verified the accuracy of this approximation both on measured data of the DCMZ and on a highly accurate simulation of the system. For the parameters used in the DCMZ we got very good correspondence with the model (obtaining a correlation coefficient between simulated and measured signals of 99.6%). 2.3 Hybrid Training Approach One challenge we faced when trying to match the model with the experimentally measured data was that we obtained a sufficiently good correspondence only when we very carefully fitted the values for β and φ. We can physically control these parameters, but exactly setting their numerical values turned out not to be trivial in the experiments, especially since they tend to show slight drifting behavior over longer periods of time (in the order of hours). As a consequence, it turned out to be a challenge to train parameters in simulation, and simply apply them directly on the DCMZ. Therefore, we applied a hybrid approach between gradient descent and the RC approach. We train both the input and output masks in simulations. Next, we only use the input masks for the physical setup. After recording all the data, we retrained the output weights using gradient descent, this time on the measured data itself. The idea is that the input encoding will produce highly useful features for the system even when it is trained on a model that may show small, systematic differences with the physical setup. 2.4 Input Limitations One additional physical constraint is the fact that the voltages that can be generated by the electronic part of the system are limited within a range set by its supply voltage. The output voltage of the electronic part serves as the input of the Mach-Zehnder interferometer, and corresponds to the term a(t−D)+z(t) in the argument of the squared sine in Equation 1 (the offset phase φ is controlled by a separate voltage source). The voltage range we were able to cover before the amplifiers started to saturate, roughly corresponded to a range of [−π\/2 · · ·π\/2] in Equation 1: one full wavelength. Instead of accounting for the saturation of the amplifiers in our simulations, we made sure that when the input argument z(t) went outside of this range, we mapped it back into this range by adding or subtracting π. Note that this has no effect on Equation 1 due to the periodicity of the squared sine. Due to the 2088 Photonic Delay Systems as Machine Learning Implementations addition of the input signal with the delayed feedback a(t−D), there is still a chance that the total argument falls out of the range [−π\/2 · · ·π\/2], but in practice such occurrences turned out to be rare, and could safely be ignored. 3. Experiments We tested the use of BPTT for training the input masks both in simulation and in exper- iment on two benchmark tasks. First, we considered the often-used MNIST written digit recognition data set, where we use the dynamics of the system indirectly. Next, we applied it on the TIMIT phoneme data set. For the MNIST experiment we used Nm = 400 masking steps. For TIMIT we used Nm = 600. 3.1 MNIST To classify static images using a dynamical system, we follow an approach similar to the one introduced in Rolfe and LeCun (2013). Essentially, we repeat the same input segment several times until the state vector ai(t) of the DCMZ no longer changes. Next we choose the final instance of ai(t) to classify the image. In practice we used 10 iterations for each image in the MNIST data set (i.e., each input digit is repeated for 10 masking periods). This sufficed for ai(t) to no longer depend on its initial conditions, and in practice this meant that we were able to present all digits to the network right after each other. Input masks were trained using 106 training iterations, where for each iteration the gradient was determined on 500 randomly sampled digits. For training we used Nesterov momen- tum (Sutskever et al., 2013), with momentum coefficient 0.9, and a learning rate of 0.01 which linearly decayed to zero over the duration of the training. As regularization we only performed 1-pixel shifts for the digits. Note that these 1-pixel shifts were used for training the input masks, but we did not include them when retraining the output weights, as we only presented the DCMZ with the original 60,000 training examples. After training the input weights, we gathered both physical and simulated data for the 4 experiments as described below, and retrained the output weights to obtain a final score. Output weights are trained using the cross-entropy loss function over 106 training iterations, where for each iteration the gradient was determined on 1000 randomly sampled digits. We again used Nesterov momentum, with momentum coefficient 0.9. The learning rate was chosen at 0.002 and linearly decayed to zero. Meta-parameter optimization was performed using 10,000 randomly selected examples from the training set. We performed 4 tests on MNIST. First of all we directly compared performances between the simulated and experimental data. When we visualized the features that the trained input masks generated, we noticed that they seemed ordered (see Figure 3). Indeed, for each masking step, a single set of weights mk, which can be seen as a receptive field, is ap- plied to the input image, and the resulting signals from the receptive fields are injected into the physical setup one after each other. Apparently, the trained input masks have similar features grouped together in time. To confirm that this ordering in time is a purposeful property, we shuffled the features mk over a single masking period to obtain a new input mask without a natural ordering in the features. Next we tested (in simulation) how much the performance degraded when using these masks. Finally, we also tested classification employing masks with completely random elements, where only the scaling of the weights 2089 Hermans, Soriano, Dambre, Bienstman, and Fischer Figure 3: Depiction of the image features present in the input masks for the MNIST task. We have shown the input weights of the 400 masking steps, which we have re- shaped into a 20×20 grid of 28×28 pixel representations, corresponding to the receptive fields of each masking step (which can be considered virtual \u201Dneurons\u201D). Time (progression of the masking steps, and hence physical time) runs row by row. Notice that the order in which they occur is not random, but rather similar features are grouped in time. 2090 Photonic Delay Systems as Machine Learning Implementations Masking steps in p u t c h a n n e ls 100 200 300 400 500 600 5 10 15 20 25 30 35 40 Figure 4: Depiction of the input mask trained on the TIMIT task. We have shown the input weights of the 600 masking steps (horizontal axis) for each channel (vertical axis). For the sake of visualization we have here depicted the natural logarithm of the absolute value of the mask plus 0.1. This enhances the difference in scaling for the different channels. MNIST test error TIMIT frame error rate Experimental data 1.16% 33.2% Simulated data 1.08% 31.7% Simulated data: time-shuffled 1.41% 32.8% Simulated data: random 6.72% 40.5% Best in literature 0.23% 25.0% (Cireşan et al., 2012) (Cheng et al., 2009) Table 1: Benchmark performances for different experimental setups. was optimized (which is the RC approach). Results are presented in the middle column of Table 1. The difference between experimental and simulation results is very small. The time shuffled features do indeed cause a notable increase in the classification error rate, indicating that the optimized input masks actively make use of the internal dynamics of the system, and not just offer a generically good fea- ture set. For the sake of comparison we have added the current state-of-the-art result on MNIST. For a comprehensive overview of results on MNIST please consult http:\/\/yann.lecun.com\/ exdb\/mnist\/. Our result are comparable to the best results obtained using neural networks with a single hidden layer (denoted as a 2-layer NN on the previously mentioned website). 2091 http:\/\/yann.lecun.com\/exdb\/mnist\/ http:\/\/yann.lecun.com\/exdb\/mnist\/ Hermans, Soriano, Dambre, Bienstman, and Fischer 3.2 TIMIT We applied frame-wise phoneme recognition to the TIMIT data set (Garofolo et al., 1993). The data was pre-processed to 39-dimensional feature vectors using Mel Frequency Cepstral Coefficients (MFCCs). The data consists of the log energy, and the first 12 MFCC coeffi- cients, enhanced with their first and second derivative (the so-called delta and delta-delta features). The phonemes were clustered into a set of 39 classes, as is the common approach. Note that we did not include the full processing pipeline to include segmentation of the labels and arrive at a phoneme error rate. Here, we wish to illustrate the potential of our approach and demonstrate how realizations of physical computers can be extended to fur- ther concepts, rather than to claim state-of-the-art performance. Given that, in addition, the input masks are trained to perform frame-wise phoneme classification, including the whole processing pipeline would not be informative. Input masks are trained using 50, 000 training iterations, where for each iteration the gra- dient was determined on 200 randomly sampled sequences of a length of 50 frames. For training we again used Nesterov momentum, with momentum coefficient 0.9, and a learning rate of 0.2 which linearly decayed to zero over the duration of the training. As we were in a regime far from overfitting, we simply chose the training error for meta-parameter opti- mization. We have depicted the optimized input mask in Figure 4. Note that the training process strongly rescaled the masking weights for different input channels, putting more emphasis on the delta and delta-delta features (respectively channels 14 to 26 and 27 to 39 ). We repeated the four scenarios previously discussed: using optimized masks in simulation and experiment, using time-shuffled masks, and using random masks. The resulting frame error rates are presented in the right column of Table 1. The simulated and experimental data differ by 1.5%, a relatively small difference, indicating that input masks optimized in simulation are useful in practice, even in the presence of unavoidable discrepancies between the used model and the DCMZ. Results for random masks are significantly worse than those with optimized input masks. Comparison to literature is not straightforward as most publications do not mention frame error rate, but rather the error rate after segmentation. We included the lowest frame error rate mentioned in literature to our knowledge, though it should be stated that other works may have even lower values, even when they are not explicitly mentioned. For an overview of other results on frame error rate please check Keshet et al. (2011). The decrease in performance when using time-shuffled masks is quite modest, suggesting that in this case, most of the improvement over random masks is due directly from the features themselves, and the precise details of the dynamics of the system are less crucial than was the case in the MNIST task 3. Although further testing is needed, we suggest two possible reasons for this. First of all, the TIMIT data set we used contained the first and second derivatives of the first thirteen channels, which already provides information on the preceding and future values and acts as an effective time window. Indeed as can be seen from Figure 4, the input features amplify these derivatives. Therefore, a lot of temporal 3. Note that, when the features are shuffled in time over a single masking period, this indirectly also affects the way information is passed on between different masking periods as the communication between specific nodes between masking periods is a combined effect of the low-pass filter and the self connection. 2092 Photonic Delay Systems as Machine Learning Implementations context is already embedded in a single input frame, reducing the need for recurrent con- nections. Secondly, the lack of control over the way information is mixed over time may still pose an important obstacle to effectively use the recurrence in the system. Currently, input features are trained to perform two tasks at once: provide a good representation of the current input, and at the same time design the features in such a way that they can make use of the (fixed) dynamics present within the system. It may prove the case that the input masks do not have enough modeling power to fulfill both tasks at once, or that the way temporal mixing occurs in the network cannot be effectively put to use for this particular task. 4. Discussion and Future Work In this paper we presented an experimental survey of the use of backpropagation through time on a physical delay-coupled electro-optical dynamical system, in order to use it as a machine learning model. We have shown that such a physical setup can be approached as a special case of recurrent neural network, and consequently can be trained with gradient descent using backpropagation. Specifically, we have shown that both the input and output encodings (input and output masks) for such a system can be fully optimized in this way, and that the encodings can be successfully applied to the real physical setup. Previous research in the usage of electro-optical dynamical systems for signal processing used random input encodings, which are quite inefficient in scenarios where the input di- mensionality is high. We focused on two tasks with a relatively high input dimensionality: the MNIST written digit recognition data set and the TIMIT phoneme recognition data set. We showed that in both cases, optimizing the input encoding provides a significant performance boost over random masks. We also showed that the input encoding for the MNIST data set seems to directly utilize the inherent dynamics of the system, and hence does more than simply provide a useful feature set. Note that the comparison with Reservoir Computing is based on the constraints by a given physical setup and a given set of resources. We note that the Reservoir Computing setup could give good results on the proposed tasks too, if we were greatly scaling up its effec- tive dimensionality. This has been evidenced in, for example, Triefenbach et al. (2010), where good results on the TIMIT data set were achieved by using Echo State Networks (a particular kind of Reservoir Computing) of up to 20,000 nodes. In our setup this would be achieved by increasing the number of masking steps Nm within one masking period. In reality, however, we will face two practical problems. First of all, there are bandwidth limitations in signal generation and measurement. Parts of the signal that fluctuate rapidly would be lost when reducing the duration of a single masking step. If one would scale up by keeping the length of the mask steps fixed but use a longer physical delay, for instance a fiber of tens or hundreds of kilometers, the potential gain in performance comes at the cost of one of the systems important advantages: its speed. Also it is hard to foresee how other optical effects in such long fibers such as dispersion and attenuation, would affect performance. This would be an interesting research topic for future investigations. At the current stage we did not quantify how much the results in this paper hinge on the ability to model the system mathematically. This particular system can be modeled rather precisely, but it is unclear how fast the usefulness of the presented approach would degrade 2093 Hermans, Soriano, Dambre, Bienstman, and Fischer when the model becomes less acute. Several directions for future improvements are apparent. The most obvious one is that we could greatly simplify the training process by putting the DCMZ measurements directly in the training loop: instead of optimizing input masks in simulations, we could just as well directly use real, measured data. The Jacobians required for the backpropagation phase can be computed from the measured data. A training iteration would then consist of the following steps: sample data, produce the corresponding input to the DCMZ with the cur- rent input mask, measure the output, perform backpropagation in simulation, and update the parameters. The benefit would be that we directly end up with functional input and output masks, without the need for retraining. On top of that, data collection would be much faster. The only additional requirement for this setup would be the need for a single computer controlling both signal generation and signal measurement. The next direction for improvement would be to rethink the design of the system from a machine learning perspective. The current physical setup on which we applied backprop- agation finds its origins in reservoir computing research. As we argue in Section 2, the system can be considered as a special case of recurrent network with a fixed, specific con- nection matrix between the hidden states at different time steps. In the reservoir computing paradigm, one always uses fixed dynamical systems that remain largely unoptimized, such that in the past this fact was not particularly restrictive. However, given the possibility of fully optimizing the system that was demonstrated in this paper, the question on how to redesign this system such that we can assert more control over the recurrent connection matrix, and hence the dynamics of the system itself, becomes far more relevant. Currently we have a fixed dynamical system of which we optimize the input signal to accommodate a certain signal processing task. As explained at the end of Section 3.2, it appears that backpropagation can currently only leverage the recurrence of the system to a limited de- gree, when using a single delay loop. Therefore it would be more desirable to optimize both the input signal and the internal dynamics of the system to accommodate a certain task. Alternatively, the configuration can be easily extended to multiple delay loops, allowing for a richer recurrent connectivity. The most significant result of this paper is that we have shown experimentally that the backpropagation algorithm, a highly abstract machine learning algorithm, can be used as a tool in designing analog hardware to perform signal processing. This means that we may be able to vastly broaden the scope of research into physical and analog realizations of neural architectures. In the end this may result in systems that combine the best of both worlds: powerful processing capabilities at a tremendous speed and with a very low power consumption. Acknowledgements P.B., M.H. and J.D. acknowledge support by the interuniversity attraction pole (IAP) Photonics@be of the Belgian Science Policy Office, the ERC NaResCo Starting grant and the European Union Seventh Framework Programme under grant agreement no. 604102 (Human Brain Project). M.C.S. and I.F. acknowledge support by MINECO (Spain), Comu- nitat Autònoma de les Illes Balears, FEDER, and the European Commission under Projects TEC2012-36335 (TRIPHOP), and Grups Competitius. M.H. and I.F. acknowledge support 2094 Photonic Delay Systems as Machine Learning Implementations from the Universitat de les Illes Balears for an Invited Young Researcher Grant. In addition, we acknowledge Prof. L. Larger for developing the optoelectronic delay setup. References Lennert Appeltant, Guy Van der Sande, Jan Danckaert, and Ingo Fischer. Constructing optimized binary masks for reservoir computing with delay systems. Scientific Reports, 4:3629, 2014. Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in opti- mizing recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624\u20138628. IEEE, 2013. Daniel Brunner, Miguel C Soriano, Claudio R Mirasso, and Ingo Fischer. Parallel photonic information processing at gigabyte per second data rates using transient states. Nature Communications, 4:1364, 2013. Ken Caluwaerts, Michiel D\u2019Haene, David Verstraeten, and Benjamin Schrauwen. Loco- motion without a brain: physical reservoir computing in tensegrity structures. Neural Computation, 19(1):35\u201366, 2013. Chih-Chieh Cheng, Fei Sha, and Lawrence Saul. A fast online algorithm for large margin training of online continuous density hidden markov models. In Interspeech 2009, pages 668\u2013671, 2009. Dan Cireşan, Ueli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber. Deep, big, simple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207\u2013 3220, 2010. Dan Cireşan, Ueli Meier, and Jürgen Schmidhuber. Multi-column deep neural networks for image classification. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642\u20133649. IEEE, 2012. Chrisantha Fernando and Sampsa Sojakka. Pattern recognition in a bucket. In Proceedings of the 7th European Conference on Artificial Life, pages 588\u2013597, 2003. John Garofolo, National Institute of Standards, Technology (US, Linguistic Data Con- sortium, Information Science, Technology Office, United States, and Defense Advanced Research Projects Agency). TIMIT Acoustic-phonetic Continuous Speech Corpus. Lin- guistic Data Consortium, 1993. Helmut Hauser, Auke J Ijspeert, Rudolf M Füchslin, Rolf Pfeifer, and Wolfgang Maass. Towards a theoretical foundation for morphological computation with compliant bodies. Optics Express, 105(5-6):355\u2013370, 2011. Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neural networks. In Advances in Neural Information Processing Systems, pages 190\u2013198, 2013. 2095 Hermans, Soriano, Dambre, Bienstman, and Fischer Michiel Hermans, Joni Dambre, and Peter Bienstman. Optoelectronic systems trained with backpropagation through time. IEEE Transactions in Neural Networks and Learning Systems, 2014a. in press. Michiel Hermans, Benjamin Schrauwen, Peter Bienstman, and Joni Dambre. Automated design of complex dynamic systems. PloS One, 9(1):e86696, 2014b. Herbert Jaeger. Short term memory in echo state networks. Technical Report GMD Report 152, German National Research Center for Information Technology, 2001. Herbert Jaeger and Harald Haas. Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless telecommunication. Science, 308:78\u201380, April 2 2004. Joseph Keshet, David McAllester, and Tamir Hazan. Pac-bayesian approach for minimiza- tion of phoneme error rate. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 2224\u20132227, 2011. Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106\u20131114, 2012. Laurent Larger, Jean-Pierre Goedgebuer, and Vladimir Udaltsov. Ikeda-based nonlinear delayed dynamics for application to secure optical transmission systems using chaos. Comptes Rendus Physique, 5(6):669\u2013681, 2004. Laurent Larger, Miguel C Soriano, Daniel Brunner, Lennert Appeltant, Jose M Gutiérrez, Luis Pesquera, Claudio R Mirasso, and Ingo Fischer. Photonic information processing beyond turing: an optoelectronic implementation of reservoir computing. Optics Express, 3:20, 2012. Mantas Lukosevicius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. Computer Science Review, 3(3):127\u2013149, 2009. Wolfgang Maass, Thomas Natschläger, and Henri Markram. Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Computation, 14(11):2531\u20132560, 2002. James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1033\u20131040, 2011. Yvan Paquot, Francois Duport, Antoneo Smerieri, Joni Dambre, Benjamin Schrauwen, Marc Haelterman, and Serge Massar. Optoelectronic reservoir computing. Scientific Reports, 2:1\u20136, 2012. Ali Rodan and Peter Tino. Minimum complexity echo state network. Neural Networks, IEEE Transactions on, 22(1):131\u2013144, 2011. Jason Tyler Rolfe and Yann LeCun. Discriminative recurrent sparse auto-encoders. In International Conference on Learning Representations (ICLR), 2013. 2096 Photonic Delay Systems as Machine Learning Implementations David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning internal representations by error propagation. MIT Press, Cambridge, MA, 1986. Miguel C Soriano, Silvia Ort́ın, Daniel Brunner, Laurent Larger, Claudio Mirasso, Ingo Fischer, and Lúıs Pesquera. Opto-electronic reservoir computing: tackling noise-induced performance degradation. Optics Express, 21(1):12\u201320, 2013. Jochen Steil. Backpropagation-Decorrelation: Online recurrent learning with O(N) com- plexity. In Proceedings of the International Joint Conference on Neural Networks, vol- ume 1, pages 843\u2013848, 2004. Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning, pages 1017\u20131024, 2011. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1139\u20131147, 2013. Fabian Triefenbach, Azaraksh Jalalvand, Benjamin Schrauwen, and Jean-Pierre Martens. Phoneme recognition with large hierarchical reservoirs. In Advances in Neural Informa- tion Processing Systems 23, pages 2307\u20132315, 2010. Kristof Vandoorne, Wouter Dierckx, Benjamin Schrauwen, David Verstraeten, Roel Baets, Peter Bienstman, and Jan van Campenhout. Toward optical signal processing using photonic reservoir computing. Optics Express, 16(15):11182\u201311192, 2008. Kristof Vandoorne, Pauline Mechet, Thomas Van Vaerenbergh, Martin Fiers, Geert Mor- thier, David Verstraeten, Benjamin Schrauwen, Joni Dambre, and Peter Bienstman. Ex- perimental demonstration of reservoir computing on a silicon photonics chip. Artificial Life, 5, 2014. Lionel Weicker, Thomas Erneux, Otti DHuys, Jan Danckaert, Maxime Jacquot, Yanne Chembo, and Laurent Larger. Strongly asymmetric square waves in a time-delayed sys- tem. Physical Review E, 86(5):055201, 2012. 2097 Introduction Physical System Input and Output Encoding Converting the System to a Trainable Machine Learning Model Hybrid Training Approach Input Limitations Experiments MNIST TIMIT Discussion and Future Work ","flair":"null\tnull"}
{"author":"treebranchleaf","created":"Tue Nov 15 07:59:39 EST 2016","text":"Your typical paper goes: \"We invent method X - we try X on MNIST (or some other small dataset), and show an improvement on not using X.  Then we try it on a larger datasets (CIFAR, ImageNet, etc) and show that the effect still holds\".\n\nI'm looking for examples of basic methods (eg. activation function, optimizer choice, resnet-connections, regularization methods, etc) that work on MNIST but do not work on larger problems.  Or the opposite - they show no or negative advantage on MNIST but are useful at scale.\n\nI'm not talking so much about methods that become intractable at scale (eg SVMs), or domain-specific design choices (eg methods that assume natural images as input), but more basic design choices for deep networks.\n\nThe reason I ask is that I'm questioning whether we really need huge datasets and long training times to develop basic methods.","flair":"null\tnull"}
{"author":"cbfinn","created":"Mon Oct 17 03:08:57 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.04286 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.RO < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Robotics Title: Sim-to-Real Robot Learning from Pixels with Progressive Nets Authors: Andrei A. Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, Raia Hadsell (Submitted on 13 Oct 2016) Abstract: Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards. Subjects: Robotics (cs.RO); Learning (cs.LG) Cite as: arXiv:1610.04286 [cs.RO]   (or arXiv:1610.04286v1 [cs.RO] for this version) Submission history From: Andrei Rusu [view email] [v1] Thu, 13 Oct 2016 22:42:10 GMT (1603kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"xingdongrobotics","created":"Tue Oct 11 18:26:14 EDT 2016","text":"As an example of mine, I am doing a semester project related to GAN with possible variations, and currently I am reading GAN, DCGAN papers, and would like to write the code compatible to original paper as a preparation to my own project. \n\nI tried several days, and realize that if I want to write code from scratch, it will take too much time before getting an executable network, (e.g. only under numpy and tensorflow). So I would like to listen to advice that if it is still recommend to just use Keras\/TF-Slim to ease and accelerate the implementation, and when it is done, then to write detailed code in a \"top-down\" fashion. \n\nThe main reason why I hesitate to either write code totally from scratch or simply use TF-Slim\/Keras, is that I found many recent paper authors' repo actually does not often use these high-level wrapper, but include lots of detailed code. ","flair":"one\tDiscussion"}
{"author":"darkconfidantislife","created":"Thu Oct 13 23:07:22 EDT 2016","text":"Hi there guys, I read the double Q-learning paper but I'm not sure if I understood it correctly or I thought I understood but in reality I didn't. Could someone explain it to me so I can confirm? So far, I think that they basically have two Q-functions that have different experience sets and alternate at every step when bootstrapping. Is this understanding correct?\n\nThanks in advance :)","flair":"one\tDiscussion"}
{"author":"siddharth-agrawal","created":"Sun Oct 09 09:25:21 EDT 2016","text":"What is the procedure for converting log-likelihood values to bits\/dim? Clarification of just one example will be enough. The log-likelihood value reported in the NICE paper (https:\/\/arxiv.org\/abs\/1410.8516) for CIFAR-10 is 5371.78. The same value is reported as 4.48 in the PixelRNN paper (https:\/\/arxiv.org\/abs\/1601.06759).","flair":"one\tDiscussion"}
{"author":"infstudent","created":"Mon Oct 17 02:34:44 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.04161 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Why Deep Neural Networks? Authors: Shiyu Liang, R. Srikant (Submitted on 13 Oct 2016) Abstract: Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1\/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1\/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1\/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a \"small\" number of neurons. Subjects: Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.04161 [cs.LG]   (or arXiv:1610.04161v1 [cs.LG] for this version) Submission history From: Shiyu Liang [view email] [v1] Thu, 13 Oct 2016 16:34:30 GMT (340kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"ldrude","created":"Mon Oct 17 09:09:14 EDT 2016","text":"When training a sequence to sequence model or maybe a CTC based ASR system, you are either forced to use a batch size of one, or need to account for varying sequence lengths for each batch element in every single network element in your model.\n\nTensorflow's tf.nn.dynamic_rnn() function handles this, by just updating the state of an RNN, when the current time index is still valid for a batch entry. Otherwise, the old state is kept and it outputs zeros for all newer frames. A blog post about this is provided in [1] and was already discussed in \/r\/MachineLearning. The lasagne wrapper uses a mask_input for the lasagne.layers.RecurrentLayer [3].\n\nNevertheless, this does not solve the problem for other parts of a model. A model might as well contain CNN layers, where one dimension is dynamic. Even a regular RNN might have a final linear layer, which also needs to account for different sequence lengths.\n\nAn often used technique is to use masking in the loss function. Chainer for example does this, by introducing an ignore label [2].\n\nThe problem becomes more apparent, when you want to use normalization (i.e. batch normalization or normalization along the time axis). In case of a batch normalization, gradients incoming to an underlying linear layer may change its weight matrix along invalid paths.\n\nSorting your data in approximately equal lengths utterances or using bucketing is a good idea, but does not entirely solve the problem.\n\nTLDR: Can different sequence lengths be handled at a framework level or does every network element need to be changed in order to be sequence-lengths-ready?\n\n[1] https:\/\/danijar.com\/variable-sequence-lengths-in-tensorflow\/\n[2] https:\/\/github.com\/pfnet\/chainer\/blob\/285e6558640425f61b6aa8c00564ccf37643babb\/chainer\/functions\/loss\/softmax_cross_entropy.py#L63\n[3] http:\/\/lasagne.readthedocs.io\/en\/latest\/modules\/layers\/recurrent.html#lasagne.layers.CustomRecurrentLayer","flair":"one\tDiscussion"}
{"author":"PachecoAndre","created":"Sun Oct 30 12:54:50 EDT 2016","text":" Ranking of classification algorithms in terms of mean- standard deviation using A-TOPSIS André G. C. Pachecoa, Renato A. Krohlinga,b aGraduate Program in Computer Science, PPGI & bProduction Engineering Department UFES - Federal University of Espirito Santo Av. Fernando Ferrari 514 - CEP 29060-270, Vitória, ES, Brazil apacheco.comp@gmail.com bkrohling.renato@gmail.com Abstract In classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, i.e., how can we rank the algorithms? In machine learning it is common run the algorithms several times and then a statistic is calculated in terms of means and standard deviations. In order to compare the performance of the algorithms, it is very common to employ statistical tests. However, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. In this paper, we present the so called A-TOPSIS, based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), to solve the problem of ranking and comparing classification algorithms in terms of means and standard deviations. We use two case studies to illustrate the A-TOPSIS for ranking classification algorithms and the results show the suitability of A-TOPSIS to rank the algorithms. The presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. Finally, to encourage researchers to use the A-TOPSIS for ranking algorithms we also presented in this work an easy-to-use A-TOPSIS web framework. Keywords machine learning, classification algorithms, comparison of algorithms, ranking, TOPSIS. 1 Introduction In machine learning, more precisely in classification problems, it is common applying different algorithms to many benchmarks several times. Normally, the performance of the algorithms are analyzed by means of mean and standard deviation of some known metric, such as the classification accuracy. Next, we need to compare the algorithms and a difficult question arises: how to compare these algorithms effectively? The first answer to this question is to use the statistical tests, i.e., parametric and\/or nonparametric. The statistical tests can detect if there are differences between the performances of the algorithms (Derrac et al., 2011; García et al. 2009). One problem is if there are differences, which algorithm is the best one? which is the second better, and which one is the worst? Using nonparametric statistical tests, it is necessary to make pairwise and multiple comparisons among the algorithms. Obviously, the number of tests required increases greatly with the number of algorithms being analyzed. This is problematic, firstly because of the tiresome work of comparing each pair of algorithms; secondly, and more importantly, the probability of making a mistake increases. In addition, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. Recently, Krohling, Lourenzutti and Campos (2015) presented a new approach to support the selection of the best algorithms by using the Hellinger distance (Hellinger, 1909; Lourenzutti and Krohling, 2014). This approach, called Hellinger-TOPSIS, provides a rank order of the algorithms in a very easy and direct way, by means of the mean and the standard deviation of the performance of the algorithms. However, the Hellinger-TOPSIS presents some shortcomings. Firstly, the mean and the standard deviation of the performance of the algorithms have the same importance. Usually, the mean of the performance is more important than the standard deviation and in the Hellinger-TOPSIS we can not control the influence of these two parameters. Second, if any algorithm in the group is deterministic, i.e, the results obtained are described just by the means and we need to compare it with others stochastic ones, the Hellinger-TOPSIS cannot handle with such a case, because in the algorithm the standard deviation must be different of zero. In our previous work, we proposed the A-TOPSIS (Krohling and Pacheco, 2015), a new approach that provides a rank order of the evolutionary algorithms in cases where the performance of the mailto:krohling.renato@gmail.com algorithms are expressed in terms of means and standard deviations. In this work, we extend our previous approach by providing an in-depth investigation for two case studies for classification problems and developing an easy-to-use web framework for A-TOPSIS. The remainder of this paper is organized as follows: Section 2 describes the TOPSIS. In Section 3, we present the approach based on TOPSIS to deal with data matrix consisting of performance of algorithms in terms of means and standard deviations and we briefly describe the framework. In Section 4, we present simulation results for two case studies involving the classification task in order to illustrate the suitability of the presented approach. In Section 5, conclusions and directions for future work are given. 2 TOPSIS - Technique for Order Preference by Similarity to Ideal Solution The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) developed by Hwang & Yoon (1981) is a technique to evaluate the performance of alternatives through the similarity with the ideal solution. According to this technique, the best alternative would be one that is closest to the positive-ideal solution and farthest from the negative-ideal solution. The positive-ideal solution is the one that maximizes the benefit criteria and minimizes the cost criteria. The negative-ideal solution maximizes the cost criteria and minimizes the benefit criteria. In summary, the positive-ideal solution is composed of all the best values attainable for the criteria, and the negative-ideal solution consists of all the worst values attainable for the criteria. The interested reader shall refer to Behzadian et al. (2012) for a broad survey about TOPSIS. Let us consider the decision matrix A, which consists of alternatives and criteria, described by: 1 ... m A A A  1 11 1 1 ... n n m mn C C x x x x           (1) where 1 2, , , mA A A are viable alternatives, and 1 2, , , nC C C are criteria, ijx indicates the rating of the alternative iA with respect to criterion .jC The weight vector  1 2, ,..., nW w w w is composed of the individual weights ( 1,..., ),jw j n for each criterion jC and satisfies 1 1. n j j w   In general, the criteria can be classified into two types: benefit and cost. The benefit criterion means that a higher value is better, while for the cost criterion the opposite is valid. The data of the decision matrix A come from different sources, so it is necessary to normalize it in order to transform it into a dimensionless matrix, which allows the comparison of the various criteria. In this work, we use the normalized decision matrix with 1,..., , and 1,..., .ij mxn R r i m j n      The normalized value ijr is calculated as: 2 1 , with 1,..., ; 1,..., . ij ij m ij i x r i m j n x      (2) or max , with 1,..., ; 1,..., . ij ij i x r i m j n x    (3) The normalized decision matrix R represents the relative rating of the alternatives. After normalization, one calculates the weighted normalized decision matrix ij mxn P p    with 1,..., , and 1,...,i m j n  by multiplying the normalized decision matrix by its associated weights. The weighted normalized value ijp is calculated as: with 1,..., , and 1,..., .ij j ijp w r i m j n    (4) The TOPSIS is described in the following steps (Hwang and Yoon, 1981; Krohling and Campanharo, 2011): Step 1: Identify the positive ideal solutions A (benefits) and negative ideal solutions A (costs) as follows: 1 2 ( , ,..., )nA p p p     (5) 1 2 ( , ,..., )nA p p p     (6) where 1 2max , ;min ,j ij ij ii p p j J p j J         1 2min , ;max ,j ij ij i i p p j J p j J         1J and 2J represent the criteria benefit and cost, respectively. Step 2: Calculate the Euclidean distances from the positive ideal solution A (benefits) and the negative ideal solution A (costs) of each alternative iA , respectively as follows: 2 1 ( ) n i ij j d d     (7) 2 1 ( ) n i ij j d d     (8) where ,with 1,..., .ij j ijd p p i m     ,with 1,..., .ij j ijd p p i m     Step 3: Calculate the relative closeness coefficients i for each alternative iA with respect to the positive ideal solution as given by: i i i i d d d       (9) Step 4: Rank the alternatives according to the relative closeness. The best alternatives are those that have higher value i and therefore should be chosen. Next, we describe the A-TOPSIS approach involving two matrices. 3 A-TOPSIS - An approach based on TOPSIS for ranking algorithms The decision matrix consisting of alternatives and criteria is described by 11 1 11 11 1 1 1 1 1 ( , ) ( , ) ( , ) ( , ) n n n m mn m m mn mn x x D x x                            where 1 2, , , mA A A are alternatives, 1 2 , ,..., nC C C are criteria, ijx indicates the rating of the alternative iA with respect to criterion jC described in terms of its mean and standard deviations , ij ij  , respectively. In the context of comparison of algorithms, the alternatives consists of the algorithms and the criteria are the benchmark problems. One could interpret this problem as consisting of two decision matrices given by  , .D M M  11 1 1 n m mn M                11 1 1 n m mn M                 In this context, we develop a new framework combining the TOPSIS for ranking algorithms in terms of mean and standard deviations as illustrated in Fig. 1. Figure 1: Illustration of TOPSIS approach for ranking algorithms in terms of mean and standard deviations. 3.1 A-TOPSIS algorithm The steps of the algorithm are described as follows: Step 1: Normalize the matrices M and .M Step 2: Identify the positive ideal solutions A (benefits) and negative ideal solutions A (costs) for each matrix as follows: 1 2 ( , ,..., )nA p p p     (10) 1 2 ( , ,..., )nA p p p     (11) where 1 2max , ;min ,j ij ij ii p p j J p j J         1 2min , ;max ,j ij ij i i p p j J p j J         1J and 2J represent the criteria benefit and cost, respectively. Step 3: Calculate the Euclidean distances from the positive ideal solution A (benefits) and the negative ideal solution A of each alternative iA , respectively as follows: 2 1 ( , ) with 1,..., . n j iji j d p p i m      (12) 2 1 ( ) with 1,..., . n i j ij j d p p i m      (13) Step 4: Calculate the relative closeness coefficients for each alternative i with respect to positive ideal solution as: with 1,..., . i i i i d i m d d        (14) Step 5: After calculating the vector i for both decision matrices, we obtain a data matrix that is made up of the two vectors of the relative closeness coefficients, as given by: 1 2 1 1 1 2 m m C                 (15) In this case, to each of the vectors, it is assigned a weight    1 2, , ,W w w w w   where and w w  represent the weight assigned to the criteria means, and standard deviations, respectively, which satisfies 1.w w   One can now obtain the weighted relative-closeness coefficients matrix by introducing the importance weights to each one of the relative-closeness coefficient vector, as given by: 1 2 1 1 2 1 1 2 1 2 m m w w C w w                 (16) From this stage on, the method continues by applying the standard TOPSIS to the resulting matrix in order to identify the global ranking. Step 6: Identify the global positive ideal solution GA  and the global negative ideal solution GA  , respectively, as follows: 1 2 1 2 ( , ) max , ;min .l lG G G i i ii A p p l J l J             (17) 1 2 1 2 ( , ) min , ;max .l lG G G i i i i A p p l J l J             (18) where 1J and 2J represent the criteria benefit and cost, respectively. Step 7: Calculate to each alternative iA the distances from the global positive ideal solution GA  and from the global negative ideal solution GA  , respectively, as follows: 2 2 1 ( ) with 1,...., . l l Gi i G l d p i m      (19) 2 2 1 ( ) with 1,...., . l l Gi i G l d p i m      (20) Step 8: Calculate the global relative-closeness coefficients Gi for each alternative iA with respect to global positive ideal solution GA  as: Gi Gi Gi Gi d d d       (21) Step 9: Rank the alternatives according to the relative closeness coefficients. The best alternatives are those that have higher value Gi and therefore should be chosen. 3.2 A-TOPSIS web framework In order to encourage researchers and practitioners in different areas of knowledge to use the A- TOPSIS for ranking algorithms, we provide an easy-to-use web framework. As shown in figure 2, to use this framework the user needs to set the matrices M and M as .csv files and the value of the weights for each one. Thereby, the framework provides the graph bar rank and the values of the closeness coefficients. Figure 2: The A-TOPSIS framework The A-TOPSIS framework can be easily used by accessing the web address http:\/\/www.inf.ufes.br\/~agcpacheco\/alg-ranking\/. 3 Simulation Results In this section, we present two case studies involving classification problems. In order to compare our results, we also apply the Hellinger-TOPSIS for each case and we used the non-parametric Friedman test followed by Wilcoxon test as a pos hoc, both with 0.05valuep  (Derrac et al., 2011). As the Hellinger- TOPSIS is not able to handle with standard deviation equal to zero, we set a very small value as standard deviation in cases where this occur. 3.1 Case study I In this case study, we have an ensemble of classifiers, containing four classifiers: feedforward neural network (FNN), extreme learning machine (ELM), discriminative restricted Boltzmann machine (DRBM) and K-nearest neighbors (KNN). In addition, we have three aggregation methodologies: average of the supports (AVG), majority voting (MV) and Choquet integral (CHO). So, the goal is to obtain a rank of the classifiers according to 12 benchmarks. In this case, the alternatives are the classifiers and the criteria are the benchmarks. In table 1 is shown the performance of the classifiers, in terms of mean and standard deviation of the classification accuracy. Obviously, to apply A-TOPSIS for this case the criterion is set as benefit, i.e., the higher the value is, the better. http:\/\/www.inf.ufes.br\/~agcpacheco\/alg-ranking\/ As we can see in table 1, the KNN algorithm does not have standard deviation, it was used with just one value of K. Because of this, we divided this study in two parts: first, we remove the KNN and consider the remaining classifiers. Secondly, we consider the standard deviation of the KNN as zero. Next, we carry out a sensitivity study by varying the weights for mean and standard deviation, respectively, and present the results for each part of the study. 3.1.1 Case study I \u2013 Part I Removing the KNN of the decision matrix, in table 2 we present the results of the rank by varying the weights. Weight variation Ranking [mean, std] [0.5, 0.5] CHO MV AVG ELM DRBM FNN [0.6, 0.4] CHO MV AVG FNN ELM DRBM [0.7, 0.3] CHO MV AVG FNN ELM DRBM [0.8, 0.2] CHO MV AVG FNN ELM DRBM [0.1, 0.9] CHO MV AVG FNN ELM DRBM [1, 0] CHO MV AVG FNN ELM DRBM Table 2: Rank by varying the values of the weight \u2013 Case study I, Part I As we can see, the first, second and third place in the rank do not change regardless the weight. In fact, the only change in the rank occurs when the values of the weights become [0.6, 0.4]. In this case, the FNN rises to the fourth place, the ELM goes down to the second place and the DRBM goes to the last place. From the values of the weight [0.6, 0.4] to [1, 0] the rank keeps the same. In figure 3 is illustrated the raking in bar graph for each value of weights for this part of the study. Table 1: The classifiers performance for each benchmark \u2013 Case study I Classifiers Benchmarks Susy Higgs Covtype DNA Isolet Cancer Cred. Aus Diabetic Iris Spam Statlog Wine FNN 78,14 ± 0,65 63,21 ± 1,19 75,22 ± 1,09 91,36 ± 1,34 89,41 ± 1,7 94,87 ± 0,62 83,05 ± 1,07 71,02 ± 2,31 95,62 ± 1,36 93,61 ± 0,8 99,59 ± 0,06 95,91 ± 2,16 DRBM 76,39 ± 0,32 63,4 ± 0,3 66,25 ± 0,17 93,45 ± 0,07 93,74 ± 0,16 95,39 ± 0,28 82,88 ± 0,8 60,48 ± 1,69 89,4 ± 0,95 90,8 ± 0,56 92,08 ± 0,04 95,15 ± 1,28 ELM 79,39 ± 0,29 63,99 ± 0,09 76,01 ± 0,11 90,59 ± 0,75 86,81 ± 0,6 95,07 ± 0,5 83,38 ± 0,81 72,37 ± 1,09 94,81 ± 1,96 89,43 ± 0,57 98,3 ± 0,08 91,13 ± 2,8 KNN 70,88 ± 0 59,84 ± 0 75,81 ± 0 85,98 ± 0 88,24 ± 0 95,23 ± 0 67,63 ± 0 61,73 ± 0 95,55 ± 0 72,75 ± 0 98,73 ± 0 67,92 ± 0 AVG 78,38 ± 0,59 64,01 ± 0,96 71,84 ± 0,42 92,18 ± 0,92 93,73 ± 0,26 95,38 ± 0,35 83,14 ± 0,55 68,84 ± 3,03 95,11 ± 1,47 93,4 ± 0,61 98,73 ± 0,01 96,16 ± 1,44 MV 78,14 ± 0,54 63,75 ± 0,64 75,75 ± 0,24 92,64 ± 0,57 93,93 ± 0,31 94,93 ± 0,42 83,15 ± 0,98 70,05 ± 2,24 95,62 ± 1,42 92,02 ± 0,37 99,32 ± 0,04 95,22 ± 1,76 CHO 78,58 ± 0,51 64,7 ± 0,72 76,85 ± 0,29 93,69 ± 0,38 93,75 ± 0,16 95,57 ± 0,35 83,52 ± 0,85 71,42 ± 0,97 95,7 ± 1,29 93,78 ± 0,65 98,73 ± 0,01 97,18 ± 1,34 Figure 3: Rank in bar graph for each values of the weights \u2013 Case study I, Part I We compare the results obtained by A-TOPSIS with the Hellinger - TOPSIS. In this case, the rank become stable when the values of the weights are [0.6, 0.4]. Therefore, we decide to choose these weights. In table 3 is presented the rank for each methodology, which is also depicted in figure 4 by means of bar graph. According to the results of the table 3, both methods obtained the same rank. Method Ranking A-TOPSIS CHO MV AVG FNN ELM DRBM Hellinger -TOPSIS CHO MV AVG FNN ELM DRBM Table 3: Rank comparison between A-TOPSIS and H-TOPSIS \u2013 Part I Figure 4: Rank in bar graph for A-TOPSIS and H-TOPSIS \u2013 Case study I, Part I The Friedman test for this part of the case study I provides 0.00005valuep  leading to reject 0 .H Then, we performe pairwise comparisons by means of the Wilcoxon test, presented in table 4. According to the results of the Wilcoxon test, the CHO classifier is significantly different comparing to the other ones. Furthermore, the DRBM classifier is significantly different comparing to AVG, MV and CHO. The results of the statistical tests indicate that in this case, the CHO classifier is the best and the DRBM is the worst one. This finding is consistent with the results obtained by A-TOPSIS. Nonetheless, this statistical test cannot provide a rank with all the classifiers as A-TOPSIS does. Pairwise p Pairwise p FNN - CHO 0.009277 ELM - CHO 0.042480 DRBM - AVG 0.015137 AVG - CHO 0.000977 DRBM - MV 0.026855 MV -CHO 0.009277 DRBM - CHO 0.000488 - - Table 4: Wilcoxon test pairwise comparison with p less than 0.05 \u2013 Case study I, part I 3.1.2 Case study I - Part II Considering the KNN in table 1, the results of the rank by varying the weights are presented in table 5. Weight variation Ranking [mean, std] [0.5, 0.5] CHO MV KNN DRBM AVG > ELM FNN [0.6, 0.4] CHO MV AVG ELM FNN DRBM KNN [0.7, 0.3] CHO MV AVG FNN ELM DRBM KNN [0.8, 0.2] CHO MV AVG FNN ELM DRBM KNN [0.1, 0.9] CHO MV AVG FNN ELM DRBM KNN [1, 0] CHO MV AVG FNN ELM DRBM KNN Table 5: Rank by varying the values of the weights \u2013 Case study I, Part II As we can notice in table 5, for all weights the first and the second place in the rank do not change. When the weights of the mean and the standard deviation are equals, the KNN reaches the third place in the rank. However, when the values of the weights become [0.6, 0.4], only 10% of variation, the KNN goes to the last place. Moreover, for these values of the weights, the ELM rises to the fourth place. Nevertheless, when the values turn to [0.7, 0.3], the ELM and the FNN change its positions. From the values of the weights [0.7, 0.3] to [1, 0], the rank does not change. In figure 5 is illustrated the rank in bar graph for each value of weights for this part of the study. Finally, we compare the results obtained by A-TOPSIS with the Hellinger TOPSIS. We decide to choose the values of the weights as [0.7, 0.3]. As shown in table 5, for these values of the weights, the rank become stable and does not change when we vary the weights. The rank obtained for A-TOPSIS and Hellinger-TOPSIS is presented in table 6 and illustrated in figure 6. Method Ranking A-TOPSIS CHO MV AVG FNN ELM DRBM KNN Hellinger - TOPSIS CHO ELM MV FNN AVG DRBM KNN Table 6: Rank comparison between A-TOPSIS and H-TOPSIS \u2013 Case study I, Part II In table 6, we can easily check that the ranking of the alternatives CHO, FNN, DRBM and KNN are the same in both methods. Thereby, the ranking of the best and worst alternatives are kept. On the other hand, the ranking of the alternatives MV, AVG and ELM have changed position. Figure 5: Rank in bar graph for each values of the weight \u2013 Case study I, Part II Figure 6: Rank in bar graph for A-TOPSIS and H-TOPSIS \u2013 Case study I, Part II Similarly to the previous part, we performed the Friedman test, which obtained 0.00007,valuep  leading to reject 0 .H Next, we applied the Wilcoxon test and the results are shown in table 7. As in the part I, the CHO classifier is significantly different comparing to the other ones, according to Wilcoxon test. In addition, the KNN is also significantly different from the others, except DRBM. Finally, the DRBM is significantly different comparing to AVG, MV, CHO. The results of the statistical tests indicate that the best classifier is CHO and the worst are the KNN and DRBM. Also in this case, this finding is consistent with the rank obtained by A-TOPSIS. Pairwise p Pairwise p FNN - KNN 0.004883 ELM - CHO 0.042480 FNN - CHO 0.009277 KNN - AVG 0.009766 DRBM - AVG 0.015137 KNN - MV 0.003418 DRBM - MV 0.026855 KNN - CHO 0.000977 DRBM - CHO 0.000488 AVG - CHO 0.000977 ELM - KNN 0.042480 MV - CHO 0.009277 Table 7: Wilcoxon test pairwise comparison with p less than 0.05 \u2013 Case study I, part II 3.2 Case study II This case study presented by Wen et al. (2013) consists in a classification problem with 8 classifiers performed to 10 benchmarks as shown in table 8. Similar to the case study 1, our goal is to find the rank of the classifiers according to the mean and the standard deviation of the classifiers performance. It is worth mentioning that in this case study the authors used the error rate as accuracy. The A-TOPSIS can easily handle with this just changing the criterion from benefit to cost, i.e., the smaller the value is, the better. Firstly, we carry out a sensitivity study by varying the weights for mean and standard deviation, respectively, and the rank of the classifiers for each weight is shown in table 9 and in figure 7 by means of bar graph. Weight variation Ranking [mean, std] [0.5, 0.5] REC LPC EKNN HKNN LNC ALH FKNN KNN [0.6, 0.4] REC HKNN LNC LPC EKNN ALH FKNN KNN [0.7, 0.3] REC HKNN LNC LPC EKNN ALH FKNN KNN [0.8, 0.2] REC HKNN LNC LPC EKNN ALH FKNN KNN [0.9, 0.1] REC HKNN LNC LPC EKNN ALH FKNN KNN [1, 0] REC HKNN LNC LPC ALH EKNN FKNN KNN Table 9: Rank by varying the values of the weights \u2013 Case study II As we can notice in table 9 and in figure 7, the first and the last place are the same for all weights. When the weights are varied from [0.5, 0.5] to [0.6, 0.4] the positions of the classifiers HKNN, LNC, LPC and EKNN are changed. For the values of the weights from [0.6, 0.4] to [0.9, 0.1] the rank does not change. Finally, when the weights become [1, 0] the classifiers EKNN and ALH switch their positions. Classifiers Benchmarks B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 KNN 3.49 ± 0.49 3.35 ± 0.39 25.83 ± 0.70 19.42 ± 0.79 30.82 ± 1.40 14.10 ± 1.60 4.40 ± 0.60 3.94 ± 0.37 18.96 ± 1.71 29.84 ± 0.55 FKNN 3.49 ± 0.24 3.13 ± 0.30 26.28 ± 0.61 15.96 ± 0.68 30.45 ± 1.82 14.10 ± 1.04 4.53 ± 0.56 2.40 ± 0.36 19.04 ± 0.93 30.50 ± 1.03 EKNN 2.26 ± 0.56 2.96 ± 0.57 25.76 ± 1.01 10.77 ± 1.25 30.71 ± 0.51 14.10 ± 1.24 5.07 ± 0.37 3.90 ± 0.32 19.19 ± 1.21 31.16 ± 1.40 LMC 2.70 ± 0.45 2.35 ± 0.47 24.85 ± 0.86 11.11 ± 1.05 31.59 ± 1.84 11.90 ± 1.47 4.27 ± 0.76 2.00 ± 0.24 19.41 ± 1.90 26.84 ± 1.08 LPC 2.60 ± 0.49 2.68 ± 0.58 25.03 ± 0.68 12.36 ± 0.59 32.0 ± 1.83 11.90 ± 1.21 4.00 ± 0.82 2.10 ± 0.31 19.04 ± 1.0 27.10 ± 0.40 HKNN 2.04 ± 0.84 2.57 ± 0.73 25.31 ± 0.83 11.44 ± 0.55 29.63 ± 2.34 11.81 ± 1.66 4.00 ± 0.82 1.98 ± 0.49 20.44 ± 1.29 24.64 ± 1.28 ALH 2.48 ± 0.51 3.07 ± 0.44 29.92 ± 0.88 11.62 ± 0.79 31.37 ± 2.36 10.86 ± 1.28 4.67 ± 0.94 2.00 ± 0.32 22.30 ± 1.27 26.72 ± 1.20 REC 1.58 ± 0.46 2.13 ± 0.32 24.35 ± 0.75 6.10 ± 0.43 28.48 ± 1.20 10.76 ± 0.93 4.00 ± 0.00 0.79 ± 0.13 18.52 ± 0.74 24.40 ± 0.90 Table 8: The classifiers performance for each benchmark - Case study 2 Figure 7: Rank in bar graph for each values of the weight \u2013 Case study II Again, we compare the results obtained by A-TOPSIS with the Hellinger - TOPSIS. We decided to choose the values of the weights as [0.7, 0.3]. As shown in table 9, for these values, the rank become stable and does not change when we vary the weights. The rank obtained for A-TOPSIS and Hellinger-TOPSIS is presented in table 10 and illustrated in figure 8 by means of bar graph. Method Ranking A-TOPSIS REC HKNN LNC LPC EKNN ALH FKNN KNN Hellinger - TOPSIS REC LNC  HKNN LPC ALH FKNN EKNN KNN Table 10: Rank comparison between A-TOPSIS and H-TOPSIS \u2013 Case study II In table 10, we can notice that there are some differences in the ranks. For both ranks, the first and the last places are kept. In the Hellinger-TOPSIS, the ranking for the classifiers LNC and HKNN are too close, because of this, they are tied in second place. Furthermore, as we can see in figure 8, even though we can distinguish the ranking for the classifiers FKNN, EKNN, LPC and ALH, the values of the closeness coefficients are too close. This issue does not occur in A-TOPSIS. Figure 8: Rank in bar graph for A-TOPSIS and H-TOPSIS \u2013 Case study II To the case study II we also performed the Friedman test that provides 0.00001,valuep  leading to reject 0H one more time. The pos hoc obtained by Wilcoxon test is shown in table 11. In this case study, the classifier REC is significantly different comparing to the other ones. Moreover, the KNN classifier is significantly different from LMC, LPC, HKNN and REC, and the FKNN from LMC, HKNN, REC and ALH. The statistical tests point that the best classifier in the group is REC and the worst are KNN and EKNN. In this case study, the results obtained by the statistical test and the A-TOPSIS are consistent. Pairwise p Pairwise p KNN - LMC 0.019531 EKNN - REC 0.001953 KNN - LPC 0.037109 LMC - REC 0.001953 KNN - HKNN 0.027344 LPC - REC 0.003906 KNN - REC 0.001953 HKNN - ALH 0.027344 FKNN - LMC 0.048828 HKNN - REC 0.003906 FKNN - HKNN 0.027344 ALH - REC 0.001953 FKNN - REC 0.001953 - - Table 11: Wilcoxon test pairwise comparison with p less than 0.05 \u2013 Case study II 4 Concluding remarks In this work, we present an application of the A-TOPSIS algorithm to compare performance of classification algorithms by means of the mean and the standard deviation. In order to illustrate the method, two case studies involving classification problems is presented. The rank provided by A-TOPSIS is compared with Hellinger-TOPSIS and we carried out the nonparametric statistical tests of Friedman and Wilcoxon in order to analyze the rank order. The results for the case studies show the effectiveness of the method. Despite the case studies are only for classification problems, the presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. In terms of computational burden, the A-TOPSIS consists of a very simple computation procedure. It is worth to note that, the TOPSIS is a well-established and reliable methodology, which guarantee the A- TOPSIS effectiveness. Finally, in order to encourage researchers and practitioners in different areas of knowledge, especially in machine learning, to use the A-TOPSIS, we provided a web framework to rank algorithms in an easy way. Acknowledgments. A.G.C. Pacheco would like to thank the financial support of the Brazilian agency CAPES and R.A. Krohling thanks the financial support of the Brazilian agency CNPq under grant nr. 309161\/2015-0. References Behzadian, M., Otaghsara, S. K., Yazdani, M., & Ignatius, J. A state-of the-art survey of TOPSIS applications, Expert Systems with Applications, 39 (2012) 13051-13069. Demsar, J. Statistical comparisons of classifiers over multiple data sets, Journal of Machine Learning Research, 7 (2006) 1-30. Derrac, J., Garcia, S., Molina, D. & Herrera, F. A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms, Swarm and Evolutionary Computation, 1 (2011) 3-18. García, S., Molina, D., Lozano, M. & Herrera, F. A study on the use of nonparametric tests for analyzing the evolutionary algorithms\u2019 behaviour: A case study on the CEC\u20192005 special session on real parameter optimization, Journal of Heuristics, 15 (2009) 617-644. Hellinger, E. Neue Begründung der Theorie quadratischer Formen von unendlich vielen Veränderlichen (in German). Journal für die reine und angewandte Mathematik 136 (1909) 210-271. Hwang, C. L. & Yoon, K. P. Multiple attribute decision making methods and applications. Springer- Verlag, Berlin (1981). Krohling, R.A. & Campanharo, V.C. Fuzzy TOPSIS for group decision making: A case study for accidents with oil spill in the sea, Expert Systems with Applications, 38 (2011) 4190-4197. Krohling, R.A. & Pacheco, A.G.C. A-TOPSIS - An approach based on TOPSIS for ranking evolutionary algorithms. Published in International Conference on Information Technology and Quantitative Management (ITQM 2015), Rio de Janeiro, Brazil, July 21-24, Procedia Computer Science, 55 (2015) 308- 317. Krohling, R.A., Lourenzutti, R. & Campos, M. Ranking and comparing evolutionary algorithms with Hellinger-TOPSIS. Applied Soft Computing, 36 (2015) 217-226. Lourenzutti, R. & Krohling, R.A. The Hellinger distance in multicriteria decision making: An illustration to the TOPSIS and TODIM methods, Expert Systems with Applications, 41 (2014) 4414-4421. Wen, G., Chen, X., Jiang, L., & Li, H. Performing classification using all kinds of distances as evidences. In 12th IEEE International Conference in Cognitive Informatics & Cognitive Computing, (2013) 168-174. http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0957417410010389 http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0957417410010389 ","flair":"three\tResearch"}
{"author":"datartai","created":"Mon Nov 21 11:27:15 EST 2016","text":" Home Blog Portuguese Home Blog Portuguese Posted on November 21, 2016 How (not) to forecast an election We use a hierarchical Bayesian model to show how a simple pro-Trump bias has a huge effect on forecasting election results. Unlike other countries, where presidential elections follow a simple majority rule, the American election follows a different set of rules that makes it very hard for outsiders to understand what is going on. However, the idea is quite simple: each state has a number of electoral college votes, totaling 538 votes across the nation. The winner candidate of each state election takes all of its electoral college votes (with two exceptions). If a candidate receives 270 electoral votes or more overall, he or she wins the election. This makes the election an interesting forecasting problem. Instead of predicting the total number of votes in each candidate, the problem is forecasting the results of each state individually. Prediction is very difficult, especially if it's about the future.1 Our interest is not to forecast the election, which is not a random variable anymore (spoiler alert: Trump won). We want to show how a Bayesian forecasting model works, and propose a modification that explains why statistical models would fail so badly if there were polling biases. Polling In theory, a poll should randomly sample the voting population. However, in practice there are several possible polling problems that can compromise their use in naive statistical analyses: Shy Tory2 \/ Bradley3 \/ \u201Cshame\u201D effect: Voters are not comfortable with revealing their particular preference. Selection bias: People who answer the pollsters are from an unrepresentative subgroup (e.g. people who answer landline phones). Stratification errors: Wrong subpopulation determination when doing stratified sampling. Candidate and (non)-voting correlation: As voting is not mandatory, the presidential preference of some people may be correlated with their chance of (not) voting. Temporal preference change: The preferences change over time (a lot of people make up their minds in the last week). Sample noise: Any sample from a population will be noisy and its statistics will not be identical to the population statistics. Simple forecasts usually only consider the last item when estimating the margins of error or uncertainty. If you only consider this and nothing more, multiple polls will be treated as independent and unbiased, and the more polls you use, the smaller the forecasting errors will be, until there is almost certainty. The other effects are biases that will make the average of multiple polls unreliable and possibly useless. As they are not directly available for modeling, the only way to estimate them is by making strong assumptions or using polling results from previous elections. We do not attempt to model exactly those issues here, we rather include all of them in a bias term that is shared across all polls, and show that even a small bias term favorable to Trump completely changes the forecast. Forecast We leave the details of our hierarchical Bayesian forecasting model at the end, for the advanced reader. Now we show the results of its forecasts. However, depending on which polls we include, we have very different results. The polls to use in a forecasting model should be recent and of good quality (conducted with appropriate methodology and with past successes). As a proxy of quality, we use the grades from 538.4 We found that the simple choice of which polls to use has a very large impact on the probability of victory of each candidate: Polling choice Clinton Trump Neither Last poll with good grade 87.3% 10.5% 2.2% Polls with the best 3 grades (over the last month) 99.3% 0.6% 0.1% All polls from last week 100.0% 0.0% 0.0% We consider polls with good grades to be above B, but this is not possible for some states, so we use a more tolerant definition in those cases. When we say last week or last month, we mean from the Election Day. By aggregating the polls with different weights (e.g. according to their quality or distance from election), we would have endless forecasting possibilities, which explains the diversity found in the media before the election: New York Times5: 85% Clinton, 15% Trump Huffington Post6: 98% Clinton, 1.7% Trump Princeton Election Consortium7: 93% Clinton 5388: 71.4% Clinton, 28.6% Trump Of the mainstream forecasters, only 538 cannot be included in the same bucket of certain Clinton victory. Notice that the other forecasts are consistent with our own results shown in the previous table. How come the forecasters, including us, made such egregious mistakes? As in many statistical problems, the answer lies with the data: garbage in, garbage out. Bias impact We encompass all possible polling issues in a general bias term. We use a bias term that is favorable to Trump on election day because that is clearly what happened on November 8. This can be interpreted as a hidden preference for Trump that is not captured by the polls by all the issues explained before. Instead of fixing the bias to an arbitrary value, we use a uniform random variable. We start with zero bias, where Clinton is almost surely the victor, and increase its span until Trump is almost certainly the victor: We see that even a small polling bias has a huge effect on the election forecast. This explains why 538 had a more favorable Trump forecast as they included a polling bias and did not treat the polls as independent samples9, but this also indicates that even 538 probably underestimated the polling biases. You can check for yourself how the bias impacts the results of each state election on the map below. For each state we forecast the predicted percentage of votes of each candidate, with adjustable bias. Note that percentage of votes is different from the probability of winning in the state, which would also take the uncertainty of the prediction into account. 0 0 Bias: Hierarchical Bayesian forecasting model We can use a hierarchical Bayesian model to aggregate the information of each state poll to form a globally coherent forecast. Overall, each poll\u2019s likelihood is modeled as a multinomial, with Dirichlet prior (per state) and uniform hyperprior (per nation). This way, prior information is shared across states and we can use weakly informative hyperpriors. We start with an overall national preference over each candidate, modeled as three independent wide uniform distributions: $$ \\alpha_c \\sim Uniform(0, 1000) \\\\ c \\in [Trump, Clinton, Ind.] $$ Then, we have the voting intention in each state, with the national preference as prior: $$ \\theta_s = [\\theta^{Trump}_s, \\theta^{Clinton}_s, \\theta^{Ind.}_s] \\\\ \\theta_s \\sim Dirichlet(\\alpha) \\\\ s \\in States $$ Finally, each poll is modeled as one independent sample of the voting intention of the state: $$ poll_{s,d} \\sim Multinomial(\\theta_s, N_{poll_{s,d}}) \\\\ d \\in Dates $$ We infer the posteriors of the unknown parameters (state voting intentions and national preferences) given the observed variables (the polls we decided to include). The posterior is our knowledge of the unseen variables after observing statistically related variables. Depending on the choice of which polls to include as observations, as we explained before, the posteriors and thus the forecast will be different. To forecast the probability of each candidate winning the state election, we use the same multinomial likelihood that was used for the inference. However, now the voting intentions are the posterior given the polls, and number of voters is chosen to match 2012 election numbers. Thus, for each state we sample the predicted number of votes of each candidate on election day using the following formula: $$ election_s \\sim Multinomial(\\theta_s, N^{voters}_s) $$ The candidate with more votes takes all the electoral colleges of the state (we ignore the particularities of Nebraska and Maine). We sum the electoral colleges votes of each candidates, and if a candidate wins 270 votes or more, he or she is the winner. We repeat this process multiple times in order to determine the probability of each candidate winning the election. To add the bias term in our forecast to account for all the polling issues already cited, we make a simple change to the predictive model: $$ \\theta^{bias}_s \\leftarrow [\\theta^{Trump}_s + \\text{bias}, \\theta^{Clinton}_s - \\text{bias}, \\theta^{Ind.}_s] \\\\ bias \\sim Uniform(0, \\epsilon) \\\\ election_s \\sim Multinomial(\\theta^{bias}_s, N^{voters}_s) \\\\ $$ This bias always stochastically favors Trump. We must subtract the same value from Clinton in order to guarantee \\( \\theta^{bias} \\) remains a valid probability simplex. In our experiments above, we vary \\( \\epsilon \\) from 0 to 5%. Code Take a look at our code and feel free to play with it. Here is how we implemented our model in Stan10: data { int nb_polls; \/\/ Number of polls int nb_states; \/\/ Number of states (51 because of D.C.) int nb_candidates; \/\/ Number of candidates (3: Trump, Clinton, Ind.) int polls_states[nb_polls]; \/\/ Poll -> state map int votes[nb_polls, nb_candidates]; \/\/ Polled votes for each candidate int nb_voters[nb_states]; \/\/ Number of voters for forecasting real bias; \/\/ Polling bias } parameters { simplex[nb_candidates] theta[nb_states]; \/\/1 - Trump, 2 - Clinton, 3 - Ind. vector[nb_candidates] alpha; } model { for(c in 1:nb_candidates) alpha[c] ~ uniform(0, 1000); \/\/ Weakly informative hyperprior for(s in 1:nb_states) theta[s] ~ dirichlet(alpha); \/\/ Dirichlet prior per state for(p in 1:nb_polls) \/\/ Multinomial observations (polled values) votes[p] ~ multinomial(theta[polls_states[p]]); } generated quantities { int votes_pred[nb_states, nb_candidates]; \/\/ Predicted number of votes on election day real epsilon[nb_states]; \/\/ Bias random variable simplex[nb_candidates] theta_bias[nb_states]; \/\/ Biased voting intentions \/\/ The deltas below are used to ensure that the biased thetas form a valid simplex real delta_t[nb_states]; real delta_h[nb_states]; real delta[nb_states]; for(s in 1:nb_states) { if(bias == 0.0) epsilon[s] Conclusion We have found that even a modestly sophisticated statistical model does very little to counter unreliable data. A proper forecasting model for the American elections must include polling biases, as we have shown in a general way. To arrive at a precise number, you must either make assumptions on the polling methodology, or calibrate the polls weights using their historical reliability. This could be the reason that 538 had Trump winning at the highest probability of the mainstream media. We have also to consider that when you forecast 70% probability of winning, the prediction is expected to fail 30% of the time, so it is hard to evaluate models and forecasters using only one observation. Even though the 2016 election was one of most surprising polling misses in recent years, the result was not a black swan. Nassim Nicholas Taleb pointed out, before the election, that the mainstream forecasts were not reliable due to their significant volatility. According to his model based on option theory, this volatility should have pulled the probabilities toward 50-5011. As a follow-up, if there is interest, we want to explore a time-series model where the voting intentions follow a random walk. To do this, we need to change the underlying model to allow the unseen random walk influence the polling results. Following Andrew Gelman\u2019s suggestion, we can change the Dirichlet prior to a softmax prior, and then we can make the softmax parameters follow a random walk 12. Authors Pedro Tabacof ptabacof at datart.com.br Ramon Oliveira roliveira at datart.com.br Please enable JavaScript to view the comments powered by Disqus. datart.com.br contato@datart.com.br (19) 98243-9668 Social Github Linkedin Twitter Subscribe to our newsletter Submit ","flair":"four\tProject"}
{"author":"code_kansas","created":"Fri Nov 18 13:48:01 EST 2016","text":"The core of this framework is the use of the Binarized Neural Network (BNN) described in Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. This framework seemed ideal for use with embedded systems such as an Arduino (or Raspberry Pi), but to my knowledge this wasn't already available. The system consists of two parts: The two sample scripts, and demonstrate how to train a model to discriminate an XOR function. The model uses a lot more weights than would theoretically be necessary for this task, but together they demonstrate how to adapt the code to other use cases. The outputs show the predictions for an XOR function. This is how the file was generated. Encoding weights \/ activations with values of -1 and 1 as binary values: . Then matrix multiplication done using the XOR operation. Here's an example: Using binary weights and activations of -1 and 1: Using binary weights and activations of -1 and 1: Because the operations are done this way, I made it so that matrix dimensions must be multiples of the integer sizes. Padding can be used to make data line up correctly (although if someone wants to change this, LMK). This was a project that I worked on for CalHacks 3.0 (although I never submitted it).","flair":"four\tProject"}
{"author":"fmcm","created":"Sat Oct 22 15:19:31 EDT 2016","text":"Hi there!\n\nFirst time here, please be gentle.\n\nI'm currently working on a project - \"Using machine learning in safety and reliability\".\n\n**Some background:**\n\nIf you have a piece of equipment it and operate it, it will eventually fail. So you note down the failure time (operating hours) and replace it.After some time (especially if you have the identical equipment several times) you collect a collection of failure times.\n\nThe usual approach is to use Minitab or R or whatever software you fancy and fit a model to the data (Exponential, Weibull, Gamma, etc.). So ideally you find a model which fits rather nice and then for the future you can describe the behaviour of your equipment with just one or two parameters.\n\nThis is often displayed in form of a Cumulative Failure Probability plot. See for example here:\n\nhttp:\/\/support.minitab.com\/en-us\/minitab\/17\/cum_failure_plot_def.png\n\nAt time 0 you have a 0% chance of being failed. It increases in a certain shape and will ultimately reach 100%.\n\n\n**Task:**\n\nReality is different. Equipment does not follow a model perfectly. But thanks to the modern world of Machine Learning, that approach might not be necessary.\n\nMy current task is:\n\n* Have a bunch of failure dates (randomly generated and by some real life data)\n* Feed that into a machine learning algorithm without implying any failure distribution model\n* Get a model which will predict the percentage of being failed at a given time.\n\n\n**Where I am now:**\n\nSo far I have used python and scikit-learn to built my very first python program ever (learning python AND machine learning at the same time here!).\n\nI randomly draw failure dates which perfectly follow a Weibull distribution so far. Later I want to implement some randomness and noise, but for the start, I want to keep to \"pure\" values so I know what to expect.\n\nI feed it my failure times and it creates a failure distribution curve. The more samples I use, the better it fits (obviously).\n\nSee following pictures:\nWeibull with Alpha = 1.2 and Lambda = 0.0002\nBlue line = The Weibull curve\nRed Dots = The random data points\nRed Line = The fitting line of my machine learning algorithm\n\nFirst picture with 200 samples, second one with 20 samples.\n\nSee http:\/\/imgur.com\/a\/Ln34X for illustration of where I am now.\n\nTill now I'm fairly happy. I actually managed to write two programs: One to generate a CSV with random Weibull values and one with the actual machine learning routine included. I used the RandomForestRegressor model provided by scikit-learn.\n\n**Future tasks:**\n\n* Implement proper data preprocessing (normalize)\n* Compare to traditional parametric model fitting\n* Include some covariates (\"My device is running at temperature X and at load level Y, what are the chances of it being failed at time Z?\")\n\n**My Questions:**\n\nDo you have any general advices on such a \"simple\" machine learning task? Is my approach so far reasonable or am I missing something obvious?\n\nIn reality you don't have hundreds of data points, just in the range 10-60. Are there any advices with handling \"Little Data\"?\n\nThe cumulative failure probability curve is per definition always rising. The probability to be failed at time X+1 is always higher than at X. How can I tell that to my machine learning routine?\n\nI am grateful for every input you can give as I am eager to learn more - both in python\/numpy\/scipy\/scikit-learn and machine learning theory in general.","flair":"four\tProject"}
{"author":"SoMuchQuestions","created":"Thu Nov 17 04:35:57 EST 2016","text":"For a work related project, I need to use a recursive neural network (not recurrent). This architecture is well described at page 400 of: \n\n&gt; Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. Chapter 10, [http:\/\/www.deeplearningbook.org\/contents\/rnn.html](http:\/\/www.deeplearningbook.org\/contents\/rnn.html).\n\nThe architecture is shown in the following screenshot taken from this very same book: \n[Architecture of the recursive Neural Network](http:\/\/i.imgur.com\/Umz2i0p.png)\n\nIn my case : I want the tree structure to be dependent of the data (each individual of the training set has its own tree structure) and the length of the sequence not constant.\n\nAny ideas on how i should proceed ? My best guess right now would be to implement the whole thing myself using Theano. The project found [here](https:\/\/github.com\/ofirnachum\/tree_rnn) is a great start\n\nFor the record, I also posted an [issue ](https:\/\/github.com\/fchollet\/keras\/issues\/4395) on keras github.","flair":"four\tProject"}
{"author":"Niourf","created":"Tue Nov 15 07:59:50 EST 2016","text":"Surprise is an open source Python library that provides with tools to build and evaluate the performance of many recommender system prediction algorithms. Its goal is to make life easy(-ier) for reseachers, teachers and students who want to play around with new recommender algorithms ideas and teach\/learn more about recommender systems. Surprise was designed with the following purposes in mind: The name SurPRISE (roughly) stands for Simple Python RecommendatIon System Engine. The easiest way is to use pip (you'll need numpy): Or you can clone the repo and build the source (you'll need Cython and numpy): Here is a simple example showing how you can (down)load a dataset, split it for 3-folds cross-validation, and compute the MAE and RMSE of the SVD algorithm. surprise surprise Dataset surprise evaluate # Load the movielens-100k dataset (download it if needed), # and split it into 3 folds for cross-validation. data Dataset.load_builtin() data.split() # We'll use the famous SVD algorithm. algo SVD() # Evaluate performances of our algorithm on the dataset. perf evaluate(algo, data, [, ]) (perf) The following table shows the average RMSE and MAE and total execution time of various algorithms on a 5-folds cross-validation procedure. The dataset is the Movielens 100k dataset. All experiments are run on a laptop with Intel Core i3 1.7 GHz, 4Go Ram. The documentation with many other usage examples is available online on ReadTheDocs. This project is licensed under the GPLv3 license. Any kind of feedback\/criticism would be greatly appreciated (software design, documentation, improvement ideas, spelling mistakes, etc...). Please feel free to contribute and send pull requests!","flair":"four\tProject"}
{"author":"mlwohls","created":"Fri Nov 18 08:20:45 EST 2016","text":"I'm trying to build a Word2Vec type model in Tensorflow with my Vocab_Size &gt; 1,000,000 'words' (the 'words' are actually the id's of Wikipedia articles and I'm looking at the cross references between articles as the \"surrounding words\").  The problem I'm having is in constructing the embeddings matrix.  I keep getting OOM errors in TensorFlow.  The network is pretty small, and my memory usage feels like it should fit in my GTX 1080 (avail memory ~ 7.3GB).\nBatch Size = 1000 (I've tried a tiny batch size as well)\n\n- Input size (1000,1)  [single indexes so (batch,1)]\n- Embed_Lookup Table (1000000, 300)  [I've tried embed lengths even at 50]\n- Hidden_Layer_Weights [24 nodes] (300, 24)\n- Output_Layer_Weights (24, 1000000)\n\nSo by my math, total memory usage should be approx (in MB, using 4 bytes per Float32):\n\n- Input = 4 * 1000 \/ 1024^2  &gt; 0.1MB\n- Embed_Lookup= 4 * 1000000 * 300 \/ 1024^2 ~= 1,144MB\n- Hidden_Layer= 4 * 300 * 24 \/ 1024^2 &lt; 0.1MB\n- Output_Layer=  4 * 24 * 1000000 \/ 1024^2 ~= 92MB\nTotal Memory of Model ~= 1236MB ~= 1.2 GB\n\nEven accounting for a 2x memory usage on back-prop, that model should easily fit on my GPU.  Am I looking at this wrong? Or maybe I have my GPU\/CUDA\/CuDNN\/TF configured wrong?\nThanks!","flair":"one\tDiscussion"}
{"author":"to4life2","created":"Thu Nov 24 14:20:25 EST 2016","text":"Hey guys, \n\nI am looking for relevant papers and machine learning approaches\/algorithms to tackle the following problem which I am trying to accomplish on my own hardware platform. Here is the description:\n\n- I have a simple robot arm with 6 joints and a gripper. \n- It is mounted on a table, on this table is an object (say, a small wooden block) as well as a bucket. \n- Mounted on the robot are multiple cameras (at the base and on top of the gripper) as well as on a few \"stands\" that overlook the table. Say there are between 3 - 6 RGB video sources at 640x480 pixels at 24 fps. \n- The plan is to first manually program the robot - by moving joints individually (not physically but programmatically) - to complete the task of picking up the block and dropping it in the bucket. I will record this data: the motor commands (inputs) and camera video feeds (observations) during this \"training demonstration\". \n- From there I want to use machine learning algorithms to let the robot arm automatically replicate the task again, if say its starting position is different, the block is placed at a different initial start location\/orientation, the bucket is slightly moved, etc. \n- We can assume that the object to picked up, and the bucket, will start within the reach\/workspace of the robot arm. \n- Bonus: while competing the sequenced task, I want to minimize objectives like the time taken to complete it, and the total amount of time each motor joint is powered, to minimize electrical power used.\n\nIs this problem fairly well defined? If not please tell me and I'll clarify further. The eventual goal is to just set up the scene with the robot on the table, the block and bucket nearby, then press \"Go\" on our computer and the robot will run its sequence of learned motor commands, using the video feeds and its learned algorithm, to complete the task, able to work with minor variations in the scene. \n\nI have a background in machine learning, mathematics, and programming, but I do not know what recommended, say, newer deep learning algorithms and software frameworks might be a good fit for this job. \n\nEDIT - please also feel free to recommend computing hardware and\/or framework, my team has a decent budget towards this research project. So we can buy e.g. an awesome GPU, or get cloud computing, etc.","flair":"four\tProject"}
{"author":"Pieranha","created":"Mon Oct 24 17:53:15 EDT 2016","text":"I need to model the dynamics of time-evolving bipartite graphs, where one side, U, has nodes m=100 and the other side, V, has nodes n=5. There's roughly 50 time steps. The goal is to categorize millions of such graphs into one of 10 well-known labels, which are supplied with the graphs.\n\nNodes in U have the same meaning across graphs, whereas this is not the case for nodes in V (i.e. nodes in U are ordered, whereas nodes in V are unordered). The graphs are unweighted, start with a single link and 1-3 links are added with every time step. Links cannot be removed. Each graph has the same # of nodes, but different # of timesteps. Most importantly, the link creation dynamics are wildly different. Capturing these dynamics are critical to do the labeling properly.\n\nWhat are some ways this could be modeled? I'm in the brainstorming phase so all ideas are highly appreciated!","flair":"one\tDiscussion"}
{"author":"bronzestick","created":"Thu Nov 17 14:57:12 EST 2016","text":"In several applications using LSTMs\/RNNs (such as char-rnn or Alex Graves handwriting generation), at training time we learn a recurrent network that predicts the next input as the output of the previous time-step. At prediction time, to do multi-step prediction we feed the last input and the output we obtain is considered to be the input at the next time-step. This process is continued until we get predictions for as many steps as we need.\nI understand that since the LSTM is trained to do one-step prediction very accurately, this model still ends up with good predictions for multiple steps after the last input. But, as is bound to happen, the uncertainty\/error in the predictions grows at each step and we could end up with very inaccurate predictions after 2-3 steps.\nI am currently trying the model introduced by Graves for a sequence prediction and I observe that my one-step prediction is quite accurate but the error blows up after 2 steps.\nI was wondering, is there a better way to do this, that has been explored in the past? Note that the problem is not to generate new text or handwriting, but conditioned on a part of the sequence, predict the next few time-steps.","flair":"one\tDiscussion"}
{"author":"cooijmanstim","created":"Sun Oct 30 22:43:28 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.09038 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Professor Forcing: A New Algorithm for Training Recurrent Networks Authors: Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio (Submitted on 27 Oct 2016) Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. Comments: NIPS 2016 Accepted Paper Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1610.09038 [stat.ML]   (or arXiv:1610.09038v1 [stat.ML] for this version) Submission history From: Alex Lamb [view email] [v1] Thu, 27 Oct 2016 23:54:31 GMT (597kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"Juffin","created":"Fri Oct 07 10:23:09 EDT 2016","text":"Also, this property doesn't depend on pixel's position. I want to approximate this property with a convolutional neural network. What kind of CNN should I use to calculate properties of all the pixels of an image?\n\nI don't want to calculate it for every neighbourhood separately because that would cause a lot of extra convolutions, since the CNN for every neighbourhood would be the same.\n\nAre such algorithms implemented in any CNN package?\n\n","flair":"null\tnull"}
{"author":"acanai","created":"Tue Nov 15 01:17:42 EST 2016","text":"Paper: https:\/\/arxiv.org\/abs\/1603.06277Code: https:\/\/github.com\/mattjj\/svaeHow to combine the complementary strengths of probabilistic graphical models and neural networks? We compose latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We use this framework to automatically segment and categorize mouse behavior from raw depth video.Matthew Johnson, David Duvenaud, Alex Wiltschko, Bob Datta, Ryan P. Adams Neural Information Processing Systems, 2016","flair":"three\tResearch"}
{"author":"jordo45","created":"Tue Oct 04 10:53:09 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.00163 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.AI cs.CV stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets Authors: Petar Veličković, Duo Wang, Nicholas D. Lane, Pietro Liò (Submitted on 1 Oct 2016 (v1), last revised 17 Oct 2016 (this version, v2)) Abstract: In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and\/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests. Comments: To appear in the 7th IEEE Symposium Series on Computational Intelligence (IEEE SSCI 2016), 8 pages, 6 figures. Minor revisions, in response to reviewers' comments Subjects: Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1610.00163 [stat.ML]   (or arXiv:1610.00163v2 [stat.ML] for this version) Submission history From: Petar Veličković [view email] [v1] Sat, 1 Oct 2016 18:01:35 GMT (410kb,D) [v2] Mon, 17 Oct 2016 14:51:36 GMT (411kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"yacob_uk","created":"Thu Oct 06 18:04:47 EDT 2016","text":"This might not be the best sub for this question... but I'm looking for a very aggressive stoplist for text parsing. \n\nI'm interested in removing most if not all of the non \"interesting\"* words from a keyword returning processing step, which I'm not in control of. \n\nI'm currently taking a very hamfisted approach by merging all the english language stoplists I can find, but I wondered if there was a better\/cleaner\/safer way of getting the linguistic mundanities filtered out of the keyword lists. \n\nMy interest is actually broader than just English, Te Reo Maori is also a language I'm keen to try and get a stoplist for, followed by other Pacific Island languages.  I suspect these will be much harder to establish.\n\n*What is interesting... that in an of itself is a problem. I think I'm currently interested in names, places, concepts, nouns, some verbs.... There is another facet of this which is looking at generating named entity lists from some institutional authorities (e.g. Lib of Congress), but thats another whole topic. ","flair":"null\tnull"}
{"author":"alexeyr","created":"Sun Nov 13 17:13:55 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.06940 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.AI < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Artificial Intelligence Title: Safety Verification of Deep Neural Networks Authors: Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu (Submitted on 21 Oct 2016 (v1), last revised 5 Nov 2016 (this version, v2)) Abstract: Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop the first SMT-based automated verification framework for feed-forward multi-layer neural networks that works directly with the code of the network, exploring it layer by layer. We define safety for a region around a data point in a given layer by requiring that all points in the region are assigned the same class label. Working with a notion of a manipulation, a mapping between points that intuitively corresponds to a modification of an image, we employ discretisation to enable exhaustive search of the region. Our method can guarantee that adversarial examples are found for the given region and set of manipulations. If found, adversarial examples can be shown to human testers and\/or used to fine-tune the network, and otherwise the network is declared safe for the given parameters. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. Subjects: Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1610.06940 [cs.AI]   (or arXiv:1610.06940v2 [cs.AI] for this version) Submission history From: Xiaowei Huang [view email] [v1] Fri, 21 Oct 2016 20:16:16 GMT (2541kb,D) [v2] Sat, 5 Nov 2016 16:05:08 GMT (2448kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"n1ghtw1sh","created":"Mon Oct 03 03:21:59 EDT 2016","text":"Data analysis seeks to learn from experience.\u202F Better inferences require better thinking and better tools. Practical advice about how to   make more credible conclusions based on data.\u202FWhat we can expect\u202Fin the future, and what we should aspire to in the future. ","flair":"null\tnull"}
{"author":"guukizl","created":"Wed Oct 12 06:13:43 EDT 2016","text":"This is mostly related to academia I guess. \n\nI am just gonna get it out and humiliate myself. \n\nI just wanted to ask if such a thing happens where in Math undergraduates (senior) are doing internships in ML. Consider my example, like I have taken courses in Statistics (two in number covering theoretical statistics ('statistical inference' by casella and berger)), linear algebra, calc apart from other pure math courses(eg.analysis) and have had experience with specifically use of Neural networks in ML in my last summer (you can pretty much sum that up by [Neural Networks and Deep Learning](http:\/\/www.neuralnetworksanddeeplearning.com) and recently I had experience in appying RNNs (used LSTMs anyway) for time series data (a semester project). \n\n\nSo is it, that it is too soon (like I haven't had formal intro to Statistical Learning or Optimization yet) for me to get into any \"math\" intensive i don't know research\/project in ML and only masters and phd people do this kind of stuff or are math people (undergrads) getting into ML which is not just all about application?\n\n\nI guess by a math undergraduate I also mean that he\/she is not your classic programmer\/cs major. As in he\/she is able to implement anything in say python (or R) but he\/she just would be much more interested in the mathematical intricacies of the models itself rather than cool implementations and usage of these algorithms on real data and analysing them. \n\nThank you.\n\n","flair":"one\tDiscussion"}
{"author":"anantzoid","created":"Sat Nov 19 09:21:11 EST 2016","text":"This is a Tensorflow implementation of Conditional Image Generation with PixelCNN Decoders which introduces the Gated PixelCNN model based on PixelCNN architecture originally mentioned in Pixel Recurrent Neural Networks. The model can be conditioned on latent representation of labels or images to generate images accordingly. Images can also be modelled unconditionally. It can also act as a powerful decoder and can replace deconvolution (transposed convolution) in Autoencoders and GANs. A detailed summary of the paper can be found here. These are some conditioned samples generated by the authors of the paper: This is the architecture for Gated PixelCNN used in the model: The gating accounts for remembering the context and model more complex interactions, like in LSTM. The network stack on the left is the Vertical stack that takes care of blind spots that occure while convolution due to the masking layer (Refer the Pixel RNN paper to know more about masking). Use of residual connection significantly improves the model performance. This implementation consists of the following models based on the Gated PixelCNN architecture: To only generate images append the flag after the command. To train the any model on CIFAR-10 dataset, add the flag. Refer for other available flags for hyperparameter tuning. The system was trained on a single AWS p2.xlarge spot instance. The implementation was only done on MNIST dataset. Generation of samples based on CIFAR-10 images took the authors 32 GPUs trained for 60 hours. To visualize the graph and loss during training, run:","flair":"four\tProject"}
{"author":"rmltestaccount","created":"Tue Oct 11 17:22:31 EDT 2016","text":" LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 1 Oracle Performance for Visual Captioning Li Yao1 li.yao@umontreal.ca Nicolas Ballas1 nicolas.ballas@umontreal.ca Kyunghyun Cho3 kyunghyun.cho@nyu.edu John R. Smith2 jsmith@us.ibm.com Yoshua Bengio1 yoshua.bengio@umontreal.ca 1 Université de Montréal 2 IBM T.J. Watson Research 3 New York University Abstract The task of associating images and videos with a natural language description has attracted a great amount of attention recently. The state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates performances that an oracle can obtain. In order to disentangle the contribution from visual model from the language model, our oracle assumes that high- quality visual concept extractor is available and focuses only on the language part. We demonstrate the construction of such oracles on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite the simplicity of the model and the training procedure, we show that current state-of-the-art models fall short when being compared with the learned oracle. Furthermore, it suggests the inability of current models in capturing important visual concepts in captioning tasks. 1 Introduction With standard datasets publicly available, such as COCO and Flickr [15, 22, 38] in im- age captioning, and YouTube2Text, MVAD and MPI-MD [12, 27, 30] in video caption- ing, the field has been progressing in an astonishing speed. For instance, the state-of-the- art results on COCO image captioning has been improved rapidly from 0.17 to 0.31 in BLEU [3, 9, 10, 17, 18, 23, 25, 34, 36]. Similarly, the benchmark on YouTube2Text has been repeatedly pushed from 0.31 to 0.50 in BLEU score [1, 26, 28, 32, 33, 35, 37, 39]. While obtaining encouraging results, captioning approaches involve large networks, usually leveraging convolution network for the visual part and recurrent network for the language side. It therefore results model with a certain complexity where the contribution of the dif- ferent component is not clear. Instead of proposing better models, the main objective of this work is to develop a method that offers a deeper insight of the strength and the weakness of popular visual captioning c© 2016. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. Citation Citation {Hodosh, Young, and Hockenmaier} 2013 Citation Citation {Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{á}r, and Zitnick} 2014 Citation Citation {Young, Lai, Hodosh, and Hockenmaier} Citation Citation {Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, and Saenko} 2013 Citation Citation {Rohrbach, Rohrbach, Tandon, and Schiele} 2015{} Citation Citation {Torabi, Pal, Larochelle, and Courville} 2015 Citation Citation {Bengio, Vinyals, Jaitly, and Shazeer} 2015 Citation Citation {Devlin, Gupta, Girshick, Mitchell, and Zitnick} 2015{} Citation Citation {Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, and Darrell} 2015 Citation Citation {Karpathy and Fei-Fei} 2014 Citation Citation {Kiros, Salakhutdinov, and Zemel} 2014 Citation Citation {Mao, Xu, Yang, Wang, and Yuille} 2015 Citation Citation {Qiprotect unhbox voidb@x penalty @M {}Wu, Shen, vanprotect unhbox voidb@x penalty @M {}den Hengel, Liu, and Dick} 2015 Citation Citation {Vinyals, Toshev, Bengio, and Erhan} 2014 Citation Citation {Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, and Bengio} 2015{} Citation Citation {Ballas, Yao, Pal, and Courville} 2016 Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} Citation Citation {Rohrbach, Qiu, Titov, Thater, Pinkal, and Schiele} 2013 Citation Citation {Venugopalan, Rohrbach, Donahue, Mooney, Darrell, and Saenko} 2015{} Citation Citation {Venugopalan, Xu, Donahue, Rohrbach, Mooney, and Saenko} 2015{} Citation Citation {Xu, Venugopalan, Ramanishka, Rohrbach, and Saenko} 2015{} Citation Citation {Yao, Torabi, Cho, Ballas, Pal, Larochelle, and Courville} 2015 Citation Citation {Yu, Wang, Huang, Yang, and Xu} 2015 2 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING models. In particular, we propose a trainable oracle that disentangles the contribution of the visual model from the language model. To obtain such oracle, we follow the assumption that the image and video captioning task may be solved with two steps [11, 28] . Consider the model P(w|v) where v refers to usually high dimensional visual inputs, such as representa- tions of an image or a video, and w refers to a caption, usually a sentence of natural language description. In order to work well, P(w|v) needs to form higher level visual concept, either explicitly or implicitly, based on v in the first step, denoted as P(a|v), followed by a lan- guage model that transforms visual concept into a legitimate sentence, denoted by P(w|a). a referes to atoms that are visually perceivable from v. The above assumption suggests an alternative way to build an oracle. In particular, we assume the first step is close to perfect in the sense that visual concept (or hints) is observed with almost 100% accuracy. And then we train the best language model conditioned on hints to produce captions. Using the proposed oracle, we compare the current state-of-the-art models against it, which helps to quantify their capacity of visual modeling, a major weakness, apart from the strong language modeling. In addition, when being applied on different datasets, the oracle offers insight on the intrinsic difficulty and blessing of them, a general guideline when designing new algorithms and developing new models. Finally, we also relax the assumption to investigate the case where visual concept may not be realistically predicted with 100% accuracy and demonstrate a quantity-accuracy trade-off in solving visual captioning tasks. 2 Related work Visual captioning The problem of image captioning has attracted a great amount of atten- tion lately. Early work focused on constructing linguistic templates or syntactic trees based on a set of concept from visual inputs [20, 21, 24]. Another popular approach is based on caption retrieval in the embedding space such as Devlin et al. [9], Kiros et al. [18]. Most re- cently, the use of language models conditioned on visual inputs have been widely studied in the work of Fang et al. [11] where a maximum entropy language model is used and in Don- ahue et al. [10], Karpathy and Fei-Fei [17], Mao et al. [23], Vinyals et al. [34], Xu et al. [36] where recurrent neural network based models are built to generate natural language descrip- tions. The work of Devlin et al. [8] advocates to combine both types of language models. Furthermore, CIDEr [31] was proposed as an alternative evaluation metric for image caption- ing and is shown to be more advantageous compared with BLEU and METEOR. To further improve the performance, Bengio et al. [3] suggests a simple sampling algorithm during training, which was one of the winning recipes for MSR-COCO Captioning challenge 1, and Jia et al. [16] suggests the use of extra semantic information to guide the language generation process. Similarly, video captioning has made substantial progress recently. Early models such as Barbu et al. [2], Kojima et al. [19], Rohrbach et al. [28] tend to focus on constrained do- mains with limited appearance of activities and objects in videos. They also rely heavily on hand-crafted video features, followed by a template-based or shallow statistical machine translation approaches to produce captions. Borrowing success from image captioning, re- cent models such as Donahue et al. [10], Rohrbach et al. [26], Venugopalan et al. [32, 33], Xu et al. [35], Yao et al. [37], Yu et al. [39] and most recently Ballas et al. [1] have adopted a 1http:\/\/mscoco.org Citation Citation {Fang, Gupta, Iandola, Srivastava, Deng, Doll{á}r, Gao, He, Mitchell, Platt, etprotect unhbox voidb@x penalty @M {}al.} 2015 Citation Citation {Rohrbach, Qiu, Titov, Thater, Pinkal, and Schiele} 2013 Citation Citation {Kulkarni, Premraj, Ordonez, Dhar, Li, Choi, Berg, and Berg} 2013 Citation Citation {Kuznetsova, Ordonez, Berg, Berg, and Choi} 2012 Citation Citation {Mitchell, Han, Dodge, Mensch, Goyal, Berg, Yamaguchi, Berg, Stratos, and Daum{é}protect unhbox voidb@x penalty @M {}III} 2012 Citation Citation {Devlin, Gupta, Girshick, Mitchell, and Zitnick} 2015{} Citation Citation {Kiros, Salakhutdinov, and Zemel} 2014 Citation Citation {Fang, Gupta, Iandola, Srivastava, Deng, Doll{á}r, Gao, He, Mitchell, Platt, etprotect unhbox voidb@x penalty @M {}al.} 2015 Citation Citation {Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, and Darrell} 2015 Citation Citation {Karpathy and Fei-Fei} 2014 Citation Citation {Mao, Xu, Yang, Wang, and Yuille} 2015 Citation Citation {Vinyals, Toshev, Bengio, and Erhan} 2014 Citation Citation {Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, and Bengio} 2015{} Citation Citation {Devlin, Cheng, Fang, Gupta, Deng, He, Zweig, and Mitchell} 2015{} Citation Citation {Vedantam, Zitnick, and Parikh} 2015 Citation Citation {Bengio, Vinyals, Jaitly, and Shazeer} 2015 Citation Citation {Jia, Gavves, Fernando, and Tuytelaars} 2015 Citation Citation {Barbu, Bridge, Burchill, Coroian, Dickinson, Fidler, Michaux, Mussman, Narayanaswamy, Salvi, etprotect unhbox voidb@x penalty @M {}al.} 2012 Citation Citation {Kojima, Tamura, and Fukunaga} 2002 Citation Citation {Rohrbach, Qiu, Titov, Thater, Pinkal, and Schiele} 2013 Citation Citation {Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, and Darrell} 2015 Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} Citation Citation {Venugopalan, Rohrbach, Donahue, Mooney, Darrell, and Saenko} 2015{} Citation Citation {Venugopalan, Xu, Donahue, Rohrbach, Mooney, and Saenko} 2015{} Citation Citation {Xu, Venugopalan, Ramanishka, Rohrbach, and Saenko} 2015{} Citation Citation {Yao, Torabi, Cho, Ballas, Pal, Larochelle, and Courville} 2015 Citation Citation {Yu, Wang, Huang, Yang, and Xu} 2015 Citation Citation {Ballas, Yao, Pal, and Courville} 2016 http:\/\/mscoco.org LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 3 more general encoder-decoder approach with end-to-end parameter tuning. Videos are input into a specific variant of encoding neural networks to form a higher level visual summary, followed by a caption decoder by recurrent neural networks. Training such type of models are possible with the availability of three relatively large scale datasets, one collected from YouTube by Guadarrama et al. [12], the other two constructed based on Descriptive Video Service (DVS) on movies by Torabi et al. [30] and Rohrbach et al. [27]. The latter two have recently been combined as the official dataset for Large Scale Movie Description Challenge (LSMDC) 2. Capturing higher-level visual concept The idea of using intermediate visual concept to guide the caption generation has been discussed in Qi Wu et al. [25] in the context of image captioning and in Rohrbach et al. [26] for video captioning. Both work trained classifiers on a predefined set of visual concepts, extracted from captions using heuristics from linguis- tics and natural language processing. Our work resembles both of them in the sense that we also extract similar constituents from captions. The purpose of this study, however, is different. By assuming perfect classifiers on those visual atoms, we are able to establish the performance upper bounds for a particular dataset. Note that a simple bound is suggested by Rohrbach et al. [26] where METEOR is measured on all the training captions against a particular test caption. The largest score is picked as the upper bound. As a comparison, our approach constructs a series of oracles that are trained to generate captions given different number of visual hints. Therefore, such bounds are clear indication of models\u2019 ability of capturing concept within images and videos when performing caption generation, instead of the one suggested by Rohrbach et al. [26] that performs caption retrieval. 3 Oracle Model The construction of the oracle is inspired by the observation that P(w|v) =∑a Pθ (w|a)P(a|v) where w = {w1, . . . ,wt} denotes a caption containing a sequence of words having a length t. v denotes the visual inputs such as an image or a video. a denotes visual concepts which we call \u201Catoms\u201D. We have explicitly factorized the captioning model P(w|v) into two parts, P(w|a), which we call the conditional language model given atoms, and P(a|v), which we call conditional atom model given visual inputs. To establish the oracle, we assume that the atom model is given, which amounts to treat P(a|v) as a Dirac delta function that assigns all the probability mass to the observed atom a. In other words, P(w|v) = Pθ (w|a). Therefore, with the fully observed a, the task of image and video captioning reduces to the task of language modeling conditioned on atoms. This is arguably a much easier task compared with the direct modeling of P(w|v), therefore a well-trained model could be treated as a performance oracle of it. Information contained in a directly influences the difficulty of modeling Pθ (w|a). For instance, if no atoms are available, Pθ (w|a) reduces to unconditional language modeling, which could be considered as a lower bound of P(w|v). By increasing the amount of information a carries, the modeling of Pθ (w|a) becomes more and more straightforward. 2https:\/\/goo.gl\/2hJ4lw Citation Citation {Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, and Saenko} 2013 Citation Citation {Torabi, Pal, Larochelle, and Courville} 2015 Citation Citation {Rohrbach, Rohrbach, Tandon, and Schiele} 2015{} Citation Citation {Qiprotect unhbox voidb@x penalty @M {}Wu, Shen, vanprotect unhbox voidb@x penalty @M {}den Hengel, Liu, and Dick} 2015 Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} https:\/\/goo.gl\/2hJ4lw 4 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 3.1 Oracle Parameterization Given a set of atoms a(k) that summarize the visual concept appearing in the visual in- puts v, this section describes the detailed parameterization of the model Pθ (w|a(k)) with θ denoting the overall parameters. In particular, we adopt the commonly used encoder- decoder framework [7] to model this conditional based on the following simple factorization Pθ (w|a(k)) = ∏Tt=1 Pθ (wt |w<t ,a(k)). Recurrent neural networks (RNNs) are natural choices when outputs are identified as sequences. We borrow the recent success from a variant of RNNs called Long-short term memory networks (LSTMs) first introduced in Hochreiter and Schmidhuber [14], formulated as the following  p(wt | w<t ,a(k)))ht ct = ψ(ht−1,ct−1,wt−1,a(k)), (1) where ht and ct represent the RNN state and memory of LSTMs at timestep t respectively. Combined with the atom representation, Equ. (1) is implemented as following ft = σ(W f Ew [wt−1]+U f ht−1 +A f Φ(a(k))+b f ), it = σ(WiEw [wt−1]+Uiht−1 +AiΦ(a(k))+bi), ot = σ(WoEw [wt−1]+Uoht−1 +AoΦ(a(k))+bo), c̃t = tanh(WcEw [wt−1]+Ucht−1 +AcΦ(a(k))+bc), ct = ft � ct−1 + it � c̃t , ht = ot � ct , where Ew denotes the word embedding matrix, as apposed to the atom embedding matrix Ea, W, U, A and b are parameters of the LSTM. With the LSTM\u2019s state ht , the probability of the next word in the sequence is pt = softmax(Up tanh(Wpht +bp)+d) with parameters Up, Wp, bp and d. The overall training criterion of the oracle is θ = argmax θ Uk(θ) = log N ∏ n=1 Pθ (w(n)|a(n,k)) = N ∑ n=1 T ∑ t=1 logPθ (w (n) t |w (n) <t ,a(n,k)), (2) given N training pairs (w(n),a(n,k)). θ represents parameters in the LSTM. 3.2 Atoms Construction Each configuration of a may be associated with a different distribution Pθ (w|a), therefore a different oracle model. We define configuration as an orderless collection of unique atoms. That is, a(k) = {a1, . . . ,ak} where k is the size of the bag and all items in the bag are different from each other. Considering the particular problem of image and video captioning, atoms are defined as words in captions that are most related to actions, entities, and attributes of entities (in Figure 1). The reason of using these three particular choices of language components as atoms is not an arbitrary decision. It is reasonable to consider these three types among the most visually perceivable ones when human describes visual content in natural language. We further verify this by conducting a human evaluation procedure to Citation Citation {Cho, Vanprotect unhbox voidb@x penalty @M {}Merri{ë}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio} 2014 Citation Citation {Hochreiter and Schmidhuber} 1997 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 5 identify \u201Cvisual\u201D atoms from this set and show that a dominant majority of them indeed match human visual perception, detailed in Section 5.1. Being able to capture these important concepts is considered as crucial in getting superior performance. Therefore, comparing the performance of existing models against this oracle reveals their ability of capturing atoms from visual inputs when P(a|v) is unknown. A set of atoms a(k) is treated as \u201Ca bag of words\u201D. As with the use of word embedding matrix in neural language modeling [4], the ith atom a(k)i is used to index the atom embedding matrix Ea[a (k) i ] to obtain a vector representation of it. Then the representation of the entire set of atoms is Φ(a(k)) = ∑ki=1 Ea[a (k) i ]. 4 Contributing factors of the oracle The formulation of Section 3 is generic, only relying on the assumption the two-step visual captioning process, independent of the parameterization in Section 3.1. In practice, however, one needs to take into account several contributing factors to the oracle. Firstly, atoms, or visual concepts, may be defined as 1-gram words, 2-gram phrases and so on. Arguably a mixture of N-gram representations has the potential to capture more com- plicated correlations among visual concepts. For simplicity, this work uses only 1-gram representations, detailed in Section 5.1. Secondly, the procedure used to extract atoms needs to be reliable, extracting mainly visual concepts, leaving out non-visual concepts. To en- sure this, the procedure used in this work is verified with human evaluation, detailed in 5.1. Thirdly, the modeling capacity of the conditional language Pθ (w|a(k)) has a direct influence on the obtained oracle. Section 3.1 has shown one example of many possible parameteriza- tions. Lastly, the oracle may be sensitive to the training procedure and its hyper-parameters (see Section 5.2). While it is therefore important to keep in mind that proposed oracle conditions on the above factors, quite surprisingly, however, with the simplest procedure and parameterization we show in the experimental section that oracle serves their purpose reasonably well. 5 Experiments We demonstrate the procedure of learning the oracle on three standard visual captioning datasets. MS COCO [22] is the most commonly used benchmark dataset in image caption- ing. It consists of 82,783 training and 40,504 validation images. each image accompanied by 5 captions, all in one sentence. We follow the split used in Xu et al. [36] where a sub- set of 5,000 images are used as validation, and another subset of 5,000 images are used for testing. YouTube2Text is the most commonly used benchmark dataset in video captioning. It consists of 1,970 video clips, each accompanied with multiple captions. Overall, there are 80,000 video and caption pairs. Following Yao et al. [37], it is split into 1,200 clips for training, 100 for validation and 670 for testing. Another two video captioning datasets have been recently introduced in Torabi et al. [30] and Rohrbach et al. [27]. Compared with YouTube2Text, they are both much larger in the number of video clips, most of which are associated with one or two captions. Recently they are merge together for Large Scale Movie Description Challenge (LSMDC). 3 We therefore name this particular dataset LSMDC. The 3https:\/\/goo.gl\/2hJ4lw Citation Citation {Bengio, Ducharme, Vincent, and Janvin} 2003 Citation Citation {Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{á}r, and Zitnick} 2014 Citation Citation {Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, and Bengio} 2015{} Citation Citation {Yao, Torabi, Cho, Ballas, Pal, Larochelle, and Courville} 2015 Citation Citation {Torabi, Pal, Larochelle, and Courville} 2015 Citation Citation {Rohrbach, Rohrbach, Tandon, and Schiele} 2015{} https:\/\/goo.gl\/2hJ4lw 6 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING official splits contain 91,908 clips for training, 6,542 for validation and 10,053 for testing. 5.1 Atom extraction visual'input' caption' entity' action' attribute' ! A! blue! train! sitting! along! side!of!a!green!forest.! train,! ! side,! ! forest! sit! blue,!green! ! Old! train! left! out! on! the! ground!has!graffiti!all!over!it! train,! it,! ground! leave,! graffiti,! have! old! Two! abandoned! blue! and! white! train! cars! next! to! trees.! car,! train,! tree! abandon! blue,! white,! next! ! A! man! with! a! skateboard! under!him,!not!touching,!are! in!the!air.! air,! skateboard,! him,! man! touch! NA! Two!guys!are!skateboarding! and! performing! jumps! and! tricks.! jump,! trick,! guy! perform,! skateboard! NA! Two! men! doing! tricks! on! skateboards! at! the! skate! park! trick,! part,! men,! skateboard! do! skate! ! ! visual'input' caption' entity' action' attribute' ! A! blue! train! sitting! along! side!of!a!green!forest.! train,! ! side,! ! forest! sit! blue,!green! ! Old! train! left! out! on! the! ground!has!graffiti!all!over!it! train,! it,! ground! leave,! graffiti,! have! old! Two! abandoned! blue! and! white! train! cars! next! to! trees.! car,! train,! tree! abandon! blue,! white,! next! ! A! man! with! a! skateboard! under!him,!not!touching,!are! in!the!air.! air,! skateboard,! him,! man! touch! NA! Two!guys!are!skateboarding! and! performing! jumps! and! tricks.! jump,! trick,! guy! perform,! skateboard! NA! Two! men! doing! tricks! on! skateboards! at! the! skate! park! trick,! park,! men,! skateboard! do! skate! ! Figure 1: Given ground truth cap- tions, three categories of visual atoms (entity, action and attribute) are automatically extracted using NLP Parser. \u201CNA\u201D denotes the empty atom set. Visual concepts in images or videos are summarized as atoms that are provided to the caption language model. They are split into three categories: actions, entities, and attributes. To identify these three classes, we utilize Stanford natural language parser 4 to automati- cally extract them. After a caption is parsed, we apply simple heuristics based on the tags produced by the parser, ignoring the phrase and sentence level tags 5: Use words tagged with {\u201CNN\u201D, \u201CNNP\u201D, \u201CNNPS\u201D ,\u201CNNS\u201D, \u201CPRP\u201D} as entity atoms. Use words tagged with {\u201CVB\u201D, \u201CVBD\u201D, \u201CVBG\u201D, \u201CVBN\u201D, \u201CVBP\u201D, \u201CVBZ\u201D} as action atoms. Use words tagged with {\u201CJJ\u201D, \u201CJJR\u201D, \u201CJJS\u201D} as attribute atoms. After atoms are identified, they are lemmatized with NLTK lemmatizer 6 to unify them to their original dictionary format 7. Figure 1 illustrates some results. We extracted atoms for COCO, YouTube2Text and LSMDC. This gives 14,207 entities, 4,736 actions and 8,671 attributes for COCO, 6,922 entities, 2,561 actions, 2,637 attributes for YouTube2Text, and 12,895 entities, 4,258 actions, 8550 attributes for LSMDC. Note that although the total number of atoms of each categories may be large, atom frequency varies. In addition, the language parser does not guarantee the perfect tags. Therefore, when atoms are being used in training the oracle, we sort them according to their frequency and make sure to use more frequent ones first to also give priority to atoms with larger coverage, detailed in Section 5.2 below. We conducted a simple human evaluation 8 to confirm that extracted atoms are indeed predominantly visual. As it might be impractical to evaluate all the extracted atoms for all three datasets, we focus on top 150 frequent atoms. This evaluation intends to match the last column of Table 2 where current state-of-the-art models have the equivalent capacity of capturing perfectly less than 100 atoms from each of three categories. Subjects are asked to cast their vote independently. The final decision of an atom being visual or not is made by majority vote. Table 1 shows the ratio of atoms flagged as visual by such procedure. 5.2 Training After the atoms are extracted, they are sorted according to the frequency they appear in the dataset, with the most frequent one leading the sorted list. Taking first k items from this list 4http:\/\/goo.gl\/lSvPr 5complete list of tags: https:\/\/goo.gl\/fU8zDd 6http:\/\/www.nltk.org\/ 7available at https:\/\/goo.gl\/t7vtFj 8details available at https:\/\/goo.gl\/t7vtFj http:\/\/goo.gl\/lSvPr https:\/\/goo.gl\/fU8zDd http:\/\/www.nltk.org\/ https:\/\/goo.gl\/t7vtFj https:\/\/goo.gl\/t7vtFj LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 7 Table 1: Human evaluation of proportion of atoms that are voted as visual. It is clear that extracted atoms from three categories contain dominant amount of visual elements, hence verifying the procedure described in Section 3.2. Another observation is that entities and actions tend to be more visual than attributes according to human perception. entities actions attributes COCO 92% 85% 81% YouTube2Text 95% 91% 72% LSMDC 83% 87% 78% gives the top k most frequent ones, forming a bag of atoms denoted by a(k) where k is the size of the bag. Conditioned on the atom bag, the oracle is maximized as in Equ (2). To form captions, we used a vocabulary of size 20k, 13k and 25k for COCO, YouTube2Text and LSMDC respectively. For all three datasets, models were trained on training set with different configuration of (1) atom embedding size, (2) word embedding size and (3) LSTM state and memory size. To avoid overfitting we also experimented weight decay and Dropout [13] to regularize the models with different size. In particular, we experimented with ran- dom hyper-parameter search by Bergstra and Bengio [5] with range [128,1024] on (1), (2) and (3). Similarly we performed random search on the weight decay coefficient with a range of [10−6,10−2], and whether or not to use dropout. Optimization was performed by SGD, minibatch size 128, and with Adadelta [40] to automatically adjust the per-parameter learn- ing rate. Model selection was done on the standard validation set, with an early stopping patience of 2,000 (early stop if no improvement made after 2,000 minibatch updates). We report the results on the test splits. 5.3 Interpretation All three metrics \u2013 BLEU, METEOR and CIDER are computed with Microsoft COCO Eval- uation Server [6]. Figure 2 summarizes the learned oracle with an increasing number of k. comparing oracle performance with existing models We compare the current state-of- the-art models\u2019 performance against the established oracles in Figure 2. Table 2 shows the comparison on three different datasets. With Figure 2, one could easily associate a particular performance with the equivalent number of atoms perfectly captured across all 3 atom cat- egories, as illustrated in Table 2, the oracle included in bold. It is somehow surprising that state-of-the-art models have performance that is equivalent to capturing only a small amount of \u201CENT\u201D and \u201CALL\u201D. This experiment highlights the shortcoming of the state-of-art visual models. By improving them, we could close the performance gap that we currently have with the oracles. quantify the diminishing return As the number of atoms k in a(k) increases, one would expect the oracle to be improved accordingly. It is however not yet clear the speed of such improvement. In other words, the gain in performance may not be proportional to the number of atoms given when generating captions, due to atom frequencies and language modeling. Figure 2 quantifies this effect. The oracle on all three datasets shows a significant gain at the beginning and diminishes quickly as more and more atoms are used. Row 2 of Figure 2 also highlights the difference among actions, entities and attributes in generating captions. For all three datasets tested, entities played much more important roles, Citation Citation {Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov} 2012 Citation Citation {Bergstra and Bengio} 2012 Citation Citation {Zeiler} 2012 Citation Citation {Chen, Fang, Lin, Vedantam, Gupta, Dollar, and Zitnick} 2015 8 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 10 50 10 0 30 0 70 0 11 00 21 00 31 00 40 00 number of atoms 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 sc o re BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR 5 10 20 60 10 0 30 0 70 0 11 00 13 00 15 00 19 00 number of atoms 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 sc o re BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR 10 50 10 0 30 0 70 0 11 00 21 00 31 00 40 00 number of atoms 0.0 0.1 0.2 0.3 0.4 0.5 sc o re BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR 10 50 10 0 30 0 70 0 11 00 21 00 31 00 40 00 number of atoms 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 C ID E r actions entities attributes combined 5 10 20 60 10 0 30 0 70 0 11 00 13 00 15 00 19 00 number of atoms 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 C ID E r actions entities attributes combined 10 50 10 0 30 0 70 0 11 00 21 00 31 00 40 00 number of atoms 0.00 0.05 0.10 0.15 0.20 0.25 M E T E O R actions entities attributes combined Figure 2: Learned oracle on COCO (left), YouTube2Text (middle) and LSMDC (right). The number of atoms a(k) is varied on x-axis and oracles are computed on y-axis on testsets. The first row shows the oracles on BLEU and METEOR with 3k atoms, k from each of the three categories. The second row shows the oracles when k atoms are selected individually for each category. CIDEr is used for COCO and YouTube2Text as each test example is associated with multiple ground truth captions, argued in [31]. For LSMDC, METEOR is used, as argued by Rohrbach et al. [26]. even more so than action atoms in general. This is particularly true on LSMDC where the gain of modeling attributes is much less than the other two categories. Although visual atoms dominant the three atom categories shown in Section 5.1, as they increase in number, more and more non-visual atoms may be included, such as \u201Cliving\u201D, \u201Ctry\u201D, \u201Cfind\u201D and \u201Cfree\u201D which are relatively difficult to be associated with a particular part of visual inputs. Excluding non-visual atoms in the conditional language model can further tighten the oracle bound as less hints are provided to it. The major difficulty lies in the labor of hand-separating visual atoms from non-visual ones as to the our best knowledge this is difficult to automate with heuristics. atom accuracy versus atom quantity We have assumed that the atoms are given, or in other words, the prediction accuracy of atoms is 100%. In reality, one would hardly expect to have a perfect atom classifier. There is naturally a trade-off between number of atoms one would like to capture and the prediction accuracy of it. Figure 3 quantifies this trade-off on COCO and LSMDC. It also indicates the upper limit of performance given different level of atom prediction accuracy. In particular, we have replaced a(k) by â(k)r where r portion of a(k) are randomly selected and replaced by other randomly picked atoms not appearing in a(k). The case of r = 0 corresponds to those shown in Figure 2. And the larger the ratio r, the worse the assumed atom prediction is. The value of r is shown in the legend of Figure 3. According to the figure, in order to improve the caption generation score, one would have two options, either by keeping the number of atoms fixed while improving the atom prediction accuracy Citation Citation {Vedantam, Zitnick, and Parikh} 2015 Citation Citation {Rohrbach, Rohrbach, and Schiele} 2015{} LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 9 Table 2: Measure semantic capacity of current state-of-the-art models. One could easily map the reported metric to the number of visual atoms captured. This establishes an equivalence between a model, the proposed oracle and a model\u2019s semantic capacity. (\u201CENT\u201D for entities. \u201CACT\u201D for actions. \u201CATT\u201D for attributes. \u201CALL\u201D for all three categories combined. \u201CB1\u201D for BLEU-1, \u201CB4\u201D for BLEU-4. \u201CM\u201D for METEOR. \u201CC\u201D for CIDEr. Note that the CIDEr is between 0 and 10. The learned oracle is denoted in bold. B1 B4 M C ENT ACT ATT ALL COCO 0.740.80 0.31 0.35 0.26 0.30 0.94 1.4 ∼200 ∼2100 >4000 ∼ 50 YouTube2Text 0.8150.88 0.499 0.58 0.326 0.40 0.658 1.2 ∼60 ∼500 >1900 ∼ 20 LSMDC N\/A0.45 N\/A 0.12 0.07 0.22 N\/A N\/A ∼40 ∼50 ∼4000 ∼10 or by keeping the accuracy while increasing the number of included atoms. As state-of-art visual model already model around 1000 atoms, we hyphotesize that we could gain more in improving the atoms accuracy rather than increase the number of atom detected by those models. 10 50 10 0 30 0 70 0 11 00 15 00 21 00 25 00 31 00 40 00 number of atoms 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 C ID E r 0 0.05 0.1 0.2 0.3 0.4 0.5 0.6 10 50 10 0 30 0 70 0 11 00 15 00 21 00 25 00 31 00 40 00 number of atoms 0.00 0.05 0.10 0.15 0.20 0.25 M E T E O R 0 0.05 0.1 0.2 0.3 0.4 0.5 0.6 Figure 3: Learned oracles with dif- ferent atom precision (r in red) and atom quantity (x-axis) on COCO (left) and LSMDC (right). The num- ber of atoms â(k)r is varied on x-axis and oracles are computed on y-axis on testsets. CIDEr is used for COCO and METEOR for LSMDC. It shows one could increase the score by either improving P(a(k)|v) with a fixed k or increase k. It also shows the maximal error bearable for different score. intrinsic difficulties of particular datasets Figure 2 also reveals the intrinsic properties of each dataset. In general, the bounds on YouTube2Text are much higher than COCO, with LSMDC the lowest. For instance, from the first column of the figure, taking 10 atoms respectively, BLUE-4 is around 0.15 for COCO, 0.30 for YouTube2Text and less than 0.05 for LSMDC. With little visual information to condition upon, a strong language model is required, which makes a dramatic difference across three datasets. Therefore the oracle, when compared across different datasets, offer an objective measure of difficulties of using them in the captioning task. 6 Discussion This work formulates oracle performance for visual captioning. The oracle is constructed with the assumption of decomposing visual captioning into two consecutive steps. We have assumed the perfection of the first step where visual atoms are recognized, followed by the 10 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING second step where language models conditioned on visual atoms are trained to maximize the probability of given captions. Such an empirical construction requires only automatic atom parsing and the training of conditional language models, without extra labeling or costly human evaluation. Such an oracle enables us to gain insight on several important factors accounting for both success and failure of the current state-of-the-art models. It further reveals model in- dependent properties on different datasets. We furthermore relax the assumption of prefect atom prediction. This sheds light on a trade-off between atom accuracy and atom coverage, providing guidance to future research in this direction. Importantly, our experimental re- sults suggest that more efforts are required in step one where visual inputs are converted into visual concepts (atoms). Despite its effectiveness shown in the experiments, the empirical oracle is constructed with the simplest atom extraction procedure and model parameterization in mind, which makes such a construction in a sense a \u201Cconservative\u201D oracle. Acknowledgments The authors would like to acknowledge the support of the following agencies for research funding and computing support: IBM T.J. Watson Research, NSERC, Calcul Québec, Com- pute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano [29] , for developing such a powerful tool for scientific computing. References [1] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville. Delving deeper into convolu- tional networks for learning video representations. ICLR, 2016. [2] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, et al. Video in sentences out. UAI, 2012. [3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099, 2015. [4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137\u2013 1155, 2003. [5] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 2012. [6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evalua- tion server. arXiv 1504.00325, 2015. [7] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014. Citation Citation {Theano Development Team} 2016 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 11 [8] Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809, 2015. [9] Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, and C Lawrence Zit- nick. Exploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467, 2015. [10] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Sub- hashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convo- lutional networks for visual recognition and description. CVPR, 2015. [11] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, et al. From captions to visual concepts and back. CVPR, 2015. [12] Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venu- gopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. Youtube2text: Rec- ognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In ICCV, 2013. [13] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature de- tectors. arXiv preprint arXiv:1207.0580, 2012. [14] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computa- tion, 9(8):1735\u20131780, 1997. [15] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 2013. [16] Xu Jia, Efstratios Gavves, Basura Fernando, and Tinne Tuytelaars. Guiding long-short term memory for image caption generation. arXiv preprint arXiv:1509.04942, 2015. [17] A Karpathy and L Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2014. [18] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014. [19] Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga. Natural language description of human activities from video images based on concept hierarchy of actions. IJCV, 2002. [20] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. Babytalk: Understanding and generating simple image descriptions. PAMI, 2013. 12 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING [21] Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers- Volume 1, pages 359\u2013368. Association for Computational Linguistics, 2012. [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra- manan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer, 2014. [23] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR, 2015. [24] Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daumé III. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Confer- ence of the European Chapter of the Association for Computational Linguistics, pages 747\u2013756. Association for Computational Linguistics, 2012. [25] Qi Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, and Anthony Dick. What value high level concepts in vision to language problems? arXiv 1506.01144, 2015. [26] Anna Rohrbach, Marcus Rohrbach, and Bernt Schiele. The long-short story of movie description. 2015. [27] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. CVPR, 2015. [28] Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. Translating video content to natural language descriptions. In ICCV, 2013. [29] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs\/1605.02688, May 2016. URL http: \/\/arxiv.org\/abs\/1605.02688. [30] Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron Courville. Using descrip- tive video services to create a large data source for video annotation research. arXiv: 1503.01070, 2015. [31] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus- based image description evaluation. CVPR, 2015. [32] Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence \u2013 video to text. In ICCV, 2015. [33] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. NAACL, 2015. [34] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. CVPR, 2014. http:\/\/arxiv.org\/abs\/1605.02688 http:\/\/arxiv.org\/abs\/1605.02688 LI YAO et al.: ORACLE PERFORMANCE FOR VISUAL CAPTIONING 13 [35] Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus Rohrbach, and Kate Saenko. A multi-scale multiple instance video description network. arXiv 1505.05914, 2015. [36] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML, 2015. [37] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville. Describing videos by exploiting temporal structure. In ICCV, 2015. [38] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descrip- tions to visual denotations: New similarity metrics for semantic inference over event descriptions. ACL14. [39] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph captioning using hierarchical recurrent neural networks. arXiv 1510.07712, 2015. [40] Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. Technical report, 2012. ","flair":"three\tResearch"}
{"author":"V4wetumpka","created":"Sun Oct 02 13:19:34 EDT 2016","text":"The code in this repository implements an efficient generalization of the popular Convolutional Neural Networks (CNNs) to arbitrary graphs, presented in our paper: There is also implementations of the filters used in: Run all the notebooks to reproduce the experiments on MNIST and 20NEWS presented in the paper. To use our graph ConvNet on your data, you need: See the usage notebook for a simple example with fabricated data. Please get in touch if you are unsure about applying the model to a different setting.","flair":"null\tnull"}
{"author":"youngChange","created":"Sun Oct 16 16:22:23 EDT 2016","text":"I am a 3rd year CS student and I am trying to create an application which will auto-group photos of the same person like google photos does.\nI understand that I can use methods such as k-means, but that would be too slow. Is there a deep learning alternative I can use?.\n\nI have alright knowledge of basic neural networks and cnns having done the cs231n course. But all of the methods they discuss are supervised ones...can someone point me in the right direction? thank you.\n","flair":"one\tDiscussion"}
{"author":"hoqqanen","created":"Fri Oct 07 20:33:52 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.01644 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Understanding intermediate layers using linear classifier probes Authors: Guillaume Alain, Yoshua Bengio (Submitted on 5 Oct 2016 (v1), last revised 14 Oct 2016 (this version, v3)) Abstract: Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as \"probes\", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems. Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1610.01644 [stat.ML]   (or arXiv:1610.01644v3 [stat.ML] for this version) Submission history From: Guillaume Alain [view email] [v1] Wed, 5 Oct 2016 20:59:01 GMT (4314kb,D) [v2] Mon, 10 Oct 2016 02:33:57 GMT (10356kb,D) [v3] Fri, 14 Oct 2016 18:47:19 GMT (4453kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Octopuscabbage","created":"Sun Nov 27 14:18:23 EST 2016","text":"(If this should go in the simple questions thread let me know, I'll delete it, apologies in advance.) \n\nI've seen a lot of jobs which require masters degrees. I'm currently at the point that I have about a month to decide if I want to go to grad school in the fall. I've taken 2 graduate level machine learning courses (Introduction to Statistical Learning &amp; Neural and Adaptive Systems) as an undergrad and have done a small independent project, but is that enough? Obviously there are more components to a landing a job than degree\/qualifications, but it is a starting point.\n\n(Edit: Title should really read \"Masters Degree or Higher\")","flair":"one\tDiscussion"}
{"author":"essofluffy","created":"Thu Oct 27 16:41:21 EDT 2016","text":"Arxiver Login | Register Built by @essofluffy to make arxiv.org a bit simpler 2016\/11\/25 The Goldbach Problem for Primes That Are Sums of Two Squares Plus One math.NT 2016\/11\/25 Absence of high-temperature ballistic transport in the spin-$1\/2$ $XXX$ chain within the grand-canonical ensemble cond-mat.str-el 2016\/11\/25 Noncentrosymmetric superconductors in one dimension cond-mat.supr-con 2016\/11\/25 Long-term photometric behavior of the PMS stars V977 Cep and V982 Cep in the vicinity of NGC 7129 astro-ph.SR 2016\/11\/25 Event Shape Sorting: selecting events with similar evolution physics.data-anhep-exhep-phnucl-exnucl-th 2016\/11\/25 On preparing ground states of gapped Hamiltonians: An efficient Quantum Lovász Local Lemma quant-ph 2016\/11\/25 A Simple, Fast Diverse Decoding Algorithm for Neural Generation cs.CL 2016\/11\/25 Toeplitz and Asymptotic Toeplitz operators on $H^2(\\mathbb{D}^n)$ math.FAmath.CVmath.OA 2016\/11\/25 Investigating overdensities around z>6 galaxies through ALMA observations of [CII] astro-ph.GA 2016\/11\/25 The sum of it all: revealing collaboration patterns by combining authorship and acknowledgements cs.DL 2016\/11\/25 Joint Throughput Maximization and Power Control in Poisson CoopMAC Networks cs.NIcs.ITmath.IT 2016\/11\/25 Reasoning about Strategies: on the Satisfiability Problem cs.LO 2016\/11\/25 Structure and Bonding in Amorphous Cr1-xCx Nanocomposite Thin Films: X-ray Absorption Spectra and First-Principles Calculation cond-mat.mtrl-scicond-mat.mes-hallphysics.chem-ph 2016\/11\/25 Dissecting the End-to-end Latency of Interactive Mobile Video Applications cs.HC 2016\/11\/25 A top-squark hunter's guide hep-ph 2016\/11\/25 A note on solution-free sets of integers math.COmath.NT 2016\/11\/25 Modular forms, Schwarzian conditions, and symmetries of differential equations in physics math-phmath.MP 2016\/11\/25 Pure and Stationary Optimal Strategies in Perfect-Information Stochastic Games with Global Preferences cs.GT 2016\/11\/25 Distributed Optimization of Multi-Class SVMs stat.MLcs.LG 2016\/11\/25 Leading order covariant chiral nucleon-nucleon interaction nucl-thhep-ph 2016\/11\/25 Multimodal Latent Variable Analysis cs.CV 2016\/11\/25 Representation theory of quantized Gieseker varieties, I math.RT 2016\/11\/25 The design strategy of scientific data quality control software for Euclid mission astro-ph.IM 2016\/11\/25 Numerical simulations of necklaces in SU(2) gauge-Higgs field theory astro-ph.COhep-phhep-th 2016\/11\/25 Théorème d'Eilenberg-Zilber en homologie cyclique entière math.KT 2016\/11\/25 How Close to the Edge? Delay\/utilization tradeoffs in MEC cs.NI 2016\/11\/25 Measuring finite-range phase coherence in an optical lattice using Talbot interferometry quant-phcond-mat.quant-gas 2016\/11\/25 Randomness and disorder of chaotic iterations. Applications in information security field nlin.CDcs.CR 2016\/11\/25 On depth zero L-packets for classical groups math.RTmath.NT 2016\/11\/25 Detection of thermal radio emission from a single coronal giant astro-ph.SR All (Home Page) Physics Astrophysics: Astrophysics of Galaxies Cosmology and Nongalactic Astrophysics Earth and Planetary Astrophysics High Energy Astrophysical Phenomena Instrumentation and Methods for Astrophysics Solar and Stellar Astrophysics Condensed Matter: Disordered Systems and Neural Networks Materials Science Mesoscale and Nanoscale Physics Other Condensed Matter Quantum Gases Soft Condensed Matter Statistical Mechanics Strongly Correlated Electrons Superconductivity General Relativity and Quantum Cosmology High Energy Physics - Experiment High Energy Physics - Lattice High Energy Physics - Phenomenology High Energy Physics - Theory Mathematical Physics Nonlinear Sciences: Adaptation and Self-Organizing Systems Cellular Automata and Lattice Gases Chaotic Dynamics Exactly Solvable and Integrable Systems Pattern Formation and Solitons Nuclear Experiment Nuclear Theory Physics: Accelerator Physics Atmospheric and Oceanic Physics Atomic Physics Atomic and Molecular Clusters Biological Physics Chemical Physics Classical Physics Computational Physics Data Analysis, Statistics and Probability Fluid Dynamics General Physics Geophysics History and Philosophy of Physics Instrumentation and Detectors Medical Physics Optics Physics Education Physics and Society Plasma Physics Popular Physics Space Physics Quantum Physics Mathematics Algebraic Geometry Algebraic Topology Analysis of PDEs Category Theory Classical Analysis and ODEs Combinatorics Commutative Algebra Complex Variables Differential Geometry Dynamical Systems Functional Analysis General Mathematics General Topology Geometric Topology Group Theory History and Overview Information Theory K-Theory and Homology Logic Mathematical Physics Metric Geometry Number Theory Numerical Analysis Operator Algebras Optimization and Control Probability Quantum Algebra Representation Theory Rings and Algebras Spectral Theory Statistics Theory Symplectic Geometry Computer Science Artificial Intelligence Computation and Language Computational Complexity Computational Engineering, Finance, and Science Computational Geometry Computer Science and Game Theory Computer Vision and Pattern Recognition Computers and Society Cryptography and Security Data Structures and Algorithms Databases Digital Libraries Discrete Mathematics Distributed, Parallel, and Cluster Computing Emerging Technologies Formal Languages and Automata Theory General Literature Graphics Hardware Architecture Human-Computer Interaction Information Retrieval Information Theory Learning Logic in Computer Science Mathematical Software Multiagent Systems Multimedia Networking and Internet Architecture Neural and Evolutionary Computing Numerical Analysis Operating Systems Other Computer Science Performance Programming Languages Robotics Social and Information Networks Software Engineering Sound Symbolic Computation Systems and Control Quantitative Biology Biomolecules Cell Behavior Genomics Molecular Networks Neurons and Cognition Other Quantitative Biology Populations and Evolution Quantitative Methods Subcellular Processes Tissues and Organs Quantitative Finance Computational Finance Economics General Finance Mathematical Finance Portfolio Management Pricing of Securities Risk Management Statistical Finance Trading and Market Microstructure Statistics Applications Computation Machine Learning Methodology Other Statistics ","flair":"one\tDiscussion"}
{"author":"Sohakes","created":"Wed Oct 19 08:26:45 EDT 2016","text":"Hi everyone!\n\nI'm planning on doing my master degree in Machine Learning. I have some knowledge in the area, since I had done two undergraduate researches on it (one only tangentially related) and completed Andrew Ng's course in Coursera.\n\nAnyway, I need to make a simple research plan, maximum 30 lines long, to apply for the research. It's just so the professors analyzing my submission don't feel I'm completely lost in the area. In the end it will probably not be used, but it could, and it gives the professor some idea of what I want to do.\n\nAlthough the research plan will (probably) not be followed if I get selected for the master, I think it's important to do it well. So I was trying to come up with an interesting problem that I could tackle. With the help of some friends, I could think of a few directions to take:\n\n- Test many algorithms in CUDA to compare performance (if no one did this before)\n\n- Apply neural networks to solve some game problem (although it looks really hard to do something novel since they already somewhat solved even Go as far as I know)\n\n- Create a novel machine learning algorithm (but I have no idea what to propose)\n\n- Create a method to generate synthetic datasets based on real ones (not sure if it's a problem for a master thesis or if it's too easy)\n\nIf I could research something that is good for humanity it would be even better, but I have no idea where to start on that, and I guess most things are being researched by people way more qualified than me.\n\nThanks!","flair":"three\tResearch"}
{"author":"sensitivejimha","created":"Sun Nov 06 05:36:53 EST 2016","text":" Under review as a conference paper at ICLR 2017 SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE Gao Huang∗, Yixuan Li∗, Geoff Pleiss Cornell University {gh349, yl2363}@cornell.edu, geoff@cs.cornell.edu Zhuang Liu Tsinghua University liuzhuangthu@gmail.com John E. Hopcroft, Kilian Q. Weinberger Cornell University jeh@cs.cornell.edu, kqw4@cornell.edu ABSTRACT Ensembles of neural networks are known to give far more robust and accurate predictions compared to any of its individual networks. However, training mul- tiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal to obtain ensembles of multiple neural network at no additional training cost. We achieve this goal by letting a single neural network converge into several local minima along its optimization path and save the model parameters. To obtain repeated rapid convergence we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is surprisingly simple, yet effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields significantly lower error rates than state-of-the-art single models at no ad- ditional training cost, and almost matches the results of (far more expensive) inde- pendently trained network ensembles. On Cifar-10 and Cifar-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively. 1 INTRODUCTION Stochastic Gradient Descent (SGD) (Bottou, 2010) and its accelerated variants (Kingma & Ba, 2014; Duchi et al., 2011) have become the de-facto approaches for optimizing deep neural networks. The popularity of SGD is typically attributed to its ability to avoid and even escape spurious saddle-points and local minima (Dauphin et al., 2014). Although this ability is generally considered positive, in this paper we argue that there may in fact be some value to these local minima and that simply leaving them on the way-side may be outright wasteful. Although deep networks typically never converge to a global minimum, there is a notion of \u201Cgood\u201D and \u201Cbad\u201D local minima in terms of generalization. Keskar et al. (2016) argue that a local minimum generalizes well if it has a flat basin. SGD can avoid sharper spurious local minima because its gradients are computed from small mini-batches and are therefore inexact (Keskar et al., 2016). If the learning-rate is sufficiently large, there will be enough intrinsic random motion across gradient steps that the optimizer will not be trapped in any of the sharper local minima along its optimization path. If however the learning rate is sufficiently small, then the model will converge into the closest local minimum. These two very different behaviors of SGD are typically exploited in different phases of the optimization (He et al., 2015). Initially the learning rate is kept high to move into the general vicinity of a flat local minimum. Once this search has reached a stage in which the optimization makes no more progress, the learning-rate is dropped (once or twice), which triggers a descent and ultimately a convergence into the final local minimum. It is well established (Kawaguchi, 2016) that the number of possible local minima grows exponen- tially with the number of parameters\u2014of which modern neural networks can have millions. It is therefore not surprising that two identical architectures optimized from different initializations or ∗Authors contribute equally. 1 Under review as a conference paper at ICLR 2017 0 10 20 30 40 50 0 10 20 30 40 50 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 Single Model Standard LR Schedule 0 10 20 30 40 50 0 10 20 30 40 50 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 3 1 2 Snapshot Ensemble Cyclic LR Schedule Figure 1: Left: Illustration of SGD optimization with a typical learning rate schedule. The model converges to a minimum at the end of training. Right: Illustration of Snapshot Ensembling optimization. The model undergoes several learning rate annealing cycles, converging to and escaping from multiple local minima. We take a snapshot at each minimum for test time ensembling. even just with a different order of minibatches will converge to different solutions. Although dif- ferent local minima often lead to very similar error rates, the corresponding neural networks tend to make very different mistakes. This diversity can be exploited through ensembling, in which multiple neural networks are trained from different initializations and then combined with majority voting or averaging (Caruana et al., 2004). Ensembling often leads to drastic reductions in error rates. In fact, most high profile competitions, e.g. Imagenet (Deng et al., 2009) or Kaggle1, are won by ensem- bles of deep learning architectures. Despite its obvious advantages, the use of ensembling for deep networks is not nearly as wide-spread as for other algorithms. One likely reason for this lack of adaptation may be the additional training cost incurred by ensembling. Training deep networks is computationally expensive and can last for weeks, even on high performance hardware with GPU acceleration. As the training cost for ensembles increases linearly, ensembles can quickly becomes uneconomical for most researchers without access to industrial scale computational resources. In this paper we propose a method to achieve the seemingly contradictory goal to obtain an ensemble of multiple neural networks without incurring any additional training costs. Our approach leverages the non-convex nature of neural networks and the ability of SGD to converge into and escape from local minima on demand. Most importantly, it is compellingly simple and straight-forward to im- plement. Instead of training M neural networks independently from scratch, we let SGD converge M times into local minima along its optimization path. Each time the model converges we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum and continue the optimization. To achieve this cyclic learning, we adopt the aggressive cosine annealing procedure suggested by Loshchilov & Hutter (2016), which lowers the learning rate continuously following a cosine function. Because our final ensemble consists of snapshots of the optimization path, we refer to our approach as Snapshot Ensembling. Figure 1 presents a high-level overview of this method. In contrast to traditional ensembles, the training time for the entire ensemble is identical to the time required to train a single traditional model. During testing time, one can evaluate and average the last (and therefore most accurate) m out of M models. Our approach is naturally compatible with other methods to improve the accuracy, such as data augmentation, stochastic depth (Huang et al., 2016b), or batch normalization (Ioffe & Szegedy, 2015). In fact, Snapshot Ensemble can even be ensembled, if for example parallel resources are available during training. In this case, an ensemble of K Snapshot Ensembles yields K ×M models at K times the training cost. We evaluate the efficacy of Snapshot Ensemble on three popular state-of-the-art deep learning archi- tectures for object recognition: ResNet (He et al., 2016), Wide-ResNet (Zagoruyko & Komodakis, 2016), and DenseNet (Huang et al., 2016a). We show across four different data sets that Snapshot 1www.kaggle.com 2 www.kaggle.com Under review as a conference paper at ICLR 2017 Ensemble almost always leads to substantial reductions in error rates at the same training costs. For example, on Cifar-10 and Cifar-100 it obtains error rates of 3.44% and 17.41% respectively. 2 RELATED WORK Neural networks ensembles have been widely studied and applied in machine learning (Hansen & Salamon, 1990; Krogh et al., 1995). However, most existing works focuses on improving the generalization performance, while few of them address the cost of training ensembles. As an alternative to traditional ensembles, so-called \u201Cimplicit\u201D ensembles have high efficiency dur- ing both training and testing (Srivastava et al., 2014; Wan et al., 2013; Huang et al., 2016b; Singh et al., 2016; Krueger et al., 2016). The Dropout (Srivastava et al., 2014) technique creates an en- semble out of a single model by \u201Cdropping\u201D \u2014 or zeroing \u2014 random sets of hidden nodes during each mini-batch. At test time, no nodes are dropped, and each node is scaled by the probability of surviving during training. Srivastava et al. claim that Dropout reduces overfitting by preventing the co-adaptation of nodes. An alternative explanation is that this mechanism creates an exponential number of networks with shared weights during training, which are then implicitly ensembled at test time. DropConnect (Wan et al., 2013) uses a similar trick to create ensembles at test time by dropping connections (weights) during training instead of nodes. The recently proposed Stochastic Depth technique (Huang et al., 2016b) randomly drops layers during training to create an implicit ensembles of many networks with varying depth at test time. Stochastic Depth has a strong reg- ularization effect and tends to outperform Dropout on very deep networks (Huang et al., 2016b). Finally, Swapout (Singh et al., 2016) is a stochastic training method which generalizes Dropout and Stochastic Depth. From the perspective of model ensemble, it provides more diversified network structures for model averaging. Our proposed method similarly trains only a single model; however, the resulting ensemble is explicit in that the models do not share weights. It is also compatible with implicit techniques and the approaches can be used in conjunction. Several recent publications focus on reducing test time cost of ensembles, by transferring the \u201Cknowledge\u201D of cumbersome ensembles into a single model (Bucilu et al., 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of the single (smaller) network. In order to produce more informative guidance, a high temperature is used to soften the probability distribution over classes. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test time cost. There are several recently proposed training mechanisms to improve the power of explicit ensem- bles. Laine & Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions. Recently, Moghimi et al. show that boosting can be applied to convolutional neural networks to create strong ensembles. Our work is inspired by recent work of Loshchilov & Hutter (2016) and Smith (2016), who show that cyclical learning rates can be effective for training convolutional neural networks. The authors show that each cycle produces models which are (almost) competitive to those learned with tradi- tional learning rate schedules in a fraction of training iterations. Although the model performance temporarily suffers when the learning rate cycle is restarted, the performance eventually surpasses that of the previous cycle after annealing the learning rate. The authors suggest that cycling perturbs the parameters of a converged model, which allows the model to find a better local minimum. Our work builds upon these recent findings in non-linear function optimization, but we are not concerned with speeding up or improving the training of a single model but instead try to extract an ensemble of classifiers while following the optimization path of the final model. 3 SNAPSHOT ENSEMBLING Snapshot Ensembling produces an ensemble of accurate and diverse models from a single training process. At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average softmax predictions from each of them at test time. 3 Under review as a conference paper at ICLR 2017 Ensembles work best if their individual members have low test error and have little overlap in the set of examples they still misclassify. Almost all weight assignments along the optimization path of a neural network do not lead to low error models. In fact, it is commonly observed that the validation error drops significantly only after the learning rate has been reduced at least once, which is typically done after several hundred epochs. Our approach is inspired by the observation that training neural networks for fewer epochs\u2014dropping the learning rate earlier\u2014often increases the final error by only a small amount (Loshchilov & Hutter, 2016). This seems to suggest, that the local minima along the optimization path are already starting to become promising in terms of generalization error after only several epochs. Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Figure 2: Training loss of 100-layer DenseNet on CI- FAR10 when using standard learning rate (blue) and M = 6 cosine annealing cycles (red). The intermedi- ate models, denoted by the dotted lines, are ensembled together at the end of training. Cyclical Cosine Annealing. In our search for viable local minima we follow a cyclic anneal- ing schedule as proposed by Loshchilov & Hut- ter (2016). We lower the learning rate at a very fast pace, encouraging the model to converge towards its first local minimum after as few as 50 epochs. The optimization is then continued at a larger learning rate, perturbing the model to snap out of its local minimum and the anneal- ing cycle starts over. Formally, the learning rate α has the form: α(t) = f (mod (t− 1, dT\/Me)) , (1) where t is the iteration number, T is the total number of training iterations, and f is a mono- tonically decreasing function. In other words, we split the training process into M cycles, each of which starts with a large learning rate value. Each cycle then undergoes a full anneal- ing process before raising the learning rate at the beginning of the next cycle. The large learn- ing rate α = f(0) in each stage gives the model enough energy to escape from a critical point, and the small learning rate α = f(T\/M) will drive the model to a well behaved local minimum. The learning rate function in (1) can be any monotonically decreasing function. We adopt the shifted cosine function proposed by Loshchilov & Hutter (2016): α(t) = α0 2 ( cos ( πmod(t− 1, dT\/Me) dT\/Me ) + 1 ) , (2) where α0 is the initial learning rate. Intuitively, this function anneals the learning rate from its initial value α0to f(T\/M) = 0 over the course of a cycle. Following (Loshchilov & Hutter, 2016), we update the learning rate at each iteration rather than at every epoch. This improves the convergence of short cycles, even when a large initial learning rate is used . Snapshot Ensembling. Figure 2 depicts the training process using cyclic and traditional learning rate schedules. At the end of each training cycle, it is apparent that the model reaches a local mini- mum with respect to training the loss. Thus, before raising the learning rate, we take a \u201Csnapshot\u201D of the model weights (indicated as vertical dashed black lines). After training M cycles, we have M model snapshots, f1 . . . fM , each of which will be used in the final ensemble. It is important to highlight that the total training time of the M snapshots is the same as training a model with a standard schedule (indicated in blue). It is also fair to point out that the training loss under the standard schedule is eventually lower\u2014however, as we will show in the next section, ensembling the M models more than compensates for that. Ensembling at test-time. At test time, we compute the ensemble prediction by averaging the softmax outputs of the last m models. Let x be a test sample and let fi (x) be the softmax score of snapshot i. The output of the ensemble is a simple average of the last m models: fEnsemble = 1 m ∑m−1 0 fM−i (x) . We always ensemble the last m models because the general- ization error of the models tends to monotonically decrease. 4 Under review as a conference paper at ICLR 2017 Method C10 C100 SVHN Tiny ImageNet ResNet-110 Single model 5.52 28.02 1.96 46.50 NoCycle Snapshot Ensemble 5.49 26.97 1.78 43.69 Snapshot Ensemble (α0 = 0.1) 5.73 25.55 1.63 40.54 Snapshot Ensemble (α0 = 0.2) 5.32 24.19 1.66 39.40 Single model 5.43 23.55 1.90 39.63 Dropout 4.68 22.82 1.81 36.58 Wide-ResNet-32 NoCycle Snapshot Ensemble 5.18 22.81 1.81 38.64 Snapshot Ensemble (α0 = 0.1) 4.41 21.26 1.64 35.45 Snapshot Ensemble (α0 = 0.2) 4.73 21.56 1.51 32.90 Single model 5.24∗ 24.42∗ 1.77 39.09 Dropout 6.08 25.79 1.79∗ 39.68 DenseNet-40 NoCycle Snapshot Ensemble 5.20 24.63 1.80 38.51 Snapshot Ensemble (α0 = 0.1) 4.99 23.34 1.64 37.25 Snapshot Ensemble (α0 = 0.2) 4.84 21.93 1.73 36.61 Single model 3.74∗ 19.25∗ - - Dropout 3.65 18.77 - - DenseNet-100 NoCycle Snapshot Ensemble 3.80 19.30 - - Snapshot Ensemble (α0 = 0.1) 3.57 18.12 - - Snapshot Ensemble (α0 = 0.2) 3.44 17.41 - - Table 1: Error rates (%) on CIFAR-10 and CIFAR-100 datasets. All methods in the same group use the same amount of training time and cost. Results of our method are colored in blue, and best result for each network and each dataset is shown in bold. ∗ indicates numbers directly taken from Huang et al. (2016a). 4 EXPERIMENTS We demonstrate the effectiveness of Snapshot Ensembles on several benchmark datasets and com- pare it with competitive baselines. We run all experiments with Torch 7 (Collobert et al., 2011)2. 4.1 DATASETS CIFAR. The two CIFAR datasets (Krizhevsky & Hinton, 2009) consist of colored natural scene images sized at 32×32 pixels. CIFAR-10 (C10) and CIFAR-100 (C100)images are drawn from 10 and 100 classes respectively. For each dataset, there are 50,000 training images and 10,000 images reserved for testing. A standard data augmentation scheme is used as in (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015; Huang et al., 2016b; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32×32 images; half of the images are then horizontally mirrored. SVHN. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32×32 col- ored digit images from Google Street View, with 10 different classes for each digit. There are 73,257 images in the training set and 26,032 images in the test set. Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Huang et al., 2016a), we withhold 6,000 training images for validation, and train on the remaining images without data augmentation. Tiny ImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet ILSVRC im- ages (Deng et al., 2009), with 200 classes. Each class has 500 training images, 50 validation images and 50 test images. Each image is resized to 64 × 64, with standard data augmentation including random crops, horizontal mirroring, and altering RGB intensities (Krizhevsky et al., 2012). 4.2 TRAINING SETTING Architectures. We benchmark on several state-of-the-art architectures including residual networks (ResNet) (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016) and DenseNet (Huang 2Code to reproduce results is available at https:\/\/github.com\/gaohuang\/SnapshotEnsemble 3https:\/\/tiny-imagenet.herokuapp.com 5 https:\/\/github.com\/gaohuang\/SnapshotEnsemble https:\/\/tiny-imagenet.herokuapp.com Under review as a conference paper at ICLR 2017 1 2 3 4 5 6 # of snapshots 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 T e st E rr o r (% ) Cifar10 (α0 = 0. 1) DenseNet Baseline (Huang et al. 2016) 1 2 3 4 5 6 # of snapshots 17.0 17.5 18.0 18.5 19.0 19.5 20.0 20.5 Cifar100 (α0 = 0. 1) DenseNet Baseline (Huang et al. 2016) 1 2 3 4 5 6 # of snapshots 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 T e st E rr o r (% ) Cifar10 (α0 = 0. 2) DenseNet Baseline (Huang et al. 2016) 1 2 3 4 5 6 # of snapshots 17.0 17.5 18.0 18.5 19.0 19.5 20.0 20.5 Cifar100 (α0 = 0. 2) DenseNet Baseline (Huang et al. 2016) Figure 3: Snapshot Ensemble performance on CIFAR-10 and CIFAR-100 when trained on DenseNet-100 with restart learning rate 0.1 (left two) and 0.2 (right two) with M = 6 annealing cycles (50 epochs per each). et al., 2016a). For ResNet, we use the original 110-layer network introduced by He et al. (2015). The Wide-ResNet) is a 32-layer ResNet with each of its convolutional layer four times wider (in terms of the number of output channels) than standard ResNets. For DenseNet, our large model in use is consistent with the original setup in (Huang et al., 2016a), with depth L = 100, growth rate k = 24. In addition, we also evaluate our method on a small DenseNet, with depth L = 40 and k = 12. For Tiny ImageNet, we add a stride of 2 to the first convolution of each model to downsample the images to a 32× 32 size. We use a batch size 64 for all the architectures and on all datasets, except the ResNet-110 and Wide-ResNet are trained with batch size 128 on Tiny ImageNet. Baselines. Snapshot Ensembles incur the training cost of a single model; therefore, we compare with baselines that require the same amount of training. Firstly, we compare against a Single Model trained with a standard learning rate schedule, dropping the learning rate from 0.1 to 0.01 halfway through training, and then to 0.001 when training is at 75%. To compare against implicit ensembling methods, we test against a single DenseNet model trained with Dropout. This baseline uses the same learning rate as above, and drops nodes during training with a probability of 0.2. We then test variants of the Snapshot Ensemble algorithm trained with the cyclic cosine cycle as described by Equation (2). We test models with the max learning rate α0 set to 0.1 and 0.2. In both cases, we divide the training process into contingent learning rate cycles. We take a model snapshot at the end of each training cycle. Additionally, we train a Snapshot Ensemble with a non-cyclic learning rate schedule. This NoCycle Snapshot Ensemble, which uses the same schedule as the Single Model and Dropout baselines, is meant to highlight the impact of cyclic learning rates for our method. To accurately compare with the cyclical Snapshot Ensembles, we take six snapshots equally spaced throughout the training process. During test time, we feed samples through each model and average the softmax outputs. Training Cost. The DenseNet-40 and DenseNet-100 baseline models are trained for a total of B = 300 epochs (150 for Tiny ImageNet). Snapshot variants were trained with M = 6 cycles of B\/M = 50 epochs each (25 for Tiny ImageNet). Following the original training budget in (He et al., 2015; Zagoruyko & Komodakis, 2016), we train ResNet and Wide ResNet models for 200 epochs (150 for Tiny ImageNet). Snapshot variants of ResNet and Wide ResNet were trained with 5 cycles of 40 epochs each (6 cycles of 25 epochs for Tiny ImageNet). On SVHN dataset, we consistently train all the models using a total training budget of B = 40 epochs. Snapshot variants in this case are trained with 5 cycles of 8 epochs each. 4.3 SNAPSHOT ENSEMBLE RESULTS Accuracy. The main results are summarized in Table 1. In all but one experiment, Snapshot en- sembles achieve lower error than any of the baseline methods. Most notably, Snapshot Ensembling yields an error rate of 17.41% on CIFAR-100 using large DenseNets, far outperforming the state- of-the-art record of 19.25% under the same training cost and architecture (Huang et al., 2016a). Our method has the most success on CIFAR-100 and Tiny ImageNet, which is likely due to the com- plexity of these datasets. The softmax outputs for these datasets are high dimensional due to the large number of classes, making it unlikely that any two models make the same predictions. Never- theless, Snapshot Ensembling is also capable of improving the competitive baselines for CIFAR-10 and SVHN, reducing error by up to 1% and 0.4% on Wide ResNet. 6 Under review as a conference paper at ICLR 2017 The NoCycle Snapshot Ensemble generally has little effect on performance, and in some instances even increases the test error. This highlights the need for a cyclical learning rate to discover solutions for useful ensembling. M Test Error (%) 2 22.92 4 22.07 6 21.93 8 21.89 10 22.16 Table 2: Error rates (%) on CIFAR-100 using DenseNet-40 with varying number of cycles. Ensemble Size. In some applications, it may be beneficial to vary the size of the ensemble dynamically at test time depending on available re- sources. Figure 3 displays the performance of CIFAR DenseNets as the effective ensemble size, m, is varied. Each ensemble is comprised of snapshots from later cycles, as these snapshots have received the most training and converge to better minima. Although ensembling more models generally gives better performance, we observe significant drops in error when the second and third models are added to the ensemble. In most cases, an ensemble of two models outperforms the baseline model. Restart Learning Rate. The effect of restart learning rate can be ob- served in Figure 3. The left two plots show the performance when using restart learning rate of α0 = 0.1 at the beginning of each cycle, and the bottom panel shows that of α0 = 0.2 restart learning rate. In most cases, we find that larger restart learning rate can lead to better ensemble performance, presumably due to larger diversity introduced by the stronger perturbation inbetween cycles. 1 2 3 4 5 6 # of models 14 16 18 20 22 24 26 28 E n se m b le T e st E rr o r (% ) Cifar100, DenseNet-40 Single Model Snapshot ensemble (50 epochs per model cost) True ensemble of fully trained models (300 epochs per model cost) Figure 4: Performance comparison of Snapshot Ensembles with true ensembles. Varying Number of Cycles Given a fixed training bud- get, there is a trade-off between the number of learning rate cycles and their length. Therefore, we investigate how the number cycles M affects the ensemble perfor- mance, give a training budget of B = 300 epochs. We train a 40-layer DenseNet on CIFAR100 dataset with an initial learning rate of α0 = 0.2. As shown in Table 2, our method is relatively robust with respect to different values of M . At the extremes, M = 2 and M = 10, we find a slight degradation in performance, as the cycles are either too few or too short. In practice, we find setting M to be 4 ∼ 8 works reasonably well. Comparison with True Ensemble. We compare Snap- shot Ensemble with the expensive traditional ensemble method. Figure 4 shows the test rates of DenseNet-40 on CIFAR100. The true ensemble method averages models that are trained with 300 full epochs, each with different weight initializations. Given the same number of models at test time, the error rate of the true ensemble can be seen as an lower bound of our method. Our method achieves with comparable performance of ensembling 3 independant models, but with the training cost of one model. 4.4 DIVERSITY OF MODEL ENSEMBLES Parameter Space. We hypothesize that the cyclical learning rate schedule creates snapshots which are not only accurate but also diverse with respect to model predictions. We qualitatively measure this diversity by visualizing the local minima they fall into. For that purpose we linearly interpolate snapshot models, as described by Goodfellow et al. (2014). Let J (θ) be the test error of a model using parameters θ. Given θ1 and θ2 \u2014 the parameters from models 1 and 2 respectively \u2014 we can compute J (θ) for various combinations of these parameters: θ = λ (θ1) + (1− λ) (θ2) , where λ is a mixing coefficient. Setting λ to 1 results in a parameters that are entirely θ1 while setting λ to 0 give the parameters θ2. By sweeping the values of λ, we can examine an linear slice of the parameter space. Two models with similar convergence point will have smooth parameter interpola- tions, whereas the interpolation for models at different minima will likely be non-convex and show a \u201Cbump\u201D around λ = 0.5. In Figure 5, we draw interpolations between the final model (sixth snapshot) and all intermediate snapshots. The left two of the plots denote Snapshot Ensembles trained with a cyclical learning rate, while the right two plots denote NoCycle Snapshot Models. λ = 0 represents a model which is 7 Under review as a conference paper at ICLR 2017 Figure 5: Interpolations in parameter space between the final model (sixth snapshot) and all intermediate snapshots. λ = 0 represents an intermediate snapshot model, while λ = 1 represents entirely final model parameters. Left: Snapshot Ensemble, with cosine annealing cycles (α0 = 0.2 every B\/M = 50 epochs). Right: NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). entirely snapshot parameters, while λ = 1 represents a model which is entirely the final parameters. From this figure, it is clear that there are differences between cyclical and non-cyclical learning rate schedules. Firstly, all of the cyclical snapshots achieve roughly the same error as the final cyclical model, as the error is similar for λ = 0 and λ = 1. Additionally, it appears that most snapshots do not lie in the same minimum as the final model. Thus the snapshots are likely to misclassify different samples. Conversely, the first three snapshots achieve much higher error than the final model. This can be observed by the sharp minima around λ = 1, which suggests that mixing in any amount of the snapshot parameters will worsen performance. While the final two snapshots achieve low error, the figures suggests that they lie in the same minimum as the final model, and therefore likely add limited diversity to the ensemble. 1 2 3 4 5 6 6 5 4 3 2 1 0.88 0.9 0.91 0.92 0.93 1 0.88 0.91 0.92 0.93 1 0.93 0.88 0.91 0.93 1 0.93 0.92 0.89 0.92 1 0.93 0.92 0.91 0.9 1 0.92 0.91 0.91 0.9 1 0.9 0.89 0.88 0.88 0.88 Cosine with restart (CIFAR100) 1 2 3 4 5 6 6 5 4 3 2 1 0.82 0.83 0.84 0.99 0.99 1 0.82 0.83 0.84 0.99 1 0.99 0.82 0.83 0.84 1 0.99 0.99 0.76 0.78 1 0.84 0.84 0.84 0.78 1 0.78 0.83 0.83 0.83 1 0.78 0.76 0.82 0.82 0.82 Standard lr scheduling (CIFAR100) Figure 6: Pairwise correlation of softmax outputs be- tween any two snapshots for small DenseNet. Left: Snapshot Ensemble, with cosine annealing cycles (restart with 0.2 every 50 epochs). Right: NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs). Activation space. To further explore the diver- sity of models, we compute the pairwise corre- lation of softmax outputs for every pair of snap- shots. Figure 6 displays the average correla- tion for both for cyclical snapshots and for non- cyclical snapshots. Firstly, there are large corre- lations between the last 3 snapshots of the non- cyclical training schedule (right). These snap- shots are taken after dropping the learning rate, suggesting that each snapshot has converged to the same minimum. Though there is more diversity amongst the earlier snapshots, these snapshots have much higher error rates and are therefore not ideal for ensembling. Conversely, there is less correlation between all cyclical snapshots (left). Because all snapshots have similar accuracy (as can be seen in Figure 5), these difference in predictions can be exploited to create effective ensembles. As expected, correlations are strongest between consecutive snap- shots. 5 DISCUSSION We introduce Snapshot Ensemble, a simple method to obtain ensembles of neural networks without any additional training cost. Our method exploits the ability of SGD to converge to and escape from local minima, which allows the model to visit several accurate solutions over the course of training. We harness this power with cyclical learning rate schedule proposed by Loshchilov & Hutter (2016), saving snapshots of the models at each point of convergence. We show in several experiments that all acquired snapshots are accurate, yet produce different predictions from one another, and therefore are ideal for test-time ensembles. Ensembles of these snapshots significantly improve the state-of- the-art on the CIFAR-10, CIFAR-100 and SVHN datasets. Future work will explore combining Snapshot Ensembles with traditional ensembles. In particular, we will investigate how to balance growing an ensemble with new models (with random initializations) and refining existing models with further training cycles under a fixed training budget. 8 Under review as a conference paper at ICLR 2017 REFERENCES Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pp. 177\u2013186. Springer, 2010. Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001. Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535\u2013541. ACM, 2006. Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In Proceedings of the twenty-first international conference on Machine learn- ing, pp. 18. ACM, 2004. Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011. Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op- timization. In Advances in neural information processing systems, pp. 2933\u20132941, 2014. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248\u2013255. IEEE, 2009. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011. Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp. 1189\u20131232, 2001. Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013. Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014. Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12:993\u20131001, 1990. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. arXiv preprint arXiv:1512.03385, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv preprint arXiv:1603.05027, 2016. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016a. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochas- tic depth. arXiv preprint arXiv:1603.09382, 2016b. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110, 2016. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe- ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. 9 Under review as a conference paper at ICLR 2017 Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012. Anders Krogh, Jesper Vedelsby, et al. Neural network ensembles, cross validation, and active learn- ing. In Advances in Neural Information Processing Systems, volume 7, pp. 231\u2013238. MORGAN KAUFMANN PUBLISHERS, 1995. David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rose- mary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural net- works without residuals. arXiv preprint arXiv:1605.07648, 2016. Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply- supervised nets. 2015. Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with restarts. arXiv preprint arXiv:1608.03983, 2016. Mohammad Moghimi, Mohammad Saberian, Jian Yang, Li-Jia Li, Nuno Vasconcelos, and Serge Belongie. Boosted convolutional neural networks. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning, 2011. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied to house numbers digit classification. In Pattern Recognition (ICPR), 2012 21st International Con- ference on, pp. 3288\u20133291. IEEE, 2012. Saurabh Singh, Derek Hoiem, and David Forsyth. Swapout: Learning an ensemble of deep archi- tectures. arXiv preprint arXiv:1605.06465, 2016. Leslie N. Smith. No more pesky learning rate guessing games. CoRR, abs\/1506.01186, 2016. URL http:\/\/arxiv.org\/abs\/1506.01186. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014. Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015. 10 http:\/\/arxiv.org\/abs\/1506.01186 Under review as a conference paper at ICLR 2017 Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 1058\u20131066, 2013. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 11 Introduction Related Work Snapshot Ensembling Experiments Datasets Training Setting Snapshot Ensemble results Diversity of model ensembles Discussion ","flair":"three\tResearch"}
{"author":"syllogism_","created":"Mon Oct 03 07:50:46 EDT 2016","text":" Blog About Request Kemal Şanlı displaCy.js: An open-source NLP visualiser for the modern web by Ines Montani on October 3, 2016With new offerings from Google, Microsoft and others, there are now a range of excellent cloud APIs for syntactic dependencies. A key part of these services is the interactive demo, where you enter a sentence and see the resulting annotation. We're pleased to announce the release of displaCy.js, a modern and service-independent visualisation library. We hope this makes it easy to compare different services, and explore your own in-house models. Here's an example of a sentence rendered by the new SVG-based displaCy:The History of displaCyWe launched displaCy as a visualiser for our NLP library spaCy in 2015 and open-sourced the code in August 2016. The first version relied on an old CSS hack. The new version uses SVG to produce flexible and easily exportable output. RobotsNNSinINpopularJJcultureNNareVBPthereRBtoTOremindVBusPRPofINtheDTawesomenessNNofINunboundedJJhumanJJagencyNNnsubjprepamodpobjadvmodauxadvcldobjprepdetpobjprepamodamodpobjUsing displacy.js in your projects Simply include displacy.js and initialize a new instance specifying the API and settings. The parse(text, model, settings) method renders a parse generated by spaCy as an SVG in the container. By default, it expects spaCy's services, which you can download and run for free. If you're using Google's NLP API instead, set format to 'google'.A note on compatibilitydisplaCy is written in ECMAScript 6. For full, cross-browser compatibility, make sure to use a compiler like Babel. For more info, see this compatibility table. \/\/ Your API const api = 'http:\/\/localhost:8000'; \/\/ Init displaCy const displacy = new displaCy(api, { container: '#displacy', format: 'spacy', distance: 300, offsetX: 100 }); \/\/ Parse sentence displacy.parse('This is a sentence.', 'en', { collapsePunct: false, collapsePhrase: false, color: '#ffffff', bg: '#000000' }); For a full list of available settings, see the Readme. Alternatively, you can also use render(parse, settings) to manually render a JSON-formatted set of arcs and words. displaCy logs the JSON representation of each parse to the console for quick copy-pasting: How displaCy works A dependency visualisation consists of three main components: words and their corresponding part-of-speech tags displayed horizontally in order arcs of different lengths connecting two words with corresponding labels showing their relation type an arrow head at the start or end of each arc indicating its direction All three components can be implemented using the SVG elements <path> and <text>, with <tspan> to separate spans of text and <textPath> to wrap the arc label along the rounded arc path. Let's take a look at the first word, \"Robots\", and the arrow connecting it to \"are\". This is a simplified example of the markup displaCy generates:About SVGThe Scalable Vector Graphics format has been around since the early 2000s. Unlike other image formats, SVG uses XML markup that's easy to manipulate using CSS or JavaScript. SVG even offers powerful color filters and dynamic cropping and with improving browser support, has replaced icon fonts on many sites. Example SVG markup (excerpt)<svg xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\"> <!-- Token --> <text y=\"440\" text-anchor=\"middle\"> <tspan x=\"150\" fill=\"currentColor\">Robots<\/tspan> <tspan x=\"150\" dy=\"2em\" fill=\"currentColor\">NNS<\/tspan> <\/text> <!-- Arc --> <path id=\"arrow-0\" d=\"M150,400 C150,0 950,0 950,400\" stroke-width=\"2px\" stroke=\"currentColor\" fill=\"none\"><\/path> <!-- Arc label --> <text dy=\"1em\"> <textPath xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" xlink:href=\"#arrow-0\" startOffset=\"50%\" text-anchor=\"middle\" fill=\"currentColor\">nsubj<\/textPath> <\/text> <!-- Arrow head --> <path d=\"M150,402 L144,392 156,392\" fill=\"currentColor\"><\/path> <\/svg> The above markup was generated from JSON data that looked like this: Example JSON from API (excerpt){ \"arcs\": [ { \"dir\": \"left\", \"end\": 4, \"label\": \"nsubj\", \"start\": 0 } ], \"words\": [ { \"tag\": \"NNS\", \"text\": \"Robots\" } ] } To translate the JSON-format into SVG markup, we need two main functions: one to draw the words, and one to draw the arcs. Drawing the Words Rendering the words is pretty straightforward because they are independent of the overall sentence. Each word needs two coordinates: x, the distance from the left, and y, the distance from the top. Starting at a fixed offset from the left, the first word will be placed at offsetX, the second word at offsetX + distance, the third word at offsetX + 2 * distance and so on. This can be broken down into a simple formula, offsetX + i * distance. Let's not focus too much on the y coordinates for now, as they're pretty much identical for all components \u2014 for the words, I'm merely adding a little spacing so they're not too close to the arrows. const offsetX = 150; \/\/ distance from the left const distance = 300; \/\/ distance between words \/\/ Iterate over words const markup = words.map(({ text, tag }, i) => ` <text y=\"${offsetY + wordSpacing}\" text-anchor=\"middle\"> <tspan x=\"${offsetX + i * distance}\">${text}<\/tspan> <tspan x=\"${offsetX + i * distance}\" dy=\"2em\">${tag}<\/tspan> <\/text> `.trim()).join(''); Drawing the Arcs Each arc comes with the index of its start and end point, making it trivial to calculate its length: end - start. This makes the start point of an arc offsetX + start * distance and the end point offsetX + (end - start) * distance. Now if we add these numbers to our path definition, we get a nice straight line connecting both points: <path stroke=\"currentColor\" d=\"M150,400 950,400\" \/> The curves are a little trickier. For each curve, we need to add four additional values to the path definition: the x and y coordinates of the left and right cubic bézier control points. To show how this looks, I've forked this great demo from SitePoint. You can move the control points around and see how it affects the <path> markup: View on CodePen The curve's height needs to adapt to the arc's length. An arrow spanning over three words needs to be higher than an arrow spanning over two \u2013 otherwise they'll overlap. Depending on the grammatical strucutre of the sentence, we usually end up with a lot of arcs with lengths of 1 and 2, connecting words to their next and second next neighbours, then one or two arrows with lengths of 3 or 4, and maybe a large one with a length of 10. Long dependencies like that are especially common with relative clauses, questions and punctuation, and in languages like German, where verbs and verb prefixes are often placed at the end of the clause. If we use only the length of an arc to calculate its curve, we quickly run into a problem when visualising complex sentences: the largest arrows become huge and produce too much whitespace, rendering the visualisation pretty much unusable.Bugfix noteThis issue was also the cause of one of the main bugs in the old displaCy. I used a hack to decrease the overall arc height by a certain percentage if the sentence had particularly long arcs. However, this would sometimes cause the smallest arcs to become invisible. The longest arc is huge compared to the others and produces too much whitespace.The largest arrow's height here is relative to its length of 21, even though a height relative to a length of 8 would have sufficed to still make it higher than the second largest one. We can solve this by generating a list of all occuring lengths in order. When rendering the arc, we can now use the index of each length (+ 1, to start with level 1). \/\/ Create an array from a set to avoid duplicate values and sort const levels = [...new Set( arcs.map(({ end, start }) => end - start).sort((a, b) => a - b) )]; \/\/ [1, 2, 3, 4, 5, 6, 7, 21] \/\/ Get level for arc const arc = { dir: \"right\", end: 28, label: \"punct\", start: 7 }; const level = levels.indexOf(arc.end - arc.start) + 1); \/\/ 8 The same sentence with a much better result \u2013 the largest arc is still the biggest.We can now generate arrows and their curves relative to the overall levels: \/\/ Get highest level (index + 1 of highest length value) const highestLevel = levels.indexOf(levels.slice(-1)[0]) + 1; const offsetX = 150; \/\/ distance from the left const distance = 300; \/\/ distance between words const startX = offsetX + start * distance; const startY = distance \/ 2 * highestLevel; const endpoint = offsetX + (end - start) * distance; const curve = startY - (end - start) * distance \/ 2; \/\/ Combine values for path definition const d = `M${startX},${startY} C${startX},${curve} ${endpoint},${curve} ${endpoint},${startY}`; \/\/ Generate path markup const path = `<path d=\"${d}\" stroke-width=\"2px\" fill=\"none\" stroke=\"currentColor\" ><\/path>`; The arrow head is simply a path forming a triangle that is placed either at the start or the end of the arc. To wrap the label along the middle of the arc path, we can take advantage of the <textPath> element and link it to the id of the arc: <path id=\"arrow-0\" d=\"...\"><\/path> <textPath xlink:href=\"#arrow-0\" startOffset=\"50%\" text-anchor=\"middle\">Label<\/textPath> Styling the Visualisation with CSS To allow custom styling, all elements contained in the SVG come with tags and data attributes. By default, the currentColor of the element is used for colouring, meaning you only need to change the color property in CSS. For example, arrows have the class .displacy-arrow as well as a data-label and data-dir attribute. Using a combination of those selectors and some basic CSS logic, you can create pretty powerful templates to style the elements based on their role and function in the parse. \/* Make all noun phrases (tags that start with \"NN\") green *\/ .displacy-tag[data-tag^=\"NN\"] { color: green; } \/* Hide all tags for verbs (tags that start with \"VB\") that are NOT the base form (\"VB\") *\/ .displacy-tag[data-tag^=\"VB\"]:not([data-tag=\"VB\"]) { display: none; } For more CSS examples, see the Readme. Using displaCy with Harp or Node \/ Express Since SVG graphics consists of basic XML, we can use a templating engine like Jade (Pug) to dynamically generate the markup. For this blog, I wrote a simple mixin that generates a static inline SVG for any given JSON representation of a parse. It's even more compact than displacy.js (less than 50 lines!) and is available here. It works with Jade-based static site generators like Harp, or Node applications using Express, which natively supports Jade templates. To use the mixin, include it at the top of your file and call +displacy() with the full parse object as its argument: include _displacy +displacy({ arcs: [ ... ], words: [ ... ] }) To add custom class names to individual arcs, you can add a style: \"classname\" to the respective arc object. We've used this feature in this post to illustrate a correct dependency vs. an incorrect dependency in one graphic. What's next? We're planning support for more annotation formats like CoreNLP. In the meantime, you can can add your own custom converter. We've also launched a modern and lightweight Named Entity Visualiser \u2014 stay tuned for another in-depth blog post! Get started! displaCy.js (GitHub repo) spaCy REST server (GitHub repo) displaCy.js (npm) displaCy demo (npm) About the Author Ines Montani Ines is a developer specialising in web applications for AI technology, letting humans get knowledge to and from machine learning models. She's been working on the spaCy project since its first release. Before founding Explosion AI, she was a freelance front-end developer and strategist, using her four years executive experience in ad sales and digital marketing. Join our mailing list Stay in the loop! Sign up Explosion.ai Explosion AI is a digital studio specialising in Artificial Intelligence and Natural Language Processing. We design custom algorithms, applications and data assets. We're the makers of spaCy, the leading open source NLP library. Read more... Legal \/ Imprint Latest Blog Posts Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models The spaCy user survey: results and analysis Building your bot's brain with Node.js and spaCy spaCy v1.0: Deep Learning with custom pipelines and Keras ","flair":"null\tnull"}
{"author":"halfeatenscone","created":"Tue Sep 27 12:47:13 EDT 2016","text":"This page shows visualizations of some width-3 1-d convolutional filters from Google's lm_1b language model. Each column corresponds to one position in the filter, and shows the characters with the most positive weights. Use the checkbox in the bottom-right to also see the most negative weights (may be slow). Below that are examples of words for which the filter emits the highest values. A filter's response is its maximum value over all substrings it sees in the word. So if a filter has high weights on 'c' in the first position, then 'a', then 't', it will assign equally high scores to 'cat', 'fatcat', 'concatenate', etc. The portion of the string in blue is the substring the filter is responding to. '^' and '$' represent beginning and end of word markers, respectively. '_' is a padding character. Literal versions of those characters are escaped with a backslash. Use the links at the top to see filters of other widths. Check out my blog post here for a bit more context.","flair":"null\tnull"}
{"author":"madary","created":"Tue Oct 04 23:14:40 EDT 2016","text":"I have been trying to use a shallow (3 hidden layers) CNN, which works well for the MNIST handwriting recognition, but  not too well for detecting whether a screen shot is from a particular game.\nI trained on frames of videos from few games and on providing random sceen shots its expected to classify it among the trained games. \nMy question : is it fundamentally impossible to classify using this approach ? I think the images are too complex (too many features) and the network too shallow. **Is my intuition correct ?**\n \nI even thought of object detection to create a signature for each game but that seems like an overkill.","flair":"null\tnull"}
{"author":"yuanyuanji","created":"Wed Nov 09 07:41:17 EST 2016","text":"A unified view on multi-class support vector machines (SVMs) is presented, covering most prominent variants including the one- vs-all approach and the algorithms proposed by Weston & Watkins, Crammer & Singer, Lee, Lin, & Wahba, and Liu & Yuan. The unification leads to a template for the quadratic training problems and new multi-class SVM formulations. Within our framework, we provide a comparative analysis of the various notions of multi-class margin and margin-based loss. In particular, we demonstrate limitations of the loss function considered, for instance, in the Crammer & Singer machine. We analyze Fisher consistency of multi- class loss functions and universal consistency of the various machines. On the one hand, we give examples of SVMs that are, in a particular hyperparameter regime, universally consistent without being based on a Fisher consistent loss. These include the canonical extension of SVMs to multiple classes as proposed by Weston & Watkins and Vapnik as well as the one-vs-all approach. On the other hand, it is demonstrated that machines based on Fisher consistent loss functions can fail to identify proper decision boundaries in low-dimensional feature spaces. We compared the performance of nine different multi-class SVMs in a thorough empirical study. Our results suggest to use the Weston & Watkins SVM, which can be trained comparatively fast and gives good accuracies on benchmark functions. If training time is a major concern, the one-vs-all approach is the method of choice.","flair":"three\tResearch"}
{"author":"olaf_nij","created":"Sat Oct 08 11:18:48 EDT 2016","text":"To organize this subreddit and weed out submitters that do not read the rules. All posts from here on out will require tags.\nCurrent Tags: [News], [Research], [Discussion], [Project]\n\nOpen to other tag suggestions.\n\nDemo post here: https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/56hnjf\/research_equality_of_opportunity_in_machine\/","flair":"null\tnull"}
{"author":"julian88888888","created":"Fri Nov 04 12:49:31 EDT 2016","text":" 3\/4 Free Articles leftRemaining Register for more | Subscribe + Save! MENU SUGGESTED TOPICS SUBSCRIBE  Hi, SIGN IN Items added to cart Your Shopping Cart is empty. Visit our store MY LIBRARY TOPIC FEEDS PURCHASES ACTIVITY ACCOUNT SETTINGS LOG OUT SUGGESTED TOPICS Loading... Technology The Competitive Landscape for Machine Intelligence Shivon Zilis James Cham November 02, 2016 SAVE SHARE COMMENT TEXT SIZE PRINT PDF 8.95 BUY COPIES The Competitive Landscape for Machine Intelligence Loading... Three years ago, our venture capital firm began studying startups in artificial intelligence. AI felt misunderstood, burdened by expectations from science fiction, and so for the last two years we\u2019ve tried to capture the most-important startups in the space in a one-page landscape. (We prefer the more neutral term \u201Cmachine intelligence\u201D over \u201CAI.\u201D) In past years, we heard mostly from startup founders and academics \u2014 people who pay attention to early, far-reaching trends in technology. But this year was different. This year we\u2019ve heard more from Fortune 500 executives with questions about machine intelligence than from startup founders. Click or Tap to See a Larger Version Click or Tap to See a Larger Version These executives are asking themselves what to do. Over the past year, machine intelligence has exploded, with $5 billion in venture investment, a few big acquisitions, and hundreds of thousands of people reading our earlier research. As with the internet in the 1990s, executives are realizing that this new technology could change everything, but nobody knows exactly how or when. If this year\u2019s landscape shows anything, it\u2019s that the impact of machine intelligence is already here. Almost every industry is already being affected, from agriculture to transportation. Every employee can use machine intelligence to become more productive with tools that exist today. Companies have at their disposal, for the first time, the full set of building blocks to begin embedding machine intelligence in their businesses. And unlike with the internet, where latecomers often bested those who were first to market, the companies that get started immediately with machine intelligence could enjoy a lasting advantage. So what should the Fortune 500 and other companies be doing to get started?   Make Talent More Productive One way to immediately begin getting the value of machine intelligence is to support your talent with readily available machine intelligence productivity tools. Some of the earliest wins have been productivity tools tuned to specific areas of knowledge work \u2014 what we call \u201CEnterprise Functions\u201D in our landscape. With these tools, every employee can get some of the powers previously available only to CEOs. These tools can aid with monitoring and predicting (e.g., companies like Clari forecasting client-by-client sales to help prioritize deals) and with coaching and training (Textio\u2019s* predictive text-editing platform to help employees write more-effective documents). Find Entirely New Sources of Data The next step is to use machine intelligence to realize value from new sources of data, which we highlight in the \u201CEnterprise Intelligence\u201D section of the landscape. These new sources are now accessible because machine intelligence software can rapidly review enormous amounts of data in a way that would have been too difficult and expensive for people to do. Imagine if you could afford to have someone listen to every audio recording of your salespeople and predict their performance, or have a team look at every satellite image taken from space and determine what macroeconomic indicators could be gleaned from them. These data sources might already be owned by your company (e.g., transcripts of customer service conversations or sensor data predicting outages and required maintenance), or they might be newly available in the outside world (data on the open web providing competitive information).   Rethink How You Build Software Let\u2019s say you\u2019ve tried some new productivity tools and started to mine new sources of data for insight. The next frontier in capturing machine intelligence\u2019s value is building a lasting competitive advantage based on this new kind of software. But machine intelligence is not just about better software; it requires entirely new processes and a different mindset. Machine intelligence is a new discipline for managers to learn, one that demands a new class of software talent and a new organizational structure. Most IT groups think in terms of applications and data. New machine intelligence IT groups will think about applications, data, and models. Think of software as the combination of code, data, and a model. \u201CModel\u201D here means business rules, like rules for approving loans or adjusting power consumption in data centers. In traditional software, programmers created these rules by hand. Today machine intelligence can use data and new algorithms to generate a model too complex for any human programmer to write. With traditional software, the model changes only when programmers explicitly rewrite it. With machine intelligence, companies can create models that evolve much more regularly, allowing you to build a lasting advantage that strengthens over time as the model \u201Clearns.\u201D Think of these models as narrowly focused employees with great memories and not-so-great social skills \u2014 idiot savants. They can predict how best to grow the business, make customers happier, or cut costs. But they\u2019ll often fail miserably if you try to apply them to something new, or, worse, they may degrade invisibly as your business and data change. All of this means that the discipline of creating machine intelligence software differs from traditional software, and companies need to staff accordingly. Luckily, though finding the right talent may be hard, the tools that developers need to build this software is readily available. For the first time, there is a maturing \u201CStack\u201D (see our landscape) of building blocks that companies can use to practice the new discipline of machine intelligence. Many of these tools are available as free, open-source libraries from technology companies such as Google (TensorFlow), Microsoft (CNTK), or Amazon (DSSTNE). Others make it easier for data scientists to collaborate (see \u201CData Science\u201D) and manage machine intelligence models (\u201CMachine Learning\u201D). If your CEO is struggling to answer the question of how machine intelligence will change your industry, take a look at the range of markets in our landscape. The startups in these sections give a sense of how different industries may be altered. Machine intelligence\u2019s first useful applications in an industry tend to use data that previously had lain dormant. Health care is a prime example: We\u2019re seeing predictive models that run on patient data and computer vision that diagnoses disease from medical images and gleans lifesaving insights from genomic data. Next up will be finance, transportation, and agriculture because of the volume of data available and their sheer economic value. Your company will still need to decide how much to trust these models and how much power to grant them in making business decisions. In some cases the risk of an error will be too great to justify the speed and new capabilities. Your company will also need to decide how often and with how much oversight to revise your models. But the companies that decide to invest in the right models and successfully embed machine intelligence in their organization will improve by default as their models learn from experience. Economists have long wondered why the so-called computing revolution has failed to deliver productivity gains. Machine intelligence will finally realize computing\u2019s promise. The C-suites and boardrooms that recognize that fact first \u2014 and transform their ways of working accordingly \u2014 will outrun and outlast their competitors. *The authors\u2019 fund has invested in this company. Shivon Zilis is a partner and founding member of Bloomberg Beta, which invests heavily in the future of work. She focuses on early-stage data and machine intelligence investments. James Cham is a Partner at Bloomberg Beta where he invests in data-centric and machine learning-related companies. This article is about TECHNOLOGY Follow this topic Following Related Topics: Analytics Technology Loading... Partner Center THE LATEST MOST POPULAR ALL TOPICS VIDEO MAGAZINE ARCHIVE STORE SUBSCRIBER EXCLUSIVES MY LIBRARY Subscribe + Save! EXPLORE HBR The Latest Most Popular All Topics Magazine Archive Video Audio Webinars Subscriber Exclusives My Library Newsletters HBR STORE Article Reprints Books Cases Collections Magazine Issues HBR Guide Series HBR 20-Minute Managers HBR Must Reads Tools ABOUT HBR Contact Us Advertise with Us Subscribe Information for Booksellers\/Retailers Masthead Global Editions Media Inquiries Guidelines for Authors HBR Analytic Services HBR SUBSCRIBER ASSISTANCE U.S.\/Canada: 800.274.3214 harvard@cdsfulfillment.com hbr.org\/subscriberservices International: +44.1858.438.412 hbr@subscription.co.uk subscription.co.uk\/hbr\/help Asia Pacific: +612.8296.5401 India Today Group: +0120.2479945 hbrcare@intoday.com hbrsasia.org HBR.ORG CUSTOMER ASSISTANCE U.S.\/Canada: 800.988.0886 International: 617.783.7500 Email: customerservice@harvardbusiness.org Customer Service Help & FAQs Copyright Permissions FOLLOW HBR Facebook Twitter LinkedIn Google+ Your Newsreader About Us Careers Privacy Policy Copyright Information Trademark Policy Harvard Business Publishing: Higher Education Corporate Learning Harvard Business Review Copyright © 2016 Harvard Business School Publishing. All rights reserved. Harvard Business Publishing is an affiliate of Harvard Business School. ","flair":"two\tNews"}
{"author":"benjaminwilson","created":"Fri Oct 07 06:41:14 EDT 2016","text":"Adagrad is a learning regime that maintains separate learning rates for each individual model parameter. It is used, for instance, in GloVe (perhaps incorrectly), in LightFM, and in many other places besides. Below is an example showing that Adagrad models evolve in a manner that depends upon the choice of co-ordinate system (i.e. orthonormal basis) for the parameter space. This dependency is no problem when the parameter space consists of many one-dimensional, conceptually unrelated features lined up beside one another, because such parameter spaces have only one natural orientation of the co-ordinate axes. It is a problem, however, for the use of Adagrad in models like GloVe and LightFM. In these cases the parameter space consists of many feature vectors (of dimension, say, 100) concatenated together. A learning regime should not depend upon the arbitrary choice of orthonormal basis in this 100-dimensional feature space. For feature spaces like these, I would propose instead maintaining a separate learning rate for each feature vector (for example, in the case of GloVe, there would be one learning rate per word vector per layer). The learning rate of a feature vector would dampen the initial learning rate by the accumulation of the squared norms of the previous gradient updates of the feature vector. The evolution of Adagrad would then be independent of the choice of basis in the feature space (as distinct from the entire parameter space). In the case of GloVe this means that a simultaneous rotation of all the word vectors in both layers during training does not alter the resulting model. This proposal would have the further advantage of greatly reducing the number of learning rates that have to be stored in memory. I don\u2019t know if this proposal would have regret minimisation properties analogous to Adagrad. I haven\u2019t read the original paper of Duchi et al. (2011), and what I am proposing might be subsumed there by the full-rank case (thanks to Alexey Rodriguez for pointing this out). Perhaps a block diagonal matrix could be used instead of a diagonal one.","flair":"null\tnull"}
{"author":"siddharth-agrawal","created":"Sat Oct 01 02:27:35 EDT 2016","text":"Watch Google's Jeff Dean talk about Google Brain and the Brain Residency Program. Jeff will will discuss the current state of the Google Brain Team, Tensorflow, the future directions of Brain and tell you a bit more about our Google Brain Residency Program (2017 application open now! https:\/\/goo.gl\/Z4TtnQ). Check out g.co\/brain for more info! The first part of the video gives an overview of the work going on in the Google Brain team, and at 32:56 is a discussion about the specifics of the Brain Residency program.Links from the presentation:Brain Team link : g.co\/brainBrain Residency Program link: g.co\/brainresidencyTensorFlow: tensorflow.orgGithub Links:TensorFlow github: https:\/\/github.com\/tensorflow\/tensorflowTensorFlow\/Magenta: https:\/\/github.com\/tensorflow\/magenta Links to papers:TensorFlow whitepaper - http:\/\/tensorflow.org\/whitepaper2015.pdfLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection - http:\/\/arxiv.org\/abs\/1603.02199Adversarial examples in the physical world - http:\/\/arxiv.org\/abs\/1607.02533Concrete problems in AI Safety - https:\/\/arxiv.org\/pdf\/1606.06565v2.pdfPhoto credits: Scott Alan Miller, \"Liesl Sleeping on the Couch.\"Craig Chew-Moulding, \"Green Kite Monster.\"Jeff Martyka, Ultimate photo.","flair":"null\tnull"}
{"author":"sybilckw","created":"Thu Oct 27 08:56:38 EDT 2016","text":"JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. This page uses JavaScript to progressively load the article content as a user scrolls. Click the View full text link to bypass dynamically loaded article content. View full text In this paper, we propose a group-aware deep feature learning (GA-DFL) approach for facial age estimation. Unlike most existing methods which utilize hand-crafted descriptors for face representation, our GA-DFL method learns a discriminative feature descriptor per image directly from raw pixels for face representation under the deep convolutional neural networks framework. Motivated by the fact that age labels are chronologically correlated and the facial aging datasets are usually lack of labeled data for each person in a long range of ages, we split ordinal ages into a set of discrete groups and learn deep feature transformations across age groups to project each face pair into the new feature space, where the intra-group variances of positive face pairs from the training set are minimized and the inter-group variances of negative face pairs are maximized, simultaneously. Moreover, we employ an overlapped coupled learning method to exploit the smoothness for adjacent age groups. To further enhance the discriminative capacity of face representation, we design a multi-path CNN approach to integrate the complementary information from multi-scale perspectives. Experimental results show that our approach achieves very competitive performance compared with most state-of-the-arts on three public face aging datasets that were captured under both controlled and uncontrolled environments. Hao Liu received the B.S. degree in software engineering from Sichuan University, China, in 2011 and the Engineering Master degree in computer technology from University of Chinese Academy of Sciences, China, in 2014. He is currently pursuing the Ph.D. degree at the department of automation, Tsinghua University. His research interests include face recognition, facial age estimation and deep learning. Jiwen Lu received the B.Eng. degree in mechanical engineering and the M.Eng. degree in electrical engineering from the Xi'an University of Technology, Xi'an, China, and the Ph.D. degree in electrical engineering from the Nanyang Technological University, Singapore, in 2003, 2006, and 2012, respectively. He is currently an Associate Professor with the Department of Automation, Tsinghua University, Beijing, China. From March 2011 to November 2015, he was a Research Scientist with the Advanced Digital Sciences Center, Singapore. His current research interests include computer vision, pattern recognition, and machine learning. He has authored\/co-authored over 130 scientific papers in these areas, where more than 50 papers are published in the IEEE Transactions journals and top-tier computer vision conferences. He serves\/has served as an Associate Editor of Pattern Recognition Letters, Neurocomputing, and the IEEE Access, a Guest Editor of Pattern Recognition, Computer Vision and Image Understanding, Image and Vision Computing and Neurocomputing, and an elected member of the Information Forensics and Security Technical Committee of the IEEE Signal Processing Society. He is\/was a Workshop Chair\/Special Session Chair\/Area Chair for more than 10 international conferences. He has given tutorials at several international conferences including ACCV\u201916, CVPR\u201915, FG\u201915, ACCV\u201914, ICME\u201914, and IJCB\u201914. He was a recipient of the First-Prize National Scholarship and the National Outstanding Student Award from the Ministry of Education of China in 2002 and 2003, the Best Student Paper Award from Pattern Recognition and Machine Intelligence Association of Singapore in 2012, the Top 10% Best Paper Award from IEEE International Workshop on Multimedia Signal Processing in 2014, and the National 1000 Young Talents Plan Program in 2015, respectively. He is a senior member of the IEEE. Jianjiang Feng is an associate professor in the Department of Automation at Tsinghua University, Beijing. He received the B.S. and Ph.D. degrees from the School of Telecommunication Engineering, Beijing University of Posts and Telecommunications, China, in 2000 and 2007. From 2008 to 2009, he was a Post Doctoral researcher in the PRIP lab at Michigan State University. He is an Associate Editor of Image and Vision Computing. His research interests include fingerprint recognition and computer vision. Jie Zhou received the BS and MS degrees both from the Department of Mathematics, Nankai University, Tianjin, China, in 1990 and 1992, respectively, and the Ph.D. degree from the Institute of Pattern Recognition and Artificial Intelligence, Huazhong University of Science and Technology (HUST), Wuhan, China, in 1995. From then to 1997, he served as a postdoctoral fellow in the Department of Automation, Tsinghua University, Beijing, China. Since 2003, he has been a full professor in the Department of Automation, Tsinghua University. His research interests include computer vision, pattern recognition, and image processing. In recent years, he has authored more than 100 papers in peer-reviewed journals and conferences. Among them, more than 30 papers have been published in top journals and conferences such as the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Image Processing, and CVPR. He is an associate editor for the International Journal of Robotics and Automation and two other journals. He received the National Outstanding Youth Foundation of China Award. He is a senior member of the IEEE.","flair":"three\tResearch"}
{"author":"Mandrathax","created":"Mon Nov 14 10:15:58 EST 2016","text":"This is a place to share machine learning research papers, journals, and articles that you're reading this week.\nIf it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  \n\n|Previous weeks|\n|--------------|\n|[Week 1](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4qyjiq\/machine_learning_wayr_what_are_you_reading_week_1\/)|  \n|[Week 2](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4s2xqm\/machine_learning_wayr_what_are_you_reading_week_2\/)|  \n|[Week 3](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4t7mqm\/machine_learning_wayr_what_are_you_reading_week_3\/)|  \n|[Week 4](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4ub2kw\/machine_learning_wayr_what_are_you_reading_week_4\/)| \n|[Week 5](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4xomf7\/machine_learning_wayr_what_are_you_reading_week_5\/)| \n|[Week 6](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4zcyvk\/machine_learning_wayr_what_are_you_reading_week_6\/)|\n|[Week 7](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/52t6mo\/machine_learning_wayr_what_are_you_reading_week_7\/)|\n|[Week 8](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/53heol\/machine_learning_wayr_what_are_you_reading_week_8\/)|\n|[Week 9](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/54kvsu\/machine_learning_wayr_what_are_you_reading_week_9\/)|\n|[Week 10](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/56s2oa\/discussion_machine_learning_wayr_what_are_you\/)|\n|[Week 11](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/57xw56\/discussion_machine_learning_wayr_what_are_you\/)|\n|[Week 12](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/5acb1t\/d_machine_learning_wayr_what_are_you_reading_week\/)|\n\nMost upvoted papers last week : \n\n[Learning Scalable Deep Kernels with Recurrent Structure](https:\/\/arxiv.org\/abs\/1610.08936)\n\n[Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http:\/\/people.idsia.ch\/%7Eciresan\/data\/icann2011.pdf) (PDF only)\n\n[Smart Reply: Automated Response Suggestion for Email](https:\/\/arxiv.org\/abs\/1606.04870)\n\nBesides that, there are no rules, have fun.","flair":"one\tDiscussion"}
{"author":"GawkyFuse","created":"Tue Sep 27 21:49:22 EDT 2016","text":"To better understand basic machine learning, I built an Excel Spreadsheet that takes the weights and biases trained by a TensorFlow model and uses them to classify test MNIST handwritten digits. As a beginner to machine learning, I found this exercise to be helpful with better understanding the flow of a basic model. I hope that you might find this to be helpful, too.\n\nLink to spreadsheet: https:\/\/db.tt\/DyI7LU0H","flair":"null\tnull"}
{"author":"Mandrathax","created":"Mon Oct 17 11:25:08 EDT 2016","text":"This is a place to share machine learning research papers, journals, and articles that you're reading this week.\nIf it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.  \n\n|Previous weeks|\n|--------------|\n|[Week 1](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4qyjiq\/machine_learning_wayr_what_are_you_reading_week_1\/)|  \n|[Week 2](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4s2xqm\/machine_learning_wayr_what_are_you_reading_week_2\/)|  \n|[Week 3](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4t7mqm\/machine_learning_wayr_what_are_you_reading_week_3\/)|  \n|[Week 4](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4ub2kw\/machine_learning_wayr_what_are_you_reading_week_4\/)| \n|[Week 5](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4xomf7\/machine_learning_wayr_what_are_you_reading_week_5\/)| \n|[Week 6](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4zcyvk\/machine_learning_wayr_what_are_you_reading_week_6\/)|\n|[Week 7](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/52t6mo\/machine_learning_wayr_what_are_you_reading_week_7\/)|\n|[Week 8](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/53heol\/machine_learning_wayr_what_are_you_reading_week_8\/)|\n|[Week 9](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/54kvsu\/machine_learning_wayr_what_are_you_reading_week_9\/)|\n|[Week 10](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/56s2oa\/discussion_machine_learning_wayr_what_are_you\/)|\n\nMost upvoted papers last week : \n \n[Pixel Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1601.06759)\n\n[Residual Networks are Exponential Ensembles of Relatively Shallow Networks](https:\/\/arxiv.org\/abs\/1605.06431)\n\n[Hybrid computing using a neural network with dynamic external memory](http:\/\/www.nature.com\/nature\/journal\/vaop\/ncurrent\/full\/nature20101.html)\n\n[gvnn: Neural Network Library for Geometric Computer Vision](https:\/\/arxiv.org\/abs\/1607.07405)\n\nBesides that, there are no rules, have fun.","flair":"one\tDiscussion"}
{"author":"xternalz","created":"Sun Oct 09 21:57:21 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.02357 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Xception: Deep Learning with Depthwise Separable Convolutions Authors: François Chollet (Submitted on 7 Oct 2016 (v1), last revised 11 Oct 2016 (this version, v2)) Abstract: We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \\textit{depthwise separable convolution} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1610.02357 [cs.CV]   (or arXiv:1610.02357v2 [cs.CV] for this version) Submission history From: Francois Chollet [view email] [v1] Fri, 7 Oct 2016 17:51:51 GMT (772kb,D) [v2] Tue, 11 Oct 2016 17:37:25 GMT (772kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"schorschico","created":"Fri Nov 04 12:06:49 EDT 2016","text":"What do you get if you cross aspirin with ibuprofen? Harvard chemistry professor Alán Aspuru-Guzik isn\u2019t sure, but he\u2019s trained software that could give him an answer by suggesting a molecular structure that combines properties of both drugs. The AI program could help the search for new drug compounds. Pharmaceutical research tends to rely on software that exhaustively crawls through giant pools of candidate molecules using rules written by chemists, and simulations that try to identify or predict useful structures. The former relies on humans thinking of everything, while the latter is limited by the accuracy of simulations and the computing power required. Aspuru-Guzik\u2019s system can dream up structures more independently of humans and without lengthy simulations. It leverages its own experience, built up by training machine-learning algorithms with data on hundreds of thousands of drug-like molecules. \"It explores more intuitively, using chemical knowledge it learned, like a chemist would,\" says Aspuru-Guzik. \"Humans could be better chemists with this kind of software as their assistant.\" Aspuru-Guzik was named to MIT Technology Review\u2019s list of young innovators in 2010. The new system was built using a machine-learning technique called deep learning, which has become pervasive in computing companies but is less established in the natural sciences. It uses a design known as a generative model, which takes in a trove of data and uses what it learned to generate plausible new data of its own. Generative models are more typically used to create images, speech, or text, for example in the case of Google\u2019s Smart Reply feature that suggests responses to e-mails. But last month Aspuru-Guzik and colleagues at Harvard, the University of Toronto, and the University of Cambridge published results from creating a generative model trained on 250,000 drug-like molecules. The system could generate plausible new structures by combining properties of existing drug compounds, and be asked to suggest molecules that strongly displayed certain properties such as solubility, and being easy to synthesize. Vijay Pande, a professor of chemistry at Stanford and partner with venture capital firm Andreessen Horowitz, says the project adds to the growing evidence that new ideas in machine learning will transform scientific research (see \u201CStopping Breast Cancer with Help from AI\u201D). It suggests that deep-learning software can internalize a kind of chemical knowledge, and use it to help scientists, he says. \u201CI think this could be very broadly applicable,\u201D says Pande. \u201CIt could play a role in finding or optimizing lead drug candidates, or other areas like solar cells or catalysts.\u201D The researchers have already experimented with training their system on a database of organic LED molecules, which are important for displays. But making the technique into a practical tool will require improving its chemistry skills, because the structures it suggests are sometimes nonsensical. Pande says one challenge for asking software to learn chemistry may be that researchers have not yet identified the best data format to use to feed chemical structures into deep-learning software. Images, speech, and text have proven to be a good fit\u2014as evidenced by software that rivals humans at image and speech recognition and translation\u2014but existing ways of encoding chemical structures may not be quite right. Aspuru-Guzik and his colleagues are thinking about that, along with adding new features to his system to reduce its chemical blooper rate. He also hopes that giving his system more data, to broaden its chemistry knowledge, will improve its power, in the same way that databases of millions of photos have helped image recognition become useful. The American Chemical Society\u2019s database records around 100 million published chemical structures. Before long, Aspuru-Guzik hopes to feed all of them to a version of his AI program.","flair":"three\tResearch"}
{"author":"yoyosarian","created":"Wed Oct 05 22:54:37 EDT 2016","text":"Here is an implementation of a few results seen in Early Visual Concept Learning with Unsupervised Deep Learning. This paper looks at training variational autoencoders to produce \"disentangled\" representations in the latent space. Their main experiment involves learning black and white images of different shapes such as hearts, circles and boxes in different x, y positions and sizes. They find that by tuning a constant beta that controls the KL loss they can learn good disentangled representations. This results in latent variables corresponding directly to x, y positions and sizes. In this project, I have attempted to recreate these results on a dataset of black and white images of balls such as these (one ball dataset) (two ball dataset). There are three models tested on both the one ball and the two ball dataset. The models used are fully connected, convolutional and all convolutional. The fully connected model is a replica of the model used in the paper. The only differences are the optimizer and the use of ELU instead of ReLU. The convolutional network was custom made to test how it does against the fully connected. The all convolutional model has no fully connected layer and keeps the latent encoding as a image. This seemed like a neat idea but turned out to not perform very well. For exact details on the models look at . To execute all experiments run . This will run each of the 3 models on the 2 datasets with 3 different values of beta. This totals 18 experiments. All models will get saved in . Running this on a GTX 1080 took approximately 1 day for all experiments to finish. To create the figures presented, run , , and . All plots are saved in . There are different parameters that can be messed with and are all flags in . There are three different kinds of figures produced. In Figure 2 of the paper they show the effect of changing individual latent variables on the produced images. Doing this reveals that some of the latent variables correspond with things like position and size. Here are recreations of similar results for different models on the datasets. Using their same model I was able to get similar but slightly worse results. In this figure we take on image of the ball and produce a latent encoding. Then we order which variables have the lowest to highest standard deviation and change variables individually between -3 and 3. This shows that only 3 variables have effect on the balls position. It seems as though these variables roughly correspond to some axis of ball position. For example the second variable roughly control the y position. The two ball dataset produces more entangled states It looks as though the variables learn rotations of the two balls next to each other. The second variable twists the two balls in a clockwise fashion. Over all, these are pretty good results. The other models produced more entangled encoding and all figures are included in . The all convolutional model was particularly bad and failed to learn disentangled representations. Here are the plotted average standard deviation of the latent encoding ordered from lowest to highest. This indicates how much the model is using each variable. We see from these plots that the fully connected model from the paper produced the best results only relying on a few variables heavily for the encoding. We also notice that increasing beta causes the latent variables to be used less. This makes sense because it increases the KL loss causing a stronger push for variables to learn 0 mean and 1 standard deviation. The same results are seen in the paper. Any discussion of an autoencoder would be incomplete without a look at how well it reconstructs the input. Here are two figures that show how well does for each beta value and each dataset. Again the fully connected model wins out on all tests. The trend observed in the paper about high beta values yielding high reconstruction error is confirmed in these results. This was a fun little project. I had no idea variational autoencoders where making these kinds of latent space encodings. I was really hoping that the all convolutional model would win over the fully connected in terms of reconstruction error but no such luck. It makes sense that it would have a significantly harder time producing disentangled states though. Keeping the latent encoding convolutional would make each variable effected differently by the image and probably make disentangling impossible.","flair":"null\tnull"}
{"author":"abhishkk65","created":"Mon Oct 17 14:29:35 EDT 2016","text":"When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. It\u2019s more obvious in some cases than others, but a large fraction of recent models exhibit this behavior. Mysteriously, the checkerboard pattern tends to be most prominent in images with strong colors. What\u2019s going on? Do neural networks hate bright colors? The actual cause of these artifacts is actually remarkably simple, as is a method for avoiding them. When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details. In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to \u201Cpaint\u201D a square in the larger one. (Deconvolution has a number of interpretations and different names, including \u201Ctransposed convolution.\u201D We use the name \u201Cdeconvolution\u201D in this article for brevity. For excellent discussion of deconvolution, see Dumoulin & Visin, 2016 and Shi, et al., 2016a.) Unfortunately, deconvolution can easily have \u201Cuneven overlap,\u201D putting more of the metaphorical paint in some places than others (Gauthier, 2015). In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this \u2013 as we\u2019ll discuss in more detail later \u2013 in practice neural networks struggle to avoid it completely. The overlap pattern also forms in two dimensions. The uneven overlaps on the two axes multiply together, creating a characteristic checkerboard-like pattern of varying magnitudes. In fact, the uneven overlap tends to be more extreme in two dimensions! Because the two patterns are multiplied together, the unevenness gets squared. For example, in one dimension, a stride 2, size 3 deconvolution has some outputs with twice the number of inputs as others, but in two dimensions this becomes a factor of four. Now, neural nets typically use multiple layers of deconvolution when creating images, iteratively building a larger image out of a series of lower resolution descriptions. While it\u2019s possible for these stacked deconvolutions to cancel out artifacts, they often compound, creating artifacts on a variety of scales. Stride 1 deconvolutions \u2013 which we often see as the last layer in successful models (eg. Salimans et al., 2016) \u2013 are quite effective at dampening artifacts. They can remove artifacts of frequencies that divide their size, and reduce others artifacts of frequency less than their size. However, artifacts can still leak through, as seen in many recent models. In addition to the high frequency checkerboard-like artifacts we observed above, early deconvolutions can create lower-frequency artifacts, which we\u2019ll explore in more detail later. These artifacts tend to be most prominent when outputting unusual colors. Since neural network layers typically have a bias (a learned value added to the output) it\u2019s easy to output the average color. The further a color \u2013 like bright red \u2013 is away from the average color, the more deconvolution needs to contribute. Thinking about things in terms of uneven overlap is \u2013 while a useful framing \u2013 kind of simplistic. For better or worse, our models learn weights for their deconvolutions. In theory, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced. This is a tricky balancing act to achieve, especially when one has multiple channels interacting. Avoiding artifacts significantly restricts the possible filters, sacrificing model capacity. In practice, neural networks struggle to learn to completely avoid these patterns. In fact, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! While it isn\u2019t their default behavior the way it is for uneven overlap, it\u2019s still very easy for even overlap deconvolution to cause artifacts. Completely avoiding artifacts is still a significant restriction on filters, and in practice the artifacts are still present in these models, although they seem milder. (See Dumoulin, et al., 2016, which uses stride 2 size 4 deconvolutions, as an example.) There are probably a lot of factors at play here. For example, in the case of Generative Adversarial Networks (GANs), one issue may be the discriminator and its gradients (we\u2019ll discuss this more later). But a big part of the problem seems to be deconvolution. At best, deconvolution is fragile because it very easily represents artifact creating functions, even when the size is carefully chosen. At worst, creating artifacts is the default behavior of deconvolution. Is there a different way to upsample that is more resistant to artifacts? To avoid these artifacts, we\u2019d like an alternative to regular deconvolution (\u201Ctransposed convolution\u201D). Unlike deconvolution, this approach to upsampling shouldn\u2019t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts. One approach is to make sure you use a kernel size that is divided by your stride, avoiding the overlap issue. This is equivalent to \u201Csub-pixel convolution,\u201D a technique which has recently had success in image super-resolution (Shi, et al., 2016b). However, while this approach helps, it is still easy for deconvolution to fall into creating artifacts. Another approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution (eg. Dong, et al., 2015). Both deconvolution and the different resize-convolution approaches are linear operations, and can be interpreted as matrices. This a helpful way to see the differences between them. Where deconvolution has a unique entry for each output window, resize-convolution is implicitly weight-tying in a way that discourages high frequency artifacts. We\u2019ve had our best results with nearest-neighbor interpolation, and had difficulty making bilinear resize work. This may simply mean that, for our models, the nearest-neighbor happened to work well with hyper-parameters optimized for deconvolution. It might also point at trickier issues with naively using bilinear interpolation, where it resists high-frequency image features too strongly. We don\u2019t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts. Resize-convolution layers can be easily implemented in TensorFlow using tf.image.resize_images(). For best results, use tf.pad() before doing convolution with tf.nn.conv2d() to avoid boundary artifacts. Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts. One case where we\u2019ve found this approach to help is Generative Adversarial Networks. Simply switching out the standard deconvolutional layers for nearest-neighbor resize followed by convolution causes artifacts of different frequencies to disappear. In fact, the difference in artifacts can be seen before any training occurs. If we look at the images the generator produces, initialized with random weights, we can already see the artifacts: This suggests that the artifacts are due to this method of generating images, rather than adversarial training. (It also suggests that we might be able to learn a lot about good generator design without the slow feedback cycle of training models.) Another reason to believe these artifacts aren\u2019t GAN specific is that we see them in other kinds of models, and have found that they also go away when we switch to resize-convolution upsampling. For example, consider real-time artistic style transfer (Johnson, et al., 2016) where a neural net is trained to directly generate style-transferred images. We\u2019ve found these to be vulnerable to checkerboard artifacts (especially when the cost doesn\u2019t explicitly resist them). However, switching deconvolutional layers for resize-convolution layers makes the artifacts disappear. Forthcoming papers from the Google Brain team will demonstrate the benefits of this technique in more thorough experiments and state-of-the-art results. (We\u2019ve chosen to present this technique separately because we felt it merited more detailed discussion, and because it cut across multiple papers.) Whenever we compute the gradients of a convolutional layer, we do deconvolution (transposed convolution) on the backward pass. This can cause checkerboard patterns in the gradient, just like when we use deconvolution to generate images. The presence of high-frequency \u201Cnoise\u201D in image model gradients is already known in the feature visualization community, where it\u2019s a major challenge. Somehow, feature visualization methods must compensate for this noise. For example, DeepDream (Mordvintsev, et al., 2015) seems to cause destructive interference between artifacts in a number of ways, such as optimizing many features simultaneously, and optimizing at many offsets and scales. In particular, the \u201Cjitter\u201D of optimizing at different offsets cancels out some of the checkerboard artifacts. (While much some of the artifacts are our standard checkerboard pattern, others are a less organized high-frequency pattern. We believe these to be caused by max pooling. Max pooling was previously linked to high-frequency artifacts in Henaff & Simoncelli, 2015.) More recent work in feature visualization (eg. Mordvintsev, 2016), has explicitly recognized and compensated for these high-frequency gradient components. One wonders if better neural network architectures could make these efforts unnecessary. Do these gradient artifacts affect GANs? If gradient artifacts can affect an image being optimized based on a neural networks gradients in feature visualization, we might also expect it to affect the family of images parameterized by the generator as they\u2019re optimized by the discriminator in GANs. We\u2019ve found that this does happen in some cases. When the generator is neither biased for or against checkerboard patterns, strided convolutions in the discriminator can cause them. It\u2019s unclear what the broader implications of these gradient artifacts are. One way to think about them is that some neurons will get many times the gradient of their neighbors, basically arbitrarily. Equivalently, the network will care much more about some pixels in the input than others, for no good reason. Neither of those sounds ideal. It seems possible that having some pixels affect the network output much more than others may exaggerate adversarial counter-examples. Because the derivative is concentrated on small number of pixels, small perturbations of those pixels may have outsized effects. We have not investigated this. The standard approach of producing images with deconvolution \u2013 despite its successes! \u2013 has some very conceptually simple issues that lead to artifacts in produced images. Using a natural alternative without these issues causes the artifacts to go away (Analogous arguments suggest that standard strided convolutional layers may also have issues). This seems like an exciting opportunity to us! It suggests that there is low-hanging fruit to be found in carefully thinking through neural network architectures, even ones where we seem to have clean working solutions. In the meantime, we\u2019ve provided an easy to use solution that improves the quality of many approaches to generating images with neural networks. We look forward to seeing what people do with it, and whether it helps in domains like audio, where high frequency artifacts would be particularly problematic.","flair":"four\tProject"}
{"author":"MathAndProgramming","created":"Sat Nov 26 12:59:40 EST 2016","text":"I've been doing some work with some deep conv autoencoders, and, as I imagine is the case with many other deep models, training time is a huge impediment. It's hard to iterate on model architecture when you're getting nearly one result a day.\n\nWhat sorts of things do you do to accelerate your work in these contexts? Should I be parallelizing my training all over AWS with different architectures and hyperparameters? Do you have any pretraining tricks that you've found to accelerate your work?\n\nI'm playing with some extensions of the \"Learning to learn gradient descent by gradient descent\" ideas to try to improve on RMSProp, but that's a pretty major undertaking in itself.","flair":"one\tDiscussion"}
{"author":"sybilckw","created":"Fri Nov 11 09:47:48 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.09064 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.AI < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Artificial Intelligence Title: Discovering Blind Spots of Predictive Models: Representations and Policies for Guided Exploration Authors: Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Eric Horvitz (Submitted on 28 Oct 2016) Abstract: Predictive models deployed in the world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases seen in the open world. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of optimizing the discovery of unknown unknowns of any predictive model under a fixed budget, which limits the number of times an oracle can be queried for true labels. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on instance similarity, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:1610.09064 [cs.AI]   (or arXiv:1610.09064v1 [cs.AI] for this version) Submission history From: Himabindu Lakkaraju [view email] [v1] Fri, 28 Oct 2016 02:55:14 GMT (6393kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"sybilckw","created":"Mon Oct 31 10:04:27 EDT 2016","text":"Corrected proofs are Articles in Press that contain the authors' corrections. Final citation details, e.g., volume and\/or issue number, publication year and page numbers, still need to be added and the text might change before final publication. Although corrected proofs do not have all bibliographic details available yet, they can already be cited using the year of online publication and the DOI , as follows: author(s), article title, Publication (year), DOI. Please consult the journal's reference style for the exact appearance of these elements, abbreviation of journal names and use of punctuation. When the final article is assigned to volumes\/issues of the Publication, the Article in Press version will be removed and the final version will appear in the associated published volumes\/issues of the Publication. The date the article was first made available online will be carried over.","flair":"three\tResearch"}
{"author":"bihaqo","created":"Fri Nov 11 05:18:49 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.03214 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Ultimate tensorization: compressing convolutional and FC layers alike Authors: Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, Dmitry Vetrov (Submitted on 10 Nov 2016) Abstract: Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset. Comments: NIPS 2016 workshop: Learning with Tensors: Why Now and How? Subjects: Learning (cs.LG) Cite as: arXiv:1611.03214 [cs.LG]   (or arXiv:1611.03214v1 [cs.LG] for this version) Submission history From: Alexander Novikov [view email] [v1] Thu, 10 Nov 2016 08:07:46 GMT (22kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"logicflow","created":"Sun Oct 16 20:10:10 EDT 2016","text":"Software OCR for printed documents (e.g. ABBYY FineReader) is becoming quite good, but I haven\u2019t been able to find any literature on which techniques are used. I\u2019m assuming the best results are using a CNN based pipeline but would appreciate if someone could point me to some papers on the topic. Thanks!\n","flair":"one\tDiscussion"}
{"author":"pjreddie","created":"Wed Nov 16 04:07:36 EST 2016","text":"SqueezeNet is cool but it's JUST optimizing for parameter count. When most high quality images are 10MB or more why do we care if our models are 5 MB or 50 MB? If you want a small model that's actually FAST, why not check out the Darknet reference network? It's only 28 MB but more importantly, it's only 800 million floating point operations. The original Alexnet is 2.3 billion. Darknet is 2.9 times faster and it's small and it's 4% more accurate. So what about SqueezeNet? Sure the weights are only 4.8 MB but a forward pass is still 2.2 billion operations. Alexnet was a great first pass at classification but we shouldn't be stuck back in the days when networks this bad are also this slow! But anyway, people are super into SqueezeNet so if you really insist on small networks, use this: The real winner here is clearly the Darknet reference model but if you insist on wanting a small model, use Tiny Darknet. Or train your own, it should be easy! Here's how to use it in Darknet (and also how to install Darknet): Hopefully you see something like this: The model is just some 3x3 and 1x1 convolutional layers:","flair":"null\tnull"}
{"author":"paulhendricks","created":"Tue Oct 25 10:10:58 EDT 2016","text":"OpenAI Gym is a open-source Python toolkit for developing and comparing reinforcement learning algorithms. This R package is a wrapper for the OpenAI Gym API, and enables access to an ever-growing variety of environments. If you encounter a clear bug, please file a minimal reproducible example on github. To download the code and install the requirements, you can run the following shell commands: This code is intended to be run locally by a single user. The server runs in python. To start the server from the command line, run this: For more details, please see here: https:\/\/github.com\/openai\/gym-http-api. In a separate R terminal, you can then try running the example agent and see what happens:","flair":"four\tProject"}
{"author":"dmitry_ulyanov","created":"Wed Oct 19 05:44:41 EDT 2016","text":"This is a multicore modification of Barnes-Hut t-SNE by L. Van der Maaten with python and Torch CFFI-based wrappers. This code also works faster than sklearn.TSNE on 1 core. Barnes-Hut t-SNE is done in two steps. First step: an efficient data structure for nearest neighbours search is built and used to compute probabilities. This can be done in parallel for each point in the dataset, this is why we can expect a good speed-up by using more cores. Second step: the embedding is optimized using gradient descent. This part is essentially consecutive so we can only optimize within iteration. In fact some parts can be parallelized effectively, but not all of them a parallelized for now. That is why second step speed-up will not be that significant as first step sepeed-up but there is still room for improvement. So when can you benefit from parallelization? It is almost true, that the second step computation time is constant of and depends mostly on . The first part's time depends on a lot, so for small , for large . As we are only good at parallelizing step 1 we will benefit most when is large enough (MNIST's is large, even for is not so much). I wrote multicore modification originally for Springleaf competition, where my data table was about and only several days left till the end of the competition so any speed-up was handy. Interestingly, that this code beats other implementations. We compare to (Barnes-Hut of course), L. Van der Maaten's bhtsne, py_bh_tsne repo (cython wrapper for bhtsne with QuadTree). for every run. In fact py_bh_tsne repo works at the same speed as this code when using more optimization flags for compiler. This is a benchmark for MNIST data: I did my best to find what is wrong with sklearn numbers, but it is the best benchmark I could do (you can find test script in folder). This table shows a relative to 1 core speed-up when using cores. Python and torch wrappers are available. Make sure is installed on your system and install python prerequisites: Pip installation does not copy file for some reason (experts wanted). Tested with both Python 2.7 and 3.5 (conda) and Ubuntu 14.04. Never tested on MacOS, something similar to this should be done for successful compilation. Also read this issue. You can use it as a drop-in replacement for sklearn.manifold.TSNE. Please refer to sklearn TSNE manual for parameters explanation. Only double arrays are supported for now. For this implementation is fixed to , which is the most common case (use Barnes-Hut t-SNE or sklearn otherwise). Also note that some of the parameters will be ignored for sklearn compatibility. Only these parameters are used (and they are the most important ones): You can test it on MNIST dataset with the following command: To make the computation log visible in jupyter please install () and execute this line in any cell beforehand: Memory leakages are possible if you interrupt the process. Should be OK if you let it run until the end. To install execute the following command from repository folder: You can run t-SNE like that: type only supported for now. Please cite this repository if it was useful for your research: Of course, do not forget to cite L. Van der Maaten's paper","flair":"four\tProject"}
{"author":"acanai","created":"Tue Nov 15 09:31:20 EST 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose Bayesian Optimization for Probabilistic Programs (NIPS 2016 Spotlight) Tom Rainforth SubscribeSubscribedUnsubscribe55 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 941 views 11 Like this video? Sign in to make your opinion count. Sign in 12 0 Don't like this video? Sign in to make your opinion count. Sign in 1 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 11, 2016 Spotlight video for 30th Conference on Advances in Neural Information Processing Systems (2016). Paper is available here http:\/\/www.robots.ox.ac.uk\/~twgr\/asse.... Other links: Code - https:\/\/bitbucket.org\/twgr\/bopp\/ - http:\/\/www.robots.ox.ac.uk\/~fwood\/ang... Author's websites - http:\/\/www.robots.ox.ac.uk\/~twgr\/ - http:\/\/www.tuananhle.co.uk\/ - http:\/\/www.ccs.neu.edu\/home\/jwvdm\/ - http:\/\/www.robots.ox.ac.uk\/~mosb\/ - http:\/\/www.robots.ox.ac.uk\/~fwood\/ Category People & Blogs License Standard YouTube License Show more Show less Loading... Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next \"An Overview of Probabilistic Programming\" by Vikash K. Mansinghka - Duration: 1:02:55. Strange Loop 6,286 views 1:02:55 NIPS 2016 Spotlight: Full-Capacity Unitary Recurrent Neural Networks - Duration: 2:58. Thomas Powers 32 views 2:58 Thomas Wiecki - Probabilistic Programming in Python - Duration: 45:35. EuroPython Conference 878 views 45:35 NIPS 2016 Synthesis of MCMC and Belief Propagation spotlight presentation - Duration: 3:04. ì\u2022ˆì\u201E±ìˆ˜ 22 views 3:04 103 videos Play all Studio Ghibli OSTìµœí�¬ì£¼ Interpretable Nonlinear Dynamic Modeling of Neural Trajectories [NIPS 2016 spotlight] - Duration: 3:04. Brain is (not) a computer 69 views 3:04 NIPS 2016 Spotlight: Optimal Binary Classifier Aggregation for General Losses - Duration: 2:52. Akshay Balsubramani 9 views 2:52 Data Programming NIPS 2016 Spotlight Video - Duration: 3:24. HazyResearch 220 views 3:24 Coresets for Bayesian Logistic Regression - NIPS 2016 spotlight video - Duration: 3:11. Jonathan Huggins 334 views 3:11 NIPS 2016 spotlight video - CMICOT - Duration: 3:00. Kate Gladkikh 249 views 3:00 Probabilistic Programming in Quantitative Finance by Thomas Wiecki, PhD - Duration: 49:18. Quantopian 3,770 views 49:18 NIPS 2016: Stochastic Structured Prediction under Bandit Feedback - Duration: 2:34. statnlp-hdu 345 views 2:34 NIPS 2016 Spotlight: Multimodal Residual Learning for Visual QA - Duration: 3:01. Jin-Hwa Kim 219 views 3:01 Tue Herlau NIPS 2016 spotlight - Duration: 2:59. tueherlau 10 views 2:59 MIT researcher Vikash Mansinghka on probabilistic programming - Duration: 1:09:02. Columbia Data Science 1,028 views 1:09:02 Collaborative Recurrent Autoencoder for Recommender Systems - NIPS 2016 spotlight video - Duration: 2:56. Wang Hao 15 views 2:56 NIPS 2015 Workshop (Ghahramani) 15537 Bayesian Optimization: Scalability and Flexibility - Duration: 48:24. NIPS 201 views 48:24 Nando de Freitas: Bayesian Optimization - Duration: 1:03:32. Open Data Science Initiative 1,223 views 1:03:32 3. Bayesian Optimization of Hyper Parameters - Duration: 13:30. Artificial Intelligence Courses 2,167 views 13:30 Probabilistic Programming: Generative Probabilistic Graphics Programming and New Research Directions - Duration: 1:02:01. Microsoft Research 93 views 1:02:01 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"three\tResearch"}
{"author":"KeponeFactory","created":"Tue Oct 04 10:50:13 EDT 2016","text":" YouTube-8M: A Large-Scale Video Classification Benchmark Sami Abu-El-Haija haija@google.com Nisarg Kothari ndk@google.com Joonseok Lee joonseok@google.com Paul Natsev natsev@google.com George Toderici gtoderici@google.com Balakrishnan Varadarajan balakrishnanv@google.com Sudheendra Vijayanarasimhan svnaras@google.com Google Research ABSTRACT Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learn- ing and inexpensive commodity hardware have reduced the bar- rier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Al- though large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of∼8 million videos\u2014500K hours of video\u2014annotated with a vocabulary of 4800 visual en- tities. To get the videos and their (multiple) labels, we used a YouTube video annotation system, which labels videos with the main topics in them. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals, so they repre- sent an excellent target for content-based annotation approaches. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre- trained on ImageNet to extract the hidden representation immedi- ately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. The dataset contains frame-level features for over 1.9 billion video frames and 8 million videos, making it the largest public multi-label video dataset. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using the publicly-available TensorFlow framework. We plan to release code for training a basic TensorFlow model and for computing metrics. We show that pre-training on large data generalizes to other datasets like Sports-1M and ActivityNet. We achieve state-of-the-art on Ac- tivityNet, improving mAP from 53.8% to 77.6%. We hope that the unprecedented scale and diversity of YouTube-8M will lead to ad- vances in video understanding and representation learning. 1. INTRODUCTION Large-scale datasets such as ImageNet [6] have been key en- ablers of recent progress in image understanding [20, 14, 11]. By supporting the learning process of deep networks with mil- lions of parameters, such datasets have played a crucial role for the rapid progress of image understanding to near-human level ac- curacy [30]. Furthermore, intermediate layer activations of such networks have proven to be powerful and interpretable for vari- Figure 1: YouTube-8M is a large-scale benchmark for general multi-label video classification. This screenshot of a dataset explorer depicts a subset of videos in the dataset annotated with the entity \u201CGuitar\u201D. The dataset explorer allows browsing and searching of the full vocabulary of Knowledge Graph enti- ties, grouped in 24 top-level verticals, along with corresponding videos. ous tasks beyond classification [41, 9, 31]. In a similar vein, the amount and size of video benchmarks is growing with the avail- ability of Sports-1M [19] for sports videos and ActivityNet [12] for human activities. However, unlike ImageNet, which contains a diverse and general set of objects\/entities, existing video bench- marks are restricted to action and sports classes. In this paper, we introduce YouTube-8M 1, a large-scale bench- mark dataset for general multi-label video classification. We treat the task of video classification as that of producing labels that are relevant to a video given its frames. Therefore, unlike Sports-1M and ActivityNet, YouTube-8M is not restricted to action classes alone. For example, Figure 1 shows random video examples for the Guitar entity. We first construct a visual annotation vocabulary from Knowl- edge Graph entities that appear as topic annotations for YouTube videos based on the YouTube video annotation system [2]. To en- sure that our vocabulary consists of entities that are recognizable visually, we use various filtering criteria, including human raters. The entities in the dataset span activities (sports, games, hobbies), objects (autos, food, products), scenes (travel), and events. The 1http:\/\/research.google.com\/youtube8m ar X iv :1 60 9. 08 67 5v 1 [ cs .C V ] 2 7 Se p 20 16 Figure 2: The progression of datasets for image and video understand- ing tasks. Large datasets have played a key role for advances in both areas. entities were selected using a combination of their popularity on YouTube and manual ratings of their visualness according to hu- man raters. They are an attempt to describe the central themes of videos using a few succinct labels. We then collect a sample set of videos for each entity, and use a publicly available state-of-the-art Inception network [4] to extract features from them. Specifically, we decode videos at one frame- per-second and extract the last hidden representation before the classification layer for each frame. We compress the frame-level features and make them available on our website for download. Overall, YouTube-8M contains more than 8 million videos\u2014 over 500,000 hours of video\u2014from 4,800 classes. Figure 2 illus- trates the scale of YouTube-8M, compared to existing image and video datasets. We hope that the unprecedented scale and diversity of this dataset will be a useful resource for developing advanced video understanding and representation learning techniques. Towards this end, we provide extensive experiments comparing several state-of-the-art techniques for video representation learn- ing, including Deep Networks [26], and LSTMs (Long Short-Term Memory Networks) [13] on this dataset. In addition, we show that transfering video feature representations learned on this dataset leads to significant improvements on other benchmarks such as Sports-1M and ActivityNet. In the rest of the paper, we first review existing benchmarks for image and video classification in Section 2. We present the details of our dataset including the collection process and a brief analysis of the categories and videos in Section 3. In Section 4, we review several approaches for the task of multi-label video classification given fixed frame-level features, and evaluate the approaches on the dataset. In Section 5, we show that features and models learned on our large-scale dataset generalize very well on other benchmarks. We offer concluding remarks with Section 6. 2. RELATED WORK Image benchmarks have played a significant role in advancing computer vision algorithms for image understanding. Starting from a number of well labeled small-scale datasets such as Caltech 101\/256 [8, 10], MSRC [32], PASCAL [7], image understanding research has rapidly advanced to utilizing larger datasets such as ImageNet [6] and SUN [38] for the next generation of vision algorithms. Im- ageNet in particular has enabled the development of deep feature learning techniques with millions of parameters such as the AlexNet [20] and Inception [14] architectures due to the number of classes (21841), the diversity of the classes (27 top-level categories) and the millions of labeled images available. A similar effort is in progress in the video understanding do- main where the community has quickly progressed from small, well-labeled datasets such as KTH [22], Hollywood 2 [23], Weiz- mann [5], with a few thousand video clips, to medium-scale datasets such as UCF101 [33], Thumos\u201814 [16] and HMDB51 [21], with more than 50 action categories. Currently, the largest available video benchmarks are the Sports-1M [19], with 487 sports related activities and 1M videos, the YFCC-100M [34], with 800K videos and raw metadata (titles, descriptions, tags) for some of them, the FCVID [17] dataset of 91, 223 videos manually annotated with 239 categories, and ActivityNet [12], with ∼200 human activity classes and a few thousand videos. However, almost all current video benchmarks are restricted to recognizing action and activity categories, and have less than 500 categories. YouTube-8M fills the gap in video benchmarks as follows: \u2022 A large-scale video annotation and representation learn- ing benchmark, reflecting the main themes of a video. \u2022 A significant jump in the number and diversity of annotation classes\u20144800 Knowledge Graph entities vs. less than 500 categories for all other datasets. \u2022 A substantial increase in the number of labeled videos\u2014over 8 million videos, more than 500,000 hours of video. \u2022 Availability of pre-computed state-of-the-art features for 1.9 billion video frames. We hope the pre-computed features will remove computational bar- riers, level the playing field, and enable researchers to explore new technologies in the video domain at an unprecedented scale. 3. YOUTUBE-8M DATASET YouTube-8M is a benchmark dataset for video understanding, where the main task is to determine the key topical themes of a video. We start with YouTube videos since they are a good (albeit noisy) source of knowledge for diverse categories including vari- ous sports, activities, animals, foods, products, tourist attractions, games, and many more. We use the YouTube video annotation system [2] to obtain topic annotations for a video, and to retrieve videos for a given topic. The annotations are provided in the form of Knowledge Graph entities [3] (formerly, Freebase topics [1]). They are associated with each video based on the video\u2019s metadata, context, and content signals [2]. We use Knowledge Graph entities to succinctly describe the main themes of a video. For example, a video of biking on dirt roads and cliffs would have a central topic\/theme of Mountain Biking, not Dirt, Road, Person, Sky, and so on. Therefore, the aim of the dataset is not only to understand what is present in each frame of the video, but also to identify the few key topics that best describe what the video is about. Note that this is different than typical event or scene recognition tasks, where each item belongs to a single event or scene. [38, 28] It is also different than most object recognition tasks, where the goal is to label everything visible in an image. This would produce thousands of labels on each video but without an- swering what the video is really about. The goal of this benchmark is to understand what is in the video and to summarize that into a few key topics. In the following sub-sections, we describe our vo- cabulary and video selection scheme, followed by a brief summary of dataset statistics. Figure 3: A tag-cloud representation of the top 200 entities. Font size is proportional to the number of videos labeled with the entity. Top-level Category 1st Entity 2nd Entity 3rd Entity 4th Entity 5th Entity 6th Entity 7th Entity Arts & Entertainment Concert Animation Music video Dance Guitar Disc jockey Trailer Autos & Vehicles Vehicle Car Motorcycle Bicycle Aircraft Truck Boat Beauty & Fitness Fashion Hair Cosmetics Weight training Hairstyle Nail Mascara Books & Literature Book Harry Potter The Bible Writing Magazine Alice E-book Business & Industrial Train Model aircraft Fish Water Tractor pulling Advertising Landing Computers & Electronics Personal computer Video game console iPhone PlayStation 3 Tablet computer Xbox 360 Microsoft Windows Finance Money Bank Foreign Exchange Euro United States Dollar Credit card Cash Food & Drink Food Cooking Recipe Cake Chocolate Egg Eating Games Video game Minecraft Action-adventure game Strategy video game Sports game Call of Duty Grand Theft Auto V Health Medicine Raw food Ear Glasses Injury Dietary supplement Dental braces Hobbies & Leisure Fishing Outdoor recreation Radio-controlled model Wedding Christmas Hunting Diving Home & Garden Gardening Home improvement House Kitchen Garden Door Swimming pool Internet & Telecom Mobile phone Smartphone Telephone Website Sony Xperia Google Nexus World Wide Web Jobs & Education School University High school Teacher Kindergarten Campus Classroom Law & Government Tank Firefighter President of the U.S.A. Soldier President Police officer Fighter aircraft News Weather Snow Rain News broadcasting Newspaper Mattel Hail People & Society Prayer Family Play-Doh Human Dragon Angel Tarot Pets & Animals Animal Dog Horse Cat Bird Aquarium Puppy Real Estate House Apartment Condominium Dormitory Mansion Skyscraper Loft Reference Vampire Bus River City Mermaid Village Samurai Science Nature Robot Eye Ice Biology Skin Light Shopping Toy LEGO Sledding Doll Shoe My Little Pony Nike; Inc. Sports Motorsport Football Winter sport Cycling Basketball Gymnastics Wrestling Travel Amusement park Hotel Airport Beach Roller coaster Lake Resort Full vocabulary Vehicle Concert Animation Music video Video game Motorsport Football Table 1: Most frequent entities for each of the top-level categories. 3.1 Vocabulary Construction We followed two main tenets when designing the vocabulary for the dataset; namely 1) every label in the dataset should be distin- guishable using visual information alone, and 2) each label should have sufficient number of videos for training models and for com- puting reliable metrics on the test set. For the former, we used a combination of manually curated topics and human ratings to prune the vocabulary into a visual set. For the latter, we considered only entities having at least 200 videos in the dataset. The Knowledge Graph contains millions of topics. Each topic has one or more types, that are curated with high precision. For ex- ample, there is an exhaustive list of animals with type animal and an exhaustive list of foods with type food. To start with our initial vocabulary, we manually selected a whitelist of 25 entity types that we considered visual (e.g. sport, tourist_attraction, inventions), and also blacklisted types that we thought are non-visual (e.g. mu- sic artists, music compositions, album, software). We then obtained all entities that have at least one whitelisted type and no blacklisted types, which resulted in an initial vocabulary of ∼50, 000 entities. Following this, we used human raters in order to manually prune this set into a smaller set of entities that are considered visual with high confidence, and are also recognizable without very deep do- main expertise. Raters were provided with instructions and exam- ples. Each entity was rated by 3 raters and the ratings were av- eraged. Figure 4a shows the main rating question. The process resulted in a total of ∼10, 000 entities that are considered visually recognizable and are not too fine-grained (i.e. can be recognized by non-domain experts after studying some examples). These enti- ties were further pruned: we only kept entities that have more than 200 popular videos, as explained in the next section. The final set of entities in the dataset are fairly balanced in terms of the speci- ficity of the topic they describe, and span both coarse-grained and fine-grained entities, as shown in Figure 4b. 3.2 Collecting Videos Having established the initial target vocabulary, we followed these (a) Screenshot of the question displayed to human raters. (b) Distribution of vocabulary topics in terms of specificity. Figure 4: Rater guidelines to assess how specific and visually recognizable each entity is, on a discrete scale of (1 to 5), where 1 is most visual and easily recognizable by a layperson. Each entity was rated by 3 raters. We kept only entities with a maximum average score of 2.5, and categorized them by specificity, into coarse-grained, medium-grained, and fine-grained entities, using equally sized score range buckets. steps to obtain the videos: \u2022 Collected all videos corresponding to the 10, 000 visual en- tities and have at least 1, 000 views, using the YouTube video annotation system [2]. We excluded too short (< 120 secs) or too long (> 500 secs) videos. \u2022 Randomly sampled 10 million videos among them. \u2022 Obtained all entities for the sampled 10 million videos using the YouTube video annotation system. This completes the annotations. \u2022 Filtered out entities with less than 200 videos, and videos with no remaining entities. This reduced the size of our data to 8, 264, 650 videos. \u2022 Split our videos into 3 partitions, Train : Validate : Test, with ratios 70% : 20% : 10%. We publish features for all splits, but only publish labels for the Train and Validate partitions. 3.3 Features The original size of the video dataset is hundreds of Terabytes, and covers over 500, 000 hours of video. This is impractical to process by most research teams (using a real-time video processing engine, it would take over 50 years to go through the data). There- fore, we pre-process the videos and extract frame-level features us- ing a state-of-the-art deep model: the publicly available Inception network [4] trained on ImageNet [14]. Concretely, we decode each video at 1 frame-per-second up to the first 360 seconds (6 minutes), feed the decoded frames into the Inception network, and fetch the ReLu activation of the last hidden layer, before the classification layer (layer name pool_3\/_reshape). The feature vector is 2048-dimensional per second of video. While this removes mo- tion information from the videos, recent work shows diminishing returns from motion features as the size and diversity of the video data increases [26, 35]. The static frame-level features provide an excellent baseline, and constructing compact and efficient motion features is beyond the scope of this paper. Nonetheless, we hope to extend the dataset with audio and motion features in the future. We cap processing of each video up to the first 360 seconds for storage and computational reasons. For comparison, the average length of videos in UCF-101 is 10 − 15 seconds, Sports-1M is 336 seconds and in this dataset, it is 230 seconds. Dataset Train Validate Test Total YouTube-8M 5,786,881 1,652,167 825,602 8,264,650 Table 2: Dataset partition sizes. Figure 5: Number of videos in log-scale versus entity rank in log scale. Entities were sorted by number of videos. We note that this somewhat follows the natural Zipf distribution. Afterwards, we apply PCA (+ whitening) to reduce feature di- mensions to 1024, followed by quantization (1 byte per coefficient). These two compression techniques reduce the size of the data by a factor of 8. The mean vector and covariance matrix for PCA was computed on all frames from the Train partition. We quantize each 32-bit float into 256 distinct values (8 bits) using optimally com- puted (non-uniform) quantization bin boundaries. We confirmed that the size reduction does not significantly hurt the evaluation metrics. In fact, training all baselines on the full-size data (8 times larger than what we publish), increases all evaluation metrics by less than 1%. Note that while this dataset comes with standard frame-level fea- tures, it leaves a lot of room for investigating video representation learning approaches on top of the fixed frame-level features (see Section 4 for approaches we explored). 3.4 Dataset Statistics The YouTube-8M dataset contains 4, 800 classes and a total of (a) Number of entities in each top-level category. (b) Number of train videos in log-scale per top-level category. Figure 6: Top-level category statistics of the YouTube-8M dataset. 8, 264, 650 videos. A video may be annotated with more than one class and the average number of classes per video is 1.8. Table 2 shows the number of videos for which we are releasing features, across the three datasets. We processed only the first six minutes of each video, at 1 frame- per-second. The average length of a video in the dataset is 229.6 seconds, which amounts to∼1.9 billion frames (and corresponding features) across the dataset. We grouped the 4, 800 entities into 24 top-level categories to measure statistics and illustrate diversity. Although we do not use these categories during training, we are releasing the entity-to-category mapping for completeness. Table 1 shows the top entities per cate- gory. Note that while some categories themselves may not seem vi- sual, most of the entities within them are visual. For instance, Jobs & Education includes universities, classrooms, lectures, etc., and Law & Government includes police, emergency vehicles, military- related entities, which are well represented and visual. Figure 5 shows a log-log scale distribution of entities and videos. Figures 6a and 6b show the size of categories, respectively, in terms of the number of entities and the number of videos. 3.5 Human Rated Test Set The annotations from the YouTube video annotation system can be noisy and incomplete, as they are automatically generated from metadata, anchor text, comments, and user engagement signals [2]. To quantify the noise, we uniformly sampled over 8000 videos from the Test partition, and used 3 human raters per video to ex- haustively rate their labels. We measured the precision and recall of the ground truth labels to be 78.8% and 14.5%, respectively, with respect to the human raters. Note that typical inter-rater agreement on similar annotation tasks with human raters is also around 80% so the precision of these ground truth labels is perhaps compara- ble to (non-expert) human-provided labels. The recall, however, is low, which makes this an excellent test bed for approaches that deal with missing data. We report the accuracy of our models primarily on the (noisy) Validate partition but also show some results on the much smaller human-rated set, showing that some of the metrics are surprisingly similar on the two datasets. While the baselines in section 4 show very promising results, we believe that they can be significantly improved (when evalu- ated on the human-based ground truth), if one explicitly models incorrect [29] (78.8% precision) or missing [40, 25] (14.5% recall) training labels. We believe this is an exciting area of research that this dataset will enable at scale. 4. BASELINE APPROACHES 4.1 Models from Frame Features One of the challenges with this dataset is that we only have video-level ground-truth labels. We do not have any additional information that specifies how the labels are localized within the video, nor their relative prominence in the video, yet we want to in- fer their importance for the full video. In this section, we consider models trained to predict the main themes of the video using the in- put frame-level features. Frame-level models have shown competi- tive performance for video-level tasks in previous work [19, 26]. A video v is given by a sequence of frame-level features xv1:Fv , where xvj is the feature of the j th frame from video v. 4.1.1 Frame-Level Models and Average Pooling Since we do not have frame-level ground-truth, we assign the video-level ground-truth to every frame within that video. More sophisticated formulations based on multiple-instance learning are left for future work. From each video, we sample 20 random frames and associate all frames to the video-level ground-truth. This re- sults in about 120 million frames. For each entity e, we get 120M instances of (xi, yei ) pairs, where xi ∈ R1024 is the inception fea- ture and yei ∈ 0, 1 is the ground-truth associated with entity e for the ith example. We train 4800 independent one-vs-all classifiers for each entity e. We use the online training framework after par- allelizing the work for each entity across multiple workers. During inference, we score every frame in the test video using the models for all classes. Since all our evaluations are based on video-level ground truths, we need to aggregate the frame-level scores (for each entity) to a single video-level score. The frame-level probabili- ties are aggregated to the video-level using a simple average. We choose average instead of max pooling since we want to reduce the effect of outlier detections and capture the prominence of each en- tity in the entire video. In other words, let p(e|x) be the probability of existence of e given the features x. We compute the probability Figure 7: The network architecture of the DBoF approach. Input frame features are first fed into a up-projection layer with shared pa- rameters for all frames. This is followed by a pooling layer that con- verts the frame-level sparse codes into a video-level representation. A few hidden layers and a classification layer provide the final video-level predictions. pv(e|xv1:Fv ) of the entity e associated with the video v as pv(e|xv1:Fv ) = 1 Fv Fv∑ j=1 p(e|xvj ). (1) 4.1.2 Deep Bag of Frame (DBoF) Pooling Inspired by the success of various classic bag of words represen- tations for video classification [23, 36], we next consider a Deep Bag-of-Frames (DBoF) approach. Figure 7 shows the overall ar- chitecture of our DBoF network for video classification. The N - dimensional input frame level features from k randomly selected frames of a video are first fed into a fully connected layer of M units with RELU activations. Typically, with M > N , the input features are projected onto a higher dimensional space. Crucially, the parameters of the fully connected layer are shared across the k input frames. Along with the RELU activation, this leads to a sparse coding of the input features in the M -dimensional space. The obtained sparse codes are fed into a pooling layer that aggre- gates the codes of the k frames into a single fixed-length video rep- resentation. We use max pooling to perform the aggregation. We use a batch normalization layer before pooling to improve stabil- ity and speed-up convergence. The obtained fixed length descriptor of the video can now be classified into the output classes using a Logistic or Softmax layer with additional fully connected layers in between. The M -dimensions of the projection layer could be thought of as M discriminative clusters which can be trained in a single network end to end using backpropagation. The entire network is trained using Stocastic Gradient Descent (SGD) with logistic loss for a logistic layer and cross-entropy loss for a softmax layer. The backpropagated gradients from the top layer train the weight vectors of the projection layer in a discrimina- tive fashion in order to provide a powerful representation of the in- put bag of features. A similar network was proposed in [26] where the convolutional layer outputs are pooled across all the frames of a video to obtain a fixed length descriptor. However, the net- work in [26] does not use an intermediate projection layer which we found to be a crucial difference when learning from input frame features. Note that the up-projection layer into sparse codes is sim- ilar to what Fisher Vectors [27] and VLAD [15] approaches do but the projection (i.e., clustering) is done discriminatively here. We also experimented with Fisher Vectors and VLAD but were not able to obtain competitive results using comparable codebook sizes. Hyperparameters: We considered values of {2048, 4096, 8192} for the number of units in the projection layer of the network and found that larger values lead to better results. We used 8192 for all datasets. We used a single hidden layer with 1024 units between the pooling layer and the final classification layer in all experiments. The network was trained using SGD with AdaGrad, a learning rate of 0.1, and a weight decay penalty of 0.0005. 4.1.3 Long Short-Term Memory (LSTM) We take a similar approach to [26] to utilize LSTMs for video- level prediction. However, unlike that work, we do not have access to the raw video frames. This means that we can only train the LSTM and Softmax layers. We experimented with the number of stacked LSTM layers and the number of hidden units. We empirically found that 2 layers with 1024 units provided the highest performance on the validation set. Similarly to [26], we also employ linearly increasing per-frame weights going from 1\/N to 1 for the last frame. During the training time, the LSTM was unrolled for 60 itera- tions. Therefore, the gradient horizon for LSTM was 60 seconds. We experimented with a larger number of unroll iterations, but that slowed down the training process considerably. In the end, the best model was the one trained for the largest number of steps (rather than the most real time). In order to transfer the learned model to ActivityNet, we used a fully-connected model which uses as inputs the concatenation of the LSTM layers\u2019 outputs as computed at the last frame of the videos in each of these two benchmarks. Unlike traditional trans- fer learning methods, we do not fine-tune the LSTM layers. This approach is more robust to overfitting than traditional methods, which is crucial for obtaining competitive performance on Activ- ityNet due to its size. We did perform full fine-tuning experiments on Sports-1M, which is large enough to fine-tune the entire LSTM model after pre-training. 4.2 Video level representations Instead of training classifiers directly on frame-level features, we also explore extracting a task-independent fixed-length video-level feature vector from the frame-level features xv1:Fv for each video v. There are several benefits of extracting fixed-length video features: 1. Standard classifiers can apply: Since the dimensionality of the representations are fixed across videos, we may train standard classifiers like logistic, SVM, mixture of experts. 2. Compactness: We get a compact representation for the en- tire video, thereby reducing the training data size by a few orders of magnitude. 3. More suitable for domain adaptation: Since the video- level representations are unsupervised (extracted independently of the labels), these representations are far less specialized to the labels associated with the current dataset, and can gener- alize better to new tasks or video domains. Formally, a video-level feature ϕ(xv1:Fv ) is a fixed-length repre- sentation (at the video-level). We explore a simple aggregation technique for getting these video-level representations. We also experimented with Fisher Vectors (FV) [27] and VLAD [15] ap- proaches for task-independent video-level representations but were not able to achieve competitive results for FV or VLAD representa- tions of similar dimensionality. We leave it as future work to come up with compact FV or VLAD type representations that outperform the much simpler approach described below. 4.2.1 First, second order and ordinal statistics From the frame-level features xv1:Fv where x v j ∈ R1024, we ex- tract the mean µv ∈ R1024 and the standard-deviation σv ∈ R1024. Additionally, we also extract the top 5 ordinal statistics for each dimension. Formally, TopK(x v(j)1:Fv ) returns a K dimensional vector where the pth dimension contains the pth highest value of the feature-vector\u2019s jth dimension over the entire video. We denote TopK(x v 1:Fv ) to be aKD dimensional vector obtained by concate- nating the ordinal statistics for each dimension. Thus, the resulting feature-vector ϕ(xv1:Fv ) for the video becomes: ϕ(xv1:Fv ) =  µ(xv1:Fv )σ(xv1:Fv ) TopK(x v 1:Fv )  . (2) 4.2.2 Feature normalization Standardization of features has been proven to help with online learning algorithms[14, 37] as it makes the updates using Stochas- tic Gradient Descent (SGD) based algorithms (like Adagrad) more robust to learning rates, and speeds up convergence. Before training our one-vs-all classifiers on the video-level rep- resentation, we apply global normalization to the feature vectors ϕ(xv1:Fv ) (defined in equation 2). Similar to how we processed the frame features, we substract the mean ϕ(.) then use PCA to decor- relate and whiten the features. The normalized video features are now approximately multivariate gaussian with zero mean and iden- tity covariance. This makes the gradient steps across the various dimensions independent, and learning algorithm gets an unbiased view of each dimension (since the same learning rate is applied to each dimension). Finally, the resulting features are L2 normal- ized. We found that these normalization techniques make our mod- els train faster. 4.3 Models from Video Features Given the video-level representations, we train independent bi- nary classifiers for each label using all the data. Exploiting the structure information between the various labels is left for future work. A key challenge is training these classifiers at the scale of this dataset. Even with a compact video-level representation for the 6M training videos, it is unfeasible to train batch optimization classifiers, like SVM. Instead, we use online learning algorithms, and use Adagrad to perform model updates on the weight vectors given a small mini-batch of examples (each example is associated with a binary ground-truth value). 4.3.1 Logistic Regression Given D dimensional video-level features, the parameters Θ of the logistic regression classifier are the entity specific weights we. During scoring, given x ∈ RD+1 to be the video-level feature of the test example, the probability of the entity e is given as p(e|x) = σ(wTe x). The weights we are obtained by minimizing the total log-loss on the training data given as: λ\u2016we\u201622 + N∑ i=1 L(yi,e, σ(wTe xi)), (3) where σ(.) is the standard logistic, σ(z) = 1\/(1 + exp(−z)). 4.3.2 Hinge Loss Since training batch SVMs on such a large dataset is impossible, we use the online SVM approach. As in the conventional SVM framework, we use ±1 to represent negative and positive labels respectively. Given binary ground-truth labels y (0 or 1), and pre- dicted labels ŷ (positive or negative scalars), the hinge loss is: L(y, ŷ) = max(0, b− (2y − 1)ŷ), (4) where b is the hinge-loss parameter which can be fine-tuned further or set to 1.0. Due to the presence of the max function, there is a discontinuity in the first derivative. This results in the subgradient being used in the updates, slowing convergence significantly. 4.3.3 Mixture of Experts (MoE) Mixture of experts (MoE) was first proposed by Jacobs and Jor- dan [18]. The binary classifier for an entity e is composed of a set of hidden states, or experts, He. A softmax is typically used to model the probability of choosing each expert. Given an ex- pert, we can use a sigmoid to model the existence of the entity. Thus, the final probability for entity e\u2019s existence is p(e|x) =∑ h∈He p(h|x)σ(u T h x), where p(h|x) is a softmax over |He|+ 1 states. In other words, p(h|x) = exp(w T h x) 1+ ∑ h\u2032∈He exp(w T h\u2032x) . The last, (|He| + 1)th, state is a dummy state that always results in the non-existence of the entity. Denote py|x = p(y = 1|x), ph|x = p(h|x) and ph = p(y = 1|x, h). Given a set of training examples (xi, gi)i=1...N for a binary classifier, where xi is the feature vec- tor and gi ∈ [0, 1] is the ground-truth, let L(pi, gi) be the log-loss between the predicted probability and the ground-truth: L(p, g) = −g log p− (1− g) log(1− p). (5) We could directly write the derivative of L [ py|x, g ] with respect to the softmax weight wh and the logistic weight uh as ∂L [ py|x, g ] ∂wh = x ph|x ( py|h,x − py|x ) ( py|x − g ) py|x(1− py|x) , (6) ∂L [ py|x, g ] ∂uh = x ph|xpy|h,x(1− py|h,x) ( py|x − g ) py|x(1− py|x) . (7) We use Adagrad with a learning rate of 1.0 and batch size of 32 to learn the weights. Since we are training independent classifiers for each label, the work is distributed across multiple machines. For MoE models, we experimented with varying number of mix- tures (1, 2, 4), and found that performance increases by 0.5%-1% on all metrics as we go from 1 to 2, and then to 4 mixtures, but the number of model parameters correspondingly increases by 2 or 4 times. We chose 2 mixtures as a good compromise and report numbers with the 2-mixture MoE model for all datasets. 5. EXPERIMENTS In this section, we first provide benchmark baseline results for the above multi-label classification approaches on the YouTube-8M dataset. We then evaluate the usefulness of video representations learned on this dataset for other tasks, such as Sports-1M sports classification and AcitvityNet activity classification. 5.1 Evaluation Metrics Mean Average Precision (mAP): For each entity, we first round the annotation scores in buckets of 10−4 and sort all the non-zero annotations according to the model score. At a given threshold τ , the precision P (τ) and recall R(τ) are given by P (τ) = ∑ t∈T I(yt ≥ τ)gt∑ t∈T I(yt ≥ τ) , (8) R(τ) = ∑ t∈T I(yt ≥ τ)gt∑ t∈T gt , (9) Input Features Modeling Approach mAP Hit@1 PERR Frame-level, {xv1:Fv} Logistic + Average (4.1.1) 11.0 50.8 42.2 Frame-level, {xv1:Fv} Deep Bag of Frames (4.1.2) 26.9 62.7 55.1 Frame-level, {xv1:Fv} LSTM (4.1.3) 26.6 64.5 57.3 Video-level, µ Hinge loss (4.3) 17.0 56.3 47.9 Video-level, µ Logistic Regression (4.3) 28.1 60.5 53.0 Video-level, µ Mixture-of-2-Experts (4.3) 29.6 62.3 54.9 Video-level, [µ;σ; Top5] Mixture-of-2-Experts (4.3) 30.0 63.3 55.8 Table 3: Results of the various benchmark baselines on the YouTube- 8M dataset. We find that binary classifiers on simple video-level rep- resentations perform substantially better than frame-level approaches. Deep learning methods such as DBoF and LSTMs do not provide a substantial boost over traditional dense feature aggregation methods because the underlying frame-level features are already very strong. Approach Hit@1 PERR Hit@5 Deep Bag of Frames (DBoF) (4.1.2) 68.6 29.0 83.5 LSTM (4.1.3) 69.1 30.5 84.7 Mixture-of-2-Experts ([µ;σ; Top5]) (4.3) 70.1 29.1 84.8 Table 4: Results of the three best approaches on the human rated test set of the YouTube-8M dataset. A comparison with the results on the validation set (Table 3) shows that the relative strengths of the different approaches are largely preserved on both sets. where I(.) is the indicator function. The average precision, ap- proximating the area under the precision-recall curve, can then be computed as AP = 10000∑ j=1 P (τj)[R(τj)−R(τj+1)], (10) where where τj = j10000 . The mean average precision is computed as the unweighted mean of all the per-class average precisions. Hit@k: This is the fraction of test samples that contain at least one of the ground truth labels in the top k predictions. If rankv,e is the rank of entity e on video v (with the best scoring entity having rank 1), andGv is the set of ground-truth entities for v, then Hit@k can be written as: 1 |V | ∑ v∈V ∨e∈Gv I(rankv,e ≤ k), (11) where ∨ is logical OR. Precision at equal recall rate (PERR): We measure the video- level annotation precision when we retrieve the same number of entities per video as there are in the ground-truth. With the same notation as for Hit@k, PERR can be written as: 1 |V : |Gv| > 0| ∑ v∈V :|Gv|>0 [ 1 |Gv| ∑ e∈Gv I(rankv,e ≤ |Gv|) ] . 5.2 Results on YouTube-8M Table 3 shows results for all approaches on the YouTube-8M dataset. Frame-level models (row 1), trained on the strong Incep- tion features and logistic regression, followed by simple averaging of predictions across all frames, perform poorly on this dataset. This shows that the video-level prediction task cannot be reduced to simple frame-level classification. Aggregating the frame-level features at the video-level using sim- ple mean pooling of frame-level features, followed by a hinge loss or logistic regression model, provides a non-trivial improvement in video level accuracies over naive averaging of the frame-level predictions. Further improvements are observed by using mixture- of-experts models and by adding other statistics, like the standard deviation and ordinal features, computed over the frame-level fea- tures. Note that the standard deviation and ordinal statistics are more meaningful in the original RELU activation space so we re- construct the RELU features from the PCA-ed and quantized fea- tures by inverting the quantization and the PCA using the provided PCA matrix, computing the collection statistics over the recon- structed frame-level RELU features, and then re-applying PCA, whitening, and L2 normalization as described in Section 4.2.2. This simple task-independent feature pooling and normalization strategy yields some of the most competitive results on this dataset. Finally, we also evaluate two deep network architectures that have produced state-of-art results on previous benchmarks [26]. The DBoF architecture ignores sequence information and treats the input video as a bag of frames whereas LSTMs use state informa- tion to preserve the video sequence. The DBoF approach with a logistic classification layer produces 2% (absolute) gains in Hit@1 and PERR metrics over using simple mean feature pooling and a single-layer logistic model, which shows the benefits of discrim- intatively training a projection layer to obtain a task-specific video- level representation. The mAP results for DBoF are slightly worse than mean pooling + logistic model, which we attribute to slower training and convergence of DBoF on rare classes (mAP is strongly affected by results on rare classes and the joint class training of DBoF is a disadvantage for those classes). The LSTM network generally performs best, except for mAP, where the 1-vs-all binary MoE classifiers perform better, likely for the same reasons of slower convergence on rare classes. LSTM does improve on Hit@1 and PERR metrics, as expected given its ability to learn long-term correlations in the time domain. Also, in [26], the authors used data augmentation by sampling multi- ple snippets of fixed length from a video and averaged the results, which could produce even better accuracies than our current results. We also considered Fisher vectors and VLAD given their recent success in aggregating CNN features at the video-level in [39]. However, for the same dimensionality as the video-level represen- tations of the LSTM, DBoF and mean features, they did not pro- duce competitive results. 5.2.1 Human Rated Test Set We also report results on the human rated test set of over 8000 videos (see Section 3.5) in Table 4 for the top three approaches. We report PERR, Hit@1, and Hit@5, since the mAP is not reliable given the size of the test set. The Hit@1 numbers are uniformly higher for all approaches when compared to the incomplete valida- tion set in Table 3 whereas the PERR numbers are uniformly lower. This is largely attributable to the missing labels in the validation set (recall of the Validation set labels is around 15% compared to ex- haustive human ratings). However, the relative ordering of the var- ious approaches is fairly consistent between the two sets, showing that the validation set results are still reliable enough to compare different approaches. 5.3 Results on Sports-1M Next, we investigate generalization of the video-level features learned using the YouTube-8M dataset and perform transfer learn- ing experiments on the Sports-1M dataset. The Sports-1M dataset [19] consists of 487 sports activities with 1.2 million YouTube videos and is one of the largest benchmarks available for sports\/activity recognition. We use the first 360 seconds of a video sampled at 1 frame per second for all experiments. To evaluate transfer learning on this dataset, in one experiment we simply use the aggregated video-level descriptors, based on the PCA matrix learned on the YouTube-8M dataset, and train MoE or Approach mAP Hit@1 Hit@5 Logistic Regression (µ) (4.3) 58.0 60.1 79.6 Mixture-of-2-Experts (µ) (4.3) 59.1 61.5 80.4 Mixture-of-2-Experts ([µ;σ; Top5]) (4.2.1) 61.3 63.2 82.6 LSTM (4.1.3) 66.7 64.9 85.6 +Pretrained on YT-8M (4.1.3) 67.6 65.7 86.2 Hierarchical 3D Convolutions [19] - 61.0 80.0 Stacked 3D Convolutions [35] - 61.0 85.0 LSTM with Optical Flow and Pixels [26] - 73.0 91.0 (a) Sports-1M: Our learned features are competitive on this dataset beating all but the approach of [26], which learned directly from the video pixels. Both [26] and [35] included motion features. Approach mAP Hit@1 Hit@5 Mixture-of-2-Experts (µ) (4.3) 69.1 68.7 85.4 +Pretrained PCA on YT-8M 74.1 72.5 89.3 Mixture-of-2-Experts ([µ;σ; Top5]) (4.2.1) NO 74.2 72.3 89.6 +Pretrained PCA on YT-8M 77.6 74.9 91.6 LSTM (4.1.3) 57.9 63.4 81.0 +Pretrained on YT-8M (4.1.3) 75.6 74.2 92.4 Ma, Bargal et al.[24] 53.8 - - Heilbron et al.[12] 43.0 - - (b) ActivityNet: Since the dataset is small, we see a substantial boost in performance by pre-training on YouTube-8M or using the transfer learnt PCA versus the one learnt from scratch on ActivityNet. Table 5: Results of transferring video representations learned on the YouTube-8M dataset to the (a) Sports-1M and (b) ActivityNet. logistic models on top using target domain training data. For the LSTM networks, we have two scenarios: 1) we use the PCA transformed features and learn a LSTM model from scratch using these features; or 2) we use the LSTM layers pre-trained on the YouTube-8M task, and fine-tune them on the Sports-1M dataset (along with a new softmax classifier). Table 5a shows the evaluation metrics for the various video-level representations on the Sports-1M dataset. Our learned features are competitive on this dataset, with the best approach beating all but the approach of [26], which learned directly from the pixels of the videos in the Sports-1M dataset, including optical flow, and made use of data augmentation strategies and multiple inferences over several video segments. We also show that even on such a large dataset (1M videos), pre-training on YouTube-8M still helps, and improves the LSTM performance by ∼1% on all metrics (vs. no pre-training). 5.4 Results on ActivityNet Our final set of experiments demonstrate the generality of our learned features for the ActivityNet untrimmed video classification task. Similar to Sports-1M experiments, we compare directly train- ing on the ActivityNet dataset against pre-training on YouTube-8M for aggregation based and LSTM approaches. As seen in Table 5b, all of the transferred features are much better in terms of all metrics than training on ActivityNet alone. Notably, without the use of mo- tion information, our best feature is better by up to 80% than the HOG, HOF, MBH, FC-6, FC-7 features used in [12]. This result shows that features learned on YouTube-8M generalize very well to other datasets\/tasks. We believe this is because of the diversity and scale of the videos present in YouTube-8M. 6. CONCLUSIONS In this paper, we introduce YouTube-8M, a large-scale video benchmark for video classification and representation learning. With YouTube-8M, our goal is to advance the field of video understand- ing, similarly to what large-scale image datasets have done for im- age understanding. Specifically, we address the two main chal- lenges with large-scale video understanding\u2014(1) collecting a large labeled video dataset, with reasonable quality labels, and (2) re- moving computational barriers by pre-processing the dataset and providing state-of-the-art frame-level features to build from. We process over 50 years worth of video, and provide features for nearly 2 billion frames from more than 8 million videos, which enables training a reasonable model at this scale within 1 day, us- ing an open source framework on a single machine! We expect this dataset to level the playing field for academia researchers, bridge the gap with large-scale labeled video datasets, and significantly accelerate research on video understanding. We hope this dataset will prove to be a test bed for developing novel video representation learning algorithms, and especially approaches that deal effectively with noisy or incomplete labels. As a side effect, we also provide one of the largest and most diverse public visual annotation vocabularies (consisting of 4800 visual Knowledge Graph entities), constructed from popularity sig- nals on YouTube as well as manual curation, and organized into 24 top-level categories. We provide extensive experiments comparing several strong base- lines for video representation learning, including Deep Networks and LSTMs, on this dataset. We demonstrate the efficacy of using a fairly unexplored class of models (mixture-of-experts) and show that they can outperform popular classifiers like logistic regression and SVMs. This is particularly true for our large dataset where many classes can be multi-modal. We explore various video-level representations using simple statistics extracted from the frame- level features and model the probability of an entity given the ag- gregated vector as an MoE. We show that this yields competitive performance compared to more complex approaches (that directly use frame-level information) such as LSTM and DBoF. This also demonstrates that if the underlying frame-level features are strong, the need for more sophisticated video-level modeling techniques is reduced. Finally, we illustrate the usefulness of the dataset by perform- ing transfer learning experiments on existing video benchmarks\u2014 Sports-1M and ActivityNet. Our experiments show that features learned on this dataset generalize well on these benchmarks, in- cluding setting a new state-of-the-art on ActivityNet. 7. REFERENCES [1] Freebase: A community-curated database of well-known people, places, and things. https:\/\/www.freebase.com. [2] Google I\/O 2013 - semantic video annotations in the Youtube Topics API: Theory and applications. https:\/\/www.youtube.com\/watch?v=wf_77z1H-vQ. [3] Knowledge Graph Search API. https:\/\/developers.google.com\/knowledge-graph\/. [4] Tensorflow: Image recognition. https:\/\/www.tensorflow.org\/tutorials\/image_recognition. [5] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. In Proceedings of the International Conference on Computer Vision (ICCV), 2005. [6] J. Deng, W. Dong, R. Socher, L. jia Li, K. Li, and L. Fei-fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009. [7] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge, 2009. https:\/\/www.freebase.com https:\/\/www.youtube.com\/watch?v=wf_77z1H-vQ https:\/\/developers.google.com\/knowledge-graph\/ https:\/\/www.tensorflow.org\/tutorials\/image_recognition [8] L. Fei-fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28, 2006. [9] R. Girshick. Fast R-CNN. In Proceedings of the International Conference on Computer Vision (ICCV), 2015. [10] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. [11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs\/1512.03385, 2015. [12] F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961\u2013970, 2015. [13] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computing, 9(8), Nov. 1997. [14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning (ICML), pages 448\u2013456, 2015. [15] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes. IEEE Trans. Pattern Anal. Mach. Intell., 34(9), Sept. 2012. [16] Y. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar. THUMOS challenge: Action recognition with a large number of classes. http:\/\/crcv.ucf.edu\/THUMOS14, 2014. [17] Y.-G. Jiang, Z. Wu, J. Wang, X. Xue, and S.-F. Chang. Exploiting feature and class relationships in video categorization with regularized deep neural networks. arXiv preprint arXiv:1502.07209, 2015. [18] M. I. Jordan. Hierarchical mixtures of experts and the em algorithm. Neural Computation, 6, 1994. [19] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1725\u20131732, Columbus, Ohio, USA, 2014. [20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1097\u20131105, 2012. [21] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: a large video database for human motion recognition. In Proceedings of the International Conference on Computer Vision (ICCV), 2011. [22] I. Laptev and T. Lindeberg. Space-time interest points. In Proceedings of the International Conference on Computer Vision (ICCV), 2003. [23] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. [24] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff. Do less and achieve more: Training cnns for action recognition utilizing action images from the web. CoRR, abs\/1512.07155, 2015. [25] V. Mnih and G. Hinton. Learning to label aerial images from noisy data. In Proceedings of the 29th Annual International Conference on Machine Learning (ICML), June 2012. [26] J. Y.-H. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snippets: Deep networks for video classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4694\u20134702, 2015. [27] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007. [28] A. Quattoni and A. Torralba. Recognizing indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. [29] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. ArXiv e-prints, Dec. 2014. [30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. [31] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR). [32] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost: Joint appearance, shape and context modeling for multi-class object. In Proceedings of the European Conference on Computer Vision (ECCV), 2006. [33] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. In CRCV-TR-12-01, 2012. [34] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L. Li. The new data and new challenges in multimedia research. CoRR, abs\/1503.01817, 2015. [35] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and M. Paluri. C3D: generic features for video analysis. CoRR, abs\/1412.0767, 2014. [36] H. Wang, M. M. Ullah, A. Kläser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recognition. In Proc. BMVC, 2009. [37] S. Wiesler, A. Richard, R. Schlüter, and H. Ney. Mean-normalized stochastic gradient for large-scale deep learning. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, pages 180\u2013184. IEEE, 2014. [38] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, A. Oliva, and J. Xiao. Sun database: Exploring a large collection of scene categories, 2013. [39] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn video representation for event detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [40] H.-F. Yu, P. Jain, P. Kar, and I. Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of The 31st International Conference on Machine Learning (ICML), pages 593\u2013601, 2014. [41] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR, abs\/1311.2901, 2013. http:\/\/crcv.ucf.edu\/THUMOS14 ","flair":"null\tnull"}
{"author":"hack777","created":"Mon Nov 07 19:39:18 EST 2016","text":"Goodfellow et al.(https:\/\/arxiv.org\/abs\/1412.6572) say that neural networks can be fooled by adversarial examples because of their highly linear nature.\nI thought neural nets were highly non-linear classifiers. Can someone shed light on what I'm missing here?","flair":"one\tDiscussion"}
{"author":"erogol","created":"Fri Nov 18 05:58:51 EST 2016","text":"Suppose I have a trained CNN and a new set of data . I like to update this model with this new data. What are the suggested ways to do this?\n\nCurrently I train the whole network with the whole data (old data + new data) with lower learning rate and cannot see any increase on the performance. Beside, this is not a scalable way when we consider large scale datasets since the whole data gets bigger and bigger as so the training time. ","flair":"one\tDiscussion"}
{"author":"Pieranha","created":"Sun Oct 16 20:11:44 EDT 2016","text":"I want to encode 1-5 sentences at a time using an approach similar to word2vec, but for paragraphs. I have 2 questions in this regard:\n\n1. Which method is the preferred approach for encoding such paragraphs? I've found Paragraph Vectors (http:\/\/www.jmlr.org\/proceedings\/papers\/v32\/le14.pdf), but the paper is a bit old in this very fast-moving field. Also, I'm particularly interested in capturing word order with the embedding, which may favor a specific approach. I'm not aware of the subtleties of the different nethods.\n\n2. I want to compare my embeddings of paragraphs. Suppose I have an embedding of a paragraph such as Paragraph Vector or an averaged word2vec. Would it still make sense to use cosine similarity between the embeddings like for word2vec?\n\nThanks!","flair":"one\tDiscussion"}
{"author":"ddcarnage","created":"Wed Oct 19 13:35:17 EDT 2016","text":"A picture is worth a thousand words. Everyone has heard that phrase, and as Wikipedia aptly puts it, it \u201Crefers to the notion that a complex idea can be conveyed with just a single still image\u201D. Think of real estate pictures. Everyone has their own taste and thinks some properties are nicer than others. But although we each know what we like in terms of style, colors and materials, it\u2019s sometimes hard to clearly communicate that to someone else. Taking it a bit further, it would be even harder to quickly encode what we like in an algorithm, so it could predict whether or not we would like a property based on its pictures. Imagine, if we had such a system, we could automatically go through real estate listings, filter them by our hard criterias like price and location, and then order them by the likelihood that we will find them pretty. This is the type of problem that DeepTeach tackles, and as you\u2019ll see, it\u2019s is a great example of how machine learning can be used for human augmentation, which means making humans more efficient by assisting them with artificial intelligence. As the Youtube demo video above shows, DeepTeach is a web application developed on MLDB that allows a user to teach a machine learning model what types of images he or she is looking for through an iterative process. The user starts with a random sample of unlabelled images, labels a few by dragging them in the positive label zone (called Target in the UI) or the negative label zone (Not Target in the UI), and then trains a model that learns what each group looks like and suggests new images for each label. The user can then approve or correct the model\u2019s predictions, and redo the procedure. Using this workflow, we\u2019re able to train a binary image classifier from unlabelled data in about a minute. The same idea naturally extends to multi-class classification. Instead of having only two groups of images like in DeepTeach, there could be many more. For example, imagine that we have an image dataset containing 5000 unlabelled images of 5 different breeds of dogs. The user could start to label images by dragging them in one of five different boxes, one for each dog breed. Since it\u2019s an iterative process, the model would quickly start to figure out what each breed looks like, and would suggest unlabelled images for each of them, just like in DeepTeach. The user can then approve or correct the suggestions, and would then get better and better suggestions. We say it\u2019s human augmentation because the machine learning works in tandem with the user and makes him more productive. The user only needs to manually label a few examples before the model understands the difference between each class. For the dog breed example above, maybe only about 50 or 100 images would need to be manually looked at in total before the model learns how to label the remaining 4900. But since we initially started with unlabelled data, the model wouldn\u2019t have been able to function without the initial user input, except to do some unsupervised learning which might not have given the results the user wanted. The human augmentation workflow that DeepTeach implements has countless useful applications. The machine learning concepts behind DeepTeach are straightforward but powerful. We use a pretrained deep convolutional neural network (ConvNet) using MLDB\u2019s TensorFlow integration to embed pictures in a conceptual space, giving us a very powerful representation of the images. The ConvNet we use is the Inception-v3 model that was trained on ImageNet. We then train a random forest using the output of the ConvNet as features and the user\u2019s input as labels. Training a random forest is lightning fast on MLDB, which allows DeepTeach to be used interactively. Using what one model learned and using it in a second model is called transfer learning, while the overall strategy of having a model query the user to clarify labels iteratively is called active learning. For more explanations on the techniques used here, I encourage you to check out our transfer learning demo notebook, or the KDNuggets guest post I wrote on this subject and that shows how to do a lot of this using MLDB. The rest of this post assumes you are roughly familiar with those concepts and will focus on what is specific to DeepTeach. The appeal of this plugin is that it allows you to build an image classifier quickly, meaning you don\u2019t have to invest a lot of time, having to look at a lot of examples. The flip side of this is that it\u2019s very easy to create a model that will overfit and perform poorly when using it in the wild on new examples. Below I explain what I did to help make sure the model generalizes to unseen data. The first thing I did was control how many input dimensions the classifier was allowed to use. Remember that we\u2019re using a ConvNet, Inception-v3 to be precise, as our feature generator. This means that when we run an image through it and use the second to last layer, it gets transformed into a 2048 dimensional vector. To help with generalization, I played a bit with the number of random features (the configuration key) that the random forest was allowed to use at each step. The default value used in DeepTeach is 10%, which is a lot lower than what I usually use. My intuition behind why this makes sense is the ConvNet features are extremely good, and so with only a handful of examples, the random forest was able to get an almost perfect accuracy with only a few decisions. By removing most of the features, it forces the model to use more indirect features that have a better chance of working with images that are a bit different to those in the positive examples training set. We\u2019re essentially making the training task harder for the model. If you think about the workflow a user goes through when using the plugin, there are at first no examples specified for either the positive or negative labels. We can expect the user to drag a few in both labels, meaning we\u2019ll have maybe 10 examples at most for our first training. That\u2019s not a lot of examples. I did two things to improve this. For positive examples, I used the embedding.neighbors function to quickly retrieve the list of the 10 closest images in the 2048 dimensional space we get from running images though Inception for each positive examples, and added them as positive to the training set at a reduced training weight. Giving an example a reduced weight means the classifier will spend less effort trying to get it right, but it still counts a bit. The weight of an example is used when calculating the error function of a classifier. We can get the neighbors of an image quickly because the images\u2019 representation are stored in a dataset of type embedding that stores an index in a Vantage Point Tree, making nearest neighbour searches very efficient. This is where the concept of similarity search ties in. For the negatives, I simply added all the remaining examples as negatives with a very low weight. This exposes the model to all the variety that exists in the dataset. It doesn\u2019t cause a problem to add everything as negative for two reasons. First, since those examples are set to have a very low training weight, you can think of image that really should be positive as noise, and machine learning excels at working with noisy data. Second, if some images are clearly positive, they\u2019ll pop up as bad suggestions in the UI and the user will be able to make the correction. We benefit from having a human augmentation workflow where the user and the AI work together. We have presented DeepTeach, an MLDB plugin that implements a human augmentation workflow allowing a user to quickly build an image classifier from unlabelled data within minutes. The same concepts apply to any type of data (text, sound, etc) and it really shows how humans can be made more efficient by orders of magnitude when integrating artificial intelligence in their workflow. This plugin uses many of MLDB\u2019s unique features, such as its TensorFlow integration, its ability to host entire web applications and its lightning fast random forest training. You can try DeepTeach by launching a free hosted instance of MLDB, or running it on your own machine. Once you\u2019re in MLDB, check the instructions on DeepTeach\u2019s repo. If you have any questions or comments, come and chat with us on Gitter!","flair":"null\tnull"}
{"author":"fhuszar","created":"Fri Nov 11 10:53:54 EST 2016","text":"I want to highlight and recommend an arXiv paper I've read a few months ago: This paper is refreshingly well written, it was pure delight to read. Therefore, I won't summarise it here, it would be robbing you of the experience of reading a much better exposition. So please go and read the original, the rest of this post are just my thoughts connecting it to representation learning. Many people who are interested in unsupervised learning today primarily hope to use it to learn representations that are useful to solve certain tasks later. For example, classify images with fewer labels. But I think most researchers would also agree that we don't have all the answers about how to do this properly. How should we formulate the goal of representation learning in a way that can be used as an objective for learning? Let me put this in the context of Marr's three levels of analysis, shown in the figure below. Advances in deep learning gave us very powerful new tools for learning rich representations, especially for raw sensory signals like images. But this mainly represents progress at the algorithmic and implementation level. I believe that we still desperately need new insights at the computational level, especially when it comes to unsupervised learning. The information bottleneck approach is a beautifully elegant approach to representation learning: specifically for deciding which aspects of observed data are relevant information, and what aspects can be thrown away. It does this by balancing two criteria (formally expressed in Equation 8 of the paper): I think this is what representation learning should be about at a high level. This framework encapsulates what should be the computational goal of representation learning. sidenote: In fact, for a truly general (but arguaby less practical) framework, I personally prefer use a more general notion of information expressed terms of Bayes risks, see e.g. this paper or section 1, Eqn. 1.16 and subsection 1.3.7 of my thesis. But this is a story for another day. The elephant in the room is, of course, that this pretty much assumes a supervised learning setting. It is assumes we know what the behaviourally relevant variables are and how they are related to observed data, or at least we have to have data to learn or approximate the joint distribution $p(x,y)$ between observed and relevant variables. The question remains how could one possibly do something like this in the unsupervised, or at least semi-supervised setting. I'm now going to look at two popular unsupervised learning techniques and try to understand how they behave in the context of learning representations. Before I go any further, I just want to say that I'm aware that none of the stuff I write about here is new, or particularly insightful, but I think it's still important to talk about. Maximum (marginal) likelihood looks at the marginal distribution of observed data $p(x)$, and tries to build a model $q(x)$ that approximates this in the $\\operatorname{KL}[p\\|q]$ sense. The representations we then use are often extracted from the conditional distributions of some hidden variables $y$ conditioned on observations $q(y|x)$. An important fact to notice is that the same marginal model $q(x)$ can be represented as the marginal of an infinite number of joint distributions $q(x,y) = q(y|x)q(x)$. You can represent a Gaussian $q(x)$ as just a Gaussian, without hidden variables, as the end-point of a Brownian motion, or even as the output of some weird nonlinear neural network with some noise fed in at the top. Therefore, unless we have some other assumptions, the likelihood alone can't possibly tell apart these different representations. This is not meant to be a crusade against maximum likelihood, the same criticism applies to other unsupervised criteria, for example, autoencoders or denoising autoencoders (DAE). DAE learns about the data distribution $p(x)$, and it will build representations that are useful to solve the denoising task. It is related to the information bottleneck in that it solves the same trade-off between compression and retaining information. However, instead of retaining information about behaviourally relevant variables, it tries to retain information about the data $x$ itself. This is really the key limitation, as it cannot (without further assumptions) tell which aspects of the data are behaviourally relevant and which aren't. It has to assume that everything is equally relevant. To summarise, a key limitation of using unsupervised criteria for representation learning is the following: So my view is that unsupervised representation learning is really hopeless without either: Slow feature analysis (SFA) is a really neat example of how priors help: SFA learns useful representations by assuming that behaviourally relevant variables change slowly over time. Another often used assumptions are sparsity and independence, the history of the former was summarised in Shakir's latest blog post. At the same time, Ladder Networks use a smart architecture and objective (a form of a prior) that allows them to solve the semi-supervised learning problem very data-efficiently (as in, using only a few labelled but lots of unlabelled examples). Through this lens, reinforcement learning can also be seen as a way of incorporating information about what is behaviourally relevant and what is not. In summary, to learn behaviorally useful representations in a fully unsupervised way, we need priors and assumptions about the representation itself. Sometimes these priors are encoded in the architecture of neural networks, sometimes they can be incorporated more explicitly as in SFA, ICA or sparse coding. If you ask me, the brain probably has a mixed strategy: it probably has strong priors\/assumptions encoded in its architecture or learning algorithm. In fact, the slowness principle behind SFA has been pretty successful as a normative model for explaining representations found in the brain: grid cells, place cells, head-direction cells, simple cells, complex cells, etc. Here is an overview. But I would be surprised if the learned representations in a fully unsupervised way. It probably employs data-efficient forms of reinforcement learning and semi-supervised learning to inform the representations built by lower-level perception about what is behaviourally relevant and what is not. One key takeaway, as always, is that the way you train your models should represent the way you want to use them later. We are often not clear about these goals, and the current terminology pretty much washes together everything on representation learning, generative models and unsupervised learning. Most ML papers today focus on algorithmic, or implementation level insights and ideas. When you read these papers, particularly in unsupervised learning, you can always try and retrofit a computational level understanding, and figure out how you could use it to solve one problem or another.","flair":"three\tResearch"}
{"author":"RaionTategami","created":"Tue Oct 25 00:22:58 EDT 2016","text":"EDIT: *I AM NOT GOING TO DO THIS!*\nThanks for the feedback everyone, the votes are in and it's a unanimous NO. I'm glad I asked, I personally would have found thing interesting but I'm clearly in the minority.\n\n----------------------------\n\nThis is partly to get the mod's feedback since I can imagine just doing this randomly could get me in to trouble. This could be pretty cool but also annoying as hell if done incorrectly. Please take a look at the questions I want to discuss below.\n\nSo this is not an original idea and I have to imagine someone has possible already done this and not told anyone. You take this:\n[A Neural Conversational Model](https:\/\/arxiv.org\/pdf\/1506.05869.pdf)\nand throw it at this:\n[i_have_every_publicly_available_reddit_comment](https:\/\/www.reddit.com\/r\/datasets\/comments\/3bxlg7\/i_have_every_publicly_available_reddit_comment\/) and let to loose on Reddit!\n\nI have an initial implementation up and running and hope to improve it using some recent techniques. E.g. I think [HYPERNETWORKS](https:\/\/arxiv.org\/pdf\/1609.09106v1.pdf) are the current state of the art in language modelling.\n\nPerhaps I should have seen this coming but...\n\n**The main problem is that version 1 is a hateful, bigoted, sexist, condescending, potty-mouthed, meme spewing monster!**\n\nHere are some *choice* outputs from my most recently trained model (these are in the form ACTUAL_REDDIT_COMMENT -&gt; RNN_GENERATED_REPLY)\n\n*Responses of the RNN do not necessarily reflect the opinions of the author*\n\n    \"Well, that's just, like, your opinion man. --&gt; Fap fap fap fap fap fap fap fap fap fap fap fap.\" \n    RU\/18 --&gt; I am trolling the most of the baby.\n    It got gonorrhea. --&gt; gay as a ball of shit.\n    things will turd around eventually --&gt; I never get reddit, I was there a lot.\n    Step 2: moisture incurs. --&gt; You should think about your penis.\n    What a jerk. Those people deserved to live. --&gt; It's what plants crave.\n    Her brother is obviously agent 47. --&gt; You gotta go full retard.\n    REDDIT IS NOT 4CHAN. --&gt; WE'LL DO IT LIVE!\n    That's less people than voted for American Idol. --&gt; A panda inside of the motherfucker.\n    --&gt; You say that like it's spelled *explainer*.\n    *throws tomato* --&gt; **NOOOOOO!!!!**\n\nI think my next step is to learn some subreddit embeddings as features so it can maybe learn that we don't swear in \/r\/MachineLearning as much?! \n\nSo questions:\n\n* Should the bot generally make it obvious that it is a bot? I initially wanted to just release it anonymously and see if it could get some positive karma. Kinda a Reddit-truing test.\n* When and how often should it respond to things? My plan was to have it randomly pick a comment very rarely (~every 2 hours?). Respond to comments that respond to it and have a way to cause it to respond by having it's username in your comment for example.\n* Is this even a good idea of should I just release it on \/r\/circlejerk, looks like it'd fit right in :)\n\nThoughts?\n\n----------------------------\nSome more examples just for fun:\n\n    Happy Birthday! --&gt; a penis comment.  It's a palind of the round.\n    Mohammad, Buddha and Ganesha all concur. --&gt; A trap\n    *fap fap fap* --&gt; **crickets**.\n    Ah yes, transparent aluminum. --&gt; I was your penis that I will not be a super funny.\n    I didn't know vaginas could do the dishes. --&gt; asked for the greatest girl.\n    HE'S A *NAGGER!* --&gt; ... was it an asshole?\n    I am a peacock and I can confirm this. --&gt; I fucking listen to the leader of all.\n    Is that the ship in BF BC2?? --&gt; hahahahahahahahahahahahahahahahahahahahahahahahaha\n    i'm naked right now --&gt; interesting.\n    Great. Now I have to masterbate. --&gt; if you're a robot dick there.\n    where's dad?      son? --&gt; **SPOILER ALERT!**\n    My gaydar was dinging when I saw that priest. --&gt; ... they do ass.\n    Why did you leave the little piano? --&gt; in the best part of the sexual porn.\n    Nikon always --&gt; How do you know what else was \"Gray and single?\"\n    Figurative big, brass balls! --&gt; Dental Plan!\n    Oh yeah. Sure. --&gt; Oh god that was going to be a meme.\n    Nickelback. --&gt; Nice try, mathematic teacher.\n    Can you cite a source for that? --&gt; Why do you also have an epic abortion in a plane?\n\nImplementation details for those interested:\n\nData munging:\n\n* Filter out all comments below 20 karma (fat lot of use that was), comments longer than 50, deleted comments etc.\n* Also had to filter out 'Robert Paulson', there are entire threads full of that guy and the model would say 'Robert Paulson' all the time.\n* of the remaining comments paired up comment -&gt; reply pairs from threads to train on, no other features yet.\n\n\nModel:\n\n* Vanilla character seq2seq LSTM.\n* RMSProp\n* LEARNING_RATE = 0.01\n* SEQ_MAX_LEN = 50\n* RNN_HIDDEN_SIZE = 1024\n* LAYERS = 3\n* CHAR_EMB_SIZE = 128\n* VOCAB_SIZE = 95\n* Trained overnight, got nowhere near through a singe epoch.","flair":"one\tDiscussion"}
{"author":"jaromiru","created":"Fri Oct 21 08:00:44 EDT 2016","text":" ヤロミル about AI Skip to content Let\u2019s make a DQN: Full DQN October 21, 2016DQNヤロミル This article is part of series Let\u2019s make a DQN. 1. Theory 2. Implementation 3. Debugging 4. Full DQN 5. Double DQN and Prioritized experience replay Introduction Up until now we implemented a simple Q-network based agent, which suffered from instability issues. In this article we will address these problems with two techniques \u2013 target network and error clipping. After implementing these, we will have a fully fledged DQN, as specified by the original paper1. Target Network During training of our algorithm we set targets for gradient descend as: We see that the target depends on the current network. A neural network works as a whole, and so each update to a point in the Q function also influences whole area around that point. And the points of Q(s, a) and Q(s\u2019, a) are very close together, because each sample describes a transition from s to s\u2019. This leads to a problem that with each update, the target is likely to shift. As a cat chasing its own tale, the network sets itself its targets and follows them. As you can imagine, this can lead to instabilities, oscillations or divergence. To overcome this problem, researches proposed to use a separate target network for setting the targets. This network is a mere copy of the previous network, but frozen in time. It provides stable values and allows the algorithm to converge to the specified target: After severals steps, the target network is updated, just by copying the weights from the current network. To be effective, the interval between updates has to be large enough to leave enough time for the original network to converge. A drawback is that it substantially slows down the learning process. Any change in the Q function is propagated only after the target network update. The intervals between updated are usually in order of thousands of steps, so this can really slow things down. On the graph below we can see an effect of a target network in MountainCar problem. The values are plotted every 1000 steps and target network was updated every 10000 steps. We can see the stair-shaped line, where the network converged to provided target values. Each drop corresponds to an update of the target network. The network more or less converged to true values after 250 000 steps. Let\u2019s look at a graph of a simple Q-network from last article (note that tracked Q function points are different): Here the algorithm converged faster, after only 100 000 steps. But the shape of the graph is very different. In the simple Q-network variant, the network went fast to the minimum of -100 and started picking the final transitions to reach true values after a while. The algorithm with target network on the other hand quickly anchored the final transitions to their true values and slowly propagated this true value further to the network. Another direct comparison can be done on the CartPole problem, where the differences are clear: The version with target network smoothly aim for the true value whereas the simple Q-network shows some oscillations and difficulties. Although sacrificing speed of learning, this added stability allows the algorithm to learn correct behaviour in much complicated environments, such as those described in the original paper1 \u2013 playing Atari games receiving only visual input. Error clipping In the gradient descend algorithm, usually Mean Squared Error (MSE) loss function is used. It is defined as: where and are targets and predicted values in i-th sample. For each sample there is the error term . This error value can be huge for a sample which is not in alignment with the current network prediction. The loss function is directly used in the backward propagation algorithm and large errors cause large changes to the network. By choosing a different loss function, we can smooth these changes. In the original paper1, clipping of the derivative of MSE to [-1 1] is proposed. This effectively means that MSE is used for errors in the [-1 1] region and Mean Absolute Error (MAE) outside. MAE is defined as: The differences between the two functions are shown in the following image: There\u2019s actually a different way of describing such a loss function, in a single quotation. It\u2019s called Huber loss and is defined as and looks like this: It behaves as in [-1 1] region and as outside, which is almost the same as the error clipping described above. This loss function is prefect for our purposes \u2013 it is fully differentiable and it\u2019s one line of code. However, the error clipping technique have its drawbacks. It it important to realize that in the back-propagation algorithm, its derivative is actually used. Outside [-1 1] region, the derivative is either -1 or 1 and therefore all errors outside this region will get fixed slowly and at the same constant rate. In some settings this can cause problems. For example in the CartPole environment, the combination of simple Q-network and Huber loss actually systematically caused the network to diverge. Implementation You can download a demonstration of DQN on the CartPole problem from github. The only changes against the old versions are that the Brain class now contains two networks model and model_ and we use the target network in the replay() function to get the targets. Also, the initialization with random agent is now used. Let\u2019s look at the performance. In the following graph, you can see smoothed reward, along with the estimated Q value in one point. Values were plotted every 1000 steps. We see that the Q function estimate is much more stable than in the simple Q-learning case. The reward tend to be stable too, but at around step 2 200 000, the reward reached an astronomical value of 69628.0, after which the performance dropped suddenly. I assume this is caused by limited memory and experience bias. During this long episode, almost whole memory was filled with highly biased experience, because the agent was holding the pole upright, never experiencing any other state. The gradient descend optimized the network using only this experience and while doing so destroyed the previously learned Q function as a whole. Conclusion This article explained the concepts of target network and error clipping, which together with Q-learning form full DQN. We saw how these concepts make learning more stable and offered a reference implementation. Next time we will look into further improvements, by using Double learning and Prioritized Experience Replay. About the author: Jaromír Janisch has graduated in 2012 as a Software Engineer from University of West Bohemia. Until 2016 he worked in several software companies, mainly as a developer. He\u2019s been always passionate about progress in artificial intelligence and decided to join the field as a researcher. He is currently preparing to start his Ph.D. studies. In this blog he strives to extract main points from sometimes complicated scientific research, reformulate them in his own words and make it more accessible for people outside the field. Mnih et al. \u2013 Human-level control through deep reinforcement learning, Nature 518, 2015 ↩ ↩ ↩ Share this article: Share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Click to share on Twitter (Opens in new window) Post navigation ← Let\u2019s make a DQN: Debugging Let\u2019s make a DQN: Double Learning and Prioritized Experience Replay → Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (Address never made public) Name Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. ","flair":"four\tProject"}
{"author":"black_knight25","created":"Sun Nov 27 20:30:22 EST 2016","text":"I've tried various of different CNN models, but my validation accuracy never seems to go above 50%. My training accuracy can go up to 90%, but I can never get the validation accuracy to go above 50%.\n\nIt should output 1 of dozen different image types. I tried creating my own models but the results were pretty bad. Later I tried many different pre-built models through TFLearn such as Alexnet, MNIST ConvNet, and CIFAR-10 ConvNet. These examples are all available on [TFLearn GitHub](https:\/\/github.com\/tflearn\/tflearn\/tree\/master\/examples). All the models I tried caused the validation accuracy to hover around 50%. I'm using 20% of my training data for validation. Any tips on how to design a model so that it doesn't overfit and to allow the validation accuracy to increase?","flair":"one\tDiscussion"}
{"author":"tsendsuren","created":"Fri Oct 21 09:41:34 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.06454v1 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1610 Change to browse by: cs cs.AI cs.NE stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: Reasoning with Memory Augmented Neural Networks for Language Comprehension Authors: Tsendsuren Munkhdalai, Hong Yu (Submitted on 20 Oct 2016) Abstract: Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. Comments: initial submission: 9 pages Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) Cite as: arXiv:1610.06454 [cs.CL]   (or arXiv:1610.06454v1 [cs.CL] for this version) Submission history From: Tsendsuren Munkhdalai [view email] [v1] Thu, 20 Oct 2016 15:17:04 GMT (110kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"alxndrkalinin","created":"Tue Nov 22 13:11:20 EST 2016","text":"This repo contains all the code required to replicate the paper: The limitations of the current system are outlined at the end of this doc. Please read it before even considering integrating our code. Note: This is not an official Google product. We use a subset of the NTU 3D Model Database models (http:\/\/3d.csie.ntu.edu.tw\/~dynamic\/database\/). Please download the model files: Next we use the binvox library (http:\/\/www.patrickmin.com\/binvox\/) to create voxelized representations of the NTU models. Download the executable for your platform and put the executable file in . Then run our script: OPTIONAL: You can view the output by using the viewvox utility (http:\/\/www.patrickmin.com\/viewvox\/). Put the viewvox executable in the directory, then: The first step is to download the custom manta fork. Next, you must build mantaflow using the cmake system. For the above cmake command setting will slow down simulation but you can view the flow fields. You will now have a binary called manta in the build directory. Install matlabnoise (https:\/\/github.com\/jonathantompson\/matlabnoise) to the SAME path that FluidNet is in. i.e. the directory structure should be: Now you're ready to generate the training data. Make sure the directory exists. For the 3D training data run: We assume that Torch7 is installed, otherwise follow the instructions here. We use the standard distro with the cuda SDK and cudnn. Note: there may be other libraries we use, so if our torch script fails the first place to look is , and make sure you have all the mandatory libraries. First compile tfluids (this is our custom CUDA & C++ library that implements a large number of the modules used in the paper): All training related code is in directory. To train a model on 3D data: This will pull data from the directory and dump the model to . To train a 2D model: At any point during the training sim you can plot test and training set loss values using the Matlab script . You can control any model or training config parameters from the command line. If you need to define nested variables the syntax is: i.e nested variables are separated. You can print a list of possible config variables using: Note: the first time the data is loaded from the manta output, it is cached to the torch\/data\/ directory. So if you need to reload new data (because you altered the dataset) then delete the cache files (). Warning: We use a 12GB card for training (Titan X). The 32 batch size used on the 3D model requires only 4.5GB during training but at startup libcudnn allocates huge temporary tensors during the first FPROP. We have seen OS crashes using cudnn when trying to allocate too much memory (i.e. when the batch size is too large). This is a cudnn \/ driver bug. For 2D models only! To run the interactive demo firstly compile LuaGL: The command line output will print a list of possible key and mouse strokes. To render the videos you will need to install Blender, but to just create the volumetric data no further tools are needed. First run our 3D example script (after training a 3D model): To control which scene is loaded, use the . This will dump a large amount of volumetric data to the file`. Now that the fluid simulation has run, you can render the frames in Blender. Note that rendering takes a few hours, while the 3D simulation is fast (with a lot of time spent dumping the results to disk). An implementation of a real-time 3D fluid render is outside the scope of this work. In addition, self-advection of the velocity field is currently carried out on the CPU and so is the slowest part of our simulator (a CUDA implementation is future work). For the mushroom cloud render, open . Next we need to re-attach the data file (because blender caches full file paths which will now be wrong). Click on the \"Smoke\" object in the \"Outliner\" window (default top right). Click on the \"Texture\" icon in the \"Properties\" window (default bottom right), it's the one that looks like a textured Square. Scroll down to \"Voxel Data\" -> \"Source Path:\" and click the file icon. Point the file path to . Next, click either the file menu \"Render\" -> \"Render Image\", or \"Render Animation\". By default the render output goes to . You can also scrub through the frame index on the time-line window (default bottom) to click a frame you want then render just that frame. The above instructions also apply to the bunny and arch examples. Note: you might need to re-center the model depending on your version of binvox (older versions of binvox placed the voxelided model in a different location). If this is the case, then click on the \"GEOM\" object in the \"Outliner\" window. Click the eye and camera icons (so they are no longer greyed out). Then press \"Shift-Z\" to turn on the geometry render preview. Now that you can see the geometry and model, you can manually align the two so they overlap. While this codebase is relatively self-contained and full-featured, it is not a \"ready-to-ship\" fluid simluator. Rather it is a proof of concept and research platform only. If you are interested in integrating our network into an existing system feel free to reach out (tompson@google.com) and we will do our best to answer your questions. The entire simulation loop is not optimized; we do not consider it fast enough for real-time applications (and we do not claim this in the paper). This is largely because we have not ported our advection code to the GPU, but also because we have not fully optimized all the components (the ConvNet pressure solve is highly optimized and we quote it's runtime in the paper). The advection code is currently the bottleneck of our system. However, please keep in mind that the primary contribution of this work is a fast approximation to the pressure solve step. This step is profiled during startup. The entire 3D simulation step takes about 80ms, which is not terrible, but we do not consider this fast enough to claim it is real-time. It would be trivial to implement advection on the GPU and reduce this runtime, but we did not have time. Our example boundary condition code is very rudimentary. It isn't a limitation of our system, rather that we have not implemented anything more sophisticated. For now, we use a Tensor mask to set pixels occupied (as geometry) or fluid. The grid boundary is assumed to be an empty region. We also have a tensor mask for pressure, velocity and density to set field values constant (this allows us to set in-flow density or velocity regions). We do not have a real-time 3D fluid render. We use an offline render instead. For our 2D \"renderer\", we simply display the RGB density field to screen and visualize the velocity vectors. It is very rudimentary. The only external forces that are supported are vorticity confinement and buoyancy. Viscosity and gravity are not supported (but could be added easily). Geometry is assumed to be static (i.e. not moving). Again, this is not a limitation of our approach, we just haven't implemented non-static geometry. This is not necessarily a limitation, but our velocity update does not match the update from mantaflow. There are some cases where manta does not properly calculate the FD of pressure near geometry boundaries. We have unit tests (including FD gradient checks) for all custom torch modules. However we do not have unit tests for everything in tfluids (some parts are tested, but not all). The two main test scripts we do have are: You should run these first if you ever get into trouble training or running the model.","flair":"four\tProject"}
{"author":"Dogsindahouse1","created":"Fri Oct 07 15:42:18 EDT 2016","text":"We have implemented the FRBNY DSGE model in a free and open-source language called Julia . The code is posted here on GitHub , a public repository hosting service. This effort is the result of a collaboration between New York Fed staff and folks from the QuantEcon project, whose aim is to coordinate development of high performance open-source code for quantitative economic modeling. You may wonder why we wrote our code, which was originally in MATLAB and made available here , in Julia. MATLAB is a widely used, mature programming language that has served our purposes very well for many years. However, Julia has two main advantages from our perspective. First, as free software, Julia is more accessible to users from academic institutions or organizations without the resources for purchasing a license. Now anyone, from Kathmandu to Timbuktu, can run our code at no cost. Second, as the models that we use for forecasting and policy analysis grow more complicated, we need a language that can perform computations at a high speed. Julia boasts performance as fast as that of languages like C or Fortran, and is still simple to learn. (Read this post , written by the creators of the language, to understand why Julia fits the bill.) We want to address hard questions with our models\u2014from understanding financial markets developments to modeling households\u2019 heterogeneity\u2014and we can do so only if we are close to the frontier of programming. We tested our code and found that the model estimation is about ten times faster with Julia than before, a very large improvement. Our ports (computer lingo for \u201Ctranslations\u201D) of certain algorithms, such as Chris Sims\u2019s gensys (which computes the model solution), also ran about six times faster in Julia than the MATLAB versions we had previously used. (These results should not be interpreted as a broad assessment of the speed of Julia relative to MATLAB, as they apply only to the code we have written.) This document written by the New York Fed and QuantEcon collaborators, who did the real work on the port, documents the speed improvements and offers an overview of the hurdles encountered in the translation of a large codebase from one language to another. We hope it will be of use to other central banks and researchers embarking on similar projects. We posted our code on GitHub because it is a natural home for open-source projects like ours. Anyone can easily download the code and\u2014most importantly from our point of view\u2014offer suggestions on how to improve it by posting enhanced versions and extensions of our routines. This release also provides an opportunity for the research community to experiment with an open-source, large-scale dynamic stochastic general equilibrium (DSGE) model that is actively used in a research and policy setting. The point of collaborative programming goes beyond the joy of sharing; it is a form of \u201Cmodel validation.\u201D We constantly test the accuracy of our code (and the process of translation into Julia led to yet another line-by-line examination), but we also believe that the best way of making sure that the code is accurate is by letting the rest of the world be our reviewers. If there are inaccuracies or inefficiencies, somebody will find them. If there is a way to make the code faster, somebody will suggest it. Finally, we want to thank our friends from QuantEcon Spencer Lyon , and Pablo Winant , who worked elbow to elbow on the code with our staff, as well as John Stachurski , who made the collaboration possible. We are not done with this project: what we posted was just the DSGE model estimation part of the code, and a lot more is yet to come. Check out our code, and stay tuned! The views expressed in this post are those of the authors and do not necessarily reflect the position of the Federal Reserve Bank of New York or the Federal Reserve System. Any errors or omissions are the responsibility of the authors. This post reflects the experience of the authors with Julia and MATLAB and does not represent an endorsement by the Federal Reserve Bank of New York or the Federal Reserve System of any particular product or service. Marco Del Negro is an assistant vice president in the Federal Reserve Bank of New York\u2019s Research and Statistics Group. Marc Giannoni is an assistant vice president in the Bank\u2019s Research and Statistics Group. Pearl Li is a undergraduate at the Wharton School of the University of Pennsylvania, and was a summer intern in the Group during the summer of 2015. Erica Moszkowski is a research analyst in the Group. Micah Smith is a senior research analyst in the Group.","flair":"null\tnull"}
{"author":"BafflesSean","created":"Sat Nov 12 14:29:02 EST 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose The Future Of Artificial Intelligence || Demis Hassabis - DeepMind Founder A.I. SubscribeSubscribedUnsubscribe Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 4,529 views 100 Like this video? Sign in to make your opinion count. Sign in 101 3 Don't like this video? Sign in to make your opinion count. Sign in 4 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 12, 2016 The Future Of Artificial Intelligence - Demis Hassabis - DeepMind Founder Click here to subscribe to A.I. : http:\/\/bit.ly\/29YHS5r Watch Other Videos : Current State Of Artificial Intelligence | World Economic Forum | Davos 2016 : https:\/\/www.youtube.com\/watch?v=exMZb... Stephen Hawking's Thoughts On Artificial Intelligence : https:\/\/www.youtube.com\/watch?v=xzi3n... Category Science & Technology License Standard YouTube License Show more Show less Loading... Advertisement Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Fujitsu Forum 2016 - Artificial Intelligence - Duration: 58:03. Fujitsu Forum 3,145 views 58:03 Artificial Intelligence And The Future | Demis Hassabis - DeepMind Founder - Duration: 48:23. A.I. 816 views New 48:23 How Neural Networks Actually Work || Geoffrey Hinton - Google's A.I. Chief - Duration: 21:12. A.I. 1,819 views New 21:12 Google s Deep Mind Explained! Self Learning A I - Duration: 13:45. THE F_YOU_TURE 178 views 13:45 The Future Of Artificial Intelligence | Prof. Fei Fei Li - Duration: 17:42. A.I. 663 views New 17:42 DealBook 2016: Artificial Intelligence - Duration: 32:58. The New York Times Conferences 8,122 views 32:58 New research about Artificial Intelligence And Robotics - A.I Documentary 2016 HD - Duration: 46:02. Documentary Documentary 1,457 views 46:02 Stuart Russell and Ya-Qin Zhang - Artificial Intelligence (AI) Hits the Mainstream - Duration: 55:09. Tech.Science.Society 205 views New 55:09 Current State Of Artificial Intelligence | World Economic Forum | Davos 2016 - Duration: 55:33. A.I. 921 views 55:33 Demis Hassabis How Deep Learning Can Give Birth to General Artificial Intelligence - Duration: 50:45. Scott Theil 290 views 50:45 Google's Deep Mind Explained! - Self Learning A.I. - Duration: 13:45. ColdFusion 775,585 views 13:45 Science Documentary 2016 | Big Data - Duration: 52:23. Science Documentaries Channel 91,751 views 52:23 S9 Ep.12 - The Amazing Progress in Artificial Intelligence Part 1- TechTalk With Solomon - Duration: 29:02. TechTalkWithSolomon 1,440 views 29:02 Artificial Intelligence and the future | AndrÃ© LeBlanc | TEDxMoncton - Duration: 10:07. TEDx Talks 193,810 views 10:07 Unfriendly Artificial Intelligence || Kurzweil Interviews Minsky - Duration: 2:44. A.I. 510 views New 2:44 Deep Neural Network Learns To Paint Like Picasso - Duration: 4:20. A.I. 436 views New 4:20 Davos 2016 - The State of Artificial Intelligence - Duration: 55:09. World Economic Forum 187,931 views 55:09 Microsoft's Artificial Intelligence Meltdown - Duration: 6:24. Journeyman Pictures 150,562 views 6:24 Udacity Talks | Yann LeCun | Director of AI Research, Facebook - Duration: 31:39. Udacity 4,920 views 31:39 Original Roots Of Artificial Intelligence | Cognitive Science | Neuroscience || Minsky & Chomsky - Duration: 2:12:39. Futurology 1,035 views 2:12:39 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"one\tDiscussion"}
{"author":"gabrielgoh","created":"Tue Nov 22 00:07:26 EST 2016","text":" Image-to-Image Translation with Conditional Adversarial Nets Phillip Isola Jun-Yan Zhu Tinghui Zhou Alexei A. Efros [Paper] [GitHub] Example results on several image-to-image translation problems. In each case we use the same architecture and objective, simply training on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either. Try our code  [GitHub] Paper Download [7MB] [Bibtex] Experiments Here we show comprehensive results from each experiment in our paper. Please see the paper for details on these experiments. Effect of the objective Cityscapes Facades Effect of the generator architecture Cityscapes Effect of the discriminator patch scale Cityscapes Facades Additional results Map to aerial Aerial to map Semantic segmentation Day to night Edges to handbags Edges to shoes Sketches to handbags Sketches to shoes Recent Related Work Generative adversarial networks have been vigorously explored in the last two years, and many conditional variants have been proposed. Please see the discussion of related work in our paper. Below we point out two papers that especially influenced this work: the original GAN paper from Goodfellow et al., and the DCGAN framework, from which our code is derived. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks NIPS, 2014. [PDF] Alec Radford, Luke Metz, Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks ICLR, 2016. [PDF] Acknowledgements We thank Richard Zhang and Deepak Pathak for helpful discussions. This work was supported in part by NSF SMA-1514512, NGA NURI, IARPA via Air Force Research Laboratory, Intel Corp, and hardware donations by nVIDIA. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL or the U.S. Government. ","flair":"three\tResearch"}
{"author":"Mandrathax","created":"Mon Nov 14 09:07:56 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.03530 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Understanding deep learning requires rethinking generalization Authors: Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals (Submitted on 10 Nov 2016) Abstract: Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. Subjects: Learning (cs.LG) Cite as: arXiv:1611.03530 [cs.LG]   (or arXiv:1611.03530v1 [cs.LG] for this version) Submission history From: Chiyuan Zhang [view email] [v1] Thu, 10 Nov 2016 22:02:36 GMT (296kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Dark_Element75","created":"Sun Oct 16 15:55:01 EDT 2016","text":" Skip to content Dark Element Artificial Intelligence & Machine Learning Menu Home About Contact Bayesian Optimization of Black Box Functions \u2013 Part 1 October 10, 2016October 16, 2016 Skip the explanation and go directly to the Github code. Foreword My goal here is to provide complete and overarching explanations and implementations of algorithms useful in Machine Learning and Artificial Intelligence Research and Development, but if you don\u2019t care about understanding it, or already understand it, then you can view my (hopefully) well-commented code on the Github page. With that said, let\u2019s begin! What are Black Box Functions? (Credit to Wikipedia) Black box functions are very much like this picture shows, no different than a normal function, except we don\u2019t know what the function is. So for instance, if we had a function like this: Then we can look at it and easily know that it\u2019s simply going to raise any input to the power 2, for any inputs. However, if this were a black box function: We have no idea what the operation(s) performed on the input(s) are, and therefore the function is a black box function. What is Black Box Optimization? Optimization of black box functions requires knowing the difference between normal functions and black box ones. Now that we know that, we can move on. Generally in optimization, you want to find the global maximum, or global minimum (however often times a local maximum\/minimum will do just fine). For instance, if we are a seller of Halloween Prop Skeletons, and we know that our sales relative to the number of skeletons produced is modeled by the function Which looks like: Really simple, right? For two-dimensional known functions, we can actually find the global maximum or minimum with complete certainty, and with this function, anyone can look at this graph and see that the company will maximize their profits by producing 420 skeletons. Sidenote: Optimizing known functions is not as easy when it comes to functions with multiple inputs, where the dimension is greater than two (e.g. three-dimensional graphs), and uses a method known as gradient descent. However, I won\u2019t be covering that in this post. Black Box Optimization is a problem no matter how many dimensions there are however, and Bayesian Black Box Optimization works regardless of dimension. Black Box Optimization would be when (more realistically) the company doesn\u2019t know an exact function for what their profit will be, so they plug in 100 values from 370-470, and end up with something like this: This time, we don\u2019t have a known function, according to our definition of black box functions. So we could try and get a function to represent this, but the end goal is to find the maximum or minimum of the function, so we instead just go for that. Unfortunately, we can\u2019t just look at this and conclude the best number of skeletons to produce is 420, not with complete certainty. For an example of why we can\u2019t have complete certainty, let\u2019s take an example function: As shown earlier. So we start by plugging in some values, and we then get some outputs, since that seems to be the easiest way to do things: Hey, this looks just like our function from earlier! After all, we can look at this and see that it matches the behavior perfectly, with the points we\u2019ve tested. But just to be sure, let\u2019s plug in a few more, to be sure! Uh-oh, suddenly our idea that the function was has been blown out of the water. But if we had stopped testing points after our first four proved our hypothesis correct, we would have naively continued, with no idea we were completely incorrect. One of the key problems with black box optimization is that we can never know with 100% certainty what our function\u2019s best input is; to do so would mean testing an often-infinite number of input values. This means we have to draw the line somewhere, since we can\u2019t test an infinite number. If we had decided that our arbitrary number of evaluations was four, and we picked these points, we would have been unaware of the true nature of this function (which by the way is actually this): (Yes, I fooled you, but it was for your own good) Another of the main reasons black box optimization is so tricky is the amount of time it can take to evaluate an input. If our skeleton manufacturer could only get the amount of profits once every Halloween, it would take us 100 years to get the graph we now have, at which point your body would likely be just a skeleton. These examples may seem strange, but they illustrate the pivotal problems: Key Problems of Black Box Optimization: 1. Cost may be high \u2013 it may take a long time for every evaluation 2. Number of inputs to test may be high \u2013 there may be an enormous amount of possible inputs for our function. Sidenote: Our second pivotal problem is often times magnified by the number of dimensions. For instance, our company may have the number of skeletons produced, number of jack o lanterns produced, and number of bags of candy corn produced as inputs, with one axis for profit. If we only had three options and it took milliseconds to test each one, we likely wouldn\u2019t even call it black box optimization, because the problem of black box optimization is only really prevalent when one of them is costly, e.g. long time to test and low input number could still be a problem, low time to test but large input number could also still be a problem; as well as if they are both prevalent.   Conventional Methods of Black Box Optimization 1. Hand \u2013 Tuning This is simply going through and choosing our next input based on the last result or past results, done by the choice of the person tuning it. 2. Grid Search What we did earlier with the skeleton manufacturer example is actually the method of black box optimization known as grid search. In our 2D example, we would represent the inputs we tested as: Or, in our  example: With this example it\u2019s not obvious why it would be called grid search, after all it\u2019s just a row. But when we have multiple dimensions for inputs (as is often the case in black box optimization) , such as with: And when we get all the possible configurations of inputs, it becomes like this: As you can see, this looks much more like a grid, which is where the search type gets its name. So grid search is when we generate all the possible combinations of inputs, given a level of detail. Level of detail \u2013 How precise we want our measurement, in the case of all numbers 1 \u2013 10, if we had our level of detail = 10, we\u2019d get 10 elements total (1, 2, 3, 4.., 10). We could have a lower number (e.g. 2 = 1, 10), or a higher number (e.g. 100 = 0.1, 0.2, 0.3, \u2026, 10.0).  It\u2019s quite possibly the simplest form of black box optimization, other than just hand-tuning. However, there are a few inherent problems with it: Problems with Grid Search 1. The search space quickly expands \u2013 The size of our \u201Cgrid\u201D is equal to getting the product of all our independent number of inputs. e.g. If we have a level of detail of 100, and have five different inputs, we end up with: . That\u2019s 10 billion different configurations, and it\u2019s not an unrealistic scenario. I have this exact situation with one of my reinforcement learning bots, and I\u2019d like to have more inputs or higher level of detail. 2. It\u2019s not intuitive \u2013 Since our search method has no predictive ability, we aren\u2019t gaining any knowledge of the problem. In order to do so, we are just picking the configuration that gave the best results. 3. Random Search  This is very similar to grid search, except instead of searching through every combination in a grid, we randomly choose from each of our domains to make a random combination of inputs, test these, and repeat the number of times we specify. This sounds crazy, but often times it works really well, because of the idea that some of our parameters have higher impact on the result than other parameters, which is almost always the case. Here\u2019s a diagram showing this exact case and why Random Search often does better than grid search in such problems (Credit to James Bergstra and Yoshua Bengio\u2019s paper on the topic) This is actually quite nice, but since it randomly searches(as a non-deterministic algorithm) we can\u2019t run the same program and get similar results every time. While random search does a really good job most of the time, I personally don\u2019t like it. The reason for my dislike is that I\u2019d prefer a method that would give the equivalent results of a good random search run, and not have such a massive amount of randomness in it. We\u2019d like one where we will get more or less the same results every run (a deterministic algorithm). Thankfully, there are many such algorithms that achieve this, albeit at the cost of being much more complicated than hand-tuning, grid search, or random search. For now, I will be covering Bayesian Optimization in part two of this post series. PART TWO Share this: Google Email Print Like this: Like Loading... Permalink. Post navigation Bayesian Optimization of Black Box Functions \u2013 Part 2  Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Search for: Follow Blog via Email Enter your email address to follow this blog and receive notifications of new posts by email. Join 2 other followers Top Posts & Pages Bayesian Optimization of Black Box Functions - Part 1 Bayesian Optimization of Black Box Functions - Appendix and Sources \/ Resources Recent Posts Bayesian Optimization of Black Box Functions \u2013 Appendix and Sources \/ Resources Bayesian Optimization of Black Box Functions \u2013 Part 3 Bayesian Optimization of Black Box Functions \u2013 Part 2 Bayesian Optimization of Black Box Functions \u2013 Part 1 Archives October 2016 RSS RSS - Posts RSS - Comments Recent Posts Bayesian Optimization of Black Box Functions \u2013 Appendix and Sources \/ Resources October 14, 2016 Bayesian Optimization of Black Box Functions \u2013 Part 3 October 13, 2016 Bayesian Optimization of Black Box Functions \u2013 Part 2 October 11, 2016 Bayesian Optimization of Black Box Functions \u2013 Part 1 October 10, 2016 Twitter Facebook Google+ GitHub WordPress.com Powered by WordPress.com. Send to Email Address Your Name Your Email Address Cancel Post was not sent - check your email addresses! Email check failed, please try again Sorry, your blog cannot share posts by email. %d bloggers like this: ","flair":"four\tProject"}
{"author":"alxndrkalinin","created":"Tue Nov 22 13:16:49 EST 2016","text":" Google Research Blog The latest news from Research at Google Zero-Shot Translation with Google\u2019s Multilingual Neural Machine Translation System Tuesday, November 22, 2016 Posted by Mike Schuster (Google Brain Team), Melvin Johnson (Google Translate) and Nikhil Thorat (Google Brain Team) In the last 10 years, Google Translate has grown from supporting just a few languages to 103, translating over 140 billion words every day. To make this possible, we needed to build and maintain many different systems in order to translate between any two languages, incurring significant computational cost. With neural networks reforming many fields, we were convinced we could raise the translation quality further, but doing so would mean rethinking the technology behind Google Translate. In September, we announced that Google Translate is switching to a new system called Google Neural Machine Translation (GNMT), an end-to-end learning framework that learns from millions of examples, and provided significant improvements in translation quality. However, while switching to GNMT improved the quality for the languages we tested it on, scaling up to all the 103 supported languages presented a significant challenge. In \u201CGoogle\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\u201D, we address this challenge by extending our previous GNMT system, allowing for a single system to translate between multiple languages. Our proposed architecture requires no change in the base GNMT system, but instead uses an additional \u201Ctoken\u201D at the beginning of the input sentence to specify the required target language to translate to. In addition to improving translation quality, our method also enables \u201CZero-Shot Translation\u201D \u2014 translation between language pairs never seen explicitly by the system. Here\u2019s how it works. Let\u2019s say we train a multilingual system with Japanese⇄English and Korean⇄English examples, shown by the solid blue lines in the animation. Our multilingual system, with the same size as a single GNMT system, shares its parameters to translate between these four different language pairs. This sharing enables the system to transfer the \u201Ctranslation knowledge\u201D from one language pair to the others. This transfer learning and the need to translate between multiple languages forces the system to better use its modeling power. This inspired us to ask the following question: Can we translate between a language pair which the system has never seen before? An example of this would be translations between Korean and Japanese where Korean⇄Japanese examples were not shown to the system. Impressively, the answer is yes \u2014 it can generate reasonable Korean⇄Japanese translations, even though it has never been taught to do so. We call this \u201Czero-shot\u201D translation, shown by the yellow dotted lines in the animation. To the best of our knowledge, this is the first time this type of transfer learning has worked in Machine Translation. The success of the zero-shot translation raises another important question: Is the system learning a common representation in which sentences with the same meaning are represented in similar ways regardless of language \u2014 i.e. an \u201Cinterlingua\u201D? Using a 3-dimensional representation of internal network data, we were able to take a peek into the system as it translates a set of sentences between all possible pairs of the Japanese, Korean, and English languages. Part (a) from the figure above shows an overall geometry of these translations. The points in this view are colored by the meaning; a sentence translated from English to Korean with the same meaning as a sentence translated from Japanese to English share the same color. From this view we can see distinct groupings of points, each with their own color. Part (b) zooms in to one of the groups, and part (c) colors by the source language. Within a single group, we see a sentence with the same meaning but from three different languages. This means the network must be encoding something about the semantics of the sentence rather than simply memorizing phrase-to-phrase translations. We interpret this as a sign of existence of an interlingua in the network. We show many more results and analyses in our paper, and hope that its findings are not only interesting for machine learning or machine translation researchers but also to linguists and others who are interested in how multiple languages can be processed by machines using a single system. Finally, the described Multilingual Google Neural Machine Translation system is running in production today for all Google Translate users. Multilingual systems are currently used to serve 10 of the recently launched 16 language pairs, resulting in improved quality and a simplified production architecture. Google Labels: Google Brain , Google Translate , Machine Learning , Machine Translation , TensorFlow    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"four\tProject"}
{"author":"funspace","created":"Wed Oct 05 15:49:08 EDT 2016","text":" Exploration Potential Jan Leike Future of Humanity Institute University of Oxford November 21, 2016 Abstract We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem\u2019s reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation. 1 Introduction Good exploration strategies are currently a major obstacle for reinforcement learning (RL). The state of the art in deep RL (Mnih et al., 2015, 2016) relies on ε-greedy policies: in every time step, the agent takes a random action with some probability. Yet ε-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game \u2018Montezuma\u2019s Revenge\u2019): it just takes too long until the agent randomwalks into the first reward. More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al., 2016). In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010). While the methods above require the agent to have a model of its environment and formalize the strategy \u2018explore by going to where the model has high uncertainty,\u2019 there are also model-free strategies like the automatic discovery of options proposed by Machado and Bowling (2016). However, none of these explicit exploration strategies take the problem\u2019s reward structure into account. Intuitively, we want to explore more in parts of the environment where the 1 ar X iv :1 60 9. 04 99 4v 3 [ cs .L G ] 1 8 N ov 2 01 6 reward is high and less where it is low. This is readily exposed in optimistic policies like UCRL (Jaksch et al., 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration\/exploitation tradeoff explicitly. In this paper, we propose exploration potential, a quantity that measures reward- directed exploration. We consider model-based reinforcement learning in partially or fully observable domains. Informally, exploration potential is the Bayes-expected absolute deviation of the value of optimal policies. Exploration potential is similar to information gain about the environment, but explicitly takes the problem\u2019s reward structure into account. We show that this leads to a exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across an environment class): a reinforcement learning agent learns to act optimal in the limit if and only if the exploration potential converges to 0. As such, exploration potential captures the essence of what it means to \u2018explore the right amount\u2019. Another exploration quantity that is both necessary and sufficient for asymptotic optimality is information gain about the optimal policy (Russo and Van Roy, 2014; Reddy et al., 2016). In contrast to exploration potential, it is not measured on the scale of rewards, making an explicit value-of-information tradeoff more difficult. For example, consider a 3-armed Gaussian bandit problem with means 0.6, 0.5, and −1. The information content is identical in every arm. Hence an exploration strategy based on maximizing information gain about the environment would query the third arm, which is easily identifiable as suboptimal, too frequently (linearly versus logarithmically). This arm provides information, but this information is not very useful for solving the reinforcement learning task. In contrast, an exploration potential based exploration strategy concentrates its exploration on the first two arms. 2 Preliminaries and Notation A reinforcement learning agent interacts with an environment in cycles: at time step t the agent chooses an action at and receives a percept et = (ot, rt) consisting of an observation ot and a reward rt ∈ [0, 1]; the cycle then repeats for t+ 1. We use æ<t to denote a history of length t− 1. With abuse of notation, we treat histories both as outcomes and as random variables. A policy is a function mapping a history æ<t and an action a to the probability π(a | æ<t) of taking action a after seeing history æ<t. An environment is a function mapping a history æ1:t to the probability ν(et | æ<tat) of generating percept et after this history æ<tat. A policy π and an environment ν generate a probability measure νπ over infinite histories, the expectation over this measure is denoted with 2 Eπν . The value of a policy π in an environment ν given history æ<t is defined as V πν (æ<t) := (1− γ)Eπν [ ∞∑ k=t γkrk ∣∣∣∣∣æ<t ] , where γ ∈ (0, 1) is the discount factor. The optimal value is defined as V ∗ν (æ<t) := supπ V π ν (æ<t), and the optimal policy is π ∗ ν := arg maxπ V π ν . We use µ to denote the true environment. We assume the nonparametric setting: letM denote a countable class of envi- ronments containing the true environment µ. Let w ∈ ∆M be a prior probability distribution onM. After observing the history æ<t the prior w is updated to the posterior w(ν | æ<t) := w(ν)ν(æ<t)\/( ∑ ρ∈Mw(ρ)ρ(æ<t)). A policy π is asymp- totically optimal in mean iff for every µ ∈ M, Eπµ[V ∗µ (æ<t)− V πµ (æ<t)]→ 0 as t→∞. 3 Exploration Potential We consider model-based reinforcement learning where the agent learns a model of its environment. With this model, we can estimate the value of any candidate policy. Concretely, let V̂ πt denote our estimate of the value of the policy π at time step t. We assume that the agent\u2019s learning algorithm satisfies on-policy value convergence (OPVC): V πµ (æ<t)− V̂ πt (æ<t)→ 0 as t→∞ µπ-almost surely. (1) This does not imply that our model of the environment converges to the truth, only that we learn to predict the value of the policy that we are following. On-policy value convergence does not require that we learn to predict off-policy, i.e., the value of other policies. In particular, we might not learn to predict the value of the µ-optimal policy π∗µ. For example, a Bayesian mixture or an MDL-based estimator both satisfy OPVC if the true environment is the environment class; for more details, see Leike (2016, Sec. 4.2.3). We define the V̂t-greedy policy as π∗V̂ := arg maxπ V̂ π t . 3.1 Definition Definition 1 (Exploration Potential). LetM be a class of environments and let æ<t be a history. The exploration potential is defined as EPM(æ<t) := ∑ ν∈M w(ν | æ<t) ∣∣∣V π∗νν (æ<t)− V̂ π ∗ ν t (æ<t) ∣∣∣ . 3 Intuitively, EP captures the amount of exploration that is still required before having learned the entire environment class. Asymptotically the posterior concen- trates around environments that are compatible with the current environment. EP then quantifies how well the model V̂t understands the value of the compatible environments\u2019 optimal policies. Remark 2 (Properties of EP). (i) EPM depends neither on the true environment µ, nor on the agent\u2019s policy π. (ii) EPM depends on the choice of the prior w and on the agent\u2019s model of the world V̂t. (iii) 0 ≤ EPM(æ<t) ≤ 1 for all histories æ<t. The last item follows from the fact that the posterior w( · | æ<t) and the value function V are bounded between 0 and 1. 3.2 Sufficiency Proposition 3 (Bound on Optimality). For all µ ∈M, V ∗µ (æ<t)− V π∗ V̂ µ (æ<t) ≤ V̂ ∗t (æ<t)− V π∗ V̂ µ (æ<t) + EPM(æ<t) w(µ | æ<t) . Proof. ∣∣∣V ∗µ − V̂ π∗µ t ∣∣∣ = w(µ | æ<t) w(µ | æ<t) ∣∣∣V ∗µ − V̂ π∗µ t ∣∣∣ ≤ ∑ ν∈M w(ν | æ<t) w(µ | æ<t) ∣∣∣V ∗ν − V̂ π ∗ ν t ∣∣∣ = EPM w(µ | æ<t) Therefore V ∗µ − V π∗ V̂ µ = V ∗ µ − V̂ π∗µ t︸ ︷︷ ︸ ≤EP(æ<t)\/w(µ|æ<t) + V̂ π∗µ t − V̂ ∗t︸ ︷︷ ︸ ≤0 + V̂ ∗t − V π∗ V̂ µ . The bound of Proposition 3 is to be understood as follows. V ∗µ (æ<t)− V π∗ V̂ µ (æ<t)︸ ︷︷ ︸ optimality of the greedy policy ≤ V̂ ∗t (æ<t)− V π∗ V̂ µ (æ<t)︸ ︷︷ ︸ OPVC + EP(æ<t)︸ ︷︷ ︸ exploration potential \/w(µ | æ<t)︸ ︷︷ ︸ posterior If we switch to the greedy policy π∗ V̂ , then V̂ ∗t − V π∗ V̂ µ → 0 due to on-policy value convergence (1). This reflects how well the agent learned the environment\u2019s response to the Bayes-optimal policy. Generally, following the greedy policy does 4 not yield enough exploration for EP to converge to 0. In order to get a policy π that is asymptotically optimal, we have to combine an exploration policy which ensures that EP→ 0 and then gradually phase out exploration by switching to the π∗ V̂ -greedy policy. Because of property (i), the agent can compute its current EP value and thus check how close it is to 0. The higher the prior belief in the true environment µ, the smaller this value will be (in expectation). 3.3 Necessity Definition 4 (Policy Convergence). Let π and π\u2032 be two policies. We say the policy π converges to π\u2032 in µπ-probability iff |V̂ πt (æ<t)− V̂ π \u2032 t (æ<t)| → 0 as t→∞ in V̂ . We assume that V̂t is continuous in the policy argument. If π converges to π\u2032 in total variation in the sense that π(a | æ<k)− π\u2032(a | æ<k)→ 0 for all actions a and k ≥ t, then π converges to π\u2032 in V̂ . Definition 5 (Strongly Unique Optimal Policy). An environment µ admits a strongly unique optimal policy iff there is a µ-optimal policy π∗µ such that for all policies π if V ∗µ (æ<t)− V πµ (æ<t)→ 0 in µπ-probability, then π converges to π∗µ in V̂ . Assuming that V̂ πt is continuous is π, an environment µ has a unique optimal policy if there are no ties in arg maxa V ∗ µ (æ<ta). Admitting a strongly unique optimal policy is an even stronger requirement because it requires that there exist no other policies that approach the optimal value asymptotically but take different actions (i.e., there is a constant gap in the argmax). For any finite-state (PO)MDP with a unique optimal policy that policy is also strongly unique. Proposition 6 (Asymptotic Optimality⇒ EP→ 0). If the policy π is asymptoti- cally optimal in mean in the environment classM and each environment ν ∈ M admits a strongly unique optimal policy, then EPM → 0 in µπ-probability for all µ ∈M. Proof. Since π is asymptotically optimal in mean inM, we have that V ∗µ −V πµ → 0 and since µ admits a strongly unique optimal policy, π converges to π∗µ in µ π- probability, thus V̂ πt − V̂ π∗µ t → 0. By on-policy value convergence V πµ − V̂ πt → 0. Therefore V ∗µ − V̂ π∗µ t = V ∗ µ − V πµ + V πµ − V̂ πt + V̂ πt − V̂ π∗µ t → 0 5 and thus Eπµ ∣∣∣V π ∗ µ µ (æ<t)− V̂ π∗µ t (æ<t) ∣∣∣→ 0 for all µ ∈M. (2) Now Eπµ[EPM(æ<t)] = Eπµ [∑ ν∈M w(ν | æ<t) ∣∣∣V π∗νν (æ<t)− V̂ π ∗ ν t (æ<t) ∣∣∣ ] ≤ 1 w(µ) Eπξ [∑ ν∈M w(ν | æ<t) ∣∣∣V π∗νν (æ<t)− V̂ π ∗ ν t (æ<t) ∣∣∣ ] = 1 w(µ) ∑ ν∈M w(ν)Eπξ [ νπ(æ<t) ξπ(æ<t) ∣∣∣V π∗νν (æ<t)− V̂ π ∗ ν t (æ<t) ∣∣∣ ] = 1 w(µ) ∑ ν∈M w(ν)Eπν ∣∣∣V π∗νν (æ<t)− V̂ π ∗ ν t (æ<t) ∣∣∣→ 0 by (2) and Hutter (2005, Lem. 5.28ii). If we don\u2019t require the condition on strongly unique optimal policies, then the policy π could be asymptotically optimal while EP 6→ 0: there might be another policy π\u2032 that is very different from any optimal policy π∗µ, but whose µ-value approaches the optimal value: V ∗µ − V π \u2032 µ → 0 as t → ∞. Our policy π could converge to π\u2032 without EP converging to 0. 4 Exploration Potential in Multi-Armed Bandits In this section we use experiments with multi-armed Bernoulli bandits to illustrate the properties of exploration potential. The class of Bernoulli bandits is Θ = [0, 1]k (the arms\u2019 means). In each time step, the agent chooses an action (arm) i ∈ {1, . . . , k} and receives a reward rt ∼ Bernoulli(θ∗i ) where θ∗ ∈ Θ is the true environment. Since Θ is uncountable, exploration potential is defined with an integral instead of a sum: EPΘ(æ<t) := ∫ Θ p(θ | æ<t)|θj(θ) − θ̂j(θ)|dθ where p(θ | æ<t) is the posterior distribution given the history æ<t, θ̂ := ∫ Θ θp(θ | æ<t)dθ is the Bayes-mean parameter, and j(θ) := arg maxi θi is the index of the best arm accoding to θ. Figure 1 shows the exploration potential of several bandit algorithms, illustrating how much each algorithm explores. Notably, optimally confident UCB (Lattimore, 6 100 101 102 103 104 10−2 10−1 time step av er ag e ex pl or at io n po te nt ia l ε-greedy, N=100 Best arm, N=100 OCUCB, N=100 Thompson sampling, N=100 Round robin, N=100 MinEP, N=100√ 2\/πt Figure 1: Exploration potential over time for different bandit algorithms in the Bernoulli bandit with arms 0.6, 0.5, 0.4, 0.4 (double logarithmic plot); shaded re- gions correspond to one standard deviation. Lower exploration potential means more exploration. The notable change in slope in around time steps 20\u201380 stems from the fact that it takes about that long to reliably distinguish the first two arms. The dashed line corresponds to the optimal asymptotic rate of t−1\/2. 2015) stops exploring around time step 700 and focuses on exploitation (because in contrast to the other algorithms it knows the horizon). Thompson sampling, round robin (alternate between all arms), and ε-greedy explore continuously (but ε-greedy is less effective). The optimal strategy (always pull the first arm) never explores and hence its exploration potential decreases only slightly. Exploration potential naturally gives rise to an exploration strategy: greedily minimize Bayes-expected exploration potential (MinEP); see Algorithm 1. This strategy unsurprisingly explores more than all the other algorithms when measured on exploration potential, but in bandits it also turns out to be a decent exploitation strategy because it focuses its attention on the most promising arms. For empirical performance see Figure 2. However, MinEP is generally not a good exploitation strategy in more complex environments like MDPs. 7 Algorithm 1 The MinEP Algorithm 1: for t ∈ N do 2: at := arg mina∈A Eet∼posterior[EP(æ<taet)] 3: take action at 4: observe percept et 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ·104 0 20 40 60 80 time step av er ag e re gr et Best arm, N=100 OCUCB, N=100 UCB1, N=100 Thompson sampling, N=100 MinEP, N=20 Figure 2: Average regret over time for different bandit algorithms in the Bernoulli bandit with arms 0.6, 0.5, 0.4, 0.4. MinEP outperforms UCB1 (Auer et al., 2002) after 10 000 steps, but neither Thompson sampling nor OCUCB. 5 Discussion Several variants on the definition exploration potential given in Definition 1 are conceivable. However, often they do not satisfy at least one of the properties that make our definition appealing. Either they break the necessity (Proposition 3), sufficiency (Proposition 6), our proofs thereof, or they make EP hard to compute. For example, we could replace |V ∗ν −V̂ π ∗ ν t | by |V ∗ν −V πν |where π is the agent\u2019s future policy. This preserves both necessesity and sufficiency, but relies on computing the agent\u2019s future policy. If the agent uses exploration potential for taking actions (e.g., for targeted exploration), then this definition becomes a self-referential equation and might be very hard to solve. Following Dearden et al. (1999), we could consider |V ∗ν − V̂ ∗t | which has the convenient side-effect that it is model-free and therefore applies to more reinforcement learning algorithms. However, in this case the necessity guarantee (Proposition 6) requires the additional condition that the agent\u2019s policy converges to the greedy policy π∗ V̂ . Moreover, this does not remove the 8 dependence on a model since we still need a model classM and a posterior. Based on the recent successes in approximating information gain (Houthooft et al., 2016), we are hopeful that exploration potential can also be approximated in practice. Since computing the posterior is too costly for complex reinforcement learning problems, we could (randomly) generate a few environments and estimate the sum in Definition 1 with them. In this paper we only scratch the surface on exploration potential and leave many open questions. Is this the correct definition? What are good rates at which EP should converge to 0? Is minimizing EP the most efficient exploration strategy? Can we compute EP more efficiently than information gain? Acknowledgments We thank Tor Lattimore, Marcus Hutter, and our coworkers at the FHI for valuable feedback and discussion. References Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002. Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Technical report, 2016. http:\/\/arxiv.org\/abs\/1606.01868. Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian explo- ration. In Uncertainty in Artificial Intelligence, pages 150\u2013159, 1999. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. Technical report, 2016. http:\/\/arxiv.org\/abs\/1605. 09674. Marcus Hutter. Universal Artificial Intelligence. Springer, 2005. Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563\u2013 1600, 2010. Tor Lattimore. Optimally confident UCB: Improved regret for finite-armed bandits. Technical report, 2015. http:\/\/arxiv.org\/abs\/1507.07880. 9 http:\/\/arxiv.org\/abs\/1606.01868 http:\/\/arxiv.org\/abs\/1605.09674 http:\/\/arxiv.org\/abs\/1605.09674 http:\/\/arxiv.org\/abs\/1507.07880 Jan Leike. Nonparametric General Reinforcement Learning. PhD thesis, Australian National University, 2016. Marlos C Machado and Michael Bowling. Learning purposeful behaviour in the absence of rewards. Technical report, 2016. http:\/\/arxiv.org\/abs\/ 1605.07700. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540): 529\u2013533, 2015. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timo- thy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Ma- chine Learning, 2016. Laurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for stochastic environments. In Algorithmic Learning Theory, pages 158\u2013172. Springer, 2013. Gautam Reddy, Antonio Celani, and Massimo Vergassola. Infomax strategies for an optimal balance between exploration and exploitation. Journal of Statistical Physics, 163(6):1454\u20131476, 2016. Dan Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In Advances in Neural Information Processing Systems, pages 1583\u2013 1591, 2014. Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2(3): 230\u2013247, 2010. Malcolm Strens. A Bayesian framework for reinforcement learning. In International Conference on Machine Learning, pages 943\u2013950, 2000. Yi Sun, Faustino Gomez, and Jürgen Schmidhuber. Planning to be surprised: Optimal bayesian exploration in dynamic environments. In Artificial General Intelligence, pages 41\u201351. Springer, 2011. 10 http:\/\/arxiv.org\/abs\/1605.07700 http:\/\/arxiv.org\/abs\/1605.07700 1 Introduction 2 Preliminaries and Notation 3 Exploration Potential 3.1 Definition 3.2 Sufficiency 3.3 Necessity 4 Exploration Potential in Multi-Armed Bandits 5 Discussion ","flair":"null\tnull"}
{"author":"downtownslim","created":"Tue Nov 22 14:10:03 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.04135 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS 1 blog link (what is this?) Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Automated Inference on Criminality using Face Images Authors: Xiaolin Wu, Xi Zhang (Submitted on 13 Nov 2016 (v1), last revised 21 Nov 2016 (this version, v2)) Abstract: We study, for the first time, automated inference on criminality based solely on still face images. Via supervised machine learning, we build four classifiers (logistic regression, KNN, SVM, CNN) using facial images of 1856 real persons controlled for race, gender, age and facial expressions, nearly half of whom were convicted criminals, for discriminating between criminals and non-criminals. All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. Above all, the most important discovery of this research is that criminal and non-criminal face images populate two quite distinctive manifolds. The variation among criminal faces is significantly greater than that of the non-criminal faces. The two manifolds consisting of criminal and non-criminal faces appear to be concentric, with the non-criminal manifold lying in the kernel with a smaller span, exhibiting a law of normality for faces of non-criminals. In other words, the faces of general law-biding public have a greater degree of resemblance compared with the faces of criminals, or criminals have a higher degree of dissimilarity in facial appearance than normal people. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1611.04135 [cs.CV]   (or arXiv:1611.04135v2 [cs.CV] for this version) Submission history From: Xi Zhang [view email] [v1] Sun, 13 Nov 2016 13:32:11 GMT (2008kb,D) [v2] Mon, 21 Nov 2016 03:57:48 GMT (878kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"evc123","created":"Wed Nov 02 07:36:59 EDT 2016","text":" Under review as a conference paper at ICLR 2017 HOLSTEP: A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING Cezary Kaliszyk University of Innsbruck cezary.kaliszyk@uibk.ac.at François Chollet, Christian Szegedy Google Research {fchollet,szegedy}@google.com ABSTRACT Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and man- ually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of de- veloping new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of baseline deep learning models suited for the tasks (including convolutional neural networks and recurrent neural networks). The results of our baseline models shows the promise of applying deep learning to HOL theorem proving. 1 INTRODUCTION As the usability of interactive theorem proving (ITP) systems (Harrison et al., 2014) grows, its use becomes a more common way of establishing the correctness of software as well as mathematical proofs. ITPs are used today for software certification projects ranging from compilers (Leroy, 2009) and operating system components (Chen et al., 2016; Klein et al., 2014), to establishing the absolute correctness of large proofs in mathematics such as the Kepler conjecture (Hales et al., 2015) and the Feit-Thomson Theorem (Gonthier et al., 2013). For results of such significance to be possible, the theorem libraries of the ITPs must contain all necessary basic mathematical properties, accompanied with formal proofs. This means that the size of many ITP libraries can be measured in dozens of thousands of theorems (Grabowski et al., 2010; Blanchette et al., 2015) and billions of individual proof steps. While the general direction of the proofs is specified by humans (by providing the goal to prove, specifying intermediate steps, or applying certain automated tactics), the majority of such proof steps are actually found by automated reasoning-based proof search (Kaliszyk & Urban, 2015), with very little application of machine learning techniques so far. At the same time, fast progress is achieved in machine learning applied to tasks that involve logical inference, such as natural question answering (Sukhbaatar et al., 2015), knowledge base completion (Socher et al., 2013), automated translation (Wu et al., 2016), and premise selection in the context of theorem proving (Alemi et al., 2016). Deep learning in particular has proven a powerful tool for embedding semantic meaning and logical relationships into geometric spaces, specifically via models such as convolutional neural networks, recurrent neural networks, and tree-recursive neural networks. These advances strongly suggest that deep learning may have become mature enough to yield significant advances in automated theorem proving. Remarkably, it has recently become possible to build a system, AlphaGo (Silver et al., 2016), blending classical AI techniques such as Monte-Carlo tree search and modern deep learning techniques, capable of playing the game of Go at super-human levels. We should note that theorem proving and Go playing are conceptually related, since both consist in searching for specific nodes in trees of states with extremely large arity and relatively large depth, which involves node evaluation decision (how valuable is this state?) and policy decisions (which node should be expanded next?). The success of AlphaGo can thus serve as 1 Under review as a conference paper at ICLR 2017 encouragement on the road to building deep learning-augmented theorem provers that would blend classical techniques developed over the past few decades with the latest machine learning advances. Fast progress in specific machine learning verticals has occasionally been achieved thanks to the release of specialized datasets (often with associated competitions, e.g. the ImageNet dataset for large-scale image classification (Deng et al., 2009)) serving as an experimental testbed and public benchmark of current progress, thus focusing the efforts of the research community. We hope that releasing a theorem proving dataset suited for specific machine learning tasks can serve the same purpose in the vertical of applying machine learning to theorem proving. 1.1 CONTRIBUTION AND OVERVIEW First, we develop a dataset for machine learning based on the proof steps used in a large interactive proof section 2. We focus on the HOL Light (Harrison, 2009) ITP, its multivariate analysis library (Harrison, 2013), as well as the formal proof of the Kepler conjecture (Hales et al., 2010). These for- malizations constitute a diverse proof dataset containing basic mathematics, analysis, trigonometry, as well as reasoning about data structures such as graphs. Furthermore these formal proof devel- opments have been used as benchmarks for automated reasoning techniques (Kaliszyk & Urban, 2014). The dataset consists of 2,013,046 training examples and 196,030 testing examples that originate from 11,400 proofs. Precisely half of the examples are statements that were useful in the currently proven conjectures and half are steps that have been derived either manually or as part of the auto- mated proof search but were not necessary in the final proofs. The dataset contains only proofs of non-trivial theorems, that also do not focus on computation but rather on actual theorem proving. For each proof, the conjecture that is being proven as well as its dependencies (axioms) and may be exploited in machine learning tasks. Furthermore, for each statement both its human-readable (pretty-printed) statement and a tokenization designed to make machine learning tasks more man- ageable are included. Next, in section 3 we discuss the proof step classification tasks that can be attempted using the dataset, and we discuss the usefulness of these tasks in interactive and automated theorem proving. These tasks includes unconditioned classification (without access to conjectures and dependencies) and conjecture-conditioned classification (with access to the conjecture) of proof steps as being useful or not in a proof. We outline the use of such classification capabilities for search space pruning and internal guidance, as well as for generation of intermediate steps or possible new lemma statements. Finally, in section 4 we propose three baseline models for the proof step classification tasks and experimentally evaluate the models on the data in section 5. The models considered include both a relatively simple regression model, as well as models based on convolutional and recurrent neural networks. 1.2 RELATED WORK The use of machine learning in interactive and automated theorem proving has so far focused on three main tasks: premise selection, strategy selection, and internal guidance. We shortly explain these tasks. Given a large library of proven facts and a user given conjecture to prove, the multi-label classifica- tion problem of selecting the facts that are most likely to lead to a successful proof of the conjecture has been usually called relevance filtering or premise selection (Alama et al., 2014). This task is crucial for the efficiency of the state-of-the-art automation techniques for ITPs (Blanchette et al., 2016). Similarly most competitive automated theorem provers today (Sutcliffe, 2016) implement the SInE classifier (Hoder & Voronkov, 2011). A second theorem proving task where machine learning has been of importance is strategy selection. With the development of automated theorem provers came many parameters that control their exe- cution. In fact, modern ATPs, such as E (Schulz, 2013) and Vampire (Kovács & Voronkov, 2013), include complete strategy description languages that allow a user to specify the orderings, weighting functions, literal selection strategies, etc. Rather than optimizing the search strategy globally, one 2 Under review as a conference paper at ICLR 2017 can choose the strategy based on the currently considered problem. For this some frameworks use machine learning (Bridge et al., 2014; Kühlwein & Urban, 2015). Finally, an automated theorem prover may use machine learning for choosing the actual inference steps. It has been shown to significantly reduce the proof search in first-order tableaux by the selection of extension steps to use (Urban et al., 2011), and has been also successfully applied in monomorphic higher-order logic proving (Färber & Brown, 2016). Data\/proof mining has also been applied on the level of interactive theorem proving tactics (Duncan, 2007) to extract and reuse repeating patterns. 2 DATASET EXTRACTION We focus on the HOL Light theorem prover for two reasons. First, it is an LCF style interactive theorem prover. In the LCF style more complicated inferences are reduced to the most primitive ones, all of which go through the small trusted core, called the kernel. This means that many modifications can be restricted to this small core, and it is relatively easy to extract proof steps and an arbitrary selected level of granularity. Second, it implements higher-order logic as its foundation, which on the one hand is powerful enough to encode most of today\u2019s formal proofs, and on the other hand allows for an easy integration of many powerful automation mechanisms (Baader & Nipkow, 1998; Paulson, 1999). When selecting the theorems to record, we choose an intermediate approach between HOL Light ProofRecording (Obua & Skalberg, 2006) and the HOL\/Import one (Kaliszyk & Krauss, 2013). The top-level values that are of type theorem based on standard proof functions are patched to record their names and remaining values are extracted from the underlying OCaml toplevel. Additionally, the intermediate theorem values that are used across proof blocks are considered theorem boundaries, avoiding any reused unrelated subproofs. All kernel-level inferences are recorded together with the necessary arguments in a trace file. The trace is processed offline to extract the facts dependencies, detect used proof boundaries, mark the used and unused steps, and mark the training and testing examples. Only proofs that have suffi- ciently many used and unused steps are considered useful for the dataset. The proof trace with additional annotations is processed again by a HOL kernel, this time saving the actual training and testing examples. Training and testing examples are grouped by proof: for each proof the conjecture (statement that is finally proved) and the dependencies of the theorem are constant, and a list of used and not used intermediate statements is provided. For each statement, whether it is the conjecture, a proof dependency, or an intermediate statement, both a fully parenthesised HOL Light human-like printout is provided, as well as a predefined to- kenization. The standard HOL Light printer uses parentheses and operator priorities to make its notations somewhat similar to textbook-style mathematics, while at the same time preserving the complete unambiguity of the order of applications (this is particularly visible for associative oper- ators). The tokenization that we propose attempts to reduce the number of parentheses. To do this we compute the maximum number of arguments that each symbol needs to be applied to, and only mark partial application. This means that fully applied functions (more than 90% of the applications) do not require neither application operators nor parentheses. Toplevel universal quantifications are eliminated, bound variables are represented by their de Bruijn indices and free variables are renamed canonically. Since the Hindley-Milner type inference mechanisms will be sufficient to reconstruct the most-general types of the expressions well enough for automated-reasoning techniques Kaliszyk et al. (2015) we erase all type information. Table 1 presents some dataset statistics. The dataset together with the description of the used format is available from: http:\/\/cl-informatik.uibk.ac.at\/cek\/holstep\/ 3 MACHINE LEARNING TASKS 3.1 TASKS DESCRIPTION This dataset makes possible several tasks well-suited for machine learning most of which are highly relevant for theorem proving: 3 http:\/\/cl-informatik.uibk.ac.at\/cek\/holstep\/ Under review as a conference paper at ICLR 2017 Train Test Positive Negative Examples 2013046 196030 1104538 1104538 Avg. length 503.18 440.20 535.52 459.66 Avg. tokens 87.01 80.62 95.48 77.40 Conjectures 9999 1411 - - Avg. dependencies 29.58 22.82 - - Table 1: HolStep dataset statistics \u2022 Predicting whether a statement is useful in the proof of a given conjecture; \u2022 Predicting the dependencies of a proof statement (premise selection); \u2022 Predicting whether a statement is an important one (human named); \u2022 Predicting which conjecture a particular intermediate statement originates from; \u2022 Predicting the name given to a statement; \u2022 Generating intermediate statements useful in the proof of a given conjecture; \u2022 Generating the conjecture the current proof will lead to. In what follows we focus on the first task: classifying proof step statements as being useful or not in the context of a given proof. This task may be further specialized into two different tasks: \u2022 Unconditioned classification of proof steps: determining how likely a given proof is to be useful for the proof it occurred in, based solely on the content of statement (i.e. by only providing the model with the step statement itself, absent any context). \u2022 Conditioned classification of proof steps: determining how likely a given proof is to be useful for the proof it occurred in, with \u201Cconditioning\u201D on the conjecture statement that the proof was aiming to attain, i.e. by providing the model with both the step statement and the conjecture statement). In the dataset, for every proof we provide the same number of useful and non-useful steps. As such, the proof step classification problem is a balanced two-class classification problem, where a random baseline would yield an accuracy of 0.5. 3.2 RELEVANCE TO INTERACTIVE AND AUTOMATED THEOREM PROVING In the interaction with an interactive theorem prover, the tasks that require most human time are: the search for good intermediate steps; the search for automation techniques able to justify the individual steps, and the searching of theorem proving libraries for the necessary simpler facts. These three problems directly correspond to the machine learning tasks proposed in the previous subsection. Being able to predict the usefulness of a statement will significantly improve many automation techniques. The generation of good intermediate lemmas or intermediate steps can improve level of granularity of the proof steps. Understanding the correspondence between statements and their names can allow users to search for statements in the libraries more efficiently (Aspinall & Kaliszyk, 2016). Premise selection and filtering are already used in many theorem proving systems, and generation of succeeding steps corresponds to conjecturing and theory exploration. 4 BASELINE MODELS For each task (conditioned and unconditioned classification), we propose three different deep learn- ing architectures, meant to provide a baseline for the classification performance that can be achived on this dataset. Our models cover a range of architecture features (from convolutional networks to recurrent networks), aiming at probing what characteristics of the data are the most helpful for usefulness classification. Our models are implemented in TensorFlow (Abadi et al., 2015) using the Keras framework (Chollet, 2015). Each model was trained on a single Nvidia K80 GPU. Training only takes a few hours per 4 Under review as a conference paper at ICLR 2017 Figure 1: Unconditioned classification model architectures. model, which makes running these experiments accessible to most people (they could even be run on a laptop CPU). We are releasing all of our benchmark code as open-source software so as to allow others to reproduce our results and improve upon our models. 4.1 UNCONDITIONED CLASSIFICATION MODELS Our three models for this task are as follow: \u2022 Logistic regression on top of learned token embeddings. This minimal model aims to determine to which extent simple differences between token distribution between useful and non-useful statements can be used to distinguish them. It provides an absolute floor on the performance achievable on this task. \u2022 2-layer 1D convolutional neural network (CNN) with global maxpooling for sequence re- duction. This model aims to determine the importance of local patterns of tokens. \u2022 2-layer 1D CNN with LSTM (Hochreiter & Schmidhuber, 1997) sequence reduction. This model aims to determine the importance of order in the features sequences. See figure 1 for a layer-by-layer description of these models. 4.2 CONDITIONED CLASSIFICATION MODELS For this task, we use versions of the above models that have two siamese branches (identical branches with shared weights), with one branch processing the proof step statement being considered, and the other branch processing the conjecture. Each branch outputs an embedding; these two embeddings (step embedding and conjecture embedding) are then concatenated and the classified by a fully- connected network. See figure 2 for a layer-by-layer description of these models. 5 Under review as a conference paper at ICLR 2017 Figure 2: Conditioned classification model architectures. 4.3 INPUT STATEMENTS ENCODING It should be noted that all of our models start with an Embedding layer, mapping tokens or characters in the statements to dense vectors in a low-dimensional space. We consider two possible encodings for presenting the input statements (proof steps and conjectures) to the Embedding layers of our models: \u2022 Character-level encoding of the human-readable versions of the statements, where each character (out of a set of 83 unique characters) in the pretty-printed statements is mapped to a 256-dimensional dense vector. This encoding yields longer statements (training state- ments are 347 character long on average). \u2022 Token-level encoding of the versions of the statements rendered with our proposed high- level tokenization scheme. This encoding yields shorter statements (training statements are 51 token long on average), while considerably increasing the size of set of unique tokens (approximately 100,000 total tokens in the training set). 6 Under review as a conference paper at ICLR 2017 Table 2: HolStep proof step classification accuracy without conditioning Logistic 1D CNN 1D CNN-LSTMregression Accuracy with char input 0.71 0.82 0.83 Accuracy with token input 0.71 0.82 0.76 Table 3: HolStep proof step classification accuracy with conditioning Logistic Siamese Siamese regression 1D CNN 1D CNN-LSTM Accuracy with char input 0.71 0.81 0.83 Accuracy with token input 0.71 0.82 0.76 5 RESULTS Experimental results are presented in tables 2 and 3 as well as figs. 3a, 3b, 4a and 4b. 5.1 INFLUENCE OF MODEL ARCHITECTURE Our unconditioned logistic regression model yields an accuracy of 71%, both with character encod- ing and token encoding (tables 2 and 3). This demonstrates that differences in token or character distributions between useful and non-useful steps alone, absent any context, is sufficient for discrim- inating between useful and non-useful statements to a reasonable extent. This also demonstrates that the token encoding is not fundamentally more informative than raw character-level statements. Additionally, our unconditioned 1D CNN model yields an accuracy of 83% to 84%, both with char- acter encoding and token encoding (tables 2 and 3). This demonstrates that patterns of characters or patterns of tokens are considerably more informative than single tokens for the purpose of usefulness classification. Finally, our unconditioned convolutional-recurrent model does not improve upon the results of the 1D CNN, which indicates that our models are not able to meaningfully leverage order in the feature sequences into which the statements are encoded. 5.2 INFLUENCE OF INPUT ENCODING For the logistic regression model and the 2-layer 1D CNN model, the choice of input encoding seems to have little impact. For the convolutional-recurrent model, the use of the high-level tokenization seems to cause a large decrease in model performance (figs. 3b and 4b). This may be due to the fact that token encoding yields shorter sequences, making the use of a LSTM less relevant. 5.3 INFLUENCE OF CONDITIONING ON THE CONJECTURE None of our conditioned models appear to be able to improve upon the unconditioned models, which indicates that our architectures are not able to leverage the information provided by the conjecture. The presence of the conditioning does however impact the training profile of our models, in particu- lar by making the 1D CNN model converge faster and overfit significantly quicker (figs. 4a and 4b). 6 CONCLUSIONS Our baseline deep learning models, albeit fairly weak, are still able to predict statement usefulness with a remarkably high accuracy, making them valuable for a number of practical theorem proving applications. This includes making tableaux-based (Paulson, 1999) and superposition-based (Hurd, 2003) internal ITP proof search significantly more efficient, which in turn would make formalization easier. 7 Under review as a conference paper at ICLR 2017 (a) Training profile of the three unconditioned base- line models with character input. (b) Training profile of the three unconditioned base- line models with token input. (a) Training profile of the three conditioned baseline models with character input. (b) Training profile of the three conditioned baseline models with token input. However, our models do not appear to be able to leverage order in the input sequences, nor do they appear to be able to leverage conditioning on the conjectures. This is due to the fact that these models are not doing any form of logical reasoning on their input statements; rather they are doing simple pattern matching at the level of n-grams of characters or tokens. This shows the need to focus future efforts on new forms deep learning models that can do reasoning, or alternatively, on systems that blend explicit reasoning with deep learning-based feature learning. 6.1 FUTURE WORK The dataset focuses on one interactive theorem prover. It would be interesting if the proposed tech- niques generalize, primarily across ITPs that use the same foundational logic, for example using the (Hurd, 2011) standard, and secondarily across fundamentally different ITPs or even ATPs. Finally, two of the proposed task for the dataset have been premise selection and intermediate sen- tence generation. It would be interesting to define more ATP-based ways to evaluate the selected premises, as well as to evaluate generated sentences (Kaliszyk et al., 2015). The set is a relatively large one when it comes to proof step classification, however the number of available premises makes the set a medium-sized set for premise selection in comparison with those of the Mizar Math- ematical Library or the seL4 development. ACKNOWLEDGEMENTS Supported by the Austrian Science Fund (FWF) grant P26201. 8 Under review as a conference paper at ICLR 2017 REFERENCES Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin- cent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Watten- berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http:\/\/tensorflow.org\/. Software available from tensor- flow.org. Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze, and Josef Urban. Premise se- lection for mathematics by corpus analysis and kernel methods. J. Autom. Reasoning, 52(2): 191\u2013213, 2014. doi: 10.1007\/s10817-013-9286-5. Alex A. Alemi, François Chollet, Geoffrey Irving, Christian Szegedy, and Josef Urban. DeepMath \u2013 Deep sequence models for premise selection. 2016. URL https:\/\/arxiv.org\/abs\/1606.04442. David Aspinall and Cezary Kaliszyk. What\u2019s in a theorem name? In Jasmin Christian Blanchette and Stephan Merz (eds.), Interactive Theorem Proving (ITP 2016), volume 9807 of LNCS, pp. 459\u2013465. Springer, 2016. doi: 10.1007\/978-3-319-43144-4. Franz Baader and Tobias Nipkow. Term rewriting and all that. Cambridge University Press, 1998. ISBN 978-0-521-45520-6. Jasmin C. Blanchette, Cezary Kaliszyk, Lawrence C. Paulson, and Josef Urban. Hammering towards QED. J. Formalized Reasoning, 9(1):101\u2013148, 2016. ISSN 1972-5787. doi: 10.6092\/issn. 1972-5787\/4593. Jasmin Christian Blanchette, Maximilian P. L. Haslbeck, Daniel Matichuk, and Tobias Nipkow. Mining the Archive of Formal Proofs. In Manfred Kerber, Jacques Carette, Cezary Kaliszyk, Florian Rabe, and Volker Sorge (eds.), Intelligent Computer Mathematics (CICM 2015), volume 9150 of LNCS, pp. 3\u201317. Springer, 2015. James P. Bridge, Sean B. Holden, and Lawrence C. Paulson. Machine learning for first-order theo- rem proving - learning to select a good heuristic. J. Autom. Reasoning, 53(2):141\u2013172, 2014. doi: 10.1007\/s10817-014-9301-5. Haogang Chen, Daniel Ziegler, Tej Chajed, Adam Chlipala, M. Frans Kaashoek, and Nickolai Zel- dovich. Using crash Hoare logic for certifying the FSCQ file system. In Ajay Gulati and Hakim Weatherspoon (eds.), USENIX 2016. USENIX Association, 2016. François Chollet. Keras. https:\/\/github.com\/fchollet\/keras, 2015. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. Hazel Duncan. The Use of Data-Mining for the Automatic Formation of Tactics. PhD thesis, Uni- versity of Edinburgh, 2007. Michael Färber and Chad E. Brown. Internal guidance for Satallax. In Nicola Olivetti and Ashish Tiwari (eds.), International Joint Conference on Automated Reasoning (IJCAR 2016), volume 9706 of LNCS, pp. 349\u2013361. Springer, 2016. doi: 10.1007\/978-3-319-40229-1. Georges Gonthier, Andrea Asperti, Jeremy Avigad, Yves Bertot, Cyril Cohen, François Garillot, Stéphane Le Roux, Assia Mahboubi, Russell O\u2019Connor, Sidi Ould Biha, Ioana Pasca, Laurence Rideau, Alexey Solovyev, Enrico Tassi, and Laurent Théry. A machine-checked proof of the odd order theorem. In Sandrine Blazy, Christine Paulin-Mohring, and David Pichardie (eds.), Interactive Theorem Proving (ITP 2013), volume 7998 of LNCS, pp. 163\u2013179. Springer, 2013. Adam Grabowski, Artur Kornilowicz, and Adam Naumowicz. Mizar in a nutshell. J. Formalized Reasoning, 3(2):153\u2013245, 2010. doi: 10.6092\/issn.1972-5787\/1980. 9 http:\/\/tensorflow.org\/ https:\/\/arxiv.org\/abs\/1606.04442 https:\/\/github.com\/fchollet\/keras Under review as a conference paper at ICLR 2017 Thomas Hales, John Harrison, Sean McLaughlin, Tobias Nipkow, Steven Obua, and Roland Zumkeller. A revision of the proof of the Kepler Conjecture. Discrete & Computational Ge- ometry, 44(1):1\u201334, 2010. Thomas C. Hales, Mark Adams, Gertrud Bauer, Dat Tat Dang, John Harrison, Truong Le Hoang, Cezary Kaliszyk, Victor Magron, Sean McLaughlin, Thang Tat Nguyen, Truong Quang Nguyen, Tobias Nipkow, Steven Obua, Joseph Pleso, Jason Rute, Alexey Solovyev, An Hoai Thi Ta, Trung Nam Tran, Diep Thi Trieu, Josef Urban, Ky Khac Vu, and Roland Zumkeller. A formal proof of the Kepler conjecture. CoRR, abs\/1501.02155, 2015. John Harrison. HOL Light: An overview. In Stefan Berghofer, Tobias Nipkow, Christian Urban, and Makarius Wenzel (eds.), Theorem Proving in Higher Order Logics (TPHOLs 2009), volume 5674 of LNCS, pp. 60\u201366. Springer, 2009. John Harrison. The HOL Light theory of Euclidean space. J. Autom. Reasoning, 50(2):173\u2013190, 2013. doi: 10.1007\/s10817-012-9250-9. John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Jörg Siekmann (ed.), Handbook of the History of Logic vol. 9 (Computational Logic), pp. 135\u2013214. Elsevier, 2014. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997. Kryštof Hoder and Andrei Voronkov. Sine qua non for large theory reasoning. In Nikolaj Bjørner and Viorica Sofronie-Stokkermans (eds.), CADE-23, volume 6803 of LNAI, pp. 299\u2013314. Springer, 2011. Joe Hurd. First-order proof tactics in higher-order logic theorem provers. In Myla Archer, Ben Di Vito, and César Muñoz (eds.), Design and Application of Strategies\/Tactics in Higher Order Log- ics (STRATA 2003), number NASA\/CP-2003-212448 in NASA Technical Reports, pp. 56\u201368, September 2003. URL http:\/\/www.gilith.com\/research\/papers. Joe Hurd. The OpenTheory standard theory library. In Mihaela Gheorghiu Bobaru, Klaus Havelund, Gerard J. Holzmann, and Rajeev Joshi (eds.), NASA Formal Methods (NFM 2011), volume 6617 of LNCS, pp. 177\u2013191. Springer, 2011. Cezary Kaliszyk and Alexander Krauss. Scalable LCF-style proof translation. In Sandrine Blazy, Christine Paulin-Mohring, and David Pichardie (eds.), Interactive Theorem Proving (ITP 2013), volume 7998 of LNCS, pp. 51\u201366. Springer, 2013. Cezary Kaliszyk and Josef Urban. Learning-assisted automated reasoning with Flyspeck. J. Autom. Reasoning, 53(2):173\u2013213, 2014. doi: 10.1007\/s10817-014-9303-3. Cezary Kaliszyk and Josef Urban. Learning-assisted theorem proving with millions of lemmas. J. Symbolic Computation, 69:109\u2013128, 2015. doi: 10.1016\/j.jsc.2014.09.032. Cezary Kaliszyk, Josef Urban, and Jiří Vyskočil. Learning to parse on aligned corpora. In Christian Urban and Xingyuan Zhang (eds.), Proc. 6h Conference on Interactive Theorem Proving (ITP\u201915), volume 9236 of LNCS, pp. 227\u2013233. Springer-Verlag, 2015. doi: 10.1007\/978-3-319-22102-1_ 15. Gerwin Klein, June Andronick, Kevin Elphinstone, Toby C. Murray, Thomas Sewell, Rafal Kolan- ski, and Gernot Heiser. Comprehensive formal verification of an OS microkernel. ACM Trans. Comput. Syst., 32(1):2, 2014. Laura Kovács and Andrei Voronkov. First-order theorem proving and Vampire. In Natasha Shary- gina and Helmut Veith (eds.), Computer-Aided Verification (CAV 2013), volume 8044 of LNCS, pp. 1\u201335. Springer, 2013. Daniel Kühlwein and Josef Urban. MaLeS: A framework for automatic tuning of automated theorem provers. J. Autom. Reasoning, 55(2):91\u2013116, 2015. doi: 10.1007\/s10817-015-9329-1. Xavier Leroy. Formal verification of a realistic compiler. Commun. ACM, 52(7):107\u2013115, 2009. 10 http:\/\/www.gilith.com\/research\/papers Under review as a conference paper at ICLR 2017 Steven Obua and Sebastian Skalberg. Importing HOL into Isabelle\/HOL. In Ulrich Furbach and Natarajan Shankar (eds.), International Joint Conference on Automated Reasoning (IJCAR 2006), volume 4130 of LNCS, pp. 298\u2013302. Springer, 2006. Lawrence C. Paulson. A generic tableau prover and its integration with Isabelle. J. Universal Computer Science, 5(3):73\u201387, 1999. Stephan Schulz. System description: E 1.8. In Kenneth L. McMillan, Aart Middeldorp, and Andrei Voronkov (eds.), Logic for Programming, Artificial Intelligence (LPAR 2013), volume 8312 of LNCS, pp. 735\u2013743. Springer, 2013. David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lil- licrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484\u2013503, 2016. URL http:\/\/www.nature.com\/nature\/journal\/v529\/n7587\/full\/nature16961.html. Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural In- formation Processing Systems 26: 27th Annual Conference on Neural Information Pro- cessing Systems 2013. Proceedings., pp. 926\u2013934, 2013. URL http:\/\/papers.nips.cc\/paper\/ 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, pp. 2431\u20132439, 2015. Geoff Sutcliffe. The CADE ATP system competition - CASC. AI Magazine, 37(2):99\u2013101, 2016. Josef Urban, Jiří Vyskočil, and Petr Štěpánek. MaLeCoP: Machine learning connection prover. In Kai Brünnler and George Metcalfe (eds.), TABLEAUX 2011, volume 6793 of LNCS. Springer, 2011. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John- son, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs\/1609.08144, 2016. URL http:\/\/arxiv.org\/abs\/1609.08144. 11 http:\/\/www.nature.com\/nature\/journal\/v529\/n7587\/full\/nature16961.html http:\/\/papers.nips.cc\/paper\/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion http:\/\/papers.nips.cc\/paper\/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion http:\/\/arxiv.org\/abs\/1609.08144 ","flair":"three\tResearch"}
{"author":"bihaqo","created":"Sun Oct 23 16:11:06 EDT 2016","text":"Hi!\n\nI\u2019m a PhD student in machine learning. I'm going to make a Coursera course with my colleagues, that will be a part of ML- and statistics-based specialization. Our MOOC will last **4-5 weeks**. We are currently discussing the main topic of the course.\n\nOur\u2019s and our research group's main expertise is in Bayesian methods (we are bayesgroup.ru after all), convolutional neural networks, and RNNs.\nWe have several ideas and need to pick one:\n\n* \u201CRecurrent Neural Networks\u201D. RNNs is a part of many deep learning courses, but lots of people have never coded\/ran an LSTM. Our course will cover main aspects, tips and tricks of RNNs and will offer some hands-on tutorials on them along with a chance to solve real-world problems. \n* \u201CHow to read deep learning papers\u201D. More and more folks are entering deep learning these days, but reading state-of-the-art papers still scares lots of people away. We plan to focus the course on reading assignments. The student will be asked to read and analyze papers, and maybe write a short essay. After that, we will analyze the very same papers in the lectures and share some tips on what to look at. We will discuss stuff like \u201Cwhy did they do this thing instead of this one?\u201D, discuss the connections to other papers and reasonable things to try next. We will also look into the official reviews for these papers (for NIPS and ICLR papers, where the reviews are available). It would be tough to make such course work, though...\n* \u201CBayesian methods for machine learning\u201C. The general form of the EM-algorithm, MCMC methods, variational inference and maybe variational autoencoders, matrix calculus (like differentiating a function w.r.t. a matrix). Coding homeworks (like [using EM algorithm to recover a villain photo from many noisy observations](http:\/\/cmp.felk.cvut.cz\/cmp\/courses\/recognition\/Labs\/em\/index_en.html)).\n* \u201CPractical neural networks\u201D. Different topic each week (CNNs, RNNs, NLP, etc.), and a Kaggle competition as a coding homework. In lectures, there would be practical tips like \u201Ctry this kind of augmentation\u201D. To see what I mean, look at this brilliant blog post: [Using convolutional neural nets to detect facial keypoints tutorial](http:\/\/danielnouri.org\/notes\/2014\/12\/17\/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial\/). It focuses on a particular Kaggle competition and goes step by step from a simplest neural network to a top notch one. Each step is motivated like \u201Cwe increased the model capacity and it indeed overfitted. Let's add dropout -- see, it helps\u201D.\n\nWhat topic do you guys think would be most beneficial for the community? Do you have any suggestions or additional ideas?\n\n**UPD**\n\nThanks everyone for the feedback, I think we will stick with the \"Practical Bayesian methods\" as most of you suggested. The rough plan is to have 3 lectures about the basic techniques (like EM and MCMC) and after each lecture show how do they apply to VAE. So after the first lecture, we will introduce VAE idea, but will not tell anything about the training, and will build up from that. The last two lectures will be dedicated to neural-Bayesian models other than VAE.\n\nBTW, if you liked any of the course ideas I mentioned above, go ahead and still it to make your own course! ","flair":"one\tDiscussion"}
{"author":"downtownslim","created":"Tue Oct 11 10:37:27 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.00768 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.CR stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: cleverhans v0.1: an adversarial machine learning library Authors: Ian Goodfellow, Nicolas Papernot, Patrick McDaniel (Submitted on 3 Oct 2016 (v1), last revised 5 Oct 2016 (this version, v2)) Abstract: cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system. Comments: Technical report for this https URL Subjects: Learning (cs.LG); Cryptography and Security (cs.CR); Machine Learning (stat.ML) Cite as: arXiv:1610.00768 [cs.LG]   (or arXiv:1610.00768v2 [cs.LG] for this version) Submission history From: Nicolas Papernot [view email] [v1] Mon, 3 Oct 2016 22:04:07 GMT (5kb) [v2] Wed, 5 Oct 2016 13:54:04 GMT (5kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"syllogism_","created":"Wed Oct 19 18:47:47 EDT 2016","text":" Blog About Request Kemal Şanlı & Freepik spaCy v1.0: Deep Learning with custom pipelines and Keras by Matthew Honnibal on October 19, 2016I'm pleased to announce the 1.0 release of spaCy, the fastest NLP library in the world. By far the best part of the 1.0 release is a new system for integrating custom models into spaCy. This post introduces you to the changes, and shows you how to use the new custom pipeline functionality to add a Keras-powered LSTM sentiment analysis model into a spaCy pipeline. The spaCy user survey has been full of great feedback about the library. The clearest finding has been the need for more tutorials. We're currently working on a new and improved tutorials section for the site. We're also prioritising tutorials for the new 1.0 functionality \u2013 like the new rule-based, entity-aware matcher, the model training APIs and the custom pipelines.spaCy user surveyThanks to all of you who have filled out the survey so far! If you're a spaCy user and you haven't seen the survey yet, you can help improve the library by giving your feedback. The custom pipelines are particularly exciting, because they let you hook your own deep learning models into spaCy. So, without further ado, here's how to use Keras to train an LSTM sentiment analysis model and use the resulting annotations with spaCy. How to add sentiment analysis to spaCy with an LSTM model using Keras There are lots of great open-source libraries for researching, training and evaluating neural networks. However, the concerns of these libraries usually end at the point where you have an evaluation score and a model file. spaCy has always been designed to orchestrate multiple textual annotation models and help you use them together in your application. spaCy 1.0 now makes it much easier to calculate those annotations using your own custom models. In this tutorial, we'll be using Keras, as it's the most popular deep learning library for Python. Let's assume you've written a custom sentiment analysis model that predicts whether a document is positive or negative. Now you want to find which entities are commonly associated with positive or negative documents. Here's a quick example of how that can look at runtime.What's Keras?Keras gives you a high-level, declarative interface to define neural networks. Models are trained using Google's TensorFlow by default. Theano is also supported. Runtime usagedef count_entity_sentiment(nlp, texts): '''Compute the net document sentiment for each entity in the texts.''' entity_sentiments = collections.Counter(float) for doc in nlp.pipe(texts, batch_size=1000, n_threads=4): for ent in doc.ents: entity_sentiments[ent.text] += doc.sentiment return entity_sentiments def load_nlp(lstm_path, lang_id='en'): def create_pipeline(nlp): return [nlp.tagger, nlp.entity, SentimentAnalyser.load(lstm_path, nlp)] return spacy.load(lang_id, create_pipeline=create_pipeline) All you have to do is pass a create_pipeline callback function to spacy.load(). The function should take a spacy.language.Language object as its only argument, and return a sequence of callables. Each callable should accept a Doc object, modify it in place, and return None. Of course, operating on single documents is inefficient, especially for deep learning models. Usually we want to annotate many texts, and we want to process them in parallel. You should therefore ensure that your model component also supports a .pipe() method. The .pipe() method should be a well-behaved generator function that operates on arbitrarily large sequences. It should consume a small buffer of documents, work on them in parallel, and yield them one-by-one. Custom Annotator Classclass SentimentAnalyser(object): @classmethod def load(cls, path, nlp): with (path \/ 'config.json').open() as file_: model = model_from_json(file_.read()) with (path \/ 'model').open('rb') as file_: lstm_weights = pickle.load(file_) embeddings = get_embeddings(nlp.vocab) model.set_weights([embeddings] + lstm_weights) return cls(model) def __init__(self, model): self._model = model def __call__(self, doc): X = get_features([doc], self.max_length) y = self._model.predict(X) self.set_sentiment(doc, y) def pipe(self, docs, batch_size=1000, n_threads=2): for minibatch in cytoolz.partition_all(batch_size, docs): Xs = get_features(minibatch) ys = self._model.predict(X) for i, doc in enumerate(minibatch): doc.sentiment = ys[i] def set_sentiment(self, doc, y): doc.sentiment = float(y[0]) # Sentiment has a native slot for a single float. # For arbitrary data storage, there's: # doc.user_data['my_data'] = y def get_features(docs, max_length): Xs = numpy.zeros((len(docs), max_length), dtype='int32') for i, doc in enumerate(minibatch): for j, token in enumerate(doc[:max_length]): Xs[i, j] = token.rank if token.has_vector else 0 return Xs By default, spaCy 1.0 downloads and uses the 300-dimensional GloVe common crawl vectors. It's also easy to replace these vectors with ones you've trained yourself, or to disable the word vectors entirely. If you've installed your word vectors into spaCy's Vocab object, here's how to use them in a Keras model: Training with Kerasdef train(train_texts, train_labels, dev_texts, dev_labels, lstm_shape, lstm_settings, lstm_optimizer, batch_size=100, nb_epoch=5): nlp = spacy.load('en', parser=False, tagger=False, entity=False) embeddings = get_embeddings(nlp.vocab) model = compile_lstm(embeddings, lstm_shape, lstm_settings) train_X = get_features(nlp.pipe(train_texts)) dev_X = get_features(nlp.pipe(dev_texts)) model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels), nb_epoch=nb_epoch, batch_size=batch_size) return model def compile_lstm(embeddings, shape, settings): model = Sequential() model.add( Embedding( embeddings.shape[1], embeddings.shape[0], input_length=shape['max_length'], trainable=False, weights=[embeddings] ) ) model.add(Bidirectional(LSTM(shape['nr_hidden']))) model.add(Dropout(settings['dropout'])) model.add(Dense(shape['nr_class'], activation='sigmoid')) model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy', metrics=['accuracy']) return model def get_embeddings(vocab): max_rank = max(lex.rank for lex in vocab if lex.has_vector) vectors = numpy.ndarray((max_rank+1, vocab.vectors_length), dtype='float32') for lex in vocab: if lex.has_vector: vectors[lex.rank] = lex.vector return vectors def get_features(docs, max_length): Xs = numpy.zeros(len(list(docs)), max_length, dtype='int32') for i, doc in enumerate(docs): for j, token in enumerate(doc[:max_length]): Xs[i, j] = token.rank if token.has_vector else 0 return Xs For most applications, I recommend using pre-trained word embeddings without \"fine-tuning\". This means that you'll use the same embeddings across different models, and avoid learning adjustments to them on your training data. The embeddings table is large, and the values provided by the pre-trained vectors are already pretty good. Fine-tuning the embeddings table is therefore a waste of your \"parameter budget\". It's usually better to make your network larger some other way, e.g. by adding another LSTM layer, using attention mechanism, using character features, etc. Attribute hooks (experimental) Earlier, we saw how to store data in the new generic user_data dict. This generalises well, but it's not terribly satisfying. Ideally, we want to let the custom data drive more \"native\" behaviours. For instance, consider the .similarity() methods provided by spaCy's Doc, Token and Span objects: Polymorphic similarity examplespan.similarity(doc) token.similarity(span) doc1.similarity(doc2) By default, this just averages the vectors for each document, and computes their cosine. Obviously, spaCy should make it easy for you to install your own similarity model. This introduces a tricky design challenge. The current solution is to add three more dicts to the Doc object:Implementation noteThe hooks live on the Doc object because the Span and Token objects are created lazily, and don't own any data. They just proxy to their parent Doc. This turns out to be convenient here \u2014 we only have to worry about installing hooks in one place. Name Description user_hooks Customise behaviour of doc.vector, doc.has_vector, doc.vector_norm or doc.sents user_token_hooks Customise behaviour of token.similarity, token.vector, token.has_vector, token.vector_norm or token.conjuncts user_span_hooks Customise behaviour of span.similarity, span.vector, span.has_vector, span.vector_norm or span.root To sum up, here's an example of hooking in custom .similarity() methods: Add custom similarity hooksclass SimilarityModel(object): def __init__(self, model): self._model = model def __call__(self, doc): doc.user_hooks['similarity'] = self.similarity doc.user_span_hooks['similarity'] = self.similarity doc.user_token_hooks['similarity'] = self.similarity def similarity(self, obj1, obj2): y = self._model([obj1.vector, obj2.vector]) return float(y[0]) What's next? The attribute hooks are likely to evolve slightly, and will certainly need a little bit of tweaking to get fully consistent. I'm also looking forward to shipping improved models for the tagger, parser and entity recogniser. Over the last twelve months, research has shown that bidirectional LSTM models are a simple and effective approach for these tasks. The resulting models should also be significantly smaller in memory. Resources Full tutorial code (GitHub) spaCy v1.0 release notes (GitHub) Install spaCy v1.0 (spaCy Docs) Install Keras (Keras Docs) Papers describing Bidirectional LSTM-based models (Semantic Scholar) About the Author Matthew Honnibal Matthew is a leading expert in AI technology, known for his research, software and writings. He completed his PhD in 2009, and spent a further 5 years publishing research on state-of-the-art natural language understanding systems. Anticipating the AI boom, he left academia in 2014 to develop spaCy, an open-source library for industrial-strength NLP. Join our mailing list Stay in the loop! Sign up Explosion.ai Explosion AI is a digital studio specialising in Artificial Intelligence and Natural Language Processing. We design custom algorithms, applications and data assets. We're the makers of spaCy, the leading open source NLP library. Read more... Legal \/ Imprint Latest Blog Posts Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models The spaCy user survey: results and analysis Building your bot's brain with Node.js and spaCy spaCy v1.0: Deep Learning with custom pipelines and Keras ","flair":"two\tNews"}
{"author":"pddpro","created":"Tue Oct 04 22:12:19 EDT 2016","text":"Please use amodern browser with JavaScript enabled to use Coursera. 请下载现代的浏览器（IE10或Google Chrome）来使用Coursera。 تحميل Lädt... Chargement... Loading... Cargando... Carregando... Загрузка... Yükleniyor... 载入中 Please use amodern browser with JavaScript enabled to use Coursera. ","flair":"null\tnull"}
{"author":"Weihua99","created":"Sat Oct 01 16:28:00 EDT 2016","text":"A TensorFlow implementation of this Nvidia paper with some changes. Download the dataset and extract into the repository folder Use to train the model Use to run the model on a live webcam feed Use to run the model on the dataset To visualize training using Tensorboard use , then open http:\/\/0.0.0.0:6006\/ into your web browser.","flair":"null\tnull"}
{"author":"ZlatanIAm","created":"Mon Nov 07 11:42:33 EST 2016","text":"Assumptions: you have a TX1 with a fresh install of JetPack 2.3 L4T. First things first. We need to remove all the fat from the install. There are tons of optimized libraries in JetPack, but so much of it takes up the valuable memory space we need to get the facial recognition app up and running. Thankfully, others have paved the way and made these steps pretty much a walk in the park. Thank you to StackOverflow user, Dwight Crowe for his stellar post on how to get Tensorflow R0.9 working on a TX1. I'm literally just going to post his exact methodology. Here we need to make an edit so that the bazel build will recognize aarch64 as ARM Now it's time to compile bazel. Now we install tensorflow R0.9. Any higher than R0.9 and it requires bazel 0.3.0, which we didn't install. You will build tensorflow once and it will fail. But by building it with the failure, it gives you the bazel .cache dir you need to place updated config.guess and config.sub files necessary for the full installation. Download the proper config files and update the .cache dir Here is where things get a bit tricky. As Dwight suggests, you'll have to change a few files so that tensorflow compiles correctly. TX1 can't do fancy constructors in cwise_op_gpu_select.cu.cc or sparse_tensor_dense_matmul_op_gpu.cu.cc Running with CUDA 8.0 requires new macros for FP16. Dwight throws some thanks to Kashif\/Mrry for pointing out the fix, so I'm throwing some thanks to whoever those people are as well. And lastly, ARM has no NUMA nodes so this needs to be added or you will get an immediate crash on starting tf.Session() So I ran into strange errors that were solved by accident. After running the above commands, bazel fails in weird places. Sometimes at a random op. Sometimes a 'cross_tool' error. Truth be told, I accidently reran the command with a different job number and the op that it had failed on previously ended up compiling just fine. And that was it. Just changing the job number. I switched between 3 and 4 a few times and it compiled just fine. Very weird. But whatever. It works. Just to verify it, repeated this process on a few devices and it always works. Now that Tensorflow is installed, remove bazel and all of bazel's caches that eat memory. All we need here are the image reading and displaying opts. Nothing else. So the compile is small and takes up minimal space. Head over to David Sandberg's tensorflow implementation of OpenFace and download the resnet model weights in the Pre-trained model section. Then download the dlib facedetector from dlib.net. BOOM! Finished with downloads and installation. Now it's time to build our embedded face detector. First thing we need to do is copy align_dlib.py from here and make some quick changes. In the 'stock' version, it looks for the 'biggest' bounding box and only processes that one. But we're going to augment it so that it will classify all bounding boxes that it finds; eg every face will classified rather than just the largest. Also, we're going to make another quick change to the face detector based on the issues from this thread, whereby the detector shears the faces and warps them slightly. It should be noted that while David Sandberg uses a version of MTCNN to detect faces, we have to use the augmented dlib version. This is done so that when the final detection system is running, the memory profile doesn't get out of wack and spontaneously kill our processes. By changing the face detector, there will be an effect on the overall detection accuracy of our system, but the difference will be minimal. The second thing we need to do is build a scanner to identify the faces you actually want to classify. One thing to note is that with the Jetson, using the camera with OpenCV can be tricky. We need to make sure open the video with this prompt in our call via OpenCV: \"\" Here's our script called scan.py. Most open source facial recognition libraries like OpenFace, home_surveillance, facenet, etc. use the model similar to the one outlined in the FaceNet paper written by Florian Schroff, Dmitry Kalenichenko, James Philbin. Here' we're no different and will be using the model that David Sandberg's facenet.uses. The model works to take an image of an individuals face and pass it through a network (the model David uses is a variant of Inception-Resnet). The goal is to make the network embed the image in a feature space so that the squared distance between images of the same identity is small and the squared distance between a pair of images from different identities is large. This is done using something called a Triplet Loss. It's probably the one of the single-most important feature of the model's structure. Rather than break down the entire model, I just want to mention what made this model stick out for me: The goal here is to promote an embedding scheme that enforces a margin between each face pair of one identity to that of all other identities. In order to ensure that the network learns properly, triplets are selected in such a way that during the forward pass, negative samples are selected in an online fashion from the current minibatch. The authors note that selecting very distant negatives can lead to a bad local minima early on, so they instead select negatives so that their distance is further away from the image's embedding than the positive example, but are still meaningful because the squared distance is close to the anchor positive distance. Thus resulting in negatives that lie inside the margin alpha and help avoid a collapsed model. TL;DR: Read the paper. It's worth it. After you've scanned the faces you want to via the TX1's camera, we're going to want to put something together to actually classify faces. This script takes on concepts from openface's web-demo as well as facenet's validate_on_lfw.py. So what's going on here? Welp, we want to train a model from all images within our training set from scan.py, then use the facenet model to build a representation of each image. After each image has been processed via the network, we train an SVM on their representations and we teach it to classify a person's processed image correctly. That trained SVM is then used to classify all the faces that the camera sees. Boomshakalaka. And there you have it. A few simple scripts, and you have an embedded detector up and running. Because everyone likes a demo in camera vertical (...a little lag due to tunneling X over ssh)","flair":"four\tProject"}
{"author":"hardmaru","created":"Wed Sep 28 14:34:37 EDT 2016","text":"In this post, I introduce and discuss binary stochastic neurons, implement trainable binary stochastic neurons in Tensorflow, and conduct several simple experiments on the MNIST dataset to get a feel for their behavior. Binary stochastic neurons offer two advantages over real-valued neurons: they can act as a regularizer and they enable conditional computation by enabling a network to make yes\/no decisions. Conditional computation opens the door to new and exciting neural network architectures, such as the choice of experts architecture and heirarchical multiscale neural networks, which I plan to discuss in future posts. If you\u2019d prefer to read this as an IPython notebook, I\u2019ve uploaded it here. A binary stochastic neuron is a neuron with a noisy output: some proportion \\(p\\) of the time it outputs 1, otherwise 0. An easy way to turn a real-valued input, \\(a\\), into this proportion, \\(p\\), is to set \\(p = \\text{sigm}(a)\\), where \\(\\text{sigm}\\) is the logistic sigmoid, \\(\\text{sigm}(x) = \\frac{1}{1 + \\exp(-x)}\\). Thus, we define the binary stochastic neuron, \\(\\text{BSN}\\), as: where \\(\\textbf{1}_{x}\\) is the indicator function on the truth value of \\(x\\) and \\(z \\sim U[0,1]\\). For any single trial, the binary stochastic neuron generally has a derivative of 0 and cannot be trained by simple backpropagation. To see this, consider that if \\(z \\neq \\text{sigm}(a)\\) in the \\(\\text{BSN}\\) function above, there exists a neighborhood around \\(a\\) such that the output of \\(\\text{BSN}(a)\\) is unchanged (i.e., the derivative is 0). We get around this by estimating the derivative with respect to the expected loss, rather than calculating the derivative with respect to the outcome of a single trial. We can only estimate this derivative, because in any given trial, we only see the loss value with respect to the given noise \u2013 we don\u2019t know what the loss would have been given another level of noise. We call a method that provides such an estimate an \u201Cestimator\u201D. An estimator is unbiased if the expectation of its estimate equals the expectation of the derivative it is estimating; otherwise, it is biased. In this post we implement the two estimators discussed in Bengio et al. (2013): The tricky part of implementing a binary stochastic neuron in Tensorflow is not the forward computation, but the implementation of the REINFORCE and straight through estimators. Each requires replacing the gradient of one or more Tensorflow operations. The official approach to this is to write a new op in C++, which seems wholly unnecessary. There are, however, two workable unofficial approaches, one of which is a trick credited to Sergey Ioffe, and another that uses , an experimental feature of Tensorflow that is documented here. We will use , which works well for our purposes. We\u2019ve now set up a good foundation from which we can run a number of simple experiments. The experiments are as follows: Recall that the REINFORCE estimator estimates the expectation of \\(\\frac{\\partial L}{\\partial a}\\) as \\((\\text{BSN}(a) - \\text{sigm}(a))(L - c)\\), where \\(c\\) is a constant. The non-variance-adjusted form of REINFORCE uses \\(c = 0\\), whereas the variance-adjusted form uses the variance minimizing result stated above. Naturally we should prefer the least variance, and the experimental results below agree. It seems that both forms of REINFORCE often break down for learning rates greater than or equal to 0.3 (compare to the learning rate of 1.0 that used in Experiment 0). After a few trials, variance-adjusted REINFORCE appears to be more resistant to such failures. In terms of performance at lower learning rates, a learning rate of about 0.05 provided the best results. The results show that the variance-adjusted REINFORCE learns faster, but that its non-variance adjusted eventually catches up. This result is consistent with the mathematical result that they are both unbiased estimators. Performance is predictably worse than it was for the plain logistic sigmoid in Experiment 0, although there is almost no generalization gap, consistent with the hypothesis that binary stochastic neurons can act as regularizers. Recall that one variant of the straight-through estimator uses the identity function as the backpropagated gradient when the neuron emits a 1 (pass-through), and another variance multiplies that by the gradient of the logistic sigmoid that the neuron calculates (sigmoid-adjusted). In Bengio et al. (2013), it was remarked that, surprisingly, the former performs better. My results below agree; we see that the pass-through variant edges out the sigmoid-adjusted variant at higher learning rates, and interestingly, at a learning rate of 1, the sigmoid-adjusted variant performs very poorly and fails to converge. Recall that Chung et al. (2016) improves upon the sigmoid-adjusted variant of the ST estimator by using the slope-annealing trick, which slowly increases the slope of the logistic sigmoid as training progresses. Using the slope-annealing trick with an annealing rate of 1.1 times per epoch (so the slope at epoch 20 is \\(1.1^{19} \\approx 6.1\\)), we\u2019re able to match and even surpass the results of the pass-through ST estimator. The slope-annealed estimator is still more sensitive to the learning rate (performing similarly to the sigmoid-adjusted variant at a learning rate of 1.0), but it appears that it can be fine tuned to perform better than the pass-through variant. Note that the slope annealed neuron used here is not the same as the one used by Chung et al. (2016), who employ a deterministic step function and use a hard sigmoid in place of a sigmoid for the backpropagation. We now directly compare the variance-adjusted REINFORCE and slope-annealed ST, both at their best learning rates. In this setting, despite being a biased estimator, the straight-through estimator displays faster learning, less variance, and better overall results than the variance-adjusted REINFORCE estimator. Similar to how dropout is not applied at inference when using dropout for training, it makes sense that we might replace the stochastic sigmoid with a deterministic step function at inference when using binary neurons. We might go even further than that, and use deterministic neurons during training, which is the approach taken by Chung et al. (2016). The following three combinations are compared below, using the pass-through straight through estimator, without slope annealing: The results show that stochastic inference and deterministic inference, when combined with stochastic training, are closely comparable (with the latter having a slight edge), but that deterministic training lags behind stochastic training. Similar results hold for the REINFORCE estimator. The introduction of slope annealing, however, as per Chung et al. (2016), closes the gap between stochastic and deterministic training. Although stochastic and deterministic training converge to a similar result, deterministic training results in faster training early on. Note that the slope annealed neuron used here is not exactly the same as the one used by Chung et al. (2016); this one uses a regular sigmoid, whereas Chung et al. use a hard sigmoid. Next, I look at how each estimator interacts with depth. From a theoretical perpective, there is reason to think the straight-through estimator will suffer from depth; as noted by Bengio et al. (2013), it is not even guaranteed to have the same sign as the expected gradient during backpropagation. It turns out that if we keep the learning rate constant, both estimators start to fail as we increase depth. However, if we lower the learning rate dramatically (300x for the ST estimator and 25x for the REINFORCE estimator), we can start to get the deeper networks to train. In constrast with the results of earlier experiments, the bias of the straight through estimator starts to show and the REINFORCE estimator is the clear winner at higher depths. I now test the \u201Cunpublished result\u201D put forth at the end of Hinton et al.\u2019s Coursera Lecture 9c, which states that we can improve upon the performance of an overfitting multi-layer sigmoid net by turning its neurons binary stochastic neurons with a straight-through estimator. Note that Hinton states that the binary stochastic net might take several times longer to train, which we\u2019ll see is true. To test the claim, we will need a dataset that is easier to overfit than MNIST, and so the following experiment uses the MNIST validation set for training (10x smaller than the MNIST training set and therefore much easier to overfit). The hidden layer size is also increased by a factor of 10 to increase overfitting. It took me quite a bit of playing with hyperparameters to get this result, but we can see below that the stochastic net has a clear advantage in terms of the generalization gap and results in a better final fit. Note that 60 epochs are trained below in order to give the stochastic net time to catch up. When testing with a deeper architecture (i.e., 2 hidden layers), I was able to match but not beat the performance of a sigmoidal network with a stochastic one, and this was after 200 epochs of training. Given that I had to run this experiment several times to cook up this result, I see no compelling reason to use binary stochastic neurons over other methods (dropout, weight decay or weight noise) for purposes of regularization. In this post we introduced, implemented and experimented with binary stochastic neurons in Tensorflow. We saw that with a single binary stochastic layer, the biased straight-through estimator outperforms the unbiased REINFORCE estimator, but that with deeper stochastic architectures, the REINFORCE estimator does better. We explored the variants of each estimator, and showed that the slope-annealed straight through estimator is better than other straight through variants, and that it is worth using the variance-adjusted REINFORCE estimator over the not variance-adjusted REINFORCE estimator. Finally, we explored the potential use for binary stochastic neurons as regularizers, and concluded that it is probably not worth the effort. In future posts, I will look at how we can take advantage of binary stochastic neurons to make hard decisions within our neural network architecture. This opens the door to conditional computation and some interesting new architectures such as the choice of experts and heirarchichal multiscale RNN architectures.","flair":"null\tnull"}
{"author":"poorasian","created":"Sun Oct 09 15:25:09 EDT 2016","text":"Can't seem to find anything. Most I've found is some guy's blog who wrote something for sympy a few years ago but I can't seem to get his code to run: https:\/\/zulko.wordpress.com\/2012\/04\/15\/symbolic-matrix-differentiation-with-sympy\/\n\nIdeally it would be able to do at least some of the stuff in the matrix cookbook (https:\/\/www.math.uwaterloo.ca\/~hwolkowi\/matrixcookbook.pdf) and then be able to apply stuff like the chain rule and product rule.","flair":"one\tDiscussion"}
{"author":"rd11235","created":"Wed Nov 09 12:56:45 EST 2016","text":"From section 5.1:\n\n\"We use orthogonal initialization for all weight matrices, except for the hidden-to-hidden weight matrix which we initialize to be the identity matrix, as this yields better generalization performance on this task for both models.\"\n\nThis means that they used \"orthogonal initialization\" for the input-to-state matrix, which isn't square. Anyone know what they're doing here? My first guess would be to choose a random matrix that has all singular values of 1, but this doesn't seem to make any sense (for example it wouldn't even be length preserving).","flair":"one\tDiscussion"}
{"author":"foboi1122","created":"Sat Oct 01 05:22:55 EDT 2016","text":"I was reading [this](https:\/\/arxiv.org\/pdf\/1602.02282.pdf) paper and I noticed they used two layers for encoder\/decoder, but up to five layers for latent variables. \n\nMy question is how do you calculate loss for more than one latent later? Do you just tack an extra KL Divergence to the loss function for each additional latent layer?","flair":"null\tnull"}
{"author":"kmike84","created":"Thu Nov 24 13:53:53 EST 2016","text":"https:\/\/github.com\/TeamHG-Memex\/eli5\n\nCurrently it supports scikit-learn, lightning and sklearn-crfsuite; it also contains an alternative implementation of LIME algorithm, so it can be used to explain predictions of black-box algorithms as well (the feature is experimental though). Focus is on text data, but other data is also supported; there are plans to add support for more ML frameworks and expand it to images. The best place to get started is the [overview](http:\/\/eli5.readthedocs.io\/en\/latest\/overview.html) and a [tutorial](http:\/\/eli5.readthedocs.io\/en\/latest\/tutorials\/sklearn-text.html).\n\nFeedback is welcome!","flair":"four\tProject"}
{"author":"gabrielgoh","created":"Tue Oct 11 12:18:23 EDT 2016","text":"A Role for Topology in Data Science: The mathematical discipline of topology offers a new approach to data analysis that is especially important in today\u2019s world of complex, high-dimensional, noisy data. Topological methods extend and enhance traditional machine learning techniques to enable more nuanced data exploration, more refined segmentation, and more effective modeling. This talk will describe a topological method for detecting the underlying shape of any dataset, then give examples applying this technique in practice.","flair":"three\tResearch"}
{"author":"flukeskywalker","created":"Fri Oct 28 06:43:14 EDT 2016","text":"This repository contains code accompanying the paper Recurrent Highway Networks (RHNs). RHNs are an extension of Long Short Term Memory Networks with forget gates to enable the learning of deep recurrent state transitions. We provide implementations in Tensorflow, Torch7 and Brainstorm libraries, and welcome additional implementations from the community. The recurrent state transition in typical recurrent networks is modeled with a single step non-linear function. This can be very inefficient in principle for modeling complicated transitions, requiring very large networks. Increased recurrence depth allows RHNs to model complex transitions more efficiently achieving substantially improved results. Moreover, using depth d in the recurrent state transition is much more powerful than stacking d recurrent layers. The figures below illustrate that if we consider the functions mapping one hidden state to another T time steps apart, its maximum depth scales as the product of d and T instead of the sum. Of course, in general RHNs can also be stacked to get the best of both worlds. The score (perplexity) of word-level language models on the Penn Treebank dataset dramatically improves as recurrence depth increases while keeping the model size fixed. WT refers to tying the input and output weights for regularization. See the paper by Inan and Khosravi for the original idea, their paper for a polished version of the original idea and the paper by Press and Wolf, published a month after the original idea, for details. *We used 1000 samples for MC dropout as done by Gal for LSTMs, but we've only evaluated the depth 5 model so far. Tensorflow code for RHNs is built by heavily extending the LSTM language modeling example provided in Tensorflow. It supports Variational RHNs as used in the paper, which use the same dropout mask at each time step and at all layers inside the recurrence. Note that this implementation uses the same dropout mask for both the H and T non-linear transforms in RHNs while the Torch7 implementation uses different dropout masks for different transformations. The Theano implementation can be configured either way. We recommend installing Tensorflow in a virtual environment. In addition to the usual Tensorflow dependencies, the code uses Sacred so you need to do: To reproduce SOTA results on enwik8 (Wikipedia), first download the dataset from http:\/\/mattmahoney.net\/dc\/enwik8.zip and unzip it into the directory, then run: This is a Sacred experiment, so you check the hyperparameter options using the command, e.g. Torch7 code is based on Yarin Gal's adaptation of Wojciech Zaremba's code implementing variational dropout. The main additions to Gal's code are the Recurrent Highway Network layer, the initial biasing of T-gate activations to facilitate learning and a few adjustments to other network parameters such as and dropout probabilities. We recommend installing Torch from the official website. To ensure the code runs some packages need to be installed: To run on the enwik8 dataset, first download and prepare the data (see data\/README for details): Then you can train by running: The Theano code's configuration and usage is similar to that of the Tensorflow code. In this implementation two configuration options were added: As with the Tensorflow code, the SOTA results on Penn Treebank and on enwik8 (Wikipedia) can be reproduced: An RHN layer implementation is also provided in Brainstorm. This implementation does not use variational dropout. It can be used in a Brainstorm experiment by simply importing from brainstorm_rhn.py. If you use RHNs in your work, please cite us:","flair":"four\tProject"}
{"author":"anantzoid","created":"Sat Oct 29 05:44:19 EDT 2016","text":"Example #1 \u2014 Old Station: view comparison in 24-bit HD, original photo CC-BY-SA @siv-athens. As seen on TV! What if you could increase the resolution of your photos using technology from CSI laboratories? Thanks to deep learning and , it's now possible to train a neural network to zoom in to your images at 2x or even 4x. You'll get even better results by increasing the number of neurons or training with a dataset similar to your low resolution image. The catch? The neural network is hallucinating details based on its training from example images. It's not reconstructing your photo exactly as it would have been if it was HD. That's only possible in Hollywood \u2014 but using deep learning as \"Creative AI\" works and it is just as cool! Here's how you can get started... The main script is called , which you can run with Python 3.4+ once it's setup as below. The argument that lets you specify which GPU or CPU to use. For the samples above, here are the performance results: The default is to use , if you have NVIDIA card setup with CUDA already try . On the CPU, you can also set environment variable to , which is most useful when running the script multiple times in parallel. A list of example command lines you can use with the pre-trained models provided in the GitHub releases: Here's a list of currently supported models, image types, and zoom levels in one table. Pre-trained models are provided in the GitHub releases. Training your own is a delicate process that may require you to pick parameters based on your image dataset. The easiest way to get up-and-running is to install Docker. Then, you should be able to download and run the pre-built image using the command line tool. Find out more about the image on its Docker Hub page. Here's the simplest way you can call the script using , assuming you're familiar with using argument to mount folders you can use this directly to specify files to enhance: Single Image \u2014 In practice, we suggest you setup an alias called to automatically expose the folder containing your specified image, so the script can read it and store results where you can access them. This is how you can do it in your terminal console on OSX or Linux: Multiple Images \u2014 To enhance multiple images in a row (faster) from a folder or wildcard specification, make sure to quote the argument to the alias command: If you want to run on your NVIDIA GPU, you can instead change the alias to use the image which comes with CUDA and CUDNN pre-installed. Then run it within nvidia-docker and it should use your physical hardware! This project requires Python 3.4+ and you'll also need and (numerical computing libraries) as well as installed system-wide. If you want more detailed instructions, follow these: Afterward fetching the repository, you can run the following commands from your terminal to setup a local environment: After this, you should have , and installed in your virtual environment. You'll also need to download this pre-trained neural network (VGG19, 80Mb) and put it in the same folder as the script to run. To de-install everything, you can just delete the folder. Example #3 \u2014 Specialized super-resolution for faces, trained on HD examples of celebrity faces only. The quality is significantly higher when narrowing the domain from \"photos\" in general. This code uses a combination of techniques from the following papers, as well as some minor improvements yet to be documented (watch this repository for updates): Special thanks for their help and support in various ways: There's a Python extension compiler called Cython, and it's missing or improperly installed. Try getting it directly from the system package manager rather than PIP. This happens when you're running without a GPU, and the CPU libraries were not found (e.g. ). The neural network expressions cannot be evaluated by Theano and it's raising an exception. You need to install Lasagne and Theano directly from the versions specified in , rather than from the PIP versions. These alternatives are older and don't have the required features. It seems your terminal is misconfigured and not compatible with the way Python treats locales. You may need to change this in your or other startup script. Alternatively, this command will fix it once for this shell instance.","flair":"null\tnull"}
{"author":"Indy20161","created":"Wed Nov 16 03:31:40 EST 2016","text":"Someone(without phd) with publications at top conferences(eg NIPS,ICML for someone in ML) is considered equivalent to someone holding a phd(eg in ML)? If yes, is it possible to do so(publish at top conferences without phd)? Has anyone done this?","flair":"one\tDiscussion"}
{"author":"Guanoco","created":"Sun Oct 16 11:26:05 EDT 2016","text":"Hello all.\n\nI want to be able to classify a time series of the response of a system when it is turned on and until it reaches some steady state.\n\nNow I have time series of different types of systems (let's say of different quality of the systems).\nI want to be able to predict the quality of the system based on the time series.\n\nI know I could use some feature engineering, but I would much rather not have to. \nThe other catches are\n1)time series are not necessarily of the same length between quality classes and even in the same class\n2)i cannot standardize the time series to make them zero mean unit variance\n3)I have less than 500 time series for 2 classes","flair":"one\tDiscussion"}
{"author":"Lajamerr_Mittesdine","created":"Tue Sep 27 18:39:04 EDT 2016","text":" Google Research Blog The latest news from Research at Google A Neural Network for Machine Translation, at Production Scale Tuesday, September 27, 2016 Posted by Quoc V. Le & Mike Schuster, Research Scientists, Google Brain Team Ten years ago, we announced the launch of Google Translate, together with the use of Phrase-Based Machine Translation as the key algorithm behind this service. Since then, rapid advances in machine intelligence have improved our speech recognition and image recognition capabilities, but improving machine translation remains a challenging goal. Today we announce the Google Neural Machine Translation system (GNMT), which utilizes state-of-the-art training techniques to achieve the largest improvements to date for machine translation quality. Our full research results are described in a new technical report we are releasing today: \u201CGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\u201D [1]. A few years ago we started using Recurrent Neural Networks (RNNs) to directly learn the mapping between an input sequence (e.g. a sentence in one language) to an output sequence (that same sentence in another language) [2]. Whereas Phrase-Based Machine Translation (PBMT) breaks an input sentence into words and phrases to be translated largely independently, Neural Machine Translation (NMT) considers the entire input sentence as a unit for translation.The advantage of this approach is that it requires fewer engineering design choices than previous Phrase-Based translation systems. When it first came out, NMT showed equivalent accuracy with existing Phrase-Based translation systems on modest-sized public benchmark data sets. Since then, researchers have proposed many techniques to improve NMT, including work on handling rare words by mimicking an external alignment model [3], using attention to align input words and output words [4] and breaking words into smaller units to cope with rare words [5,6]. Despite these improvements, NMT wasn't fast or accurate enough to be used in a production system, such as Google Translate. Our new paper [1] describes how we overcame the many challenges to make NMT work on very large data sets and built a system that is sufficiently fast and accurate enough to provide better translations for Google\u2019s users and services. Data from side-by-side evaluations, where human raters compare the quality of translations for a given source sentence. Scores range from 0 to 6, with 0 meaning \u201Ccompletely nonsense translation\u201D, and 6 meaning \u201Cperfect translation.\" The following visualization shows the progression of GNMT as it translates a Chinese sentence to English. First, the network encodes the Chinese words as a list of vectors, where each vector represents the meaning of all words read so far (\u201CEncoder\u201D). Once the entire sentence is read, the decoder begins, generating the English sentence one word at a time (\u201CDecoder\u201D). To generate the translated word at each step, the decoder pays attention to a weighted distribution over the encoded Chinese vectors most relevant to generate the English word (\u201CAttention\u201D; the blue link transparency represents how much the decoder pays attention to an encoded word). Using human-rated side-by-side comparison as a metric, the GNMT system produces translations that are vastly improved compared to the previous phrase-based production system. GNMT reduces translation errors by more than 55%-85% on several major language pairs measured on sampled sentences from Wikipedia and news websites with the help of bilingual human raters. An example of a translation produced by our system for an input sentence sampled from a news site. Go here for more examples of translations for input sentences sampled randomly from news sites and books. In addition to releasing this research paper today, we are announcing the launch of GNMT in production on a notoriously difficult language pair: Chinese to English. The Google Translate mobile and web apps are now using GNMT for 100% of machine translations from Chinese to English\u2014about 18 million translations per day. The production deployment of GNMT was made possible by use of our publicly available machine learning toolkit TensorFlow and our Tensor Processing Units (TPUs), which provide sufficient computational power to deploy these powerful GNMT models while meeting the stringent latency requirements of the Google Translate product. Translating from Chinese to English is one of the more than 10,000 language pairs supported by Google Translate, and we will be working to roll out GNMT to many more of these over the coming months. Machine translation is by no means solved. GNMT can still make significant errors that a human translator would never make, like dropping words and mistranslating proper names or rare terms, and translating sentences in isolation rather than considering the context of the paragraph or page. There is still a lot of work we can do to serve our users better. However, GNMT represents a significant milestone. We would like to celebrate it with the many researchers and engineers\u2014both within Google and the wider community\u2014who have contributed to this direction of research in the past few years. Acknowledgements: We thank members of the Google Brain team and the Google Translate team for the help with the project. We thank Nikhil Thorat and the Big Picture team for the visualization. References: [1] Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean. Technical Report, 2016. [2] Sequence to Sequence Learning with Neural Networks, Ilya Sutskever, Oriol Vinyals, Quoc V. Le. Advances in Neural Information Processing Systems, 2014. [3] Addressing the rare word problem in neural machine translation, Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics, 2015. [4] Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. International Conference on Learning Representations, 2015. [5] Japanese and Korean voice search, Mike Schuster, and Kaisuke Nakajima. IEEE International Conference on Acoustics, Speech and Signal Processing, 2012. [6] Neural Machine Translation of Rare Words with Subword Units, Rico Sennrich, Barry Haddow, Alexandra Birch. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016. Google Labels: Google Brain , Google Translate , Machine Learning , Machine Translation , TensorFlow    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"TheWittyCat","created":"Wed Nov 16 17:02:36 EST 2016","text":"https:\/\/github.com\/frownyface\/dnc\n\nThis is my first time implementing a paper - would like thoughts and critiques about code style and actual implementation. Also if anyone has an insight for the TODO section, it would be greatly appreciated.\n\nRe-submit to follow rules and see if it garners any attention. ","flair":"four\tProject"}
{"author":"spruceabtuse","created":"Fri Nov 18 18:46:20 EST 2016","text":"SAN FRANCISCO\u2014Intel Corp. to intensify its efforts in a hot new computing market, is taking the unusual step of relying on technology developed by a startup it acquired. The chip maker, determined to counter rival Nvidia Corp.\u2019s lead in the field known as deep learning, said it would begin shipping chips in 2017 that it acquired through its purchase earlier this year of startup Nervana Systems. Intel said it plans to integrate...","flair":"two\tNews"}
{"author":"davis685","created":"Tue Oct 11 07:03:34 EDT 2016","text":" Tuesday, October 11, 2016 Easily Create High Quality Object Detectors with Deep Learning A few years ago I added an implementation of the max-margin object-detection algorithm (MMOD) to dlib. This tool has since become quite popular as it frees the user from tedious tasks like hard negative mining.  You simply label things in images and it learns to detect them.  It also produces high quality detectors from relatively small amounts of training data.  For instance, one of dlib's example programs shows MMOD learning a serviceable face detector from only 4 images. However, the MMOD implementation in dlib used HOG feature extraction followed by a single linear filter. This means it's incapable of learning to detect objects that exhibit complex pose variation or have a lot of other variability in how they appear.  To get around this, users typically train multiple detectors, one for each pose.  That works OK in many cases but isn't a really good general solution. Fortunately, over the last few years convolutional neural networks have proven themselves to be capable of dealing with all these issues within a single model. So the obvious thing to do was to add an implementation of MMOD with the HOG feature extraction replaced with a convolutional neural network.  The new version of dlib, v19.2, contains just such a thing.  On this page you can see a short tutorial showing how to train a convolutional neural network using the MMOD loss function.  It uses dlib's new deep learning API to train the detector end-to-end on the very same 4 image dataset used in the HOG version of the example program.  Happily, and very much to the surprise of myself and my colleagues, it learns a working face detector from this tiny dataset. Here is the detector run over an image not in the training data: I expected the CNN version of MMOD to inherit the low training data requirements of the HOG version of MMOD, but working with only 4 training images is very surprising considering other deep learning methods typically require many thousands of images to produce any kind of sensible results. The detector is also reasonably fast for a CNN.  On the CPU, it takes about 370ms to process a 640x480 image.  On my NVIDIA Titan X GPU (the Maxwell version, not the newer Pascal version) it takes 45ms to process an image when images are processed one at a time.  If I group the images into batches then it takes about 18ms per image. To really test the new CNN version of MMOD, I ran it through the leading face detection benchmark, FDDB.  This benchmark has two modes, 10-fold cross-validation and unrestricted.  Both test on the same dataset, but in the 10-fold cross-validation mode you are only allowed to train on data in the FDDB dataset.  In the unrestricted mode you can train on any data you like so long as it doesn't include images from FDDB.  I ran the 10-fold cross-validation version of the FDDB challenge.  This means I trained 10 CNN face detectors, each on 9 folds and tested on the held out 10th.  I did not perform any hyper parameter tuning.  Then I ran the results through the FDDB evaluation software and got this plot: The X axis is the number of false alarms produced over the entire 2845 image dataset.  The Y axis is recall, i.e. the fraction of faces found by the detector. The green curve is the new dlib detector, which in this mode only gets about 4600 faces to train on. The red curve is the old Viola Jones detector which is still popular (although it shouldn't be, obviously). Most interestingly, the blue curve is a state-of-the-art result from the paper Face Detection with the Faster R-CNN, published only 4 months ago. In that paper, they train their detector on the very large WIDER dataset, which consists of 159,424 faces, and arguably get worse results on FDDB than the dlib detector trained on only 4600 faces. As another test, I created the dog hipsterizer, which I made a post about a few days ago.  The hipsterizer used the exact same code and parameter settings to train a dog head detector.  The only difference was the training data consisted in 9240 dog heads instead of human faces.  That produced the very high quality models used in the hipsterizer.  So now we can automatically create fantastic images such as this one :) Barkhaus dogs looking fancy As one last test of the new CNN MMOD tool I made a dataset of 6975 faces.  This dataset is a collection of face images selected from many publicly available datasets (excluding the FDDB dataset).  In particular, there are images from ImageNet, AFLW, Pascal VOC, the VGG dataset, WIDER, and face scrub.  Unlike FDDB, this new dataset contains faces in a wide range of poses rather than consisting of mostly front facing shots.  To give you an idea of what it looks like, here are all the faces in the dataset tightly cropped and tiled into one big image: Using the new dlib tooling I trained a CNN on this dataset using the same exact code and parameter settings as used by the dog hipsterizer and previous FDDB experiment. If you want to run that CNN on your own images you can use this example program. I tested this CNN on FDDB's unrestricted protocol and found that it has a recall of 0.879134, which is quite good. However, it produced 90 false alarms.  Which sounds bad, until you look at them and find that it's finding labeling errors in FDDB.  The following image shows all the \"false alarms\" it outputs on FDDB.  All but one of them are actually faces. Finally, to give you a more visceral idea of the difference in capability between the new CNN detector and the old HOG detector, here are a few images where I ran dlib's default HOG face detector (which is actually 5 HOG models) and the new CNN face detector. The red boxes are CNN detections and blue boxes are from the older HOG detector. While the HOG detector does an excellent job on easy faces looking at the camera, you can see that the CNN is way better at handling not just the easy cases but all faces in general.  And yes, I ran the HOG detector on all the images, it's just that it fails to find any faces in some of them. Posted by Davis King at 6:59 AM 95 comments : Joseph Kogut said... That's quite impressive, nice work. Any plans for a Python binding? October 11, 2016 at 11:28 AM dasinesnaps© said... Wow. Really good work. October 11, 2016 at 2:24 PM Davis King said... The API makes heavy use of C++11, so no general Python binding is possible. Although I might make very narrow bindings to specific models at some point. October 11, 2016 at 3:03 PM Moji said... Awesome job!! October 11, 2016 at 3:24 PM Davis King said... Thanks! :) October 11, 2016 at 3:25 PM ngap wei Tham said... Thanks, it is awesome!!! This is a good deep learning library for c++ programmers, it is easy to use(nice, clean api), support gpgpu, provides nice examples and ease to build, it is like a dream come true. October 12, 2016 at 1:31 AM Kyle McDonald said... This is incredible, thanks for sharing your great work! I think I see two non-faces, the \"4\" jersey, but also the square just above it. I was about to try this out, but I'm having some growing pain building dlib on my OS X 10.11 laptop (1. using Anaconda creates X11 problems, I think I figured that out 2. I got CUDA 8.0 built with Xcode 7.3 and I've been using it Python, dlib finds CUDA, but says it can't compile anything and can't find cuDNN). If I have time to dig into these issues more and can provide more precise info I'll post an issue on GitHub. October 12, 2016 at 9:21 AM Davis King said... Thanks. The thing above the 4 is a wooden carving of some guy's head, which is a little hard to see from the crop. October 12, 2016 at 9:45 AM HChu said... Thanks for the latest update. Finally able to run the deep learning example with Visual Studio 2015. The original file names from MNIST data set contain '.' while you hard coded '-'. October 12, 2016 at 1:38 PM Davis King said... No problem. I'm looking at the MNIST site (http:\/\/yann.lecun.com\/exdb\/mnist\/) and the files don't have any . in them. October 12, 2016 at 1:57 PM MeTroFuN said... What is the test time performance compared to HOG based detector? October 12, 2016 at 3:19 PM Davis King said... If you run the CNN on the GPU then it's about the same speed as HOG (which only runs on CPU) October 12, 2016 at 3:21 PM florisdesmedt said... Great work!! I have a question about the receptive field you mention in the code comment. You mention it is a little above 50x50, but I'm seem not able to reproduce this size, I end up with a receptive field of 117x117 pixels. Best regards October 13, 2016 at 3:24 AM Davis King said... Well, the first few layers downsample the image by a factor of 8 so you can think of it as turning it into 8x8 cells. I realize the cells have receptive fields that are larger than 8x8, but most of the action of their focus is in a smaller area. Then the detector pulls a 6x6 grid of them at the end, after some filtering. I should probably change the wording of the example to be a little less confusing :) October 13, 2016 at 6:59 AM florisdesmedt said... Thank you for the explanation, it helps my understanding to how to change the network for other objects. The results of experiments I did so far with the face model are remarkable. Keep on the good work ;). I have another question though. I am trying to train a pedestrian model of 22x50. I have altered all of the annotations to more or less this aspect ratio (rounded dimention values). When training with these annotations, I get the error \"Encountered a ground truth rectangle with a width and height of 71 and 90...aspect ratio is to different from the detection window\", although these dimentions are not present in the annotation file. Are the ground truth annotations cropped somehow when they overlap with other annotations or the image boundary maybe? And will this problem still appear for 'ignored' annotations? Best regards October 13, 2016 at 3:49 PM Davis King said... No problem. Glad you like dlib :) No, there isn't any cropping or anything like that. You must have a box in your dataset that you don't expect. If that error says it's there then it's there. October 13, 2016 at 3:54 PM Unknown said... Hi, I just finished reading your MMOD paper and it is a brilliant concept. I am curious how the margin based optimization of MMOD is combined with gradient descent based optimization of neural nets. Is there more material about the convnet version I can read somewhere ? October 14, 2016 at 10:09 AM Davis King said... Thanks. The convnet version literally just optimizes the same objective function as in the MMOD paper. Except instead of using a cutting plane solver it uses vanilla SGD. October 14, 2016 at 10:30 AM Rastaxe said... Great work! I am trying your code with CUDA 8.0 (Ubuntu 16.04 - GeForce GT 520), but when I ran the dnn_mmod_ex program I got this error: Error while calling cudnnCreate(&handles[new_device_id]) in file \/home\/ale\/dlib-19.2\/dlib\/dnn\/cudnn_dlibapi.cpp:102. code: 6, reason: A call to cuDNN failed Can you help me to solve this issue? Thanks! October 14, 2016 at 11:43 AM Davis King said... I don't know what's wrong. You probably have something wrong with your computer or cuda install. October 14, 2016 at 12:01 PM Matt said... @florisdesmedt -- I saw the same issue when training an object detector that is rectangular (2x1 aspect ratio). I believe the issue is the random crop rotation. After rotation (e.g., at a 45 degree angle) the region is no longer rectangular. It's basically at a 1x1 aspect ratio, and the training fails. This isn't really a problem for square things, since 45 degree rotation, and it's still within the tolerance. I reduced the random crop rotation to 5 degrees (from default 30) and it worked: cropper.set_max_rotation_degrees(0); October 14, 2016 at 4:58 PM Unknown said... @ Davis, great job. I see that you use \"num of false alarms v.s. recall\" figure to compare fasterRCNN, dlib MMOD CNN, and Violajones. May I know if you have similar figures to compare fasterRCNN, dlib MMOD CNN and dlib HOG? October 15, 2016 at 3:20 AM Unknown said... @ Davis, great job. I see that you use \"num of false alarms v.s. recall\" figure to compare fasterRCNN, dlib MMOD CNN, and Violajones. May I know if you have similar figures to compare fasterRCNN, dlib MMOD CNN and dlib HOG? October 15, 2016 at 3:20 AM Peter Chen said... @ Davis, great job. I see that you use \"num of false alarms v.s. recall\" figure to compare fasterRCNN, dlib MMOD CNN, and Violajones. May I know if you have similar figures to compare fasterRCNN, dlib MMOD CNN and dlib HOG? October 15, 2016 at 3:21 AM Davis King said... The MMOD paper has some more such plots on the FDDB dataset, including one for HOG. October 15, 2016 at 7:51 AM opterix said... This is awesome!! I tried the dog hipsterizer with CPU mode and it works pretty well... I was trying to train a detector from scratch. I compiled dlib with cudnn library I used: > g++ -std=c++11 -O3 -I.. ..\/dlib\/all\/source.cpp -lpthread -lX11 -lcudnn -ljpeg -lpng -L\/~\/external\/cuda\/lib64\/ -D DLIB_JPEG_SUPPORT dnn_mmod_ex.cpp it generated a binary but I don't see any speed improvement compared with the CPU mode. How can you compile the example? Thanks for sharing October 15, 2016 at 5:19 PM Davis King said... You didn't compile any of the CUDA code. You have to use the nvidia nvcc compiler and do a number of other things. You should use cmake to compile the project, it will do all the right stuff. October 15, 2016 at 7:52 PM ngap wei Tham said... Hi, how could we reuse trained network to extract features from images and then train our own standard machine learning classifier (such as softmax or SVM). Like the overfeat example of sklearn_theano.Thanks October 15, 2016 at 9:29 PM florisdesmedt said... This comment has been removed by the author. October 16, 2016 at 5:53 AM Davis King said... Yes, you can get the output of the network's last layer, or any other layer. Read the documentation to see how. In particular, there are two introduction examples that are important to read to understand the API. October 16, 2016 at 6:22 AM ngap wei Tham said... >there are two introduction examples that are important to read to understand the API. Do you mean dnn_introduction1_ex.cpp and dnn_introduction2_ex.cpp? Please correct me if I am wrong. We can use layer(pnet).get_output(); to extract the features from images and use it to train our own classifier? Do you have any plan on providing more pre-train network?Like the network of Deep face recognition used?We could implement face descriptor based on it if it exist. Forgive me if I ask anything stupid. October 16, 2016 at 9:56 AM Davis King said... Yes, those introductions. There is also an entire API reference http:\/\/dlib.net\/faq.html#Whereisthedocumentationforobjectfunction I'm going to be adding more things to the deep learning toolkit for the foreseeable future. That will probably include more examples with pretrained models. October 16, 2016 at 10:02 AM Matt said... I just used dlib to train a neural network for the first time using this library. It was super easy. A lot of the nonsense you have to deal with in other libraries is taken care of for you automatically. Plus the code has basically 0 dependencies, I can deploy it in an embedded device (No Python, lua, lmdb, or a whole menu of other stuff). I've already said this once before, but really *great* job on this library. Dlib deserves a lot more attention as a first-class neural net library. October 16, 2016 at 10:07 AM Davis King said... Thanks :) Yeah, I wanted to make a library that professionals would want to use. Most of the other tools are obviously written by grad students who don't know how to code well. Dlib will also automatically adjust the learning rate as well as detect convergence, and so knows when to stop. So there shouldn't be much fiddling with learning rates. Just run it and wait. Although you do need to set the \"steps without progress threshold\", but setting it to something big like 10,000 or 8,000 should always be fine. I have it set to something smaller in the examples just so they run faster. October 16, 2016 at 10:24 AM GoodLaw57 said... Hi, I have a question about dnn_introduction_ex.cpp. using net_type = loss_multiclass_log< fc<10, relu> >>>>>>>>>>>>; Why the first fc output size is 120 but not 784? Size of the mnist dataset is 28*28. I check the source codes of max_pool, by default it prefer zero padding. first max_pool 28*28 become 14*14 second max_pool 14*14 become 7*7 final size should be 7*7*16 = 784 October 17, 2016 at 5:31 AM Davis King said... I think you are confusing input sizes with output sizes. October 17, 2016 at 6:36 AM ngap wei Tham said... >I'm going to be adding more things to the deep learning toolkit for the foreseeable future. That will probably include more examples with pretrained models. Thanks, looking forward to it. Do you have any plan to support parallel programming on cpu in the future? Which part is the most time consuming part of cpu? Maybe unfold some for loops can make cpu part become faster October 17, 2016 at 7:48 AM ngap wei Tham said... Hi, Kyle McDonald, I have the same issue as yours, I use a brute force solution to fix this issue I change the line \"if (CUDA_FOUND AND cudnn AND cudnn_include AND COMPILER_CAN_DO_CPP_11 AND cuda_test_compile_worked AND cudnn_test_compile_worked)\" to \"if(1)\". Then everything work as expected(I confirm cuda and cudnn work on my machine before I change it to if(1)). But to make the program compile, I need to link to cublas.lib cublas_device.lib cudnn.lib curand.lib and make sure my program know where to locate cublas64_80.dll cudart64_80.dll cudnn64_5.dll curand64_80.dll Following are the time comparison of cpu vs gpu CPU : elapsed time: 13213s GPU : elapsed time: 528s It is around 25 times faster than cpu. I enable arch:AVX when I use cpu to do the training, but I think there are still room to improve the speed, because I notice dlib only leverage one cpu to train the network. October 18, 2016 at 1:44 AM Davis King said... You said the error was this: *** cuDNN V5.0 OR GREATER NOT FOUND. DLIB WILL NOT USE CUDA. *** *** If you have cuDNN then set CMAKE_PREFIX_PATH to include cuDNN's folder. So it's not finding cudnn. Did you set CMAKE_PREFIX_PATH to include cuDNN's folder so cmake can find it? October 18, 2016 at 6:37 AM cool-rat said... Good stuff. I am trying to use the program to detect my palm instead of my face. I have given it a total of 4 images (3 training and 1 testing) just to see what happens. I get the following error num training images: 3 num testing images: 1 detection window width,height: 34,47 overlap NMS IOU thresh: 0 overlap NMS percent covered thresh: 0 done training training results: 1 0 0 testing results: Error detected at line 31. Error detected in file filetree\/dlib\/dlib\/..\/dlib\/dnn\/validation.h. Error detected in function const matrix dlib::test_object_detection_function(loss_mmod &, const image_array_type &, const std::vector > &, const dlib::test_box_overlap &, const double) [SUBNET = dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::add_layer, dlib::input_rgb_image_pyramid >, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, void>, image_array_type = std::__1::vector, dlib::row_major_layout>, std::__1::allocator, dlib::row_major_layout> > >]. Failing expression was is_learning_problem(images,truth_dets) == true. matrix test_object_detection_function() invalid inputs were given to this function is_learning_problem(images,truth_dets): false images.size(): 1 ----------------------- Now I get the same error, when I swap out a training image for a testing image. That is if the testing image and result was faulty, I should not get a result of (1,0,0) when I swap out the faulty testing image with a training image... Any clues... Thanks! October 19, 2016 at 1:27 AM innked. xD said... Cool Davis! :-) October 19, 2016 at 4:06 AM Davis King said... How is test_object_detection_function() supposed to test and image if you don't give it any truth data? If you looked at the documentation for is_learning_problem() (or test_object_detection_function) you would find out it's complaining because you haven't given any truth detections. October 19, 2016 at 6:44 AM florisdesmedt said... Hi, I'm experimenting with batchprocessing of images. I have found out that this can easily be done by giving a vector of matrix elements to the functor net (assuming net is the network). But I want to alter the adjustment_threshold also. For a single image I can alter it using: --- \/\/temporary create a vector to work with iterators std::vector> images; images.push_back(img); \/\/ create a tensor with all the data to process (single image) resizable_tensor temp_tensor; net.to_tensor(&images[0], &images[0]+1, temp_tensor); \/\/ run the network net.subnet().forward(temp_tensor); \/\/ convert the output of the network to detections (adjusting the threshold with -0.6) std::vector dets; net.loss_details().to_label(temp_tensor, net.subnet(), &dets,-0.6); --- For batch-processing, I assume the vector \"images\" should contain all the images, such that a tensor is created that contains all the data of the whole batch. But how can I perform the last step (...to_label())? I assume I have to iterate through the samples of the tensor \"temp_tensor\" to obtain the detections for the respective image, so call the to_label function multiple times? Is there an iterator that easily loops over the samples that I can use? Best regards October 19, 2016 at 9:24 AM Davis King said... You give an iterator to a range of std::vectors of rectangles. It's nothing fancy. You should find a C++ tutorial or book that explains iterators and then it will be clear. October 19, 2016 at 9:31 AM florisdesmedt said... ok, it was quit easy :S , I was making it way to complicated. Thanx October 19, 2016 at 9:52 AM florisdesmedt said... This comment has been removed by the author. October 19, 2016 at 9:52 AM cool-rat said... Thanks for the response Davis. Appreciate it. I did see that 'is_learning_problem(images,truth_dets): ' is coming out false. But I dont see why. I generated the testing.xml and training.xml the same way using the imglab tool. That is, I picked the 'truth' box the same way for both training and testing. Here is what the testing.xml contains: imglab dataset Created by imglab tool. Did it need anything other than the Box definition. Note that when I swap out the two lines of image file and box top from the testing to the training xml, it is ok.. I know the program has found the testing.xml because it knows that there is only 1 image in the testing. That is the initial comments in the code is here: num training images: 3 num testing images: 1 detection window width,height: 35,45 overlap NMS IOU thresh: 0 overlap NMS percent covered thresh: 0 So it has found the testing.xml, and the image...the image and box sizes are ok, because I can swap them between training and testing...what else is could be going wrong? Thanks October 19, 2016 at 2:43 PM cool-rat said... The xml statements dint come through in the last post, so replacing < with [ in the xml statements They have [images] [image file='\/Volumes\/hdname\/third.jpg'] [box top='271' left='234' width='182' height='262'\/] [\/image] [\/images] October 19, 2016 at 3:36 PM ngap wei Tham said... What if I want to detect the objects with different aspect ratio? Like cigarette alarm, the aspect ratio of cigarette will vary become different under different views. Easiest solution I could think of is apply cnn to train the network->apply selective search to find interesting candidate->use the classifier to determine it is cigarette or object X. October 21, 2016 at 4:36 AM Davis King said... I would use a square box right now. Although, in the near future I'm going to add support for multiple aspect ratios into the MMOD tool. But right now it only does one aspect ratio at a time. October 21, 2016 at 8:52 PM ngap wei Tham said... Thanks for your help. >I would use a square box right now. Ok, I will give this a shot, if the results are good I will tell you. >Although, in the near future I'm going to add support for multiple aspect ratios into the MMOD tool. Looking forward to that. It would be another big surprise(present) if we could use MMOD to detect objects of different aspect ratio I check the file loss.h, there are some loss class do not used in the examples. Like loss_binary_hinge, loss_binary_log_, loss_metric_. What are they for?Under what situations we should try to use them?Thanks October 22, 2016 at 1:53 AM Davis King said... Read the documentation. http:\/\/dlib.net\/faq.html#Whereisthedocumentationforobjectfunction Although loss_metric_ is not documented yet though because I only added it a few days ago and it's still in development. October 22, 2016 at 8:25 AM ngap wei Tham said... Ah, thanks. I always jump into header file when I find something confuse, but I cannot found much comments in loss.h(I though the link just the same as header file before I give it a try). October 22, 2016 at 9:24 AM ngap wei Tham said... >support for multiple aspect ratios into the MMOD tool Is it possible for MMOD tool to support detection of different type of objects too(banana, car, pedestrian, book etc)? Like allow the detector output the top 5 error? October 24, 2016 at 8:57 AM Davis King said... Yes, that's something I'm planning on adding soon. October 25, 2016 at 6:34 AM Rafal Kapela said... I have the same problem as before: Error while calling cudnnCreate(&handles[new_device_id]) in file \/home\/ale\/dlib-19.2\/dlib\/dnn\/cudnn_dlibapi.cpp:102. code: 6, reason: A call to cuDNN failed cuDNN was detected during cmake compilation of dlib: -- Found cuDNN: \/usr\/local\/cuda-8.0\/lib64\/libcudnn.so LD_LIBRARY_PATH also includes ducnn libraries. CPU version seem to work. Any idead what could be wrong??? October 26, 2016 at 8:53 AM Davis King said... If you google for cudnn error code 6 will you will find out it's the architecture mismatch error. Sounds like your GPU is too old. October 26, 2016 at 8:59 AM Rafal Kapela said... Thanks. I appreciate your time. October 26, 2016 at 9:08 AM Ked Su said... Is the example code, \"dnn_mmod_ex,\" resource consuming? I used the code to train my dataset, which contains 1000 images, and found that the program took 5G GPU memory, 6G main memory, and 40% CPU computing for 8 cores. Is it normal? How can I adjust the code for using less resource, for example 10% CPU computing. Thanks. October 27, 2016 at 2:10 AM Davis King said... Deep learning takes a lot of computer power. We wouldn't bother to use GPU computing if that weren't the case. October 27, 2016 at 7:00 AM florisdesmedt said... Some parameters that could be changed to use less resources (which I have used to keep the training process from swapping) is making the cropping size smaller, and the amounts of crops in each iteration. Both will however have an influence on the resulting detection accuracy I presume. October 27, 2016 at 8:34 AM Masaki said... Thank you very much for the nice post! I got this error when I was running the training. Could anyone help me? >> Encountered a truth rectangle with a width and height of 19 and 40. The image pyramid and sliding window can't output a rectangle of this shape. This is because the rectangle is smaller than the detection window which has a width and height of 30 and 54. October 30, 2016 at 1:20 PM florisdesmedt said... I had a similar problem before. When you try to train a model that is not square shaped, you have to limit the amount of rotation in annotation augmentation. The command for this can be found in an earlier post. October 30, 2016 at 4:49 PM Nectar said... Hi Davis, With this deep learning API, can you also get the associated landmarks for the face? If so, is it possible that I use the API to train on a different set of landmarks? Thanks! November 1, 2016 at 11:40 AM florisdesmedt said... Hi, I have some problems with getting the training working properly. I almost always get a testresult of 1 0 0 (so no detections are found, with an obvious full precision), also on the trainingsset. Does the training data has to be of some specified format? I have tried different datasets, model sizes, ... Now I'm experimenting with the head annotations of the Town Centre dataset (more or less square annotations). I started with using the full images and using all annotations per image (no luck there). I saw that the trainingsdata for the facedetector uses all square patches (250x250 or 350x350) with the annotation positioned in the center taking between 23% and 45% of the patches. Is this required? (I tried also this format without success so far). I also saw that in most of these patches the other annotations are set to ignore, with a few exceptions (which lead to a non-zero NMS threshold), is there some logic there which are ignored? Is the model-size, cropped patch-size, ... related to size the annotations have in the training data? best regards November 2, 2016 at 4:39 AM Davis King said... There are no hard coded limits or special formatting requirements in the code. You do have to make training data that make sense in the usual machine learning way though. November 2, 2016 at 6:36 AM Email Address said... Man amazing work Davis! Do you have any plans to support GPU for face\/object detection? Speeding up the training as step 1 is pretty fantastic. Just curious if you have plans to support OpenCL or something similar for detecting faces in images and reduce the need for clusters and put the power in the hands of the average guys! I plan to look into dlib's core and see how feasible it is to do myself November 4, 2016 at 11:41 PM ngap wei Tham said... >Do you have any plans to support GPU for face\/object detection? I think dlib already implement the libs by cudnn? By the way, today I deploy a small example on a windows laptop without cuda support, it crash suddenly(I put cublas64_80.dll, cudart64_80.dll, cudnn64_5.dll, curand64_80.dll in the folder of exe). Is this a normal case?Thanks November 5, 2016 at 4:55 AM mohanraj said... can tell me the steps to compile the dlib19.2 with deep learning support November 5, 2016 at 1:51 PM Davis King said... Use cmake, this page tells you what to type: http:\/\/dlib.net\/compile.html Also, if anyone is getting cuda errors then it's probably because cuda isn't installed correctly on your computer. I would try reinstalling it. November 5, 2016 at 9:24 PM Rafal Kapela said... Hi Guys, Didn't have a chance to thank you for this work. This is amazing. Sorry for a rookie question but I struggle with something I don't understand in your example. Using imglab tool I defined my train\/test object xmls but the dnn_mmod_ex keeps saying me: \"Encountered a truth rectangle located at [(10, 51) (23, 70)] that is too close to the edge of the image to be captured by the CNN features.\" Depending on the cropper and mmod_options settings the values in brackets change. It seens pretty self-explanatory comment so I wanted to know which exactly region causes the error. So I endded up with one image in the train xml that has one region in the center of it. Definitelly it is not too close to the edge of the image. I noticed that when the regions are too small this error can appear. Could you please shed more light on this? November 7, 2016 at 6:43 AM Davis King said... Post this as a github issue and I'll take a look at it. Include all the files necessary to reproduce what you are seeing. November 7, 2016 at 8:54 AM bright 910570 said... @Davis King Thanks for doing such amazing job! This can detect human face perfectly. However I'd like to further operation with the object detected(say, recognition), but I don't quite understand what operation I could deal with instead of showing overlay. Sorry for this kind of stupid question.Is there any reference suggested? November 8, 2016 at 12:53 AM ngap wei Tham said... > However I'd like to further operation with the object detected(say, recognition) Face recognition is much difficult than face detection, if you are interesting about this topic, you can google \"Deep face\", \"vgg face\" and \"DeepID\". DeepID provide state of the art performance with relatively small training example. Hi Davis, is it possible to switch to cpu operation if the computer do not support cuda?Thanks November 8, 2016 at 1:14 AM Samarth Tandon said... What changes have to be made to process a bunch of images as given in comment . Please help matrix img; load_image(img, argv[i]); \/\/ Upsampling the image will allow us to detect smaller faces but will cause the \/\/ program to use more RAM and run longer. while(img.size() < 1800*1800) pyramid_up(img); \/\/ Note that you can process a bunch of images in a std::vector at once and it runs \/\/ much faster, since this will form mini-batches of images and therefore get \/\/ better parallelism out of your GPU hardware. However, all the images must be \/\/ the same size. To avoid this requirement on images being the same size we \/\/ process them individually in this example. auto dets = net(img); November 8, 2016 at 6:12 AM ngap wei Tham said... >What changes have to be made to process a bunch of images as given in comment . Please help Never tried it(do not have a strong gpu), but I guess this should work. std::vector imgs; \/\/this is acronym to dynamic array in c++ size_t constexpr batch_size = 10; while(img.size() < 1800*1800) pyramid_up(img); \/\/move will take the resource of img, do not move it if you want to reuse img imgs.emplace_back(std::move(img)); if(imgs.size() >= 10){ auto dets = net(imgs); imgs.clear()\/\/ reset vector } November 8, 2016 at 6:43 AM Davis King said... Yes, that's what you do. Pack them into a std::vector and give them to the network. CMake will automatically use the CPU if CUDA isn't available. But you can toggle it by going into cmake-gui and setting the DLIB_USE_CUDA option. November 8, 2016 at 6:49 AM ngap wei Tham said... >CMake will automatically use the CPU if CUDA isn't available. I think you are talking about compile time? The problem I met is at runtime, on the target host do not support cuda, the program will crash if I compile the app with cuda. Is it possible to switch to from gpu mode to cpu mode if the target host do not support cuda?Thanks November 8, 2016 at 10:17 AM Davis King said... No, you have to compile for a particular target architecture. November 8, 2016 at 11:01 AM Rafal Kapela said... Hi Davis, A quick question - what negative loss during training means in dlib implementation. I see that in loss.h:82 we have const float temp = 1-y*out_data[i]; so negative loss means that out_data is bigger than 1 ? Overfitting or jump outside local mimimum? Shall we prevent it by adding: while ((trainer.get_learning_rate() >= 1e-4) && (trainer.get_average_loss() > 0)) ? Or maybe decrease of iterations without progress threshold? Add more train examples so that train starts with bigger loss? November 14, 2016 at 3:36 PM Rasel said... Hi Davis, Thank you for such a nice tool. All the CNN tools I have explored so far this is easiest one. I built dlib 19.2 with cuda 8.0, cuDNN 5.1, visual studio 2015 update 3 and without openCV. Then I tried \"dnn_mmod_face_detection_ex\" which is working perfectly. But getting error while running \"dnn_mmod_ex\". For training and testing I use the face dataset provided with dlib. Here is the error message, num training images: 4 num testing images: 5 detection window width,height: 40,40 overlap NMS IOU thresh: 0.0781701 overlap NMS percent covered thresh: 0.257122 step#: 0 learning rate: 0.1 average loss: 0 steps without apparent progress: 0 Error while calling cudaMalloc(&data, new_size*sizeof(float)) in file C:\\AtiqWorkSpace\\dlib_CNN\\dlib-19.2\\dlib\\dnn\\gpu_data.cpp:191. code: 2, reason: out of memory Any idea why this is happening.I ran the tool in Amazon EC2 machine which should have sufficient memory. November 14, 2016 at 5:28 PM Rafal Kapela said... Hi Rasel, Try to reduce number of crops in: cropper(150, images_train, face_boxes_train, mini_batch_samples, mini_batch_labels); November 15, 2016 at 4:11 AM Rasel said... Thanks Rafal. Reducing number of crops resolve the memory issue. I tried 80 and able to train and test but the test result is very poor. No face detection at all. I am confused whether it is because of the number of crops (mini_batch_size) or any other issue. November 15, 2016 at 11:28 AM Adithya Apuroop said... Hello Davis, I want to use max margin object detection for face detection. For this, I tried modifying dnn_mmod_ex.cpp file that is provided in the examples. However I would like the detector to detect more faces, even at the cost of some false positives. I understand that I have to change the adjust_threshold parameter, but where do I change it from? I tried to read the documentation but could not find anything related to this. Any help would be much appreciated! November 15, 2016 at 6:56 PM Davis King said... If you want to find faces use one of the pretrained face detectors that comes with dlib. They example programs with the word face in the name show you how to do it. November 15, 2016 at 8:25 PM Unknown said... can Max-margin loss apply to caffe? November 15, 2016 at 9:32 PM Adithya Karavadi said... I really need to train my own DNN model for this, as the faces I'm detecting are from grayscale low resolution images and the pretrained model does not work well for this. I tried going through example programs you suggested but could not find any settings for the threshold that lets me have more detections. Any particular example you are talking about? November 15, 2016 at 10:29 PM Adithya Karavadi said... Hi Rasel, did you try increasing the iterations without progress value to something like 1000? November 15, 2016 at 10:34 PM Rasel said... Hi Adithya, Thanks for your suggestions. I did not increase the number of iterations without progress. I am running the training again and this time I am going to use the network configuration and all the parameters describe in pretrained face detectors example. November 16, 2016 at 12:49 PM Baris EKICI said... Hi There, How can I change the aspect ratio? Tried to change the some settings but 2:1 ratio always there. November 20, 2016 at 4:54 PM Baris EKICI said... I'm processing the images via random_cropper_ex. Actually some images are red in same random crops and not in some other crops. I have resized the images and made the width 600px by keeping the aspect ratio. Trying to figure out which images is better for dnn training, but still couldnt figure out yet. November 21, 2016 at 3:53 AM Rasel said... Hi, I trained a model with the same CNN architecture of dnn_mmod_face_detection_ex. And the parameters, mmod_options= 40*80 [The objects to be detected are rectangular not square like face] iterations_without_progress_threshold= 8000 max_rotation_degrees =15 total_crop =50 Training was successful. But got following error message while testing, Error detected at line 591. Error detected in file C:\/AtiqWorkSpace\/dlib_CNN\/dlib-19.2\/dlib\/dnn\/cuda_dlib.cu . Error detected in function void dlib::cuda::affine_transform(dlib::tensor &, con st dlib::tensor &, const dlib::tensor &, const dlib::tensor &). Failing expression was A.nr()==B.nr() && B.nr()==src.nr() && A.nc()==B.nc() && B .nc()==src.nc() && A.k() ==B.k() && B.k()==src.k(). A.nr(): 556 B.nr(): 556 src.nr(): 5385 A.nc(): 148 B.nc(): 148 src.nc(): 1994 A.k(): 16 B.k(): 16 src.k(): 16 For testing I used the example dnn_mmod_face_detection_ex as I trained the model using the CNN architecture of this example. November 22, 2016 at 8:37 AM Davis King said... Don't train with affine layers. You should use the bn_con (the batch normalization layers) instead. November 23, 2016 at 7:54 AM bright 910570 said... HI Davis, I'd like to use tracking after object is detected in a video, and which the position of object is necessary. I wonder how to find those attributes like positions or size etc. Thank you for your contribution. November 24, 2016 at 9:54 AM Matias Arrech said... Hi Davis, thanks for the amazing job! Im reading this post and i think it can help me. Im new to this and im a little lost. I need to detect the tongue if is down, up, left or right. If i can detect like the landmark face detector with points it will be nice too. Can you give me some advice of how achieve this? November 24, 2016 at 6:06 PM Post a Comment Older Post Home Subscribe to: Post Comments ( Atom ) Dlib Homepage Library news Loading... Blog Archive ▼  2016 ( 4 ) ▼  October ( 2 ) Easily Create High Quality Object Detectors with D... Hipsterize Your Dog With Deep Learning ►  August ( 1 ) ►  June ( 1 ) ►  2015 ( 2 ) ►  June ( 1 ) ►  February ( 1 ) ►  2014 ( 8 ) ►  December ( 1 ) ►  November ( 1 ) ►  October ( 1 ) ►  August ( 1 ) ►  July ( 1 ) ►  April ( 2 ) ►  February ( 1 ) ►  2007 ( 1 ) ►  March ( 1 ) Powered by Blogger. ","flair":"four\tProject"}
{"author":"DevFRus","created":"Fri Sep 30 23:09:49 EDT 2016","text":"There are two main gaps in our understanding of neural networks: optimization hardness and generalization performance. Training a neural network requires solving a highly non-convex optimization problem in high dimensions. Current training algorithms are all based on gradient descent, which only guarantees convergence to a critical point (local minimum or saddle). In fact, Anandkumar & Ge 2016 recently proved that finding even a local minimum is NP-hard, which means that (assuming P != NP) there exist \"bad\", hard to escape, saddle points in the in the error surface. Yet, these training algorithms are empirically effective for many practical problems, and we don't know why. There have been theoretical papers such as Choromanska et al. 2016 and Kawaguchi 2016 which prove that, under certain assumptions, the local minima are essentially as good as the global minima, but the assumptions they make are somewhat unrealistic and they don't address the issue of the bad saddle points. The other main gap in our understanding is generalization performance: how well does the model perform on novel examples not seen during training? It's easy to show that in the limit of an infinite number of training examples (sampled i.i.d. from a stationary distribution), the training error converges to the expected error on novel examples (provided that you could train to the global optimum), but since we don't have infinite training examples, we are interested in how many examples are needed to achieve a given difference between training and generalization error. Statistical learning theory studies these generalization bounds. Empirically, training a large modern neural network requires a large number of training examples (Big Data, if you like buzzwords), but not that monumentally large to be practically unfeasible. But if you apply the best known bounds from statistical learning theory (for instance Gao & Zhou 2014) you typically get these unfeasibly huge numbers. Therefore these bounds are very far from being tight, at least for practical problems. One of the reason might be that these bounds tend to assume very little about the data generating distribution, hence they reflect the worst-case performance against adversarial environments, while \"natural\" environments tend to be more \"learnable\". It is possible to write distribution-dependent generalization bounds, but we don't know how to formally characterize a distribution over \"natural\" environments. Approaches such as algorithmic information theory are still unsatisfactory. Therefore we still don't know why neural networks can be trained without overfitting. Furthermore, it should be noted that these two main issues seem to be related in a still poorly understood way: the generalization bounds from statistical learning theory assume that the model is trained to the global optimum on the training set, but in a practical setting you would never train a neural network until convergence even to a saddle point, as to do so would typically cause overfitting. Instead you stop training when the error on a held-out validation set (which is a proxy for the generalization error) stops improving. This is known as \"early stopping\". So in a sense all this theoretical research on bounding the generalization error of the global optimum may be quite irrelevant: not only we can't efficiently find it, but even if we could, we would not want to, since it would perform worse on novel examples than many \"sub-optimal\" solutions. It may be the case that optimization hardness is not a flaw of neural network, on the contrary, maybe neural networks can work at all precisely because they are hard to optimize. All these observations are empirical and there is no good theory that explains them. There is also no theory that explains how to set the hyperparameters of neural networks (hidden layer width and depth, learning rates, architectural details, etc.). Practitioners use their intuition honed by experience and lots of trial and error to come up with effective values, while a theory could allow us to design neural networks in a more systematic way.","flair":"null\tnull"}
{"author":"DJTeebS","created":"Sat Oct 22 02:50:25 EDT 2016","text":"Hi \/r\/Machinelearning!\n\n\nThis is my first post on this subreddit. I am interested in what the reddit community thinks of my thesis topic which utilizes machine learning to solve a classification problem. I am investigating the development of a software system for a self-sorting smart bin that can identify and sort plastics, metals, glass, and landfill.\n\n\nOne of the biggest problems to crack is the identification and classification of items input into the bin. I intend to have an identification chamber fitted with a array of instruments and sensors which would provide valuable data streams characterizing the material of the input item:\n\n* Camera -&gt; image processing\n\n* Microphone -&gt; acoustic data\n\n* Spectrographs -&gt; spectroscopic data\n\n* Metal detector -&gt; electromagnetic data\n\n\nI have yet to select a machine learning algorithm to solve this classification problem but 'adaptive interactive modelling systems' looks very promising. Otherwise I would assume that artificial neural nets would be the way to go.\n\n\nI am seeking any critical feedback or advice that will be of help to my project!\n\n\nMany thanks,\n\nTeebS","flair":"three\tResearch"}
{"author":"clear_coprolite","created":"Tue Oct 18 16:19:02 EDT 2016","text":"AI\u2022ON is an open community dedicated to advancing Artificial Intelligence by: As a research contributor, gain experience and make an impact: As a senior researcher, outsource your backlog of projects and give back to the community: Read about our process, our values, and explore our collection of open research problems.","flair":"three\tResearch"}
{"author":"afeder_","created":"Fri Nov 04 14:48:20 EDT 2016","text":" Home Research Publications AlphaGo DQN Applied DeepMind Health DeepMind for Google News & Blog About Us Careers Research Highlighted Research AlphaGo DQN Publications Latest Research News Reinforcement learning with unsupervised auxiliary tasks Applied DeepMind Health Streams DeepMind for Google Latest Applied News Working with the NHS to build lifesaving technology Careers Home News & Blog About Us Press Terms and Conditions Privacy Policy DeepMind and Blizzard to release StarCraft II as an AI research environment Today at BlizzCon 2016 in Anaheim, California, we announced our collaboration with Blizzard Entertainment to open up StarCraft II to AI and Machine Learning researchers around the world. For almost 20 years, the StarCraft game series has been widely recognised as the pinnacle of 1v1 competitive video games, and among the best PC games of all time. The original StarCraft was an early pioneer in eSports, played at the highest level by elite professional players since the late 90s, and remains incredibly competitive to this day. The StarCraft series\u2019 longevity in competitive gaming is a testament to Blizzard\u2019s design, and their continual effort to balance and refine their games over the years. StarCraft II continues the series\u2019 renowned eSports tradition, and has been the focus of our work with Blizzard. DeepMind is on a scientific mission to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be told how. Games are the perfect environment in which to do this, allowing us to develop and test smarter, more flexible AI algorithms quickly and efficiently, and also providing instant feedback on how we\u2019re doing through scores. Over the past five years we\u2019ve helped to pioneer the use of games as AI research environments to drive our machine learning and reinforcement learning research forwards, from 2D games in Atari, to full 3D environments such as Torcs, mastering the game of Go, or our forthcoming DeepMind Labyrinth. Here's a representation of what these research environments have looked like with L-R, Atari and Labyrinth. StarCraft is an interesting testing environment for current AI research because it provides a useful bridge to the messiness of the real-world. The skills required for an agent to progress through the environment and play StarCraft well could ultimately transfer to real-world tasks. At the start of a game of StarCraft, players choose one of three races, each with distinct unit abilities and gameplay approaches. Players\u2019 actions are governed by the in-game economy; minerals and gas must be gathered in order to produce new buildings and units. The opposing player builds up their base at the same time, but each player can only see parts of the map within range of their own units. Thus, players must send units to scout unseen areas in order to gain information about their opponent, and then remember that information over a long period of time.  This makes for an even more complex challenge as the environment becomes partially observable - an interesting contrast to perfect information games such as Chess or Go. And this is a real-time strategy game - both players are playing simultaneously, so every decision needs to be computed quickly and efficiently. An agent that can play StarCraft will need to demonstrate effective use of memory, an ability to plan over a long time, and the capacity to adapt plans based on new information. Computers are capable of extremely fast control, but that doesn\u2019t necessarily demonstrate intelligence, so agents must interact with the game within limits of human dexterity in terms of \u201CActions Per Minute\u201D. StarCraft\u2019s high-dimensional action space is quite different from those previously investigated in reinforcement learning research; to execute something as simple as \u201Cexpand your base to some location\u201D, one must coordinate mouse clicks, camera, and available resources.  This makes actions and planning hierarchical, which is a challenging aspect of Reinforcement Learning. We\u2019re particularly pleased that the environment we\u2019ve worked with Blizzard to construct will be open and available to all researchers  next year. We recognise the efforts of the developers and researchers from the Brood War community in recent years, and hope that this new, modern and flexible environment - supported directly by the team at Blizzard - will be widely used to advance the state-of-the-art. We\u2019ve worked closely with the StarCraft II team to develop an API that supports something similar to previous bots written with a \u201Cscripted\u201D interface, allowing programmatic control of individual units and access to the full game state (with some new options as well).  Ultimately agents will play directly from pixels, so to get us there, we\u2019ve developed a new image-based interface that outputs a simplified low resolution RGB image data for map & minimap, and the option to break out features into separate \u201Clayers\u201D, like terrain heightfield, unit type, unit health etc. Below is an example of what the feature layer API will look like. StarCraft II DeepMind feature layer API We are also working with Blizzard to create \u201Ccurriculum\u201D scenarios, which present increasingly complex tasks to allow researchers of any level to get an agent up and running, and benchmark different algorithms and advances. Researchers will also have full flexibility and control to create their own tasks using the existing StarCraft II editing tools. We\u2019re really excited to see where our collaboration with Blizzard will take us. While we\u2019re still a long way from being able to challenge a professional human player at the game of StarCraft II, we hope that the work we have done with Blizzard will serve as a useful testing platform for the wider AI research community. Share Article LinkedIn WhatsApp SMS Reddit Author Friday, 4 November 2016 Oriol Vinyals Research Scientist, DeepMind Show all results Follow Research Research Applied Applied News & Blog News & Blog About Us About Us Careers Careers Press Terms and Conditions Privacy Policy Alphabet Inc © 2016 DeepMind Technologies Limited DeepMind.com uses cookies to help give you the best possible user experience. Find Out More ","flair":"two\tNews"}
{"author":"Mandrathax","created":"Mon Oct 31 09:48:27 EDT 2016","text":"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n\n|Previous weeks|\n|--------------|\n|[Week 1](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4qyjiq\/machine_learning_wayr_what_are_you_reading_week_1\/)|\n|[Week 2](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4s2xqm\/machine_learning_wayr_what_are_you_reading_week_2\/)|\n|[Week 3](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4t7mqm\/machine_learning_wayr_what_are_you_reading_week_3\/)|\n|[Week 4](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4ub2kw\/machine_learning_wayr_what_are_you_reading_week_4\/)|\n|[Week 5](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4xomf7\/machine_learning_wayr_what_are_you_reading_week_5\/)|\n|[Week 6](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4zcyvk\/machine_learning_wayr_what_are_you_reading_week_6\/)|\n|[Week 7](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/52t6mo\/machine_learning_wayr_what_are_you_reading_week_7\/)|\n|[Week 8](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/53heol\/machine_learning_wayr_what_are_you_reading_week_8\/)|\n|[Week 9](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/54kvsu\/machine_learning_wayr_what_are_you_reading_week_9\/)|\n|[Week 10](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/56s2oa\/discussion_machine_learning_wayr_what_are_you\/)|\n|[Week 11](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/57xw56\/discussion_machine_learning_wayr_what_are_you\/)|\n\nMost upvoted papers last week :\n\n[Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1607.01426)\n\n[Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https:\/\/arxiv.org\/abs\/1506.02142)\n\n[Conditional Image Generation with PixelCNN Decoders](https:\/\/arxiv.org\/abs\/1606.05328)\n\nBesides that, there are no rules, have fun.\n","flair":"one\tDiscussion"}
{"author":"jacobgil","created":"Sun Nov 06 14:09:13 EST 2016","text":"The YOLO (https:\/\/arxiv.org\/pdf\/1506.02640v5.pdf) detector last layer is a fully connected layer that regresses a 7x7x30 output, representing detections in each cell in a 7x7 grid.\nIsn't there a lot of redundancy here? \n\nIf you know how to regress the bounding box for a single cell in that grid, can't that be re-used for all the cells in the grid? \nWhy learn a different set of weights for each cell?\n\nOr from another angle - if the training data never contains detections in some of the cells in the grid - won't that cause YOLO to never detect anything in those cells?\n\nYou could replace that layer with a convolutional layer, so you get parameter sharing across the image.\n\nI guess this is why the SSD detector uses convolutional layers to regress the bounding boxes.\n\nIs this correct or am I missing something?","flair":"one\tDiscussion"}
{"author":"kjw0612","created":"Sun Nov 20 00:07:29 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1608.08021 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats Current browse context: cs.CV < prev | next > new | recent | 1608 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Kye-Hyeon Kim Yeongjae Cheon Sanghoon Hong Byung-Seok Roh Minje Park Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection Authors: Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park (Submitted on 29 Aug 2016 (v1), last revised 30 Sep 2016 (this version, v3)) Abstract: This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of \"CNN feature extraction + region proposal + RoI classification\", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is \"less channels with more layers\" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms\/image on Intel i7-6700K CPU with a single core and 46ms\/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012. Comments: Full details about \"PVANet 9.0\" in the VOC2012 leaderboard (this https URL). The test codes are available at this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1608.08021 [cs.CV]   (or arXiv:1608.08021v3 [cs.CV] for this version) Submission history From: Sanghoon Hong [view email] [v1] Mon, 29 Aug 2016 12:32:00 GMT (306kb,D) [v2] Mon, 19 Sep 2016 10:00:44 GMT (306kb,D) [v3] Fri, 30 Sep 2016 07:17:13 GMT (306kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"pogopuschel_","created":"Mon Oct 03 10:27:08 EDT 2016","text":"This repository provides code, exercises and solutions for popular Reinforcement Learning algorithms. These are meant to serve as a learning tool to complement the theoretical materials from Each folder in corresponds to one or more chapters of the above textbook and\/or course. In addition to exercises and solution, each folder also contains a list of learning goals, a brief concept summary, and links to the relevant readings. All code is written in Python 3 and uses RL environments from OpenAI Gym. Advanced techniques use Tensorflow for neural network implementations.","flair":"null\tnull"}
{"author":"elanmart","created":"Tue Oct 25 13:01:25 EDT 2016","text":"The Microsoft Cognitive Toolkit\u2014previously known as CNTK\u2014empowers you to harness the intelligence within massive datasets through deep learning by providing uncompromised scaling, speed and accuracy with commercial-grade quality and compatibility with the programming languages and algorithms you already use. Hear about the team that developed the Cognitive Toolkit, or read more below.","flair":"two\tNews"}
{"author":"liviu-","created":"Tue Oct 18 15:47:24 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.03483 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat stat.CO References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Learning in Implicit Generative Models Authors: Shakir Mohamed, Balaji Lakshminarayanan (Submitted on 11 Oct 2016 (v1), last revised 4 Nov 2016 (this version, v2)) Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. Subjects: Machine Learning (stat.ML); Learning (cs.LG); Computation (stat.CO) Cite as: arXiv:1610.03483 [stat.ML]   (or arXiv:1610.03483v2 [stat.ML] for this version) Submission history From: Shakir Mohamed [view email] [v1] Tue, 11 Oct 2016 19:59:39 GMT (125kb) [v2] Fri, 4 Nov 2016 15:11:20 GMT (25kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"shagunsodhani","created":"Tue Oct 25 14:57:43 EDT 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 61 Fork 14 shagunsodhani\/CNN-Sentence-Classifier Code Issues 0 Pull requests 0 Projects 0 Pulse Graphs Implementation of \"Convolutional Neural Networks for Sentence Classification\" paper 14 commits 1 branch 0 releases 1 contributor MIT Python 100.0% Python Clone or download Clone with HTTPS Use Git or checkout with SVN using the web URL. Download ZIP Find file Branch: master Switch branches\/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 00213ed Nov 1, 2016 shagunsodhani changed the order of saving the model and the weights so that trainin\u2026 \u2026 \u2026g can be terminated early Permalink Failed to load latest commit information. app changed the order of saving the model and the weights so that trainin\u2026 Nov 1, 2016 sample_dataset Added sample dataset and updated filereader to read the dataset Oct 23, 2016 .gitignore added support for checkpointing model. Fixes #2 Oct 23, 2016 LICENSE Initial commit Oct 22, 2016 README.md Added summary of the paper Nov 1, 2016 README.md CNN-Sentence-Classifier Simplified implementation of \"Convolutional Neural Networks for Sentence Classification\" paper Usage Install Keras Repository contains \"Movie reviews with one sentence per review\" (Pang and Lee, 2005) dataset in sample_dataset. Alternatively, to use some other dataset, make two files input.txt where each line is a sentence to be classified label.txt where each line is the label for corresponding line in input.txt Make model folder by running mkdir model Refer this to train or download Glove embeddings and this for Word2Vec embeddings. Run python3 app\/train.py --data_dir=path_to_folder_containing_input.txt_and_label.txt --embedding_file_path=path_to_embedding_vectors_file --model_name=name_of_model_from_the_paper For example, if data is in data folder, embedding file is vectors.txt and model is cnn_static, run python3 app\/train.py --data_dir=data --embedding_file_path=vectors.txt --model_name=cnn_static To define your own model, pass model_name as self, define your model in app\/model\/model.py and invoke from model_selector function (in model.py). All supported arguments can be seen in here References Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014) Summary of paper Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"sybilckw","created":"Thu Oct 13 10:09:18 EDT 2016","text":"Under President Obama\u2019s leadership, America continues to be the world\u2019s most innovative country, with the greatest potential to develop the industries of the future and harness science and technology to help address important challenges. Over the past 8 years, President Obama has relentlessly focused on building U.S. capacity in science and technology. This Thursday, President Obama will host the White House Frontiers Conference in Pittsburgh to imagine the Nation and the world in 50 years and beyond, and to explore America\u2019s potential to advance towards the frontiers that will make the world healthier, more prosperous, more equitable, and more secure.  Today, to ready the United States for a future in which Artificial Intelligence (AI) plays a growing role, the White House is releasing a report on future directions and considerations for AI called Preparing for the Future of Artificial Intelligence. This report surveys the current state of AI, its existing and potential applications, and the questions that progress in AI raise for society and public policy. The report also makes recommendations for specific further actions. A companion National Artificial Intelligence Research and Development Strategic Plan is also being released, laying out a strategic plan for Federally-funded research and development in AI. Preparing for the Future of Artificial Intelligence details several policy opportunities raised by AI, including how the technology can be used to advance social good and improve government operations; how to adapt regulations that affect AI technologies, such as automated vehicles, in a way that encourages innovation while protecting the public; how to ensure that AI applications are fair, safe, and governable; and how to develop a skilled and diverse AI workforce. The publication of this report follows a series of public-outreach activities spearheaded by the White House Office of Science and Technology Policy (OSTP) in 2016, which included five co-hosted public workshops held across the country, as well as a Request for Information (RFI) in June 2016 that received 161 responses. These activities helped inform the focus areas and recommendations included in the report. Advances in AI technology hold incredible potential to help America stay on the cutting edge of innovation. Already, AI technologies have opened up new markets and new opportunities for progress in critical areas such as health, education, energy, and the environment. In recent years, machines have surpassed humans in the performance of certain specific tasks, such as some aspects of image recognition. Although it is very unlikely that machines will exhibit broadly-applicable intelligence comparable to or exceeding that of humans in the next 20 years, experts forecast that rapid progress in the field of specialized AI will continue, with machines reaching and exceeding human performance on an increasing number of tasks. One of the most important issues raised by AI is its impact on jobs and the economy. The report recommends that the White House convene a study on automation and the economy, resulting in a follow-on public report that will be released by the end of this year. In the coming years, AI will continue contributing to economic growth and will be a valuable tool for improving the world in fields as diverse as health care, transportation, the environment, criminal justice, and economic inclusion. The Administration believes that it is critical that industry, civil society, and government work together to develop the positive aspects of the technology, manage its risks and challenges, and ensure that everyone has the opportunity to help in building an AI-enhanced society and to participate in its benefits. To read the Future of AI report, click here. And tune-in for the White House Frontiers Conference on October 13 for more on the #FutureofAI, including discussions with leading experts on harnessing the potential of AI, including data science, machine learning, automation, and robotics to engage and benefit all Americans. Watch the conference live and learn more at: http:\/\/www.frontiersconference.org. Ed Felten is a Deputy U.S Chief Technology Officer in the White House Office of Science and Technology Policy. Terah Lyons is a Policy Advisor to the U.S. Chief Technology Officer in White House Office of Science and Technology Policy.","flair":"two\tNews"}
{"author":"spruceabtuse","created":"Wed Nov 23 10:52:59 EST 2016","text":" Menu Search Follow Us Facebook Instagram Twitter Youtube Flipboard LinkedIn Google+ RSS More Youtube Flipboard LinkedIn Google+ RSS Got a tip? Let us know. News Channels Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Topics All Galleries All Timelines Video Shows Apps News Crunch Report Disrupt SF 2016 Gadgets Reviews Interviews TC Features All Shows All Videos Events TechCrunch Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store News About Mobile World Congress CES All Events Crunchbase Trending Tesla Google Facebook News Startups Mobile Gadgets Enterprise Social Europe Message Us Search TechCrunch Search TechCrunch Search Search × Hi! You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you. Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot. Thanks, TC Team Cyber Monday SaleGet A $200 Holiday Gift Guide Item Free With Disrupt London Early Bird Ticket Purchase Today Only - Get Yours Now Artificial Intelligence Relax, artificial intelligence isn\u2019t coming for your job Japan looks to create a superfast supercomputer for deep learning Google\u2019s AI translation tool seems to have invented its own secret internal language Browse more... AI AI-powered virtual assistant, Mezi, pivots to focus on travel Adobe makes big bets on AI and the public cloud Zugata raises $7 million to make annual performance reviews obsolete Browse more... huawei Huawei goes over-the-top with a Porsche Design version of its Mate 9 Huawei\u2019s Mate 9 has a 5.9-inch display, a giant battery and is coming to the States, eventually Huawei\u2019s Fit is a no-frills activity tracker Browse more... Huawei puts $1M into a new AI research partnership with UC Berkeley Posted Oct 11, 2016 by Ingrid Lunden (@ingridlunden) 0 SHARES Next Story Samsung kills the Galaxy Note 7 for good Artificial intelligence continues to have its moment in the spotlight, with a surge of interest in startups and efforts from huge tech companies to push the boundaries of how we might best use machine learning, computer vision and other areas of AI in the future. The latest development on that front comes from China\u2019s Huawei, which today announced that it would form a research partnership with UC Berkeley focused on AI, and fund it to the initial tune of $1 million. The alliance, between Huawei\u2019s Noah\u2019s Ark Laboratory and Berkeley Artificial Intelligence Research (BAIR), is being billed as a \u201Cstrategic partnership into basic research\u201D, and it will cover areas like deep learning, reinforcement learning, machine learning, natural language processing and computer vision. \u201CThe two parties believe that this strategic partnership will fuel the advancement of AI technology and create completely new experiences for people, thus contributing greatly to society at large,\u201D Huawei notes. Some of these areas of AI you will have heard a lot about already. Machine learning has become a central part of a lot of basic large-scale computing projects, from bots to search engines and more. Computer vision is being applied in areas like facial recognition tech, AR, VR and self-driving applications. NLP is what makes services like Amazon\u2019s Alexa, Apple\u2019s Siri, and Microsoft\u2019s Cortana work. Others are still in an early phase: reinforcement learning, for example, sits on top of AI systems to help them learn to make better decisions. To date it is still only being worked on by a handful of companies, so that is one key area where Huawei could potentially get a jump by partnering with a university directly. AI has become a central part of how many tech companies are thinking about their next generation of devices and services. But one of the challenges for companies like Huawei \u2014 one of the world\u2019s biggest phone and telecoms equipment makers, and highly ambitious in the wider world of consumer electronics \u2014 is that it has become a race for talent. Competitors like Google, Apple, Amazon and Facebook have been working hard to not only develop a lot of the cutting-edge work in-house, but they have been working closely with universities, as well as snapping up companies and talent coming straight out of universities, to advance their in-house work. Google back in 2014, off the back of its acquisition of DeepMind, formed a partnership with Oxford University, contributing funding to AI research there, and acqui-hired two of the startups that had been incubated at Oxford as part of that effort. Apple has also been known to tap and buy startups that are fresh out of the research labs, such as its acquisition of Faceshift last year. And Amazon is running university competitions to lure out interesting ideas in AI. More recently, several large tech companies (pointedly not including Huawei) formed a non-profit group to explore the opportunities and pitfalls of AI. You could say that now Huawei is rising to challenge by working with researchers at Berkeley on their own AI projects. Berkeley has a history of extensive work and research in AI, and while the BAIR lab is a relatively new face for that effort, it already works with 11 other big companies as \u201Cindustrial partners\u201D, which means Huawei is not exactly getting free reign as a corporate partner. (The others are Facebook, Microsoft, Samsung, Sony, Adobe, Amazon, Yahoo Japan, Nvidia, Intel, Siemens \u2014 itself investing $1 billion into AI research \u2014 and Citris.) The lab also has 2 dozen faculty and 100 grad students. Huawei\u2019s expressively named Noah\u2019s Ark lab, meanwhile, has been around for about four years and is part of the company\u2019s effort to put some $8.1 billion annually into R&D. Huawei is very focused on advancing on this front, and has over 75,000 employees working solely on R&D \u2014 perhaps one reason why MIT Technology Review named it the 10th \u201Csmartest company\u201D in a recent survey. Maybe fittingly for an organization called Noah\u2019s Ark, expanding research to Berkeley appears to be part of a wider remit to plant seeds for Huawei research globally: another recent outpost was created in Paris. It\u2019s just early days, and so it remains to be seen whether Huawei can attract and bottle some of the more interesting AI tech that it will come across in this newest partnership with Berkeley. But at the very least, forging the alliance will put it in a position to try. 0 SHARES Advertisement Advertisement Crunchbase Huawei Technologies Founded 1987 Overview Huawei Technologies is a telecom solutions company that offers infrastructure application software and devices with wireline, wireless, and IP technologies. The company\u2019s products and solutions have been deployed in over 100 countries and have served 45 of the world's top 50 telecom operators as well as one third of the world's population. Huawei Technologies has 3 divisions in the United States: \u2026 Location Plano, TX Categories Security, Web Hosting, Wireless, Mobile Website http:\/\/huawei.com Full profile for Huawei Technologies U.C. Berkeley Founded 1868 Overview The University of California, Berkeley is a public research university located in Berkeley, California. It is the most selective \u2013 and highest ranked in U.S. News and ARWU \u2013 public university in the world for undergraduate education. Aside from its academic prestige, UC Berkeley is also well known for producing a high number of entrepreneurs. Full profile for U.C. Berkeley Newsletter Subscriptions The Daily Crunch Get the top tech stories of the day delivered to your inbox TC Weekly Roundup Get a weekly recap of the biggest tech stories Crunchbase Daily The latest startup funding announcements Enter Address Subscribe Latest Crunch Report Facebook Builds a Censorship Tool | Crunch Report Watch More Episodes AI huawei Artificial Intelligence Popular Posts Featured Stories Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 | Sarah Perez Siren Care makes a \u201Csmart\u201D sock to track diabetic health Nov 25, 2016 | Sarah Buhr Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 | Connie Loizos Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 | Ingrid Lunden Latest From Asia Uber rival Grab\u2019s first CFO is leaving the company after just seven months Nov 25, 2016 | Jon Russell China\u2019s Ctrip is buying flight search company Skyscanner for $1.74 billion Nov 23, 2016 | Jon Russell Facebook is unlikely to succeed in China, even if it compromises on free speech Nov 23, 2016 | Jon Russell Singapore\u2019s Spacemob lands $5.5M to expand its co-working spaces across Asia Pacific Nov 23, 2016 | Jon Russell Comment moderation powered by Up Next Samsung kills the Galaxy Note 7 for good Posted Oct 11, 2016 CrunchBoard Job Listings CRM Berater (m\/f) at eGym GmbH (München, Deutschland) C++ Developer - Qt Product Development (m\/f) at eGym GmbH (München, Deutschland) Full Stack Engineer at FactorChain at The Sourcery (Los Altos, CA, United States) UI Engineering @ FactorChain at The Sourcery (Los Altos, CA, United States) Senior Backend Engineer @ MeetMe at The Sourcery (San Francisco, CA, United States) More from CrunchBoard Advertisement TechCrunch News Video Events Crunchbase TechCrunch Store About Staff Contact Us Advertise With Us Send Us A Tip International China Europe Japan Follow TechCrunch Facebook Twitter Google+ LinkedIn Youtube Pinterest Tumblr Instagram StumbleUpon Feed TechCrunch Apps iOS Android Windows 8 Subscribe to The Daily Crunch Latest headlines delivered to you daily Subscribe to <span class=\"no-mobile\">Subscribe to <\/span>The Daily Crunch Enter Email Address Subscribe © 2013-2016 AOL Inc. All rights reserved. Aol Tech Privacy Policy About Our Ads Anti Harassment Policy Terms of Service Powered by WordPress.com VIP Fonts by News Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Galleries All Timelines Videos Apps News Crunch Report Disrupt SF 2016 All Shows All Videos Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store All Events Crunchbase Message Us Most Popular RAWR: Samsung Canada Wins The Internet With This Custom Galaxy S III Aug 28, 2012 by Matt Burns A Dongle Joke That Spiraled Way Out Of Control Mar 21, 2013 by Kim-Mai Cutler Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago by Ron Miller Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 by Sarah Perez Disrupting the world of science publishing 8 hours ago by Bérénice Magistretti Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 by Ingrid Lunden Xiaomi admits it doesn\u2019t make money on smartphone hardware sales Nov 25, 2016 by Darrell Etherington UPDATED: Machine learning can fix Twitter, Facebook, and maybe even America yesterday by Chris Nicholson Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 by Connie Loizos ","flair":"two\tNews"}
{"author":"pmigdal","created":"Tue Oct 11 06:25:16 EDT 2016","text":"Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images). Full Citation: Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in Neural Information Processing Systems 29. (pdf) Code repository for the experiments in this paper is available here.","flair":"three\tResearch"}
{"author":"The_Man_of_Science","created":"Thu Nov 03 18:55:48 EDT 2016","text":" Under review as a conference paper at ICLR 2017 LEARNING PYTHON CODE SUGGESTION WITH A SPARSE POINTER NETWORK Avishkar Bhoopchand, Tim Rocktäschel, Earl Barr & Sebastian Riedel Department of Computer Science University College London avishkar.bhoopchand.15@ucl.ac.uk, {t.rocktaschel,e.barr,s.riedel}@cs.ucl.ac.uk ABSTRACT To enhance developer productivity, all modern integrated development environ- ments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their re- liance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long- range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate pre- diction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past. 1 INTRODUCTION Integrated development environments (IDEs) are essential tools for programmers. Especially when a developer is new to a codebase, one of their most useful features is code suggestion: given a piece of code as context, suggest a likely sequence of next tokens. Typically, the IDE suggests an identifier or a function call, including API calls. While extensive support exists for statically-typed languages such as Java, code suggestion for dynamic languages like Python is harder and less well supported because of the lack of type annotations. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recently, methods from statistical natural language processing (NLP) have been used to train code suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis & Sutton, 2013; Tu et al., 2014). To this end, usually an n-gram language model is trained to score possible completions. Neural language models for code suggestion (White et al., 2015; Das & Shah, 2015) have extended this line of work to capture more long-range dependencies. Yet, these standard neural language models are limited by the so-called hidden state bottleneck, i.e., all context information has to be stored in a fixed-dimensional internal vector representation. This limitation restricts such models to local phenomena and does not capture very long-range semantic relationships like suggesting calling a function that has been defined many tokens before. To address these issues, we create a large corpus of 41M lines of Python code by using a heuristic for crawling high-quality code repositories from GitHub. We investigate, for the first time, the use of attention (Bahdanau et al., 2014) for code suggestion and find that, despite a substantial improvement 1 Under review as a conference paper at ICLR 2017 in accuracy, it still makes avoidable mistakes. Hence, we introduce a model that leverages long-range Python dependencies by selectively attending over the introduction of identifiers as determined by examining the Abstract Syntax Tree. The model is a form of pointer network (Vinyals et al., 2015a), and learns to dynamically choose between syntax-aware pointing for modeling long-range dependencies and free form generation to deal with local phenomena, based on the current context. Our contributions are threefold: (i) We release a code suggestion corpus of 41M lines of Python code crawled from GitHub, (ii) We introduce a sparse attention mechanism that captures very long-range dependencies for code suggestion of this dynamic programming language efficiently, and (iii) We provide a qualitative analysis demonstrating that this model is indeed able to learn such long-range dependencies. 2 METHODS We first revisit neural language models, before briefly describing how to extend such a language model with an attention mechanism. Then we introduce a sparse attention mechanism for a pointer network that can exploit the Python abstract syntax tree of the current context for code suggestion. 2.1 NEURAL LANGUAGE MODEL Code suggestion can be approached by a language model that measures the probability of observing a sequence of tokens in a Python program. For example, for the sequence S = a1, . . . , aN , the joint probability of S factorizes according to Pθ(S) = Pθ(a1) · N∏ t=2 Pθ(at | at−1, . . . , a1) (1) where the parameters θ are estimated from a training corpus. Given a sequence of Python tokens, we seek to predict the next M tokens at+1, . . . , at+M that maximize Equation 1 argmax at+1, ..., at+M Pθ(a1, . . . , at, at+1, . . . , at+M ). (2) In this work, we build upon neural language models using Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997). This neural language model estimates the probabilities in Equation 1 using the output vector of an LSTM at time step t (denoted ht here) according to Pθ(at = τ | at−1, . . . , a1) = exp (vTτ ht + bτ )∑ τ \u2032 exp (v T τ \u2032ht + bτ \u2032) (3) where vτ is a parameter vector associated with token τ in the vocabulary. Neural language models can, in theory, capture long-term dependencies in token sequences through their internal memory. However, as this internal memory has fixed dimension and can be updated at every time step, such models often only capture local phenomena. In contrast, we are interested in very long-range dependencies like referring to a function identifier introduced many tokens in the past. For example, a function identifier may be introduced at the top of a file and only used near the bottom. In the following, we investigate various external memory architectures for neural code suggestion. 2.2 ATTENTION A straight-forward approach to capturing long-range dependencies is to use a neural attention mech- anism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention mechanisms have been successfully applied to sequence-to-sequence tasks such as machine transla- tion (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rocktäschel et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al. (2016) and Tran et al. (2016). 2 Under review as a conference paper at ICLR 2017 Formally, an attention mechanism with a fixed memory Mt ∈ Rk×K of K vectors mi ∈ Rk for i ∈ [1,K], produces an attention distribution αt ∈ RK and context vector ct ∈ Rk at each time step t according to Equations 4 to 7. Furthermore, WM ,W h ∈ Rk×k and w ∈ Rk are trainable parameters. Finally, note that 1K represents a K-dimensional vector of ones. Mt = [m1 . . . mK ] ∈ Rk×K (4) Gt = tanh(WMMt + 1TK(W hht)) ∈ Rk×K (5) αt = softmax(wTGt) ∈ R1×K (6) ct =Mtα T t ∈ Rk (7) For language modeling, we populate Mt with a fixed window of the previous K LSTM output vectors. To obtain a distribution over the next token we combine the context vector ct of the attention mechanism with the output vector ht of the LSTM using a trainable projection matrixWA ∈ Rk×2k. The resulting final output vector nt ∈ Rk encodes the next-word distribution and is projected to the size of the vocabulary |V |. Subsequently, we apply a softmax to arrive at a probability distribution yt ∈ R|V | over the next token. This process is presented in Equation 9 where WV ∈ R|V |×k and bV ∈ R|V | are trainable parameters. nt = tanh ( WA [ ht ct ]) ∈ Rk (8) yt = softmax(W V nt + b V ) ∈ R|V | (9) The problem of the attention mechanism above is that it quickly becomes computationally expensive for large K. Moreover, attending over many memories can make training hard as a lot of noise is introduced in early stages of optimization where the LSTM outputs (and thus the memoryMt) are more or less random. To alleviate these problems we now turn to pointer networks and a simple heuristic for populating Mt that permits the efficient retrieval of identifiers in a large history of Python code. 2.3 SPARSE POINTER NETWORK We develop an attention mechanism that provides a filtered view of a large history of Python tokens. At any given time step, the memory consists of context representations of the previous K identifiers introduced in the history. This allows us to model long-range dependencies found in identifier usage. For instance, a class identifier may be declared hundreds of lines of code before it is used. Given a history of Python tokens, we obtain a next-word distribution from a weighed average of the sparse pointer network for identifier reference and a standard neural language model. The weighting of the two is determined by a controller. Formally, at time-step t, the sparse pointer network operates on a memoryMt ∈ Rk×K of only the K previous identifier representations (e.g. function identifiers, class identifiers and so on). In addition, we maintain a vectormt = [id1, . . . , idK ] ∈ NK of symbol ids for these identifier representations (i.e. pointers into the large global vocabulary). As before, we calculate a context vector ct using the attention mechanism (Equation 7), but on a memoryMt only containing representations of identifiers that were declared in the history. Next, we obtain a pseudo-sparse distribution over the global vocabulary from st[i] = { αt[j] ifmt[j] = i −C otherwise (10) it = softmax(st) ∈ R|V | (11) where−C is a large negative constant (e.g. −1000). In addition, we calculate a next-word distribution from a standard neural language model yt = softmax(W V ht + bV ) ∈ R|V | (12) 3 Under review as a conference paper at ICLR 2017 Figure 1: Sparse pointer network for code suggestion on a Python code snippet, showing the next- word distributions of the language model and identifier attention and their weighted combination through λ and we use a controller to calculate a distribution λt ∈ R2 over the language model and pointer network for the final weighted next-word distribution y∗t via hλt = [ ht xt ct ] ∈ R3k (13) λt = softmax(W λhλt + b λ) ∈ R2 (14) y∗t = [yt it]λt ∈ R|V | (15) Here, xt is the representation of the input token, and W λ ∈ R2×3k and bλ ∈ R2 a trainable weight matrix and bias respectively. This controller is conditioned on the input, output and context representations. This means for deciding whether to refer to an identifier or generate from the global vocabulary, the controller has access to information from the encoded next-word distribution ht of the standard neural language model, as well as the attention-weighted identifier representations ct from the current history. Figure 1 overviews this process. In it, the identifier base_path appears twice, once as an argument to a function and once as a member of a class (denoted by *). Each appearance has a different id in the vocabulary and obtains a different probability from the model. In the example, the model correctly chooses to refer to the member of the class instead of the out-of-scope function argument, although, from a user point-of-view, the suggestion would be the same in both cases. 3 LARGE-SCALE PYTHON CORPUS Previous work on code suggestion either focused on statically-typed languages (particularly Java) or trained on very small corpora. Thus, we decided to collect a new large-scale corpus of the dynamic programming language Python. According to the programming language popularity website Pypl (Carbonnelle, 2016), Python is the second most popular language after Java. It is also the 3rd most common language in terms of number of repositories on the open-source code repository GitHub, after JavaScript and Java (Zapponi, 2016). We collected a corpus of 41M lines of Python code from GitHub projects. Ideally, we would like this corpus to only contain high-quality Python code, as our language model learns to suggest code from how users write code. However, it is difficult to automatically assess what constitutes high-quality code. Thus, we resort to the heuristic that popular code projects tend to be of good quality, There are 4 Under review as a conference paper at ICLR 2017 Table 1: Python corpus statistics. Dataset #Projects #Files #Lines #Tokens Vocabulary Size Train 489 118 298 26 868 583 88 935 698 2 323 819 Dev 179 26 466 5 804 826 18 147 341 Test 281 43 062 8 398 100 30 178 356 Total 949 187 826 41 071 509 137 261 395 Figure 2: Example of the Python code normalization. Original file on the left and normalized version on the right. two metrics on GitHub that we can use for this purpose, namely stars (similar to bookmarks) and forks (copies of a repository that allow users to freely experiment with changes without affecting the original repository). Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select Python projects with more than 100 stars, sort by the number of forks descending, and take the top 1000 projects. We then removed projects that did not compile with Python3, leaving us with 949 projects. We split the corpus on the project level into train, dev, and test. Table 1 presents the corpus statistics. 3.1 NORMALIZATION OF IDENTIFIERS Unsurprisingly, the long tail of words in the vocabulary consists of rare identifiers. To improve generalization, we normalize identifiers before feeding the resulting token stream to our models. That is, we replace every identifier name with an anonymous identifier indicating the identifier group (class, variable, argument, attribute or function) concatenated with a random number that makes the identifier unique in its scope. Note that we only replace novel identifiers defined within a file. Identifier references to external APIs and libraries are left untouched. Consistent with previous corpus creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing less than five times with an $OOV$ (out of vocabulary) token. 4 EXPERIMENTS Although previous work by White et al. (2015) already established that a simple neural language model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines to confirm this observation. Specifically, we use n-gram models for n ∈ {3, 4, 5, 6} with Modified Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig, 2012). We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated backpropagation through time (Werbos, 1990) with a history of 20 identifier representations. We use 5 Under review as a conference paper at ICLR 2017 Table 2: Perplexity (PP), Accuracy (Acc) and Accuarcy among top 5 predictions (Acc@5). Model Train PP Dev PP Test PP Acc [%] Acc@5 [%] All IDs Other All IDs Other 3-gram 12.90 24.19 26.90 13.19 \u2013 \u2013 50.81 \u2013 \u2013 4-gram 7.60 21.07 23.85 13.68 \u2013 \u2013 51.26 \u2013 \u2013 5-gram 4.52 19.33 21.22 13.90 \u2013 \u2013 51.49 \u2013 \u2013 6-gram 3.37 18.73 20.17 14.51 \u2013 \u2013 51.76 \u2013 \u2013 LSTM 9.29 13.08 14.01 57.91 2.1 62.8 76.30 4.5 82.6 LSTM w\/ Attention 20 7.30 11.07 11.74 61.30 21.4 64.8 79.32 29.9 83.7 LSTM w\/ Attention 50 7.09 9.83 10.05 63.21 30.2 65.3 81.69 41.3 84.1 Sparse Pointer Network 6.41 9.40 9.18 62.97 27.3 64.9 82.62 43.6 84.5 an initial learning rate of 0.7 and decay it by 0.9 after every epoch. As additional baselines, we test a neural language model with LSTM units with and without attention. For the attention language models, we experiment with a fixed-window attention memory of the previous 20 and 50 tokens respectively, and a batch size of 75. All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using cross-entropy loss. While processing a Python source code file, the last recurrent state of the RNN is fed as the initial state of the subsequent sequence of the same file and reset between files. All models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015), gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval (−0.05, 0.05). As regularizer, we use a dropout of 0.1 on the input representations. Furthermore, we use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size of 1000. 5 RESULTS We evaluate all models using perplexity (PP), as well as accuracy of the top prediction (Acc) and the top five predictions (Acc@5). The results are summarized in Table 2. We can confirm that for code suggestion neural models outperform n-gram language models by a large margin. Furthermore, adding attention improves the results substantially (2.3 lower perplexity and 3.4 percentage points increased accuracy). Interestingly, this increase can be attributed to a superior prediction of identifiers, which increased from an accuracy of 2.1% to 21.4%. An LSTM with an attention window of 50 gives us the best accuracy for the top prediction. We achieve further improvements for perplexity and accuracy of the top five predictions by using a sparse pointer network that uses a smaller memory of the past 20 identifier representations. 5.1 QUALITATIVE ANALYSIS Figures 3a-d show a code suggestion example involving an identifier usage. While the LSTM baseline is uncertain about the next token, we get a sensible prediction by using attention or the sparse pointer network. The sparse pointer network provides more reasonable alternative suggestions beyond the correct top suggestion. Figures 3e-h show the use-case referring to a class attribute declared 67 tokens in the past. Only the Sparse Pointer Network makes a good suggestion. Furthermore, the attention weights in 3i demonstrate that this model distinguished attributes from other groups of identifiers. We give a full example of a token-by-token suggestion of the Sparse Pointer Network in Figure 4 in the Appendix. 6 RELATED WORK Previous code suggestion work using methods from statistical NLP has mostly focused on n-gram models. Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall 6 Under review as a conference paper at ICLR 2017 (a) Code snippet for referencing variable. (b) LSTM Model. (c) LSTM w\/ Attention 50. (d) Sparse Pointer Net- work. (e) Code snippet for referencing class member. (f) LSTM Model. (g) LSTM w\/ Attention 50. (h) Sparse Pointer Net- work. (i) Sparse Pointer Network attention over memory of identifier representations. Figure 3: Code suggestion example involving a reference to a variable (a-d), a long-range dependency (e-h), and the attention weights of the Sparse Pointer Network (i). into a much smaller space than the flexibility of programming languages allows. They were able to capture the repetitiveness and predictable statistical properties of real programs using language models. Subsequently, Tu et al. (2014) improved upon Hindle et al.\u2019s work by adding a cache mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of program modules. Tu et al.\u2019s idea of adding a cache mechanism to the language model is specifically designed to exploit the properties of source code, and thus follows the same aim as the sparse attention mechanism introduced in this paper. While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a corpus of 352M lines of Java code which they analysed with n-gram language models. The size of the corpus allowed them to train a single language model that was effective across multiple different project domains. White et al. (2015) later demonstrated that neural language models outperform n-gram models for code suggestion. They compared various n-gram models (up to nine grams), including Tu et al.\u2019s cache model, with a basic RNN neural language model. Khanh Dam et al. (2016) compared White et al.\u2019s basic RNN with LSTMs and found that the latter are better at code suggestion due to their improved ability to learn long-range dependencies found in source code. Our paper extends this line of work by introducing a sparse attention model that captures even longer dependencies. The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al. (2016) who equipped LSTM cells with a fixed-length memory tape rather than a single memory cell. They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al., 1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English, German and Italian and outperformed both n-gram and neural language models. Their memory encompasses representations of all possible words in the vocabulary rather than providing a sparse view as we do. An alternative to our purely lexical approach to code suggestion involves the use of probabilistic context-free grammars (PCFGs) which exploit the formal grammar specifications and well-defined, deterministic parsers available for source code. These were used by Allamanis & Sutton (2014) to extract idiomatic patterns from source code. A weakness of PCFGs is their inability to model context-dependent rules of programming languages such as that variables need to be declared before 7 Under review as a conference paper at ICLR 2017 being used. Maddison & Tarlow (2014) added context-aware variables to their PCFG model in order to capture such rules. Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions. Our use of a controller for deciding whether to generate from a language model or copy an identifier using a sparse pointer network is inspired by their latent code predictor. However, their inputs (textual descriptions) are short whereas code suggestion requires capturing very long-range dependencies that we addressed by a filtered view on the memory of previous identifier representations. 7 CONCLUSIONS AND FUTURE WORK In this paper, we investigated neural language models for code suggestion of the dynamically-typed programming language Python. We released a corpus of 41M lines of Python crawled from GitHub and compared n-gram, standard neural language models, and attention. By using attention, we observed an order of magnitude more accurate prediction of identifiers. Furthermore, we proposed a sparse pointer network that can efficiently capture long-range dependencies by only operating on a filtered view of a memory of previous identifier representations. This model achieves the lowest perplexity and best accuracy among the top five predictions. The Python corpus and code for replicating our experiment is released at TODO. The presented methods were only tested for code suggestion within the same Python file. We are interested in scaling the approach to the level of entire code projects and collections thereof, as well as integrating a trained code suggestion model into an existing IDE. Furthermore, we plan to work on code completion, i.e., models that provide a likely continuation of a partial token, using character language models (Graves, 2013). ACKNOWLEDGMENTS This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award. REFERENCES Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zhang. Tensorflow: A system for large-scale machine learning. CoRR, abs\/1605.08695, 2016. URL http:\/\/arxiv.org\/abs\/1605. 08695. Miltiadis Allamanis and Charles Sutton. Mining idioms from source code. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pp. 472\u2013483, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi: 10.1145\/2635868.2635901. URL http:\/\/doi.acm.org\/10.1145\/2635868.2635901. Miltiadis Allamanis and Charles A. Sutton. Mining source code repositories at massive scale using language modeling. In Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim (eds.), MSR, pp. 207\u2013216. IEEE Computer Society, 2013. ISBN 978-1-4673-2936-1. URL http:\/\/dblp.uni-trier.de\/db\/conf\/msr\/msr2013.html#AllamanisS13a. Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding conventions. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pp. 281\u2013293, New York, NY, USA, 2014. ACM. ISBN 978- 1-4503-3056-5. doi: 10.1145\/2635868.2635883. URL http:\/\/doi.acm.org\/10.1145\/ 2635868.2635883. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs\/1409.0473, 2014. URL http:\/\/arxiv.org\/abs\/ 1409.0473. 8 TODO http:\/\/arxiv.org\/abs\/1605.08695 http:\/\/arxiv.org\/abs\/1605.08695 http:\/\/doi.acm.org\/10.1145\/2635868.2635901 http:\/\/dblp.uni-trier.de\/db\/conf\/msr\/msr2013.html#AllamanisS13a http:\/\/doi.acm.org\/10.1145\/2635868.2635883 http:\/\/doi.acm.org\/10.1145\/2635868.2635883 http:\/\/arxiv.org\/abs\/1409.0473 http:\/\/arxiv.org\/abs\/1409.0473 Under review as a conference paper at ICLR 2017 Pierre Carbonnelle. Pypl popularity of programming language. http:\/\/pypl.github.io\/ PYPL.html, 2016. URL http:\/\/pypl.github.io\/PYPL.html. [Online; accessed 30- August-2016]. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 551\u2013561. Association for Computational Linguistics, 2016. URL http:\/\/ aclweb.org\/anthology\/D16-1053. Subhasis Das and Chinmayee Shah. Contextual code completion using machine learning. 2015. Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs\/1308.0850, 2013. URL http:\/\/arxiv.org\/abs\/1308.0850. Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and compre- hend. In Advances in Neural Information Processing Systems 28: Annual Confer- ence on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1693\u20131701, 2015. URL http:\/\/papers.nips.cc\/paper\/ 5945-teaching-machines-to-read-and-comprehend. Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of software. In Proceedings of the 34th International Conference on Software Engineering, ICSE \u201912, pp. 837\u2013847, Piscataway, NJ, USA, 2012. IEEE Press. ISBN 978-1-4673-1067-3. URL http:\/\/dl.acm.org\/citation.cfm?id=2337223.2337322. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u2013 1780, November 1997. ISSN 0899-7667. doi: 10.1162\/neco.1997.9.8.1735. URL http:\/\/dx. doi.org\/10.1162\/neco.1997.9.8.1735. Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1\u201310, Beijing, China, July 2015. Association for Computational Linguistics. URL http:\/\/www.aclweb.org\/anthology\/P15-1001. Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In David Blei and Francis Bach (eds.), Proceedings of the 32nd Inter- national Conference on Machine Learning (ICML-15), pp. 2342\u20132350. JMLR Workshop and Conference Proceedings, 2015. URL http:\/\/jmlr.org\/proceedings\/papers\/v37\/ jozefowicz15.pdf. H. Khanh Dam, T. Tran, and T. Pham. A deep language model for software code. ArXiv e-prints, August 2016. R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp. 181\u2013184 vol.1, May 1995. doi: 10.1109\/ICASSP.1995.479394. Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744, 2016. Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In International Conference on Machine Learning, 2014. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313\u2013330, 1993. Graham Neubig. Kylm - the kyoto language modeling toolkit. http:\/\/www.phontron.com\/ kylm\/, 2012. URL http:\/\/www.phontron.com\/kylm\/. [Online; accessed 23-July- 2016]. 9 http:\/\/pypl.github.io\/PYPL.html http:\/\/pypl.github.io\/PYPL.html http:\/\/pypl.github.io\/PYPL.html http:\/\/aclweb.org\/anthology\/D16-1053 http:\/\/aclweb.org\/anthology\/D16-1053 http:\/\/arxiv.org\/abs\/1308.0850 http:\/\/papers.nips.cc\/paper\/5945-teaching-machines-to-read-and-comprehend http:\/\/papers.nips.cc\/paper\/5945-teaching-machines-to-read-and-comprehend http:\/\/dl.acm.org\/citation.cfm?id=2337223.2337322 http:\/\/dx.doi.org\/10.1162\/neco.1997.9.8.1735 http:\/\/dx.doi.org\/10.1162\/neco.1997.9.8.1735 http:\/\/www.aclweb.org\/anthology\/P15-1001 http:\/\/jmlr.org\/proceedings\/papers\/v37\/jozefowicz15.pdf http:\/\/jmlr.org\/proceedings\/papers\/v37\/jozefowicz15.pdf http:\/\/www.phontron.com\/kylm\/ http:\/\/www.phontron.com\/kylm\/ http:\/\/www.phontron.com\/kylm\/ Under review as a conference paper at ICLR 2017 Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310\u20131318, 2013. URL http:\/\/jmlr.org\/ proceedings\/papers\/v28\/pascanu13.html. Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom. Reasoning about entailment with neural attention. In ICLR, 2016. Ke M. Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language modeling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pp. 321\u2013331, 2016. URL http:\/\/aclweb.org\/anthology\/N\/ N16\/N16-1036.pdf. Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2014, pp. 269\u2013280, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3056-5. doi: 10.1145\/2635868.2635875. URL http:\/\/doi.acm.org\/10.1145\/2635868.2635875. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692\u20132700, 2015a. Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pp. 2773\u20132781, 2015b. URL http:\/\/papers.nips.cc\/paper\/ 5635-grammar-as-a-foreign-language. Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990. Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshyvanyk. Toward deep learning software repositories. In Proceedings of the 12th Working Conference on Mining Software Repositories, MSR \u201915, pp. 334\u2013345, Piscataway, NJ, USA, 2015. IEEE Press. URL http:\/\/dl.acm.org\/citation.cfm?id=2820518.2820559. Carlo Zapponi. Githut - programming languages and github. http:\/\/githut.info\/, 2016. URL http:\/\/githut.info\/. [Online; accessed 19-August-2016]. 10 http:\/\/jmlr.org\/proceedings\/papers\/v28\/pascanu13.html http:\/\/jmlr.org\/proceedings\/papers\/v28\/pascanu13.html http:\/\/aclweb.org\/anthology\/N\/N16\/N16-1036.pdf http:\/\/aclweb.org\/anthology\/N\/N16\/N16-1036.pdf http:\/\/doi.acm.org\/10.1145\/2635868.2635875 http:\/\/papers.nips.cc\/paper\/5635-grammar-as-a-foreign-language http:\/\/papers.nips.cc\/paper\/5635-grammar-as-a-foreign-language http:\/\/dl.acm.org\/citation.cfm?id=2820518.2820559 http:\/\/githut.info\/ http:\/\/githut.info\/ Under review as a conference paper at ICLR 2017 APPENDIX Figure 4: Full example of code suggestion with a Sparse Pointer Network. Boldface tokens on the left show the first declaration of an identifier. The middle part visualizes the memory of representations of these identifiers. The right part visualizes the output λ of the controller, which is used for interpolating between the language model (LM) and the attention of the pointer network (Att). 11 Introduction Methods Neural Language Model Attention Sparse Pointer Network Large-scale Python Corpus Normalization of Identifiers Experiments Results Qualitative Analysis Related Work Conclusions and Future Work ","flair":"three\tResearch"}
{"author":"MetricSpade007","created":"Sat Nov 05 16:46:04 EDT 2016","text":" Under review as a conference paper at ICLR 2017 NEURO-SYMBOLIC PROGRAM SYNTHESIS Emilio Parisotto1,2, Abdel-rahman Mohamed1, Rishabh Singh1, Lihong Li1, Dengyong Zhou1, Pushmeet Kohli1 1Microsoft Research, USA 2Carnegie Mellon University, USA eparisot@andrew.cmu.edu , {asamir,risin,lihongli,denzho,pkohli}@microsoft.com ABSTRACT Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these ar- chitectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limi- tations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can au- tomatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross corre- lation I\/O network, given a set of input-output examples, produces a continuous representation of the set of I\/O examples. The second module, the Recursive- Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial pro- grams. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Ex- periments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training. 1 INTRODUCTION The act of programming, i.e., developing a procedure to accomplish a task, is a remarkable demon- stration of the reasoning abilities of the human mind. Expectedly, Program Induction is considered as one of the fundamental problems in Machine Learning and Artificial Intelligence. Recent progress on deep learning has led to the proposal of a number of promising neural architectures for this prob- lem. Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015). A common thread in this line of work is to specify the atomic operations of the network in some differentiable form, allowing efficient end-to-end training of a neural controller, or to use reinforcement learning to make hard choices about which operation to perform. While these results are impressive, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task. Motivated by the need for model interpretability and scalability to multiple tasks, we address the problem of Program Synthesis. Program Synthesis, the problem of automatically constructing pro- grams that are consistent with a given specification, has long been a subject of research in Computer Science (Biermann, 1978; Summers, 1977). This interest has been reinvigorated in recent years on 1 Under review as a conference paper at ICLR 2017 the back of the development of methods for learning programs in various domains, ranging from low-level bit manipulation code (Solar-Lezama et al., 2005) to data structure manipulations (Singh & Solar-Lezama, 2011) and regular expression based string transformations (Gulwani, 2011). Most of the recently proposed methods for program synthesis operate by searching the space of programs in a Domain-Specific Language (DSL) instead of arbitrary Turing-complete languages. This hypothesis space of possible programs is huge (potentially infinite) and searching over it is a challenging problem. Several search techniques including enumerative (Udupa et al., 2013), stochas- tic (Schkufza et al., 2013), constraint-based (Solar-Lezama, 2008), and version-space algebra based algorithms (Gulwani et al., 2012) have been developed to search over the space of programs in the DSL, which support different kinds of specifications (examples, partial programs, natural language etc.) and domains. These techniques not only require significant engineering and research effort to develop carefully-designed heuristics for efficient search, but also have limited applicability and can only synthesize programs of limited sizes and types. In this paper, we present a novel technique called Neuro-Symbolic Program Synthesis (NSPS) that learns to generate a program incrementally without the need for an explicit search. Once trained, NSPS can automatically construct computer programs that are consistent with any set of input-output examples provided at test time. Our method is based on two novel module neural architectures. The first module, called the cross correlation I\/O network, produces a continuous representation of any given set of input-output examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the input-output examples, synthesizes a program by incrementally expanding partial programs. R3NN employs a tree-based neural archi- tecture that sequentially constructs a parse tree by selecting which non-terminal symbol to expand using rules from a context-free grammar (i.e., the DSL). We demonstrate the efficacy of our method by applying it to the rich and complex domain of regular- expression-based syntactic string transformations, using a DSL based on the one used by Flash- Fill (Gulwani, 2011; Gulwani et al., 2012), a Programming-By-Example (PBE) system in Microsoft Excel 2013. Given a few input-output examples of strings, the task is to synthesize a program built on regular expressions to perform the desired string transformation. An example task that can be expressed in this DSL is shown in Figure 1, which also shows the DSL. Our evaluation shows that NSPS is not only able to construct programs for known tasks from new input-output examples, but it is also able to construct completely new programs that it had not ob- served during training. Specifically, the proposed system is able to synthesize string transformation programs for 63% of tasks that it had not observed at training time, and for 94% of tasks when 100 program samples are taken from the model. Moreover, our system is able to learn 38% of 238 real-world FlashFill benchmarks. To summarize, the key contributions of our work are: \u2022 A novel Neuro-Symbolic program synthesis technique to encode neural search over the space of programs defined using a Domain-Specific Language (DSL). \u2022 The R3NN model that encodes and expands partial programs in the DSL, where each node has a global representation of the program tree. \u2022 A novel cross-correlation based neural architecture for learning continuous representation of sets of input-output examples. \u2022 Evaluation of the NSPS approach on the complex domain of regular expression based string transformations. 2 PROBLEM DEFINITION In this section, we formally define the DSL-based program synthesis problem that we consider in this paper. Given a DSL L, we want to automatically construct a synthesis algorithm A such that given a set of input-output example, {(i1, o1), · · · , (in, on)}, A returns a program P ∈ L that conforms to the input-output examples, i.e., ∀j : 1 ≤ j ≤ n P (ij) = oj . (1) 2 Under review as a conference paper at ICLR 2017 Input v Output 1 William Henry Charles Charles, W. 2 Michael Johnson Johnson, M. 3 Barack Rogers Rogers, B. 4 Martha D. Saunders Saunders, M. 5 Peter T Gates Gates, P. String e := Concat(f1, · · · , fn) Substring f := ConstStr(s) | SubStr(v, pl, pr) Position p := (r, k,Dir) | ConstPos(k) Direction Dir := Start | End Regex r := s | T1 · · · | Tn (a) (b) Figure 1: An example FlashFill task for transforming names to lastname with initials of first name, and (b) The DSL for regular expression based string transformations. The syntax and semantics of the DSL for string transformations is shown in Figure 1(b) and Figure 7 respectively. The DSL corresponds to a large subset of FlashFill DSL (except conditionals), and allows for a richer class of substring operations than FlashFill. A DSL program takes as input a string v and returns an output string o. The top-level string expression e is a concatenation of a finite list of substring expressions f1, · · · , fn. A substring expression f can either be a constant string s or a substring expression, which is defined using two position logics pl (left) and pr (right). A position logic corresponds to a symbolic expression that evaluates to an index in the string. A position logic p can either be a constant position k or a token match expression (r, k,Dir), which denotes the Start or End of the kth match of token r in input string v. A regex token can either be a constant string s or one of 8 regular expression tokens: p (ProperCase), C (CAPS), l (lowercase), d (Digits), α (Alphabets), αn (Alphanumeric), ∧ (StartOfString), and $ (EndOfString). The semantics of the DSL programs is described in the appendix. A DSL program for the name transformation task shown in Figure 1(a) that is con- sistent with the examples is: Concat(f1,ConstStr(\u201C, \u201D), f2,ConstStr(\u201C.\u201D)), where f1 ≡ SubStr(v, (\u201C \u201D,−1,End),ConstPos(−1)) and f2 ≡ SubStr(v,ConstPos(0),ConstPos(1)). The program concatenates the following 4 strings: i) substring between the end of last whitespace and end of string, ii) constant string \u201C, \u201D, iii) first character of input string, and iv) constant string \u201C.\u201D. 3 OVERVIEW OF OUR APPROACH We now present an overview of our approach. Given a DSL L, we learn a generative model of pro- grams in the DSL L that is conditioned on input-output examples to efficiently search for consistent programs. The workflow of our system is shown in Figure 2, which is trained end-to-end using a large training set of programs in the DSL together with their corresponding input-output examples. To generate a large training set, we uniformly sample programs from the DSL and then use a rule- based strategy to compute well-formed input strings that satisfy the pre-conditions of the programs. The corresponding output strings are obtained by running the programs on the input strings. A DSL can be considered a context-free grammar with a start symbol S and a set of non-terminals with corresponding expansion rules. The (partial) grammar derivations or trees correspond to (par- tial) programs. A naı̈ve way to perform a search over the programs in a DSL is to start from the start symbol S and then randomly choose non-terminals to expand with randomly chosen expansion rules until reaching a derivation with only terminals. We, instead, learn a generative model over partial derivations in the DSL that assigns probabilities to different non-terminals in a partial derivation and corresponding expansions to guide the search for complete derivations. Our generative model uses a Recursive-Reverse-Recursive Neural Network (R3NN) to encode par- tial trees (derivations) in L, where each node in the partial tree encodes global information about every other node in the tree. The model assigns a vector representation for every symbol and every expansion rule in the grammar. Given a partial tree, the model first assigns a vector representation to each leaf node, and then performs a recursive pass going up in the tree to assign a global tree representation to the root. It then performs a reverse-recursive pass starting from the root to assign a global tree representation to each node in the tree. 3 Under review as a conference paper at ICLR 2017 R3NN DSL R3NN I\/O Encoder R3NN ... DSL DSL Program Sampler DSL Input Gen Rules i1 \u2013 o1 i2 \u2013 o2 \u2026 ik \u2013 ok {p1 i1 \u2013 o1 i2 \u2013 o2 \u2026 ik \u2013 ok {pj i1 \u2013 o1 i2 \u2013 o2 \u2026 ik \u2013 ok {pn \u2026 pj,0 pj,1 pj,2 pj \u2026 R3NN DSL R3NN I\/O Encoder R3NN ... DSL DSL Learnt program i1 \u2013 o1 i2 \u2013 o2 \u2026 ik \u2013 ok (a) Training Phase (b) Test Phase Figure 2: An overview of the training and test workflow of our synthesis appraoch. The generative process is conditioned on a set of input-output examples to learn a program that is consistent with this set of examples. We experiment with multiple input-output encoders including an LSTM encoder that concatenates the hidden vectors of two deep bidirectional LSTM networks for input and output strings in the examples, and a Cross Correlation encoder that computes the cross correlation between the LSTM tensor representations of input and output strings in the examples. This vector is then used as an additional input in the R3NN model to condition the generative model. 4 TREE-STRUCTURED GENERATION MODEL We define a program t-steps into construction as a partial program tree (PPT) (see Figure 3 for a visual depiction). A PPT has two types of nodes: leaf (symbol) nodes and inner non-leaf (rule) nodes. A leaf node represents a symbol, whether non-terminal or terminal. An inner non-leaf node represents a particular production rule of the DSL, where the number of children of the non-leaf node is equivalent to the arity of the RHS of the rule it represents. A PPT is called a program tree (PT) whenever all the leaves of the tree are terminal symbols. Such a tree represents a completed program under the DSL and can be executed. We define an expansion as the valid application of a specific production rule (e → e op2 e) to a specific non-terminal leaf node within a PPT (leaf with symbol e). We refer to the specific production rule that an expansion is derived from as the expansion type. It can be seen that if there exist two leaf nodes (l1 and l2) with the same symbol then for every expansion specific to l1 there exists an expansion specific to l2 with the same type. 4.1 RECURSIVE-REVERSE-RECURSIVE NEURAL NETWORK In order to define a generation model over PPTs, we need an efficient way of assigning probabilities to every valid expansion in the current PPT. A valid expansion has two components: first the pro- duction rule used, and second the position of the expanded leaf node relative to every other node in the tree. To account for the first component, a separate distributed representation for each produc- tion rule is maintained. The second component is handled using an architecture where the forward propagation resembles belief propagation on trees, allowing a notion of global tree state at every node within the tree. A given expansion probability is then calculated as being proportional to the inner product between the production rule representation and the global-tree representation of the leaf-level non-terminal node. We now describe the design of this architecture in more detail. The R3NN has the following parameters for the grammar described by a DSL (see Figure 3): 1. For every symbol s ∈ S, an M -dimensional representation φ(s) ∈ RM . 2. For every production rule r ∈ R, an M−dimensional representation ω(r) ∈ RM . 3. For every production rule r ∈ R, a deep neural network fr which takes as input a vector x ∈ RQ·M , with Q being the number of symbols on the RHS of the production rule r, and outputs a vector y ∈ RM . Therefore, the production-rule network fr takes as input a concatenation of the distributed representations of each of its RHS symbols and produces a distributed representation for the LHS symbol. 4. For every production rule r ∈ R, an additional deep neural network gr which takes as input a vector x\u2032 ∈ RM and outputs a vector y\u2032 ∈ RQ·M . We can think of gr as a reverse 4 Under review as a conference paper at ICLR 2017 (a) Recursive pass (b) Reverse-Recursive pass Figure 3: (a) The initial recursive pass of the R3NN. (b) The reverse-recursive pass of the R3NN where the input is the output of the previous recursive pass. production-rule network that takes as input a vector representation of the LHS and produces a concatenation of the distributed representations of each of the rule\u2019s RHS symbols. Let E be the set of all valid expansions in a PPT T , let L be the current leaf nodes of T and N be the current non-leaf (rule) nodes of T . Let S(l) be the symbol of leaf l ∈ L and R(n) represent the production rule of non-leaf node n ∈ N . 4.1.1 GLOBAL TREE INFORMATION AT THE LEAVES To compute the probability distribution over the set E, the R3NN first computes a distributed rep- resentation for each leaf node that contains global tree information. To accomplish this, for every leaf node l ∈ L in the tree we retrieve its distributed representation φ(S(l)) . We now do a standard recursive bottom-to-top, RHS→LHS pass on the network, by going up the tree and applying fR(n) for every non-leaf node n ∈ N on its RHS node representations (see Figure 3(a)). These networks fR(n) produce a node representation which is input into the parent\u2019s rule network and so on until we reach the root node. Once at the root node, we effectively have a fixed-dimensionality global tree representation φ(root) for the start symbol. The problem is that this representation has lost any notion of tree position. To solve this problem R3NN now does what is effectively a reverse-recursive pass which starts at the root node with φ(root) as input and moves towards the leaf nodes (see Figure 3(b)). More concretely, we start with the root node representation φ(root) and use that as input into the rule network gR(root) where R(root) is the production rule that is applied to the start symbol in T . This produces a representation φ\u2032(c) for each RHS node c of R(root). If c is a non-leaf node, we iteratively apply this procedure to c, i.e., process φ\u2032(c) using gR(c) to get representations φ\u2032(cc) for every RHS node cc of R(c), etc. If c is a leaf node, we now have a leaf representation φ\u2032(c) which has an information path to φ(root) and thus to every other leaf node in the tree. Once the reverse-recursive process is complete, we now have a distributed representation φ\u2032(l) for every leaf node l which contains global tree information. While φ(l1) and φ(l2) could be equal for leaf nodes which have the same symbol type, φ\u2032(l1) and φ\u2032(l2) will not be equal even if they have the same symbol type because they are at different positions in the tree. 4.1.2 EXPANSION PROBABILITIES Given the global leaf representations φ\u2032(l), we can now straightforwardly acquire scores for each e ∈ E. For expansion e, let e.r be the expansion type (production rule r ∈ R that e applies) and let e.l be the leaf node l that e.r is applied to. ze = φ\u2032(e.l) · ω(e.r) The score of an expansion is calculated using ze = φ\u2032(e.l) · ω(e.r). The probability of expansion e is simply the exponentiated normalized sum over all scores: π(e) = e ze∑ e\u2032∈E e z e\u2032 . An additional improvement that was found to help was to add a bidirectional LSTM to process the global leaf representations right before calculating the scores. The LSTM hidden states are then 5 Under review as a conference paper at ICLR 2017 used in the score calculation rather than the leaves themselves. This serves primarily to reduce the minimum length that information has to propagate between nodes in the tree. The R3NN can be seen as an extension and combination of several previous tree-based models, which were mainly developed in the context of natural language processing (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013). 5 CONDITIONING WITH INPUT\/OUTPUT EXAMPLES Now that we have defined a generation process over tree-structured programs, we need a way of conditioning this generation process on a set of input\/output examples. The set of input\/output examples provide a nearly complete specification for the desired output program, and so a good encoding of the examples is crucial to the success of our program generator. For the most part, this example encoding needs to be domain-specific, since different DSLs have different inputs (some may operate over integers, some over strings, etc.). Therefore, in our case, we use an encoding adapted to the input-output strings that our DSL operates over. We also investigate different ways of conditioning program search on the learnt example input-output encodings. 5.1 ENCODING INPUT\/OUTPUT EXAMPLES There are two types of information that string manipulation programs need to extract from input- output examples: 1) constant strings, such as \u201C@domain.com\u201D or \u201C.\u201D, which appear in all output examples; 2) substring indices in input where the index might be further defined by a regular expres- sion. These indices determine which parts of the input are also present in the output. To simplify the DSL, we assume that there is a fixed finite universe of possible constant strings that could appear in programs. Therefore we focus on extracting the second type of information, the substring indices. In earlier hand-engineered systems such as FlashFill, this information was extracted from the input- output strings by running the Longest Common Substring algorithm, a dynamic programming algo- rithm that efficiently finds matching substrings in string pairs. To extract substrings, FlashFill runs LCS on every input-output string pair in the I\/O set to get a set of substring candidates. It then takes the entire set of substring candidates and simply tries every possible regex and constant index that can be used at substring boundaries, exhaustively searching for the one which is the most \u201Cgeneral\u201D, where generality is specified by hand-engineered heuristics. In contrast to these previous methods, instead of of hand-designing a complicated algorithm to extract regex-based substrings, we develop neural network based architectures that are capable of learning to extract and produce continuous representations of the likely regular expressions given input\/output strings. 5.1.1 BASELINE LSTM ENCODER Our first I\/O encoding network involves running two separate deep bidirectional LSTM networks for processing the input and the output string in each example pair. For each pair, it then concatenates the topmost hidden representation at every time step to produce a 4HT -dimensional feature vector per I\/O pair, where T is the maximum string length for any input or output string, and H is the topmost LSTM hidden dimension. We then concatenate the encoding vectors across all I\/O pairs to get a vector representation of the en- tire I\/O set. This encoding is conceptually straightforward and has very little prior knowledge about what operations are being performed over the strings, i.e., substring, constant, etc., which might make it difficult to discover substring indices, especially the ones based on regular expressions. 5.1.2 CROSS CORRELATION ENCODER To help the model discover input substrings that are copied to the output, we designed an novel I\/O example encoder to compute the cross correlation between each input and output example repre- sentation. We used the two output tensors of the LSTM encoder (discussed above) as inputs to this encoder. For each example pair, we first slide the output feature block over the input feature block and compute the dot product between the respective position representation. Then, we sum over all overlapping time steps. Features of all pairs are then concatenated to form a 2∗ (T −1)-dimensional 6 Under review as a conference paper at ICLR 2017 vector encoding for all example pairs. There are 2 ∗ (T − 1) possible alignments in total between input and output feature blocks. We also designed the following variants of this encoder. Diffused Cross Correlation Encoder: This encoder is identical to the Cross Correlation encoder except that instead of summing over overlapping time steps after the element-wise dot product, we simply concatenate the vectors corresponding to all time steps, resulting in a final representation that contains 2 ∗ (T − 1) ∗ T features for each example pair. LSTM-Sum Cross Correlation Encoder: In this variant of the Cross Correlation encoder, instead of doing an element-wise dot product, we run a bidirectional LSTM over the concatenated feature blocks of each alignment. We represent each alignment by the LSTM hidden representation of the final time step leading to a total of 2 ∗H ∗ 2 ∗ (T − 1) features for each example pair. Augmented Diffused Cross Correlation Encoder: For this encoder, the output of each character position of the Diffused Cross Correlation encoder is combined with the character embedding at this position, then a basic LSTM encoder is run over the combined features to extract a 4∗H-dimensional vector for both the input and the output streams. The LSTM encoder output is then concatenated with the output of the Diffused Cross Correlation encoder forming a (4∗H+T ∗(T−1))-dimensional feature vector for each example pair. 5.2 CONDITIONING PROGRAM SEARCH ON EXAMPLE ENCODINGS Once the I\/O example encodings have been computed, we can use them to perform conditional generation of the program tree using the R3NN model. There are a number of ways in which the PPT generation model can be conditioned using the I\/O example encodings depending on where the I\/O example information is inserted in the R3NN model. We investigated three locations to inject example encodings: 1) Pre-conditioning: where example encodings are concatenated to the encoding of each tree leaf, and then passed to a conditioning network before the bottom-up recursive pass over the program tree. The conditioning network can be either a multi-layer feedforward network, or a bidirectional LSTM network running over tree leaves. Running an LSTM over tree leaves allows the model to learn more about the relative position of each leaf node in the tree. 2) Post-conditioning: After the reverse-recursive pass, example encodings are concatenated to the updated representation of each tree leaf and then fed to a conditioning network before computing the expansion scores. 3) Root-conditioning: After the recursive pass over the tree, the root encoding is concatenated to the example encodings and passed to a conditioning network. The updated root representation is then used to drive the reverse-recursive pass. Empirically, pre-conditioning worked better than either root- or post- conditioning. In addition, conditioning at all 3 places simultaneously did not cause a significant improvement over just pre-conditioning. Therefore, for the experimental section, we report models which only use pre- conditioning. 6 EXPERIMENTS In order to evaluate and compare variants of the previously described models, we generate a dataset randomly from the DSL. To do so, we first enumerate all possible programs under the DSL up to a specific number of instructions, which are then partitioned into training, validation and test sets. In order to have a tractable number of programs, we limited the maximum number of instructions for programs to be 13. Length 13 programs are important for this specific DSL because all larger programs can be written as compositions of sub-programs of length at most 13. The semantics of length 13 programs therefore constitute the \u201Catoms\u201D of this particular DSL. In testing our model, there are two different categories of generalization. The first is input\/output generalization, where we are given a new set of input\/output examples as well as a program with a specific tree that we have seen during training. This represents the model\u2019s capacity to be applied on new data. The second category is program generalization, where we are given both a previously unseen program tree in addition to unseen input\/output examples. Therefore the model needs to 7 Under review as a conference paper at ICLR 2017 I\/O Encoding Train Test LSTM 88% 88% Cross Correlation (CC) 67% 65% Diffused CC 89% 88% LSTM-sum CC 90% 91% Augmented diffused CC 91% 91% Table 1: The effect of different input\/output encoders on accuracy. Each result used 100 samples. There is almost no generalization error in the results. have a sufficient enough understanding of the semantics of the DSL that it can construct novel combinations of operations. For all reported results, training sets correspond to the first type of generalization since we have seen the program tree but not the input\/output pairs. Test sets represent the second type of generalization, as they are trees which have not been seen before on input\/output pairs that have also not been seen before. In this section, we compare several different variants of our model. We first evaluate the effect of each of the previously described input\/output encoders. We then evaluate the R3NN model against a simple recurrent model called io2seq, which is basically an LSTM that takes as input the input\/output conditioning vector and outputs a sequence of DSL symbols that represents a linearized program tree. Finally, we report the results of the best model on the length 13 training and testing sets, as well as on a set of 238 benchmark functions. 6.1 SETUP AND HYPERPARAMETERS SETTINGS For training the R3NN, two hyperparameters that were crucial for stabilizing training were the use of hyperbolic tangent activation functions in both R3NN and cross-correlation I\/O encoders and the use of minibatches of length 8. Due to the difficulty of batching tree-based neural networks and time-constraints, we were limited to 8 samples per batch but some preliminary experiments indicated that increasing the batch size even further improved performance. Additionally, for all results, the program tree generation is conditioned on a set of 10 input\/output string pairs. For each latent function and set of input\/output examples that we test on, we report whether we had a success after sampling 100 functions from the model and testing all 100 to see if one of these functions is equivalent to the latent function. Here we consider two functions to be equivalent with respect to a specific input\/output example set if the functions output the same strings when run on the inputs. Under this definition, two functions can have a different set of operations but still be equivalent with respect to a specific input-output set. 6.2 EXAMPLE ENCODING In this section, we evaluate the effect of several different input\/output example encoders. To control for the effect of the tree model, all results here used an R3NN with fixed hyperparameters to generate the program tree. Table 1 shows the performance of several of these input\/output example encoders. We can see that the summed cross-correlation encoder did not perform well, which can be due to the fact that the sum destroys positional information that might be useful for determining specific substring indices. The LSTM-sum and the augmented diffused cross-correlation models did the best. Surprisingly, the LSTM encoder was capable of finding nearly 88% of all programs without having any prior knowledge explicitly built into the architecture. We use 100 samples for evaluating the Train and Test sets. The training performance is sometimes slightly lower because there are close to 5 million training programs but we only look at less than 2 million of these programs during training. We sample a subset of only 1000 training programs from the 5 million program set to report the training results in the tables. The test sets also consist of 1000 programs. 6.3 IO2SEQ In this section, we motivate the use of the R3NN by testing whether a simpler model can also be used to generate programs. The io2seq model is an LSTM whose initial hidden and cell states 8 Under review as a conference paper at ICLR 2017 Sampling Train Test io2seq 44% 42% Table 2: Testing the I\/O-vector-to-sequence model. Each result used 100 samples. Sampling Train Test 1-best 60% 63% 1-sample 56% 57% 10-sample 81% 79% 50-sample 91% 89% 100-sample 94% 94% 300-sample 97% 97% Table 3: The effect of backtracking (sampling) multiple programs on accuracy. 1-best is determin- istically choosing the expansion with highest probability at each step. are a function of the input\/output encoding vector. The io2seq model then generates a linearized tree of a program symbol-by-symbol. An example of what a linearized program tree looks like is (S(e(f (ConstStr\u201C@\u201D)ConstStr)f )e)S , which represents the program tree that returns the constant string \u201C@\u201D. Predicting a linearized tree using an LSTM was also done in the context of pars- ing (Vinyals et al., 2015). For the io2seq model, we used the LSTM-sum cross-correlation I\/O conditioning model. The results in Table 2 show that the performance of the io2seq model at 100 samples per latent test function is far worse than the R3NN, at around 42% versus 91%, respectively. The reasons for that could be that the io2seq model needs to perform far more decisions than the R3NN, since the io2seq model has to predict the parentheses symbols that determine at which level of the tree a particular symbol is at. For example, the io2seq model requires on the order of 100 samples for length 13 programs, while the R3NN requires no more than 13. 6.4 EFFECT OF BACKTRACKING SEARCH For the best R3NN model that we trained, we also evaluated the effect that a different number of samples per latent function had on performance. The results are shown in Table 3. The increase of the model\u2019s performance as the sample size increases hints that the model has a notion of what type of program satisfies a given I\/O pair, but it might not be that certain about the details such as which regex to use, etc. By 300 samples, the model is nearing perfect accuracy on the test sets. 6.5 FLASHFILL BENCHMARKS We also evaluate our learnt models on 238 real-world FlashFill benchmarks obtained from the Mi- crosoft Excel team and online help-forums. These benchmarks involve string manipulation tasks described using input-output examples. We evaluate two models \u2013 one with a cross correlation en- coder trained on 5 input-output examples and another trained on 10 input-output examples. Both the models were trained on randomly sampled programs from the DSL upto size 13 with randomly generated input-output examples. The distribution of the size of smallest DSL programs needed to solve the benchmark tasks is shown in Figure 4(a), which varies from 4 to 63. The figure also shows the number of benchmarks for which our model was able to learn the program using 5 input-output examples using samples of top-2000 learnt programs. In total, the model is able to learn programs for 91 tasks (38.2%). Since the model was trained for programs upto size 13, it is not surprising that it is not able to solve tasks that need larger program size. There are 110 FlashFill benchmarks that require programs upto size 13, out of which the model is able to solve 82.7% of them. The effect of sampling multiple learnt programs instead of only top program is shown in Figure 4(b). With only 10 samples, the model can already learn about 13% of the benchmarks. We observe a steady increase in performance upto about 2000 samples, after which we do not observe any 9 Under review as a conference paper at ICLR 2017 0 5 10 15 20 25 30 35 40 45 50 4 7 9 10 11 13 15 17 19 24 25 27 30 31 37 50 59 63 N u m b er o f B en ch m ar ks Size of smallest programs for FlashFill Benchmarks Number of FlashFill Benchmarks solved Total Solved Sampling Solved Benchmarks 10 13% 50 21% 100 23% 200 29% 500 33% 1000 34% 2000 38% 5000 38% (a) (b) Figure 4: (a) The distribution of size of programs needed to solve FlashFill tasks and the perfor- mance of our model, (b) The effect of sampling for trying top-k learnt programs. Input v Output [CPT-00350 [CPT-00350] [CPT-00340] [CPT-00340] [CPT-114563] [CPT-114563] [CPT-1AB02 [CPT-1AB02] [CPT-00360 [CPT-00360] Input v Output 732606129 0x73 430257526 0x43 444004480 0x44 371255254 0x37 635272676 0x63 Input v Output John Doyle John D. Matt Walters Matt W. Jody Foster Jody F. Angela Lindsay Angela L. Maria Schulte Maria S. (a) (b) (c) Figure 5: Some example solved benchmarks: (a) cleaning up medical codes with closing brackets, (b) generating Hex numbers with first two digits, (c) transforming names to firstname and last initial. significant improvement. Since there are more than 2 million programs in the DSL of length 11 itself, the enumerative techniques with uniform search do not scale well (Alur et al., 2015). We also evaluate a model that is learnt with 10 input-output examples per benchmark. Surprisingly, this model can only learn programs for about 29% of the FlashFill benchmarks. We hypothesize that the space of consistent programs gets more constrained with additional input-output examples, which makes it harder for R3NN to learn the desired program. Another possibility is that the input- output encoder gets more confused with the additional example pairs. Our model is able to solve majority of FlashFill benchmarks that require learning programs with upto 3 Concat operations. We now describe a few of these benchmarks, also shown in Fig- ure 5. An Excel user wanted to clean a set of medical billing records by adding a missing \u201C]\u201D to medical codes as shown in Figure 5(a). Our system learns the following program given these 5 input-output examples: Concat(SubStr(v,ConstPos(0),(d,-1,End)), ConstStr(\u201C]\u201D)). The pro- gram concatenates the substring between the start of the input string and the position of the last digit regular expression with the constant string \u201C]\u201D. Another task that required user to trans- form some numbers into a hex format is shown in Figure 5(b). Our system learns the following program: Concat(ConstStr(\u201C0x\u201D),SubStr(v,ConstPos(0),ConstPos(2))). For some benchmarks with long input strings, it is still able to learn regular expressions to extract the desired sub- string, e.g. it learns a program to extract \u201CNancyF\u201D from the string \u201C123456789,freehafer ,drew ,nancy,19700101,11\/1\/2007,NancyF@north.com,1230102,123 1st Avenue,Seattle,wa,09999\u201D. Our system is currently not able to learn programs for benchmarks that require 4 or more Con- cat operations. Two such benchmarks are shown in Figure 6. The task of combining names in Figure 6(a) requires 6 Concat arguments, whereas the phone number transformation task in Fig- ure 6(b) requires 5 Concat arguments. This is mainly because of the scalability issues in training with programs of larger size. There are also a few interesting benchmarks where the R3NN models gets very close to learning the desired program. For example, for the task \u201CBill Gates\u201D → \u201CMr. Bill Gates\u201D, it learns a program that generates \u201CMr.Bill Gates\u201D (without the whitespace), and for the task \u201C617-444-5454\u201D → \u201C(617) 444-5454\u201D, it learns a program that generates the string \u201C(617 444-5454\u201D. 10 Under review as a conference paper at ICLR 2017 Input v Output 1 John James Paul John, James, and Paul. 2 Tom Mike Bill Tom, Mike, and Bill. 3 Marie Nina John Marie, Nina, and John. 4 Reggie Anna Adam Reggie, Anna, and Adam. Input v Output 1 (425) 221 6767 425-221-6767 2 206.225.1298 206-225-1298 3 617-224-9874 617-224-9874 4 425.118.9281 425-118-9281 (a) (b) Figure 6: Some unsolved benchmarks: (a)Combining names by different delimiters. (b) Transform- ing phone numbers to consistent format. 7 RELATED WORK We have seen a renewed interest in recent years in the area of Program Induction and Synthesis. In the machine learning community, a number of promising neural architectures have been pro- posed to perform program induction. These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015). These approaches represent the atomic operations of the net- work in a differentiable form, which allows for efficient end-to-end training of a neural controller. However, unlike our approach that learns comprehensible complete programs, many of these ap- proaches learn only the program behavior (i.e., they produce desired outputs on new input data). Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks. Liang et al. (2010) restrict the problem space with a probabilistic context-free grammar and introduce a new representation of programs based on combinatory logic, which allows for sharing sub-programs across multiple tasks. They then take a hierarchical Bayesian approach to learn frequently occurring substructures of programs. Our approach, instead, uses neural architectures to condition the search space of pro- grams, and does not require additional step of representing program space using combinatory logic for allowing sharing. The DSL-based program synthesis approach has also seen a renewed interest recently (Alur et al., 2015). It has been used for many applications including synthesizing low-level bitvector implemen- tations (Solar-Lezama et al., 2005), Excel macros for data manipulation (Gulwani, 2011; Gulwani et al., 2012), superoptimization by finding smaller equivalent loop bodies (Schkufza et al., 2013), protocol synthesis from scenarios (Udupa et al., 2013), synthesis of loop-free programs (Gulwani et al., 2011), and automated feedback generation for programming assignments (Singh et al., 2013). The synthesis techniques proposed in the literature generally employ various search techniques in- cluding enumeration with pruning, symbolic constraint solving, and stochastic search, while sup- porting different forms of specifications including input-output examples, partial programs, program invariants, and reference implementation. In this paper, we consider input-output example based specification over the hypothesis space de- fined by a DSL of string transformations, similar to that of FlashFill (without conditionals) (Gul- wani, 2011). The key difference between our approach over previous techniques is that our system is trained completely in an end-to-end fashion, while previous techniques require significant manual effort to design heuristics for efficient search. There is some work on guiding the program search us- ing learnt clues that suggest likely DSL expansions, but the clues are learnt over hand-coded textual features of examples (Menon et al., 2013). Moreover, their DSL consists of composition of about 100 high-level text transformation functions such as count and dedup, whereas our DSL consists of tree structured programs over richer regular expression based substring constructs. There is also a recent line of work on learning probabilistic models of code from a large number of code repositories (big code) (Raychev et al., 2015; Bielik et al., 2016; Hindle et al., 2016), which are then used for applications such as auto-completion of partial programs, inference of variable and method names, program repair, etc. These language models typically capture only the syntactic 11 Under review as a conference paper at ICLR 2017 properties of code, unlike our approach that also tries to capture the semantics to learn the desired program. The work by Maddison & Tarlow (2014) addresses the problem of learning structured generative models of source code but both their model and application domain are different from ours. The R3NN model employed in our work is related to several tree and graph structured neural net- works present in the NLP literature (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013). The Inside-Outside Recursive Neural Network (Le & Zuidema, 2014) in particular is most similar to the R3NN, where they generate a parse tree incrementally by using global leaf-level representations to determine which expansions in the parse tree to take next. 8 CONCLUSION We have proposed a novel technique called Neuro-Symbolic Program Synthesis that is able to con- struct a program incrementally based on given input-output examples. To do so, a new neural architecture called Recursive-Reverse-Recursive Neural Network is used to encode and expand a partial program tree into a full program tree. Its effectiveness at example-based program synthesis is demonstrated, even when the program has not been seen during training. These promising results open up a number of interesting directions for future research. For example, we took a supervised-learning approach here, assuming availability of target programs during train- ing. In some scenarios, we may only have access to an oracle that returns the desired output given an input. In this case, reinforcement learning is a promising framework for program synthesis. REFERENCES Alur, Rajeev, Bodı́k, Rastislav, Dallal, Eric, Fisman, Dana, Garg, Pranav, Juniwal, Garvit, Kress- Gazit, Hadas, Madhusudan, P., Martin, Milo M. K., Raghothaman, Mukund, Saha, Shamwaditya, Seshia, Sanjit A., Singh, Rishabh, Solar-Lezama, Armando, Torlak, Emina, and Udupa, Ab- hishek. Syntax-guided synthesis. In Dependable Software Systems Engineering, pp. 1\u201325. 2015. Bielik, Pavol, Raychev, Veselin, and Vechev, Martin T. PHOG: probabilistic model for code. In ICML, pp. 2933\u20132942, 2016. Biermann, Alan W. The inference of regular lisp programs from examples. IEEE transactions on Systems, Man, and Cybernetics, 8(8):585\u2013600, 1978. Bunel, Rudy, Desmaison, Alban, Kohli, Pushmeet, Torr, Philip H. S., and Kumar, M. Pawan. Adap- tive neural compilation. CoRR, abs\/1605.07969, 2016. URL http:\/\/arxiv.org\/abs\/1605.07969. Gaunt, Alexander L, Brockschmidt, Marc, Singh, Rishabh, Kushman, Nate, Kohli, Pushmeet, Tay- lor, Jonathan, and Tarlow, Daniel. Terpret: A probabilistic programming language for program induction. arXiv preprint arXiv:1608.04428, 2016. Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Gulwani, Sumit. Automating string processing in spreadsheets using input-output examples. In POPL, pp. 317\u2013330, 2011. Gulwani, Sumit, Jha, Susmit, Tiwari, Ashish, and Venkatesan, Ramarathnam. Synthesis of loop-free programs. In PLDI, pp. 62\u201373, 2011. Gulwani, Sumit, Harris, William, and Singh, Rishabh. Spreadsheet data manipulation using exam- ples. Communications of the ACM, Aug 2012. Hindle, Abram, Barr, Earl T., Gabel, Mark, Su, Zhendong, and Devanbu, Premkumar T. On the naturalness of software. Commun. ACM, 59(5):122\u2013131, 2016. Irsoy, Orzan and Cardie, Claire. Bidirectional recursive neural networks for token-level labeling with structure. In NIPS Deep Learning Workshop, 2013. 12 http:\/\/arxiv.org\/abs\/1605.07969 Under review as a conference paper at ICLR 2017 Joulin, Armand and Mikolov, Tomas. Inferring algorithmic patterns with stack-augmented recurrent nets. In NIPS, pp. 190\u2013198, 2015. Kurach, Karol, Andrychowicz, Marcin, and Sutskever, Ilya. Neural random-access machines. arXiv preprint arXiv:1511.06392, 2015. Le, Phong and Zuidema, Willem. The inside-outside recursive neural network model for dependency parsing. In EMNLP, pp. 729\u2013739, 2014. Liang, Percy, Jordan, Michael I., and Klein, Dan. Learning programs: A hierarchical Bayesian approach. In ICML, pp. 639\u2013646, 2010. Maddison, Chris J and Tarlow, Daniel. Structured generative models of natural source code. In ICML, pp. 649\u2013657, 2014. Menon, Aditya Krishna, Tamuz, Omer, Gulwani, Sumit, Lampson, Butler W., and Kalai, Adam. A machine learning framework for programming by example. In ICML, pp. 187\u2013195, 2013. Neelakantan, Arvind, Le, Quoc V, and Sutskever, Ilya. Neural programmer: Inducing latent pro- grams with gradient descent. arXiv preprint arXiv:1511.04834, 2015. Paulus, Romain, Socher, Richard, and Manning, Christopher D. Global belief recursive neural networks. pp. 2888\u20132896, 2014. Raychev, Veselin, Vechev, Martin T., and Krause, Andreas. Predicting program properties from \u201Dbig code\u201D. In POPL, pp. 111\u2013124, 2015. Reed, Scott and de Freitas, Nando. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015. Riedel, Sebastian, Bosnjak, Matko, and Rocktäschel, Tim. Programming with a differentiable forth interpreter. CoRR, abs\/1605.06640, 2016. URL http:\/\/arxiv.org\/abs\/1605.06640. Schkufza, Eric, Sharma, Rahul, and Aiken, Alex. Stochastic superoptimization. In ASPLOS, pp. 305\u2013316, 2013. Singh, Rishabh and Solar-Lezama, Armando. Synthesizing data structure manipulations from sto- ryboards. In SIGSOFT FSE, pp. 289\u2013299, 2011. Singh, Rishabh, Gulwani, Sumit, and Solar-Lezama, Armando. Automated feedback generation for introductory programming assignments. In PLDI, pp. 15\u201326, 2013. Solar-Lezama, Armando. Program Synthesis By Sketching. PhD thesis, EECS Dept., UC Berkeley, 2008. Solar-Lezama, Armando, Rabbah, Rodric, Bodik, Rastislav, and Ebcioglu, Kemal. Programming by sketching for bit-streaming programs. In PLDI, 2005. Summers, Phillip D. A methodology for lisp program construction from examples. Journal of the ACM (JACM), 24(1):161\u2013175, 1977. Udupa, Abhishek, Raghavan, Arun, Deshmukh, Jyotirmoy V., Mador-Haim, Sela, Martin, Milo M. K., and Alur, Rajeev. TRANSIT: specifying protocols with concolic snippets. In PLDI, pp. 287\u2013296, 2013. Vinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, Sutskever, Ilya, and Hinton, Geoffrey. Grammar as a foreign language. In ICLR, 2015. 13 http:\/\/arxiv.org\/abs\/1605.06640 Under review as a conference paper at ICLR 2017 JConcat(f1, · · · , fn)Kv = Concat(Jf1Kv, · · · , JfnKv) JConstStr(s)Kv = s JSubStr(v, pl, pr)Kv = v[JplKv..JprKv] JConstPos(k)Kv = k > 0? k : len(s) + k J(r, k, Start)Kv = Start of kthmatch of r in v from beginning (end if k < 0) J(r, k,End)Kv = End of kthmatch of r in v from beginning (end if k < 0) Figure 7: The semantics of the DSL for string transformations. A DOMAIN-SPECIFIC LANGUAGE FOR STRING TRANSFORMATIONS The semantics of the DSL programs is shown in Figure 7. The semantics of a Concat expression is to concatenate the results of recursively evaluating the constituent substring expressions fi. The semantics of ConstStr(s) is to simply return the constant string s. The semantics of a substring expression is to first evaluate the two position logics pl and pr to p1 and p2 respectively, and then return the substring corresponding to v[p1..p2]. We denote s[i..j] to denote the substring of string s starting at index i (inclusive) and ending at index j (exclusive), and len(s) denotes its length. The semantics of ConstPos(k) expression is to return k if k > 0 or return len + k (if k < 0). The semantics of position logic (r, k, Start) is to return the Start of kth match of r in v from the beginning (if k > 0) or from the end (if k < 0). 14 Introduction Problem Definition Overview of our Approach Tree-Structured Generation Model Recursive-Reverse-Recursive Neural Network Global Tree Information at the Leaves Expansion Probabilities Conditioning with Input\/Output Examples Encoding input\/output examples Baseline LSTM encoder Cross Correlation encoder Conditioning program search on example encodings Experiments Setup and hyperparameters settings Example encoding io2seq Effect of backtracking search FlashFill Benchmarks Related Work Conclusion Domain-specific Language for String Transformations ","flair":"three\tResearch"}
{"author":"sybilckw","created":"Thu Nov 03 05:39:59 EDT 2016","text":"The URL has moved <a href=\"https:\/\/secure.jbs.elsevierhealth.com\/action\/getSharedSiteSession?redirect=http%3A%2F%2Fwww.medicalimageanalysisjournal.com%2Farticle%2FS1361-8415%2816%2930184-0%2Fabstract&rc=0&code=medima-site\">here<\/a> ","flair":"three\tResearch"}
{"author":"ill-logical","created":"Tue Oct 11 14:14:51 EDT 2016","text":"It used to be that ConvNets would use 2x2 pooling every few layers: the number of channels would grow, while the dimensions of the image would decrease all the way to 4x4 or less.\n\nHowever, now, with the [Xception](https:\/\/arxiv.org\/abs\/1610.02357) architecture, I'm seeing lots of pooling in the beginning (299x299 -&gt; 18x18), no pooling in most of the network, and lots of pooling at the end (18x18 -&gt; 1x1).\n\nIs there a justification for this? Pinging my reddit friend, \/u\/fchollet.\n\n**Edit** post not showing up; mods notified","flair":"one\tDiscusssion"}
{"author":"Sig_Luna","created":"Fri Oct 07 09:42:38 EDT 2016","text":"Hello everyone\n\nI need some help regarding SciKit-Learn. I have a lot of data in the form of an array A and a value B.\n\nE.g:\n\n    &gt;&gt; print(A[0])\n\n\n    [1.2, 0.3, -0.2, 0.3, -1.1, -0.4, 0.7, 0.2, -2.1, 0.1]\n\n\n    &gt;&gt; print (B[0])\n\n\n    9.2\n\n\nNow I want to predict the Value B, given the Array A. Which model could I use and *how*? I don't quite have an overview over all SciKit-Learn models. I'm fresh out of Andrew Ng's course and experimenting with some private projects.\n\nEDIT: Brainfart","flair":"null\tnull"}
{"author":"open_nsfw","created":"Sun Oct 23 19:02:38 EDT 2016","text":"This paper presents a recurrent neural network architecture in which some of the recurrent weights dynamically change during the forward pass, using a hebbian-like rule. They correspond to the matrices $A(t)$ in the figure below: ![Fast weights RNN figure](http:\/\/i.imgur.com\/DCznSf4.png) These weights $A(t)$ are referred to as *fast weights*. Comparatively, the recurrent weights $W$ are referred to as slow weights, since they are only changing due to normal training and are otherwise kept constant at test time. More specifically, the proposed fast weights RNN compute a series of hidden states $h(t)$ over time steps $t$, but, unlike regular RNNs, the transition from $h(t)$ to $h(t+1)$ consists of multiple ($S$) recurrent layers $h_1(t+1), \\dots, h_{S-1}(t+1), h_S(t+1)$, defined as follows: $$h_{s+1}(t+1) = f(W h(t) + C x(t) + A(t) h_s(t+1))$$ where $f$ is an element-wise non-linearity such as the ReLU activation. The next hidden state $h(t+1)$ is simply defined as the last \"inner loop\" hidden state $h_S(t+1)$, before moving to the next time step. As for the fast weights $A(t)$, they too change between time steps, using the hebbian-like rule: $$A(t+1) = \\lambda A(t) + \\eta h(t) h(t)^T$$ where $\\lambda$ acts as a decay rate (to partially forget some of what's in the past) and $\\eta$ as the fast weight's \"learning rate\" (not to be confused with the learning rate used during backprop). Thus, the role played by the fast weights is to rapidly adjust to the recent hidden states and remember the recent past. In fact, the authors show an explicit relation between these fast weights and memory-augmented architectures that have recently been popular. Indeed, by recursively applying and expending the equation for the fast weights, one obtains $$A(t) = \\eta \\sum_{\\tau = 1}^{\\tau = t-1}\\lambda^{t-\\tau-1} h(\\tau) h(\\tau)^T$$ *(note the difference with Equation 3 of the paper... I think there was a typo)* which implies that when computing the $A(t) h_s(t+1)$ term in the expression to go from $h_s(t+1)$ to $h_{s+1}(t+1)$, this term actually corresponds to $$A(t) h_s(t+1) = \\eta \\sum_{\\tau =1}^{\\tau = t-1} \\lambda^{t-\\tau-1} h(\\tau) (h(\\tau)^T h_s(t+1))$$ i.e. $A(t) h_s(t+1)$ is a weighted sum of all previous hidden states $h(\\tau)$, with each hidden states weighted by an \"attention weight\" $h(\\tau)^T h_s(t+1)$. The difference with many recent memory-augmented architectures is thus that the attention weights aren't computed using a softmax non-linearity. Experimentally, they find it beneficial to use [layer normalization](https:\/\/arxiv.org\/abs\/1607.06450). Good values for $\\eta$ and $\\lambda$ seem to be 0.5 and 0.9 respectively. I'm not 100% sure, but I also understand that using $S=1$, i.e. using the fast weights only once per time steps, was usually found to be optimal. Also see Figure 3 for the architecture used on the image classification datasets, which is slightly more involved. The authors present a series 4 experiments, comparing with regular RNNs (IRNNs, which are RNNs with ReLU units and whose recurrent weights are initialized to a scaled identity matrix) and LSTMs (as well as an associative LSTM for a synthetic associative retrieval task and ConvNets for the two image datasets). Generally, experiments illustrate that the fast weights RNN tends to train faster (in number of updates) and better than the other recurrent architectures. Surprisingly, the fast weights RNN can even be competitive with a ConvNet on the two image classification benchmarks, where the RNN traverses glimpses from the image using a fixed policy. **My two cents** This is a very thought provoking paper which, based on the comparison with LSTMs, suggests that fast weights RNNs might be a very good alternative. I'd be quite curious to see what would happen if one was to replace LSTMs with them in the myriad of papers using LSTMs (e.g. all the Seq2Seq work). Intuitively, LSTMs seem to be able to do more than just attending to the recent past. But, for a given task, if one was to observe that fast weights RNNs are competitive to LSTMs, it would suggests that the LSTM isn't doing something that much more complex. So it would be interesting to determine what are the tasks where the extra capacity of an LSTM is actually valuable and exploitable. Hopefully the authors will release some code, to facilitate this exploration. The discussion at the end of Section 3 on how exploiting the \"memory augmented\" view of fast weights is useful to allow the use of minibatches is interesting. However, it also suggests that computations in the fast weights RNN scales quadratically with the sequence size (since in this view, the RNN technically must attend to all previous hidden states, since the beginning of the sequence). This is something to keep in mind, if one was to consider applying this to very long sequences (i.e. much longer than the hidden state dimensionality). Also, I don't quite get the argument that the \"memory augmented\" view of fast weights is more amenable to mini-batch training. I understand that having an explicit weight matrix $A(t)$ for each minibatch sequence complicates things. However, in the memory augmented view, we also have a \"memory matrix\" that is different for each sequence, and yet we can handle that fine. The problem I can imagine is that storing a *sequence of arbitrary weight matrices* for each sequence might be storage demanding (and thus perhaps make it impossible to store a forward\/backward pass for more than one sequence at a time), while the implicit memory matrix only requires appending a new row at each time step. Perhaps the argument to be made here is more that there's already mini-batch compatible code out there for dealing with the use of a memory matrix of stored previous memory states. This work strikes some (partial) resemblance to other recent work, which may serve as food for thought here. The use of possibly multiple computation layers between time steps reminds me of [Adaptive Computation Time (ACT) RNN]( http:\/\/www.shortscience.org\/paper?bibtexKey=journals\/corr\/Graves16). Also, expressing a backpropable architecture that involves updates to weights (here, hebbian-like updates) reminds me of recent work that does backprop through the updates of a gradient descent procedure (for instance as in [this work]( http:\/\/www.shortscience.org\/paper?bibtexKey=conf\/icml\/MaclaurinDA15)). Finally, while I was familiar with the notion of fast weights from the work on [Using Fast Weights to Improve Persistent Contrastive Divergence](http:\/\/people.ee.duke.edu\/~lcarin\/FastGibbsMixing.pdf), I didn't realize that this concept dated as far back as the late 80s. So, for young researchers out there looking for inspiration for research ideas, this paper confirms that looking at the older neural network literature for inspiration is probably a very good strategy :-) To sum up, this is really nice work, and I'm looking forward to the NIPS 2016 oral presentation of it!","flair":"three\tResearch"}
{"author":"DrPharael","created":"Mon Nov 21 12:29:10 EST 2016","text":" Menu Search Follow Us Facebook Instagram Twitter Youtube Flipboard LinkedIn Google+ RSS More Youtube Flipboard LinkedIn Google+ RSS Got a tip? Let us know. News Channels Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Topics All Galleries All Timelines Video Shows Apps News Crunch Report Disrupt SF 2016 Gadgets Reviews Interviews TC Features All Shows All Videos Events TechCrunch Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store News About Mobile World Congress CES All Events Crunchbase Trending Tesla Google Facebook News Startups Mobile Gadgets Enterprise Social Europe Message Us Search TechCrunch Search TechCrunch Search Search × Hi! You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you. Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot. Thanks, TC Team Cyber Monday SaleGet A $200 Holiday Gift Guide Item Free With Disrupt London Early Bird Ticket Purchase Today Only - Get Yours Now Artificial Intelligence Relax, artificial intelligence isn\u2019t coming for your job Japan looks to create a superfast supercomputer for deep learning Google\u2019s AI translation tool seems to have invented its own secret internal language Browse more... Google opens new AI lab and invests $3.4M in Montreal-based AI research Posted Nov 21, 2016 by Darrell Etherington (@etherington) 0 SHARES Next Story Spectacles finally arrive in NYC with a Central Park Snapbot store Google has invested a total of $4.5 million CAD ($3.4M US) in AI research in Montreal\u2019s Institute for Learning Algorithms, with an academic fund covering three years that will help pay for seven faculty members across various Montreal academic institutions, including the University of Montreal and McGill. The investment is also continued backing for deep learning expert Yoshua Bengio\u2019s work, and is part of Google\u2019s continued bet on Canada\u2019s strong expertise in machine learning and AI research, both of which are becoming increasingly important to its core business. To that end, along with the investment, Google is also opening a brand new deep learning and AI research group in Montreal at its existing office in the city. The new team will be a remote arm of its Google Brain team based in Mountain View, and will be led locally by Hugo Larochelle, a deep learning expert who\u2019s returning home to Montreal from a role with Twitter in Boston specifically for the new position. Google notes that its total investment in academic research in Canada to date now amounts to around $13 million Canadian over the past 10 years, and it hopes that the new investment will help with the ongoing formation of an AI supercluster in Montreal, which is becoming a hotbed for AI startups as well as academic research. Google also has significant presence in Canada via a Waterloo engineering office, and works with AI expert and University of Toronto CS professor Geoffrey Hinton on a lot of its deep learning research. 0 SHARES Advertisement Advertisement Newsletter Subscriptions The Daily Crunch Get the top tech stories of the day delivered to your inbox TC Weekly Roundup Get a weekly recap of the biggest tech stories Crunchbase Daily The latest startup funding announcements Enter Address Subscribe Latest Crunch Report Facebook Builds a Censorship Tool | Crunch Report Watch More Episodes Artificial Intelligence Popular Posts Featured Stories Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 | Sarah Perez Siren Care makes a \u201Csmart\u201D sock to track diabetic health Nov 25, 2016 | Sarah Buhr Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 | Connie Loizos Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 | Ingrid Lunden Latest From TechCrunch Disrupting the world of science publishing 8 hours ago | Bérénice Magistretti Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago | Ron Miller We\u2019re all screwed, but let\u2019s not be nihilists 12 hours ago | Jon Evans, Columnist Technology as a force for division \u2014 and unification \u2014 in politics yesterday | Richard Muirhead Comment moderation powered by Up Next Spectacles finally arrive in NYC with a Central Park Snapbot store Posted Nov 21, 2016 CrunchBoard Job Listings CRM Berater (m\/f) at eGym GmbH (München, Deutschland) C++ Developer - Qt Product Development (m\/f) at eGym GmbH (München, Deutschland) Full Stack Engineer at FactorChain at The Sourcery (Los Altos, CA, United States) UI Engineering @ FactorChain at The Sourcery (Los Altos, CA, United States) Senior Backend Engineer @ MeetMe at The Sourcery (San Francisco, CA, United States) More from CrunchBoard Advertisement TechCrunch News Video Events Crunchbase TechCrunch Store About Staff Contact Us Advertise With Us Send Us A Tip International China Europe Japan Follow TechCrunch Facebook Twitter Google+ LinkedIn Youtube Pinterest Tumblr Instagram StumbleUpon Feed TechCrunch Apps iOS Android Windows 8 Subscribe to The Daily Crunch Latest headlines delivered to you daily Subscribe to <span class=\"no-mobile\">Subscribe to <\/span>The Daily Crunch Enter Email Address Subscribe © 2013-2016 AOL Inc. All rights reserved. Aol Tech Privacy Policy About Our Ads Anti Harassment Policy Terms of Service Powered by WordPress.com VIP Fonts by News Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Galleries All Timelines Videos Apps News Crunch Report Disrupt SF 2016 All Shows All Videos Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store All Events Crunchbase Message Us Most Popular RAWR: Samsung Canada Wins The Internet With This Custom Galaxy S III Aug 28, 2012 by Matt Burns A Dongle Joke That Spiraled Way Out Of Control Mar 21, 2013 by Kim-Mai Cutler Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago by Ron Miller Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 by Sarah Perez UPDATED: Machine learning can fix Twitter, Facebook, and maybe even America yesterday by Chris Nicholson Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 by Ingrid Lunden Xiaomi admits it doesn\u2019t make money on smartphone hardware sales Nov 25, 2016 by Darrell Etherington Disrupting the world of science publishing 8 hours ago by Bérénice Magistretti How to recruit, hire and retain female engineers Nov 25, 2016 by Sharon Wienbar Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 by Connie Loizos ","flair":"two\tNews"}
{"author":"hammertime89","created":"Fri Nov 04 07:40:20 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01142 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.SY References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Using a Deep Reinforcement Learning Agent for Traffic Signal Control Authors: Wade Genders, Saiedeh Razavi (Submitted on 3 Nov 2016) Abstract: Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%. Subjects: Learning (cs.LG); Systems and Control (cs.SY) Cite as: arXiv:1611.01142 [cs.LG]   (or arXiv:1611.01142v1 [cs.LG] for this version) Submission history From: Wade Genders [view email] [v1] Thu, 3 Nov 2016 19:46:19 GMT (1981kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"pvkooten","created":"Sat Oct 01 15:46:14 EDT 2016","text":"Does anyone know of experiments with using `y` in multiple locations in a neural network (using back-propagation)? Might it not be another solution for short-term vs long-term issue? E.g. some nodes are allowed short access to y and some are having a larger distance to y, generating more complex features.\n","flair":"null\tnull"}
{"author":"Faizann24","created":"Sat Oct 22 17:05:50 EDT 2016","text":"With the growth of Machine Learning in the past few years, many tasks are being done with the help of machine learning algorithms.Unfortunately or fortunately, there has been little work done on machine learning and cyber security. So I thought of presenting some at Fsecurify. A few days ago, I had this idea about what if we could detect a malicious URL from a non-malicious URL using some machine learning algorithm. There has been some research done on the topic so I thought that I should give it a go and implement something from scratch. So lets start. The first task was gathering data. I did some surfing and found some websites offering malicious links. I set up a little crawler and crawled a lot of malicious links from various websites. The next task was finding clear URLs. Fortunately, I did not have to crawl any. There was a data set available. Don\u2019t worry if I am not mentioning the sources of the data. You\u2019ll get the data at the end of this post. So, I gathered around 400,000 URLs out of which around 80,000 were malicious and others were clean. There we have it, our data set. Lets move next. We\u2019ll be using Logistic Regression since it is fast. The first part was tokenizing the URLs. I wrote my own tokenizer function for this since URLs are not like some other document text. Some of the tokens we get are like\u2018virus\u2019,\u2019exe\u2019,\u2019php\u2019,\u2019wp\u2019,\u2019dat\u2019 etc. The next step is to load the data and store it into a list. Now that we have the data in our list, we have to vectorize our URLs. I used tf-idf scores instead of using bag of words classification since there are words in urls that are more important than other words e.g\u2018virus\u2019, \u2018.exe\u2019 ,\u2019.dat\u2019 etc. Lets convert the URLs into a vector form. We have the vectors. Lets now convert it into test and training data and go right about performing logistic regression on it. That\u2019s it. See, its that simple yet so effective. We get an accuracy of 98%. That\u2019s a very high value for a machine to be able to detect a malicious URL with. Want to test some links to see if the model gives good predictions? Sure. Lets do it. The results come out to be amazing. This is what a human would have predicted. No? The data and code is available at Github That is it. I hope you enjoyed reading. Your comments are most welcome.","flair":"four\tProject"}
{"author":"clay_pool","created":"Tue Oct 04 23:33:24 EDT 2016","text":" Skip to Main Content IEEE.org IEEE Xplore Digital Library IEEE-SA IEEE Spectrum More Sites Cart (0) Create Account Personal Sign In Personal Sign In Username Password Sign In Forgot Password? Institutional Sign In By Topic Aerospace Bioengineering Communication, Networking & Broadcasting Components, Circuits, Devices & Systems Computing & Processing Engineered Materials, Dielectrics & Plasmas Engineering Profession Fields, Waves & Electromagnetics General Topics for Engineers Geoscience Nuclear Engineering Photonics & Electro-Optics Power, Energy, & Industry Applications Robotics & Control Systems Signal Processing & Analysis Transportation Browse Books & eBooks Conference Publications Courses Journals & Magazines Standards By Topic My Settings Content Alerts My Projects Search Alerts Preferences Purchase History Search History What can I access? Get Help About IEEE Xplore Feedback Technical Support Resources and Help Terms of Use What Can I Access? Subscribe Personal Sign In Create Account IEEE Account Change Username\/Password Update Address Purchase Details Payment Options Order History View Purchased Documents Profile Information Communications Preferences Profession and Education Technical Interests Need Help? US & Canada: +1 800 678 4333 Worldwide: +1 732 981 0060 Contact & Support About IEEE Xplore Contact Us Help Terms of Use Nondiscrimination Policy Sitemap Privacy & Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2016 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions. ","flair":"null\tnull"}
{"author":"Jaden71","created":"Tue Oct 18 20:48:37 EDT 2016","text":"Nvidia is releasing a new [GPU](http:\/\/www.geforce.com\/hardware\/10series\/geforce-gtx-1050)\nsoon and I was wondering if this would be viable as a GPU for a budget hobbyist build to learn deep learning. Should this suffice or are there any other viable options that don't cost an arm and a leg? How would this compare to what I have now (solely on an i7-4790k)?\n\nThanks in advance.\n\n(I'm a poor student and don't have a lot of money so the price tags of cards such as 1080, Titan X, etc. are out of the question).","flair":"one\tDiscussion"}
{"author":"DrPharael","created":"Fri Oct 07 08:04:23 EDT 2016","text":" Google Research Blog The latest news from Research at Google Graph-powered Machine Learning at Google Thursday, October 06, 2016 Posted by Sujith Ravi, Staff Research Scientist, Google Research Recently, there have been significant advances in Machine Learning that enable computer systems to solve complex real-world problems. One of those advances is Google\u2019s large scale, graph-based machine learning platform, built by the Expander team in Google Research. A technology that is behind many of the Google products and features you may use everyday, graph-based machine learning is a powerful tool that can be used to power useful features such as reminders in Inbox and smart messaging in Allo, or used in conjunction with deep neural networks to power the latest image recognition system in Google Photos. Learning with Minimal Supervision Much of the recent success in deep learning, and machine learning in general, can be attributed to models that demonstrate high predictive capacity when trained on large amounts of labeled data -- often millions of training examples. This is commonly referred to as \u201Csupervised learning\u201D since it requires supervision, in the form of labeled data, to train the machine learning systems. (Conversely, some machine learning methods operate directly on raw data without any supervision, a paradigm referred to as unsupervised learning.) However, the more difficult the task, the harder it is to get sufficient high-quality labeled data. It is often prohibitively labor intensive and time-consuming to collect labeled data for every new problem. This motivated the Expander research team to build new technology for powering machine learning applications at scale and with minimal supervision. Expander\u2019s technology draws inspiration from how humans learn to generalize and bridge the gap between what they already know (labeled information) and novel, unfamiliar observations (unlabeled information). Known as \u201Csemi-supervised\u201D learning, this powerful technique enables us to build systems that can work in situations where training data may be sparse. One of the key advantages to a graph-based semi-supervised machine learning approach is the fact that (a) one models labeled and unlabeled data jointly during learning, leveraging the underlying structure in the data, (b) one can easily combine multiple types of signals (for example, relational information from Knowledge Graph along with raw features) into a single graph representation and learn over them. This is in contrast to other machine learning approaches, such as neural network methods, in which it is typical to first train a system using labeled data with features and then apply the trained system to unlabeled data. Graph Learning: How It Works At its core, Expander\u2019s platform combines semi-supervised machine learning with large-scale graph-based learning by building a multi-graph representation of the data with nodes corresponding to objects or concepts and edges connecting concepts that share similarities. The graph typically contains both labeled data (nodes associated with a known output category or label) and unlabeled data (nodes for which no labels were provided). Expander\u2019s framework then performs semi-supervised learning to label all nodes jointly by propagating label information across the graph. However, this is easier said than done! We have to (1) learn efficiently at scale with minimal supervision (i.e., tiny amount of labeled data), (2) operate over multi-modal data (i.e., heterogeneous representations and various sources of data), and (3) solve challenging prediction tasks (i.e., large, complex output spaces) involving high dimensional data that might be noisy. One of the primary ingredients in the entire learning process is the graph and choice of connections. Graphs come in all sizes, shapes and can be combined from multiple sources. We have observed that it is often beneficial to learn over multi-graphs that combine information from multiple types of data representations (e.g., image pixels, object categories and chat response messages for PhotoReply in Allo). The Expander team\u2019s graph learning platform automatically generates graphs directly from data based on the inferred or known relationships between data elements. The data can be structured (for example, relational data) or unstructured (for example, sparse or dense feature representations extracted from raw data). To understand how Expander\u2019s system learns, let us consider an example graph shown below. There are two types of nodes in the graph: \u201Cgrey\u201D represents unlabeled data whereas the colored nodes represent labeled data. Relationships between node data is represented via edges and thickness of each edge indicates strength of the connection. We can formulate the semi-supervised learning problem on this toy graph as follows: predict a color (\u201Cred\u201D or \u201Cblue\u201D) for every node in the graph. Note that the specific choice of graph structure and colors depend on the task. For example, as shown in this research paper we recently published, a graph that we built for the Smart Reply feature in Inbox represents email messages as nodes and colors indicate semantic categories of user responses (e.g., \u201Cyes\u201D, \u201Cawesome\u201D, \u201Cfunny\u201D). The Expander graph learning framework solves this labeling task by treating it as an optimization problem. At the simplest level, it learns a color label assignment for every node in the graph such that neighboring nodes are assigned similar colors depending on the strength of their connection. A naive way to solve this would be to try to learn a label assignment for all nodes at once -- this method does not scale to large graphs. Instead, we can optimize the problem formulation by propagating colors from labeled nodes to their neighbors, and then repeating the process. In each step, an unlabeled node is assigned a label by inspecting color assignments of its neighbors. We can update every node\u2019s label in this manner and iterate until the whole graph is colored. This process is a far more efficient way to optimize the same problem and the sequence of iterations converges to a unique solution in this case. The solution at the end of the graph propagation looks something like this: Semi-supervised learning on a graph In practice, we use complex optimization functions defined over the graph structure, which incorporate additional information and constraints for semi-supervised graph learning that can lead to hard, non-convex problems. The real challenge, however, is to scale this efficiently to graphs containing billions of nodes, trillions of edges and for complex tasks involving billions of different label types. To tackle this challenge, we created an approach outlined in Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation, published last year. It introduces a streaming algorithm to process information propagated from neighboring nodes in a distributed manner that makes it work on very large graphs. In addition, it addresses other practical concerns, notably it guarantees that the space complexity or memory requirements of the system stays constant regardless of the difficulty of the task, i.e., the overall system uses the same amount of memory regardless of whether the number of prediction labels is two (as in the above toy example) or a million or even a billion. This enables wide-ranging applications for natural language understanding, machine perception, user modeling and even joint multimodal learning for tasks involving multiple modalities such as text, image and video inputs. Language Graphs for Learning Humor As an example use of graph-based machine learning, consider emotion labeling, a language understanding task in Smart Reply for Inbox, where the goal is to label words occurring in natural language text with their fine-grained emotion categories. A neural network model is first applied to a text corpus to learn word embeddings, i.e., a mathematical vector representation of the meaning of each word. The dense embedding vectors are then used to build a sparse graph where nodes correspond to words and edges represent semantic relationship between them. Edge strength is computed using similarity between embedding vectors \u2014 low similarity edges are ignored. We seed the graph with emotion labels known a priori for a few nodes (e.g., laugh is labeled as \u201Cfunny\u201D) and then apply semi-supervised learning over the graph to discover emotion categories for remaining words (e.g., ROTFL gets labeled as \u201Cfunny\u201D owing to its multi-hop semantic connection to the word \u201Claugh\u201D). Learning emotion associations using graph constructed from word embedding vectors For applications involving large datasets or dense representations that are observed (e.g., pixels from images) or learned using neural networks (e.g., embedding vectors), it is infeasible to compute pairwise similarity between all objects to construct edges in the graph. The Expander team solves this problem by leveraging approximate, linear-time graph construction algorithms. Graph-based Machine Intelligence in Action The Expander team\u2019s machine learning system is now being used on massive graphs (containing billions of nodes and trillions of edges) to recognize and understand concepts in natural language, images, videos, and queries, powering Google products for applications like reminders, question answering, language translation, visual object recognition, dialogue understanding, and more. We are excited that with the recent release of Allo, millions of chat users are now experiencing smart messaging technology powered by the Expander team\u2019s system for understanding and assisting with chat conversations in multiple languages. Also, this technology isn\u2019t used only for large-scale models in the cloud - as announced this past week, Android Wear has opened up an on-device Smart Reply capability for developers that will provide smart replies for any messaging application. We\u2019re excited to tackle even more challenging Internet-scale problems with Expander in the years to come. Acknowledgements We wish to acknowledge the hard work of all the researchers, engineers, product managers, and leaders across Google who helped make this technology a success. In particular, we would like to highlight the efforts of Allan Heydon, Andrei Broder, Andrew Tomkins, Ariel Fuxman, Bo Pang, Dana Movshovitz-Attias, Fritz Obermeyer, Krishnamurthy Viswanathan, Patrick McGregor, Peter Young, Robin Dua, Sujith Ravi and Vivek Ramavajjala. Google Labels: Computer Vision , Expander , Graph , Machine Intelligence , Machine Learning , Multimodal Learning , Natural Language Understanding , Semi-supervised Learning    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"zdwiel","created":"Thu Nov 17 21:07:08 EST 2016","text":"Nervana graph is Nervana's library for developing frameworks that can efficiently run deep learning computations on a variety of compute platforms. it consists of three primary API components: For more information, please see the blog post announcing our preview release! Installation documentation can be found here. First ensure you have neon checked out and built. To install Nervana Graph into your neon virtual env: To uninstall Nervana Graph from your virtual env: Before checking in code, ensure no \"make style\" errors: To generate the documentation as html files: These are known issues which are being addressed: Please feel free to contribute in shaping the future of Nervana Graph.","flair":"two\tNews"}
{"author":"she89","created":"Tue Oct 25 22:46:39 EDT 2016","text":"Hi, I tried to implement a neural network and failed. I faced 2 problems:  \n1. I needed a multi-dimensional dynamic array. Using normal arrays failed, so I used Eigen.  \n2. I couldn't model the NN well. It seems too complex.   \nThe problem with 2 is it may be hard to imagine the whole possible structures in my mind. How to architect big programs?  \nI asked a professor and the reply was to use a NN library, but I found 2 problems:  \n1. License: Will I be arrested if I used a library like CNTK in a comercial product?  \n*Note: I used CNTK as an example, because I prefer C++ over Python.*  \n2. What if it wasn't enough and I've to implement something by myself?  \n  \nWhat are your advices? Should I try again or use a library? I'm planning to make something like the neuron simulator **NEST**.","flair":"one\tDiscussion"}
{"author":"gradient__dissent","created":"Tue Nov 01 16:07:09 EDT 2016","text":"Lets say I have a decent budget(500k) to spend on solving a problem that can most likely only benefit from optimization techniques used in Machine Learning.\nWhat options are available to me?  \n  \nIt seams like money goes to the cool or relevant enough proposals, not ones directed at solving a very specific problem. Is this \"seller's market\" interpretation accurate? Industry takes what it can get?   \n  \nAre the most qualified people to handle this doing so in a research capacity?  \nWhat reliable consultancies exist? Can you list a few?   \n  \n\n  \n\n\n  \n","flair":"one\tDiscussion"}
{"author":"Guim30","created":"Tue Nov 22 07:04:23 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.06355 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs cs.AI References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Invertible Conditional GANs for image editing Authors: Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, Jose M. Álvarez (Submitted on 19 Nov 2016) Abstract: Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications. Comments: Accepted paper at NIPS 2016 Workshop on Adversarial Training Subjects: Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) Cite as: arXiv:1611.06355 [cs.CV]   (or arXiv:1611.06355v1 [cs.CV] for this version) Submission history From: Guim Perarnau [view email] [v1] Sat, 19 Nov 2016 12:35:01 GMT (2392kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"rd11235","created":"Mon Oct 10 05:18:12 EDT 2016","text":"From the Hammersley-Clifford theorem, we know that an undirected graph is compatible (same independence assumptions) with a distribution if and only if it can be factorized over cliques:\n\n(1)    p(x) = 1 \/ Z   *   prod_c f_c(x_c)\n\nSubtlety: The theorem only applies to distributions that are everywhere positive, which in turn means that we can represent p(x) using an exponential:\n\n(2)    p(x) = 1 \/ Z   *   exp(  -sum_c f_c(x_c)  )\n\n(These new f_c's are not the same as the old f_c's.)\n\nNotice here that we can have model parameters inside the f_c's, and notice also that *we are not guaranteeing that the f_c's are linear in those parameters.*\n\nThis theorem is discussed in Bishop's book, in Geman and Geman's classic paper, and many other sources.\n\nBut some other sources, including but not limited to the [Wikipedia article on Markov Random Fields](https:\/\/en.wikipedia.org\/wiki\/Markov_random_field), state something else: that an undirected graph is compatible with a distribution if and only if the distribution has this form *and is log linear in its parameters*:\n\n(3)    p(x) = 1 \/ Z   *   exp(  -sum_c,k   w_c,k * f_c,k(x_c)    )\n\nwith all of the f_c's being fixed functions (in other words, with no model parameters), and with k introduced to allow multiple feature functions per clique.\n\nSo is this last statement true or not? I'm leaning toward not but would like a definitive reference if someone has one.\n\nAlso, a toy example for discussion:\n\np(x_1, x_2) = 1 \/ Z   *   exp(  ReLU(w * x_1 + w * x_2)  )\n\nWe can verify that the independence properties are consistent with the graph x_1 ----- x_2.\n\nIf the x are discrete, I can imagine transforming (2) into (3) by building a huge table of possible values and reparameterizing using many more parameters, though this doesn't seem like it'd be useful in practice. But if the x are continuous, then I see no way to equate (2) and (3).","flair":"one\tDiscussion"}
{"author":"iX6kNcrS","created":"Mon Oct 03 08:36:01 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1606.05426 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1606 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Jose Alvarez Jose M. Alvarez Lars Petersson Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: DecomposeMe: Simplifying ConvNets for End-to-End Learning Authors: Jose Alvarez, Lars Petersson (Submitted on 17 Jun 2016) Abstract: Deep learning and convolutional neural networks (ConvNets) have been successfully applied to most relevant tasks in the computer vision community. However, these networks are computationally demanding and not suitable for embedded devices where memory and time consumption are relevant. In this paper, we propose DecomposeMe, a simple but effective technique to learn features using 1D convolutions. The proposed architecture enables both simplicity and filter sharing leading to increased learning capacity. A comprehensive set of large-scale experiments on ImageNet and Places2 demonstrates the ability of our method to improve performance while significantly reducing the number of parameters required. Notably, on Places2, we obtain an improvement in relative top-1 classification accuracy of 7.7\\% with an architecture that requires 92% fewer parameters compared to VGG-B. The proposed network is also demonstrated to generalize to other tasks by converting existing networks. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1606.05426 [cs.CV]   (or arXiv:1606.05426v1 [cs.CV] for this version) Submission history From: Jose M. Alvarez [view email] [v1] Fri, 17 Jun 2016 06:48:12 GMT (1426kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"hardmaru","created":"Wed Nov 09 11:40:18 EST 2016","text":" Eric Jang Technology, A.I., Careers Tuesday, November 8, 2016 Tutorial: Categorical Variational Autoencoders using Gumbel-Softmax 256ReLU [Not supported by viewer]512ReLU [Not supported by viewer]512ReLU [Not supported by viewer]256ReLU [Not supported by viewer] In this post, I discuss our recent paper, Categorical Reparameterization with Gumbel-Softmax, which introduces a simple technique for training neural networks with discrete latent variables. I'm really excited to share this because (1) I believe it will be quite useful for a variety of Machine Learning research problems, (2) this is my first published paper ever (on Arxiv, and submitted to a NIPS workshop and ICLR as well). The TLDR; if you want categorical features in your neural nets, just let sample = softmax((logits+gumbel noise)\/temperature), and then backprop as usual using your favorite automatic differentiation software (e.g. TensorFlow, Torch, Theano). You can find the code for this article here Introduction One of the main themes in Deep Learning is to \u201Clet the neural net figure out all the intermediate features\u201D. For example: training convolutional neural networks results in the self-organization of a feature detector hierarchy, while Neural Turing Machines automatically \u201Cdiscover\u201D copying and sorting algorithms. The workhorse of Deep Learning is the backpropagation algorithm, which uses dynamic programming to compute parameter gradients of the network. These gradients are then used to minimize the optimization objective via gradient descent. In order for this to work, all of the layers in our neural network \u2014 i.e. our learned intermediate features \u2014 must be continuous-valued functions. What happens if we want to learn intermediate representations that are discrete? Many \"codes\" we want to learn are fundamentally discrete - musical notes on a keyboard, object classes (\u201Ckitten\u201D, \u201Cballoon\u201D, \u201Ctruck\u201D), and quantized addresses (\u201Cindex 423 in memory\u201D). We can use stochastic neural networks, where each layer compute the parameters of some (discrete) distribution, and its forward pass consists of taking a sample from that parametric distribution. However, the difficulty is that we can\u2019t backpropagate through samples. As shown below, there is a stochastic node (blue circle) in between $f(z)$ and $\\theta$. Left: in continuous neural nets, you can use backprop to compute parameter gradients. Right: backpropagation is not possible through stochastic nodes. Gumbel-Softmax Distribution The problem of backpropagating through stochastic nodes can be circumvented if we can re-express the sample $z \\sim p_\\theta(z)$, such that gradients can flow from $f(z)$ to $\\theta$ without encountering stochastic nodes. For example, samples from the normal distribution $z \\sim \\mathcal{N}(\\mu,\\sigma)$ can be re-written as $z = \\mu + \\sigma \\cdot \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This is also known as the \u201Creparameterization trick\u201D, and is commonly used to train variational autoencoders with Gaussian latent variables. The Gumbel-Softmax distribution is reparameterizable, allowing us to avoid the stochastic node during backpropagation. The main contribution of this work is a \u201Creparameterization trick\u201D for the categorical distribution. Well, not quite \u2013 it\u2019s actually a re-parameterization trick for a distribution that we can smoothly deform into the categorical distribution. We use the Gumbel-Max trick, which provides an efficient way to draw samples $z$ from the Categorical distribution with class probabilities $\\pi_i$: $$ \\DeclareMathOperator*{\\argmax}{arg\\,max} z = \\verb|one_hot|\\left(\\argmax_{i}{\\left[ g_i + \\log \\pi_i \\right]}\\right) $$ argmax is not differentiable, so we simply use the softmax function as a continuous approximation of argmax: $$ y_i = \\frac{\\text{exp}((\\log(\\pi_i)+g_i)\/\\tau)}{\\sum_{j=1}^k \\text{exp}((\\log(\\pi_j)+g_j)\/\\tau)} \\qquad \\text{for } i=1, ..., k. $$ Hence, we call this the Gumbel-SoftMax distribution*. $\\tau$ is a temperature parameter that allows us to control how closely samples from the Gumbel-Softmax distribution approximate those from the categorical distribution. As $\\tau \\to 0$, the softmax becomes an argmax and the Gumbel-Softmax distribution becomes the categorical distribution. During training, we let $\\tau > 0$ to allow gradients past the sample, then gradually anneal the temperature $\\tau$ (but not completely to 0, as the gradients would blow up). Below is an interactive widget that draws samples from the Gumbel-Softmax distribution. Keep in mind that samples are vectors, and a one-hot vector (i.e. one of the elements is 1.0 and the others are 0.0) corresponds to a discrete category. Click \"re-sample\" to generate a new sample, and try dragging the slider and see what samples look like when the temperature $\\tau$ is small! 1.0 re-sample TensorFlow Implementation Using this technique is extremely simple, and only requires 12 lines of Python code: Despite its simplicity, Gumbel-Softmax works surprisingly well - we benchmarked it against other stochastic gradient estimators for a couple tasks and Gumbel-Softmax outperformed them for both Bernoulli (K=2) and Categorical (K=10) latent variables. We can also use it to train semi-supervised classification models much faster than previous approaches. See our paper for more details. Categorical VAE with Gumbel-Softmax To demonstrate this technique in practice, here's a categorical variational autoencoder for MNIST, implemented in less than 100 lines of Python + TensorFlow code. In standard Variational Autoencoders, we learn an encoding function that maps the data manifold to an isotropic Gaussian, and a decoding function that transforms it back to the sample. The data manifold is projected into a Gaussian ball; this can be hard to interpret if you are trying to learn the categorical structure within your data. First, we declare the encoding network: Next, we sample from the Gumbel-Softmax posterior and decode it back into our MNIST image. Variational autoencoders minimizes reconstruction error of the data by maximizing an expectedlower bound (ELBO) on the likelihood of the data, under a generative model $p_\\theta(x)$. For a derivation, see this tutorial on variational methods. $$\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(y|x)}[\\log p_\\theta(x|y)] - KL[q_\\phi(y|x)||p_\\theta(y)]$$ Finally, we run train our VAE: ...and, that's it! Now we can sample randomly from our latent categorical code and decode it back into MNIST images: Code can be found here. Thank you for reading, and let me know if you find this technique useful! Acknowledgements I'm sincerely grateful to my co-authors, Shane Gu and Ben Poole for collaborating with me on this work. Shane introduced me to the Gumbel-Max trick back in August, and supplied the framework for comparing Gumbel-Softmax with existing stochastic gradient estimators. Ben suggested and implemented the semi-supervised learning aspect of the paper, did the math derivations in the Appendix, and helped me a lot with editing the paper. Finally, thanks to Vincent Vanhoucke and the Google Brain team for encouraging me to pursue this idea. *Chris J. Maddison, Andriy Mnih, and Yee Whye Teh at Deepmind have discovered this technique independently and published their own paper on it - they call it the \u201CConcrete Distribution\u201D. We only found out about each other\u2019s work right as we were submitting our papers to conferences (oops!). If you use this technique in your work, please cite both of our papers! They deserve just as much credit. Eric at 11:00 PM Share 6 comments: Viktor YanushNovember 9, 2016 at 11:05 AM Could you please compare your model to model called \"Discrete Variational Autoencoder\" and give some thoughts on the difference and similarity of models? https:\/\/arxiv.org\/abs\/1609.02200 ReplyDelete Replies UnknownNovember 14, 2016 at 11:44 AM +1 Delete Reply UnknownNovember 9, 2016 at 10:39 PM Great tutorial! I am wondering what happens if the number of categories is extremely large, i.e., 1 million. Gradients need to calculated for all the \\PI s and g s of a large number? Thank you, and looking forward to hearing. ReplyDelete Replies EricNovember 10, 2016 at 5:55 PM If categories are large, you will need a more efficient encoding of samples from the categorical distribution than one-hot vectors, otherwise you will have a rank>1e6 matrix multiply. A reparameterization trick for other encodings of vectors might be worth pursuing. Delete Reply UnknownNovember 14, 2016 at 6:01 PM Hi Eric, Agree with the posters above me -- great tutorial! I was wondering how this would be applied to my use case: suppose I have two dense real-valued vectors, and I want to train a VAE s.t. the latent features are categorical and the original and decoded vectors are close together in terms of cosine similarity. I'm guessing that I have to change the first term of the ELBO function, since `p_x.log_prob(x)` isn't what I care about (is that right?). Any thoughts on what the modified version would be? Thanks ReplyDelete Ero GolNovember 17, 2016 at 5:02 AM I still don't see why we cannot train the same network by enforcing the latent space a one-hot vector for the example above. So if the backprop is the problem, you can flow the error through the argmax node and you can learn the parameters still. Could you give more details what is the differentiating factor of your method. Also cloud you explain what is z values on the last figure? ReplyDelete Add comment Load more... \u203A Home View web version Powered by Blogger. ","flair":"three\tResearch"}
{"author":"NeuroBoss31","created":"Sat Oct 29 19:25:32 EDT 2016","text":" Can Peripheral Representations Improve Clutter Metrics on Complex Scenes? Arturo Deza Dynamical Neuroscience Institute for Collaborative Biotechnologies UC Santa Barbara, CA, USA deza@dyns.ucsb.edu Miguel P. Eckstein Psychological and Brain Sciences Institute for Collaborative Biotechnologies UC Santa Barbara, CA, USA eckstein@psych.ucsb.edu Abstract Previous studies have proposed image-based clutter measures that correlate with human search times and\/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores (r(44) = −0.82 ± 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = −0.19 ± 0.13, p = 0.0774) in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available1. 1 Introduction What is clutter? While it seems easy to make sense of a cluttered desk vs an uncluttered desk at a glance, it is hard to quantify clutter with a number. Is a cluttered desk, one stacked with papers? Or is an uncluttered desk, one that is more organized irrelevant of number of items? An important goal in clutter research has been to develop an image based computational model that outputs a quantitative measure that correlates with human perceptual behavior [19, 12, 23]. Previous studies have created models that output global or regional metrics to measure clutter perception. Such measures are aimed to predict the influence of clutter on perception. However, one important aspect of human visual perception is that it is not space invariant: the fovea processes visual information with high spatial detail while regions away from the central fovea have access to lower spatial detail. Thus, the influence of clutter on perception can depend on the retinal location of the stimulus and such influences will likely interact with the information content in the stimulus. The goal of the current paper is to develop a foveated clutter model that can successfully predict the interaction between retinal eccentricity and image content in modulating the influence of clutter on perceptual behavior. We introduce a foveated mechanism based on the peripheral architecture proposed by Freeman and Simoncelli [9] and stack it into a current clutter model (Feature Conges- tion [22, 23]) to generate a clutter map that arises from a calculation of information loss with retinal eccentricity but is multiplicatively modulated by the original unfoveated clutter score. The new 1https:\/\/github.com\/ArturoDeza\/Piranhas 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. ar X iv :1 60 8. 04 04 2v 1 [ cs .C V ] 1 4 A ug 2 01 6 https:\/\/github.com\/ArturoDeza\/Piranhas Figure 1: The Feature Congestion pipeline as explained in Rosenholtz et al. [23]. A color, contrast and orientation feature map for each spatial pyramid is extracted, and the max value of each is computed as the final feature map. The Feature Congestion map is then computed by a weighted sum over each feature map. The Feature Congestion score is the mean value of the map. measure is evaluated in a gaze-contingent psychophysical experiment measuring target detection in complex scenes as a function of target retinal eccentricity. We show that the foveated clutter models that account for loss of information in the periphery correlates better with human target detection (hit rate) across retinal eccentricities than non-foveated models. Although the model is presented in the context of Feature Congestion, the framework can be extended to any previous or future clutter metrics that produce clutter scores that are computed from a global pixel-wise clutter map. 2 Previous Work Previous studies have developed general measures of clutter computed for an entire image and do not consider the space-variant properties of the human visual system. Because our work seeks to model and assess the interaction between clutter and retinal location, experiments manipulating the eccentricity of a target while observers hold fixation (gaze contingent forced fixation) are most appropriate to evaluate the model. To our knowledge there has been no systematic evaluation of fixation dependent clutter models with forced fixation target detection in scenes. In this section, we will give an overview of state-of-the-art clutter models, metrics and evaluations. 2.1 Clutter Models Feature Congestion: Feature Congestion, initially proposed by [22, 23] produces both a pixel-wise clutter score map as a well as a global clutter score for any input image or Region of Interest (ROI). Each clutter map is computed by combining a Color map in CIELab space, an orientation map [14], and a local contrast map at multiple scales through Gaussian Pyramids [5]. One of the main advan- tages Feature Congestion has is that each pixel-wise clutter score (Fig. 1) and global score can be computed in less than a second. Furthermore, this is one of the few models that can output a specific clutter score for any pixel or ROI in an image. This will be crucial for developing a foveated model as explained in Section 4. Edge Density: Edge Density computes a ratio after applying an Edge Detector on the input im- age [19]. The final clutter score is the ratio of edges to total number of pixels present in the image. The intuition for this metric is straightforward: \u201Cthe more edges, the more clutter\u201D (due to objects for example). Subband Entropy: The Subband Entropy model begins by computing N steerable pyramids [24] at K orientations across each channel from the input image in CIELab color space. Once each N ×K subband is collected for each channel, the entropy for each oriented pyramid is computed pixelwise and they are averaged separately. Thus, Subband Entropy wishes to measure the entropy of each spatial frequency and oriented filter response of an image. Scale Invariance: The Scale Invariant Clutter Model proposed by Farid and Bravo [4] uses graph- based segmentation [8] at multiple k scales. A scale invariant clutter representation is given by the power law coefficient that matches the decay of number of regions with the adjusted scale parameter. 2 + + Fixation: 500 - 1000 ms (1 of 4 locations) Stimulus: 100, 200, 400, 900, 1600 ms (Remain fixated) Task 1 response (unlimited time, no feedback) Figure 2: Experiment 1: Forced Fixation Search flow diagram. A naive observer begins by fixating the image at a location that is either 1, 4, 9 or 15 deg away from the target (the observer is not aware of the possible eccentricities). After fixating on the image for a variable amount of time (100, 200, 400, 900 or 1600 ms), the observer must make a decision on target detection. ProtoObject Segmentation: ProtoObject Segmentation proposes an unsupervised metric for clut- ter scoring [26, 27]. The model begins by converting the image into HSV color space, and then proceeds to segment the image through superpixel segmentation [17, 16, 1]. After segmentation, mean-shift [11] is applied on all cluster (superpixel) medians to calculate the final amount of repre- sentative colors present in the image. Next, superpixels are merged with one another contingent on them being adjacent, and being assigned to the same mean-shift HSV cluster. The final score is a ratio between initial number of superpixels and final number of superpixels. Crowding Model: The Crowding Model developed by van der Berg et al. [25] is the only model to have used losses in the periphery due to crowding as a clutter metric. It decomposes the image into 3 different scales in CIELab color space. It then produces 6 different orientation maps for each scale given the luminance channel; a contrast map is also obtained by difference of Gaussians on the previously mentioned channel. All feature maps are then pooled with Gaussian kernels that grow linearly with eccentricity, KL-divergence is then computed between the pre and post pooling feature maps to get information loss coefficients, all coefficients are averaged together to produce a final clutter score. We will discuss the differences of this model to ours in the Discussion (Section 5). Texture Tiling Model: The Texture Tiling Model (TTM) is a recent perceptual model that accounts for losses in the periphery [21, 13] through psyhophysical experiments modelling visual search [7]: feature search, conjunction search, configuration search and asymmetric search. In essence, the Mongrels proposed by Rosenholtz et al. that simulate peripheral losses are very similar to the Metamers proposed by Freeman & Simoncelli [9]. We do not include comparisons to the TTM model since it requires additional psychophysics on the Mongrel versions of the images. 2.2 Clutter Metrics Global Clutter Score: The most basic clutter metric used in clutter research is the original clutter score that every model computes over the entire image. Edge Density & Proto-Object Segmentation output a ratio, while Subband Entropy and Feature Congestion output a score. However, Feature Congestion is the only model that outputs a dense pixelwise clutter map before computing a global score (Fig. 1). Thus, we use Feature Congestion clutter maps for our foveated clutter model. Clutter ROI: The second most used clutter metric is ROI (Region of Interest)-based, as shown in the work of Asher et al. [3]. This metric is of interest when an observer is engaging in target search, vs making a human judgement (Ex: \u201Crate the clutter of the following scenes\u201D). 2.3 Clutter Evaluations Human Clutter Judgements: Multiple studies of clutter, correlate their metrics with rankings\/ratings of clutter provided by human participants. Ideally, if clutter model A is better than clutter model B, then the correlation of model scores and human rankings\/ratings should be higher for model A than for model B. [27, 19, 25] 3 Response Time: Highly cluttered images will require more time for target search, hence more time to arrive to a decision of target present\/absent. Under the previous assumption, a high correlation value between response time and clutter score are a good sign for a clutter model. [23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image. [23, 3, 12] 3 Methods & Experiments 3.1 Experiment 1: Forced Fixation Search A total of 13 subjects participated in a Forced Fixation Search experiment where the goal was to detect a target in the subject\u2019s periphery and identify if there was a target (person) present or absent. Participants had variable amounts of time (100, 200, 400, 900, 1600 ms) to view each clip that was presented in a random order at a variable degree of eccentricities that the subjects were not aware of (1 deg, 4 deg, 9 deg, 15 deg). They were then prompted with a Target Detection rating scale where they had to rate from a scale from 1-10 by clicking on a number reporting how confident they were on detecting the target. Participants have unlimited time for making their judgements, and they did not take more than 10 seconds per judgment. There was no response feedback after each trial. Trials were aborted when subjects broke fixation outside of a 1 deg radius around the fixation cross. Each subject did 12 sessions that consisted of 360 unique images. Every session also presented the images with aerial viewpoints from different vantage points (Example: session 1 had the target at 12 o\u2019clock, while session 2 had the target at 3 o\u2019clock). To control for any fixational biases, all subjects had a unique fixation point for every trial for the same eccentricity values. All images were rendered with variable levels of clutter. Each session took about an hour to complete. The target was of size 0.5 deg×0.5 deg, 1 deg×1 deg, 1.5 deg×1.5 deg, depending on zoom level. For our analysis, we only used the low zoom and 100 ms time condition since there was less ceiling effects across all eccentricities. Stimuli Creation: A total of 273 videos were created each with a total duration of 120 seconds, where a \u2018birds eye\u2019 point-of-view camera rotated slowly around the center. While the video was in rotating motion, there was no relative motion between any parts of the video. From the original videos, a total of 360× 4 different clips were created. Half of the clips were target present, while the other half were target absent. These short and slowly rotating clips were used instead of still images in our experiment, to simulate slow real movement from a pilot point of view. All clips were shown to participants in random order. Apparatus: An EyeLink 1000 system (SR Research) was used to collect Eye Tracking data at a frequency of 1000Hz. Each participant was at a distance of 76 cm from a LCD screen on gamma display, so that each pixel subtended a visual angle of 0.022 deg \/px. All video clips were rendered at 1024× 760 pixels (22.5 deg×16.7 deg) and a frame rate of 24fps. Eye movements with velocity over 22 deg \/s and acceleration over 4000 deg \/s2 were qualified as saccades. Every trial began with a fixation cross, where each subject had to fixate the cross with a tolerance of 1 deg. 4 Foveated Feature Congestion A regular Feature Congestion clutter score is computed by taking the mean of the Feature Congestion map of the image or of a target ROI [12]. We propose a Foveated Feature Congestion (FFC) model that outputs a score which takes into account two main terms: 1) a regular Feature Congestion (FC) score and 2) a Peripheral Integration Feature Congestion (PIFC) coefficient that accounts the lower spatial resolution of the visual periphery that are detrimental for target detection. The first term is independent of fixation, while the second term will act as a non-linear gain that will either reduce or amplify the clutter score depending on fixation distance from the target. In this Section we will explain how to compute a PIFC, which will require creating a human-like peripheral architecture as explained in Section 4.1. We then present our Foveated Feature Congestion (FFC) clutter model in Section 4.2. Finally, we conclude by making a quantiative evaluation of the FFC (Section 4.3) in its ability to predict variations of target detectability across images and retinal eccentricity of the target. 4 3 6 9 12 15 18 21 240 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Eccentricity in degrees away from fovea 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Fu nc tio n va lu e Fu nc tio n va lu e Polar angle referenced from fovea 2ππ0 π\/2π\/4 3π\/4 5π\/4 3π\/2 7π\/4 (a) Top: gn(e) function. Bottom: hn(θ) function. Fu nc tio n Va lu e 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0 6 12 18 24-6-12-18-24 Eccentricity (degrees) 0 6 12 18 24 -6 -12 -18 -24 Ec ce nt ric ity (d eg re es ) (b) Peripheral Architecture. Figure 3: Construction of a Peripheral Architecture a la Freeman & Simoncelli [9] using the functions described in Section 4.1 are shown in Fig. 3(a). The blue region in the center of Fig. 3(b), represents the fovea where all information is preserved. Outer regions (in red), represent different parts of the periphery at multiple eccentricities. 4.1 Creating a Peripheral Architecture We used the Piranhas Toolkit [6] to create a Freeman and Simoncelli [9] peripheral architecture. This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2]. It is described by a set of pooling (linear) regions that increase in size with retinal eccentricity. Each pooling region is separable with respect to polar angle hn(θ) and log eccentricity gn(e), as described in Eq. 2 and Eq. 3 respectively. These functions are multiplied for every angle and eccentricity (θ, e) and are plotted in log polar coordinates to create the peripheral architecture as seen in Fig. 3. f(x) =  cos2(π2 ( x−(t0−1)\/2 t0 )); (−1 + t0)\/2 < x ≤ (t0 − 1)\/2 1; (t0 − 1)\/2 < x ≤ (1− t0)\/2 −cos2(π2 ( x−(1+t0)\/2 t0 )) + 1; (1− t0)\/2 < x ≤ (1 + t0)\/2 (1) hn(θ) = f (θ − (wθn+ wθ2 ) wθ ) ;wθ = 2π Nθ ;n = 0, ..., Nθ − 1 (2) gn(e) = f ( log(e)− [log(e0) + we(n+ 1)] we ) ;we = log(er)− log(e0) Ne ;n = 0, ..., Ne − 1 (3) The parameters we used match a V1 architecture with a scale of s = 0.25 , a visual radius of er = 24deg, a fovea of 2 deg, with e0 = 0.25 deg 2, and t0 = 1\/2. The scale defines the number of eccentricities Ne, as well as the number of polar pooling regions Nθ from 〈0, 2π]. Although observers saw the original stimuli at 0.022 deg\/pixel, with image size 1024 × 760; for modelling purposes: we rescaled all images to half their size so the peripheral architecture could fit all images under any fixation point. To preserve stimuli size in degrees after rescaling our images, our foveal model used an input value of 0.044 deg\/pixel (twice the value of experimental settings). Resizing the image to half its size also allows the peripheral architecture to consume less CPU computation time and memory. 4.2 Creating a Foveated Feature Congestion Model Intuitively, a foveated clutter model that takes into account target search should score very low when the target is in the fovea (near zero), and very high when the target is in the periphery. Thus, an observer should find a target without difficulty, achieving a near perfect hit rate in the fovea, yet the observer should have a lower hit rate in the periphery given crowding effects. Note that in the 2We remove regions with a radius smaller than the foveal radius, since there is no pooling in the fovea. 5 Figure 4: Foveated Feature Congestion flow diagram: In this example, the point of fixation is at 15 deg away from the target (bottom right corner of the input image). A Feature Congestion map of the image (top flow), and a Foveated Feature Congestion map (bottom flow) are created. The PIFC coefficient is computed around an ROI centered at the target (bottom flow; zoomed box). The Feature Congestion score is then multiplied by the PIFC coefficient, and the Foveated Feature Congestion score is returned. Sample PIFC\u2019s across eccentricities can be seen in the Supplementary Material. periphery, not only should it be harder to detect a target, but it is also likely to confuse the target with another object or region affine in shape, size, texture and\/or pixel value (false alarms). Under this assumption, we wish to modulate a clutter score (Feature Congestion) by a multiplicative factor, given the target and fixation location. We call this multiplicative term: the PIFC coefficient, which is defined over a 6 deg×6 deg ROI around the location of target t. The target itself was removed when processing the clutter maps since it indirectly contributes to the ROI clutter score [3]. The PIFC aims at quantifying the information loss around the target region due to peripheral processing. To compute the PIFC, we use the before mentioned ROI, and calculate a mean difference from the foveated clutter map with respect to the original non-foveated clutter map. If the target is foveated, there should be little to no difference between a foveated map and the original map, thus setting the PIFC coefficient value to near zero. However, as the target is farther away from the fovea, the PIFC coefficient should be higher given pooling effects in the periphery. To create a foveated map, we use Feature Congestion and apply max pooling on each pooling region after the peripheral architecture has been stacked on top of the Feature Congestion map. Note that the FFC map values will depend on the fixation location as shown in Fig. 4. The PIFC map is the result of subtracting the foveated map from the unfoveated map in the ROI, and the score is a mean distance value between these two maps (we use L1-norm, L2-norm or KL-divergence). Computational details can be seen in Algorithm 1. Thus, we can resume our model in Eq. 4: FFCf,tI = FCI × PIFC f ROI(t) (4) where FCI is the Feature Congestion score [23] of image I which is computed by the mean of the Feature Congestion map RFC , and FFC f,t I is the Foveated Feature Congestion score of the image I , depending on the point of fixation f and the location of the target t. 4.3 Foveated Feature Congestion Evaluation A visualization of each image and its respective Hit Rate vs Clutter Score across both foveated and unfoveated models can be visualized in Fig 5. Qualitatively, it shows the importance of a PIFC weighting term to the total image clutter score when performing our forced fixation search experiment. Futhermore, a quantitative bootstrap correlation analysis comparing classic metrics (Image, Target, ROI) against foveal metrics (FFC1, FFC2 and FFC3) shows that hit rate vs clutter scores are greater for those foveated models with a PIFC: Image: (r(44) = −0.19 ± 0.13, p = 0.0774), Target: (r(44) = −0.03± 0.14, p = 0.4204), ROI: (r(44) = −0.25± 0.14, p = 0.0392), FFC1 (L1-norm): 6 Algorithm 1 Computation of Peripheral Integration Feature Congestion (PIFC) Coefficient 1: procedure COMPUTE PIFC OF ROI OF IMAGE I ON FIXATION f 2: Create a Peripheral Architecture A : (Nθ, Ne) 3: Offset image I in Peripheral Architecture by fixation f : (fx, fy). 4: Compute Regular Feature Congestion (RFC) map of image I 5: Set Peripheral Feature Congestion (P fFC) ⊂ R 2 + map to zero. 6: Copy Feature Congestion values in fovea r0: P fFC(r0) = (RFC(r0)) 7: for each pooling region ri overlapping I , s.t. 1 ≤ i ≤ Nθ ×Ne do 8: Get Regular Feature Congestion (FC) values in ri 9: Set Peripheral FC value to max Regular FC value: P fFC(ri) = max(RFC(ri)) 10: end for 11: Crop PIFC map to ROI: pfFC = P f FC(ROI) 12: Crop FC map to ROI: rFC = RFC(ROI) 13: Choose Distance metric D between rFC and pfFC map 14: Compute Coefficient = mean(D(rFC , pfFC)) 15: return Coefficient 16: end procedure (1) (2) (3) (4) (25) (26) (27) (28) (50)(51) (52) (1) (2) (3) (4) (25) (26) (27) (28) (50) (51) (52) (1) (2) (3) (4) (25) (26) (27)(28) (50) (51) (52) (1)(2) (3) (4) (25) (26)(27) (28) (50) (51) (52) 1 deg 4 deg 9 deg 15 deg Feature Congestion 2.00 2.25 2.50 2.75 3.00 3.25 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Hi t R at e (a) Feature Congestion with image ID\u2019s (1) (2) (3) (4) (25) (26) (27) (28) (50)(51) (52) (1) (2) (3) (4) (25) (26) (27) (28) (50) (51) (52) (1) (2) (3) (4) (25) (26) (27) (28) (50) (51) (52) (1)(2) (3) (4) (25) (26) (27) (28) (50) (51) (52) 1 deg 4 deg 9 deg 15 deg Foveated Feature Congestion 0 3 6 9 12 15 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Hi t R at e (b) Foveated Feature Congestion with image ID\u2019s Figure 5: Fig. 5(a) shows the current limitations of global clutter metrics when engaging in Forced Fixation Search. The same image under different eccentricities has the same clutter score yet possess a different hit rate. Our proposed foveated model (Fig. 5(b)), compensates this difference through the PIFC coefficient, and modulates each clutter score depending on fixation distance from target. (r(44) = −0.82±0.04, p < 0.0001), FFC2 (L2-norm): (r(44) = −0.79±0.06, p < 0.0001), FFC3 (KL-divergence): (r(44) = −0.82± 0.04, p < 0.0001). Notice that there is no difference in correlations between using the L1-norm, L2-norm or KL- divergence distance for each model in terms of the correlation with hit rate. Table 1(Supp. Mat.) also shows the highest correlation with a 6 × 6 deg ROI window across all metrics. Note that the same analysis can not be applied to false alarms, since it is indistinguishable to separate a false alarm at 1 deg from 15 deg (the target is not present, so there is no real eccentricity away from fixation). However as mentioned in the Methods section, fixation location for target absent trials in the experiment were placed assuming a location from its matching target present image. It is important that target present and absent fixations have the same distributions for each eccentricity. 5 Discussion In general, images that have low Feature Congestion have less gain in PIFC coefficients as eccentricity increases. While images with high clutter have higher gain in PIFC coefficients. Consequently, the difference of FFC between different images increases nonlinearly with eccentricity, as observed in Fig. 6. This is our main contribution, as these differences in clutter score as a function of eccentricity do not exist for regular Feature Congestion, and these differences in scores should be able to correlate with human performance in target detection. 7 (a) FC vs Eccentricity. (b) PIFC (L1-norm) vs Eccentricity. (c) FFC vs Eccentricity. Figure 6: Feature Congestion (FC) vs Foveated Feature Congestion (FFC). In Fig. 6(a) we see that clutter stays constant across different eccentricities for a forced fixation task. Our FFC model (Fig. 6(c)) enriches the FC model, by showing how clutter increases as a function of eccentricity through the PIFC in Fig. 6(b). Feature Congestion Edge Density Subband Entropy ProtoObject Segmentation De ns e Re pr es en ta tio n Fo ve at ed Re pr es en ta tio n Figure 7: Dense and Foveated representations of multiple models assuming a center point of fixation. Our model is also different from the van der Berg et al. [25] model since our peripheral architecture uses: a biologically inspired peripheral architecture with log polar regions that provide anisotropic pooling [15] rather than isotropic gaussian pooling as a linear function of eccentricity [25]; we used region-based max pooling for each final feature map instead of pixel-based mean pooling (gaussians) per each scale (which allows for stronger differences); this final difference also makes our model computationally more efficient running at 700ms per image, vs 180s per image for the Crowding model (×250 speed up). A home-brewed Crowding Model applied to our forced fixation experiment resulted in a correlation of (r(44) = −0.23± 0.13, p = 0.0469), equivalent to using a non foveated metric such as regular Feature Congestion (r(44) = −0.19± 0.13, p = 0.0774). We finally extended our model to create foveated(FoV) versions of Edge Density(ED) [19], Sub- band Entropy(SE) [24, 23] and ProtoObject Segmentation(PS) [27] showing that correlations for all foveated versions are stronger than non-foveated versions for the same task: rED = −0.21, rED+FoV = −0.76, rSE = −0.19, rSE+FoV = −0.77, rPS = −0.30, but rPS+FoV = −0.74. Note that the highest foveated correlation is FC: rFC+FoV = −0.82, despite rFC = −0.19 under a L1-norm loss of the PIFC. Feature Congestion has a dense representation, is more bio-inspired than the other models, and outperforms in the periphery. See Figure 7. An overview of creating dense and foveated versions for previously mentioned models can be seen in the Supp. Material. 6 Conclusion In this paper we have introduced a peripheral architecture that shows detrimental effects of different eccentricities on target detection, that helps us model clutter for forced fixation experiments. We introduced a forced fixation experimental design for clutter research; we defined a biologically inspired peripheral architecture that pools features in V1; and we stacked the previously mentioned peripheral architecture on top of a Feature Congestion map to create a Foveated Feature Congestion (FFC) model \u2013 and we extended this pipeline to other clutter models. We showed that the FFC model better explains loss in target detection performance as a function of eccentricity through the introduction of the Peripheral Integration Feature Congestion (PIFC) coefficient which varies non linearly. 8 Acknowledgements We would like to thank Miguel Lago and Aditya Jonnalagadda for useful proof-reads and revisions, as well as Mordechai Juni, N.C. Puneeth, and Emre Akbas for useful suggestions. This work was supported by the Institute for Collaborative Biotechnologies through grant 2 W911NF-09-0001 from the U.S. Army Research Office. References [1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. Slic superpixels. Technical report, 2010. [2] E. Akbas and M. P. Eckstein. Object detection through exploration with a foveated visual field. arXiv preprint arXiv:1408.0814, 2014. [3] M. F. Asher, D. J. Tolhurst, T. Troscianko, and I. D. Gilchrist. Regional effects of clutter on human target detection performance. Journal of vision, 13(5):25\u201325, 2013. [4] M. J. Bravo and H. Farid. A scale invariant measure of clutter. Journal of Vision, 8(1):23\u201323, 2008. [5] P. J. Burt and E. H. Adelson. The laplacian pyramid as a compact image code. Communications, IEEE Transactions on, 31(4):532\u2013540, 1983. [6] A. Deza, E. Abkas, and M. P. Eckstein. Piranhas toolkit: Peripheral architectures for natural, hybrid and artificial systems. [7] M. P. Eckstein. Visual search: A retrospective. Journal of Vision, 11(5):14\u201314, 2011. [8] P. F. Felzenszwalb and D. P. Huttenlocher. Efficient graph-based image segmentation. International Journal of Computer Vision, 59(2):167\u2013181, 2004. [9] J. Freeman and E. P. Simoncelli. Metamers of the ventral stream. Nature neuroscience, 14(9):1195\u20131201, 2011. [10] J. Freeman, C. M. Ziemba, D. J. Heeger, E. P. Simoncelli, and J. A. Movshon. A functional and perceptual signature of the second visual area in primates. Nature neuroscience, 16(7):974\u2013981, 2013. [11] K. Fukunaga and L. D. Hostetler. The estimation of the gradient of a density function, with applications in pattern recognition. Information Theory, IEEE Transactions on, 21(1):32\u201340, 1975. [12] J. M. Henderson, M. Chanceaux, and T. J. Smith. The influence of clutter on real-world scene search: Evidence from search efficiency and eye movements. Journal of Vision, 9(1):32\u201332, 2009. [13] S. Keshvari and R. Rosenholtz. Pooling of continuous features provides a unifying account of crowding. Journal of Vision, 16(39), 2016. [14] M. S. Landy and J. R. Bergen. Texture segregation and orientation gradient. Vision research, 31(4):679\u2013691, 1991. [15] D. M. Levi. Visual crowding. Current Biology, 21(18):R678\u2013R679, 2011. [16] A. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J. Dickinson, and K. Siddiqi. Turbopixels: Fast superpixels using geometric flows. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(12):2290\u20132297, 2009. [17] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa. Entropy rate superpixel segmentation. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2097\u20132104. IEEE, 2011. [18] J. A. Movshon and E. P. Simoncelli. Representation of naturalistic image structure in the primate visual cortex. In Cold Spring Harbor symposia on quantitative biology, volume 79, pages 115\u2013122. Cold Spring Harbor Laboratory Press, 2014. [19] A. Oliva, M. L. Mack, M. Shrestha, and A. Peeper. Identifying the perceptual dimensions of visual complexity of scenes. [20] J. Portilla and E. P. Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision, 40(1):49\u201370, 2000. [21] R. Rosenholtz, J. Huang, A. Raj, B. J. Balas, and L. Ilie. A summary statistic representation in peripheral vision explains visual search. Journal of vision, 12(4):14\u201314, 2012. [22] R. Rosenholtz, Y. Li, J. Mansfield, and Z. Jin. Feature congestion: a measure of display clutter. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 761\u2013770. ACM, 2005. [23] R. Rosenholtz, Y. Li, and L. Nakano. Measuring visual clutter. Journal of vision, 7(2):17\u201317, 2007. [24] E. P. Simoncelli and W. T. Freeman. The steerable pyramid: A flexible architecture for multi-scale derivative computation. In icip, page 3444. IEEE, 1995. [25] R. van den Berg, F. W. Cornelissen, and J. B. Roerdink. A crowding model of visual clutter. Journal of Vision, 9(4):24\u201324, 2009. [26] C.-P. Yu, W.-Y. Hua, D. Samaras, and G. Zelinsky. Modeling clutter perception using parametric proto- object partitioning. In Advances in Neural Information Processing Systems, pages 118\u2013126, 2013. [27] C.-P. Yu, D. Samaras, and G. J. Zelinsky. Modeling visual clutter perception using proto-object segmenta- tion. Journal of vision, 14(7):4\u20134, 2014. 9 Supplementary Material PIFC maps Figure 8: PIFC maps across the images used for our analysis ranked from least (top) to highest (bottom) FFC clutter as shown in Fig.6. Notice how the clutter scores (heatmap values) in the PIFC increase as a function of eccentricity and is contingent on the amount of clutter in the ROI. 10 Beyond Foveated Feature Congestion We extended other clutter models to their respective peripheral versions. Since the other models: Edge Density, Subband Entropy and ProtoObject Segmentation have not been designed to produce an intermediate step with a dense clutter pixel-wise representation (unlike Feature Congestion 1), it is hard to find respective optimal dense clutter representations without losing the essence of each model. For Edge Density, we compute the magnitude of the image gradient after grayscale conversion. For Subband Entropy, we decided to keep all the respective subbands, as the model proposes as well as the coefficients that are used to compute a weighted sum over the entropies. In other words, our dense version of Subband Entropy is more of a dense \u201CSubband Energy\u201D term, since computing Entropy over a vector of a small N ×K vector space of N = 3 scales and K = 4 orientations produced very little room for variation. Finally dense ProtoObject Segmentation was computed by following the intuition of final number of superpixels over inital number of superpixels, but since this is not applicable at a pixel wise level, we decided to compute multiple ProtoObject Segmentations with different regularizer and superpixel radius parameters, and averaged all superpixel segmentation ratios \u2013 where every map was dense at a superpixel level, and each superpixel score was the initial number of pixels over the final number of initial number of pixels that belong to that superpixel after the meanshift merging stage in HSV color space. We believe that future work can be tailored towards improving dense versions of each clutter model, as well as creating new dense clutter models, that can easily be stacked with a peripheral architecture. Foveated Feature Congestion vs Hit Rate correlation Distance 4 deg 6 deg 8 deg 10 deg 12 deg L1-norm −0.80± 0.04 −0.82± 0.04 −0.81± 0.05 −0.79± 0.05 −0.76± 0.06 L2-norm −0.79± 0.05 −0.79± 0.06 −0.77± 0.06 −0.75± 0.07 −0.71± 0.07 KL-divergence −0.80± 0.04 −0.82± 0.04 −0.82± 0.04 −0.81± 0.05 −0.77± 0.06 Foveated Edge Density vs Hit Rate correlation Distance 4 deg 6 deg 8 deg 10 deg 12 deg L1-norm −0.76± 0.06 −0.73± 0.07 −0.69± 0.08 −0.65± 0.09 −0.59± 0.09 L2-norm −0.72± 0.07 −0.66± 0.08 −0.62± 0.09 −0.56± 0.10 −0.50± 0.11 KL-divergence −0.76± 0.06 −0.76± 0.06 −0.73± 0.07 −0.69± 0.08 −0.63± 0.08 Foveated Subband Entropy vs Hit Rate correlation Distance 4 deg 6 deg 8 deg 10 deg 12 deg L1-norm −0.75± 0.04 −0.77± 0.04 −0.77± 0.05 −0.76± 0.05 −0.73± 0.06 L2-norm −0.74± 0.05 −0.76± 0.05 −0.76± 0.05 −0.75± 0.06 −0.71± 0.06 KL-divergence −0.79± 0.04 −0.83± 0.04 −0.84± 0.04 −0.83± 0.04 −0.80± 0.05 Foveated ProtoObject Segmentation vs Hit Rate correlation Distance 4 deg 6 deg 8 deg 10 deg 12 deg L1-norm −0.70± 0.06 −0.74± 0.06 −0.74± 0.06 −0.72± 0.06 −0.66± 0.07 L2-norm −0.74± 0.04 −0.76± 0.05 −0.76± 0.05 −0.76± 0.06 −0.72± 0.06 KL-divergence −0.66± 0.06 −0.71± 0.05 −0.68± 0.06 −0.61± 0.07 −0.54± 0.08 Table 1: Foveated Clutter Models distance and ROI window length (deg) search. 11 1 Introduction 2 Previous Work 2.1 Clutter Models 2.2 Clutter Metrics 2.3 Clutter Evaluations 3 Methods & Experiments 3.1 Experiment 1: Forced Fixation Search 4 Foveated Feature Congestion 4.1 Creating a Peripheral Architecture 4.2 Creating a Foveated Feature Congestion Model 4.3 Foveated Feature Congestion Evaluation 5 Discussion 6 Conclusion ","flair":"three\tResearch"}
{"author":"antinucleon","created":"Tue Nov 22 01:15:11 EST 2016","text":"Today\u2019s deep learning models perform tens of thousands of operations on GPU. The input and output of each GPU kernel has to be stored in the global memory, but read and write on global memory is much slower than on on-chip register. When some special kernels executed in sequence share some data, performance and memory locality can be improved by fusing these kernels into a single, larger one, operating on on-chip register instead of global memory. For example, computing is usually with two separate kernels: However, if this pattern occurs often enough, the two kernels can be fused into one. The net result is reduction of reads\/writes to global memory as well as memory allocation for temporary variables: But, there is no straightforward answer to the question of \u201Cto fuse or not to fuse\u201D. The number of possible kernel combinations is exponential; preparing a pool of fused kernels statically is impractical and expensive. We opt to automatically generate fusion kernel code during runtime when the symbolic graph is available. Consequently, we also compile the generated kernel code during runtime. This feature is especially useful when users write customized operators, such as customized optimization function and updaters. It is worth mentioning this feature is available in Theano, which pioneers many ideas in the deep learning system. Our goal is to build it with a modular approach so that the optimization module can be reused across multiple deep learning frameworks. NNVM is a modular, decentralized and lightweight framework to help build deep learning libraries. It provides ways to construct, represent and transform generic computation graphs irrespective and independent of how it is executed. Its goal is to be a high level intermediate representation library for neural nets and computation graphs generation and optimization. More concretely, NNVM allows user to register operators and their attributes, leading to the possibility of having multiple reusable optimizations, like the following code: Rich information about the operators affords optimizations on the computation graph. In NNVM, a pass is a process that takes an existing graph with its current attributes, enriches it with more attributes, or transforms to yet another computationally equivalent graph. Notably, symbolic differentiation, memory planning, shape\/type inference are all implemented as passes. Following the abstract provided by NNVM, kernel fusion include three passes on the symbolic graph: \u201CFusion\u201D, \u201CCodeGen\u201D, \u201CRTCGen\u201D. In the fusion apart NNVM performs a traversal on the original graph, from outputs to inputs, by DFS(depth-first-search). When it detects some pattern can be fused, for instance if the current node and some children nodes are element-wise operations, it replaces them with one fused node. It also builds a mapping from the fusion node to the subgraph, in preparation for code generation. The fusion pass generates a new graph, with the fused nodes replacing the subgraphs they represent, as well as the mapping between them. In order to do code generation, we need to know how to generate its CUDA code for every node in the subgraph. For instance, for node, we need to generate \u201C+\u201D string for it. We define a series of basic AST(Abstract Syntax Tree) class for this purpose. Part of definitions are listed below: As shown above, naive ASTs can be composed to represent more complicated code structure. We register an attribute named for every element-wise operation which takes current node itself and the input ASTs and returns the output ASTs, in order to express the composition procedure between the inputs of this operation. As an example: After these preparation, we just need to take the graph after fusion, get the subgraphs by fusion nodes and the mirror mapping, then use the FCodeGen attribute of each node in the subgraph to generate the CUDA code we want. Add it as a graph attribute and register a new pass called \u201CCodeGen\u201D. The next task is easy: just compiles the generated CUDA code in the runtime. We choose to use NVRTC library, which accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded and linked with other modules by the CUDA Driver API, then we can call the compiled kernel by cuLaunchKernel. Then we register these three passes into NNVM (in three separate source files actually) : Now we\u2019ve create three passes for NNVM! After we get the symbolic graph, we just need to do on the original graph, then a new graph with some patterns fused and CUDA kernels generated. We use TinyFlow as a test case to demonstrate how this can be applied to a new deep learning framework. TinyFlow is a showcase to demonstrate how to use NNVM to build a clean, minimum and powerful computation graph-based deep learning system with same API as TensorFlow. The whole system is only 2K lines of code with CPU and GPU support. The original TinyFlow utilizes Torch for its operator backend. In this blog, we will explore how to add fusion and RTC features on TinyFlow, alongside with the Torch-backend Let us take an example, say you want to create a new activation operation. For the time being we assume it is a sigmoid: , we can create it by existing operation in TinyFlow like After we apply Fusion & CodeGen & RTCGen passes on the graph, we can use this operator with compiled kernel just like the native operators. The point is, user likes to write simple and intuitive operations. We call them imperative operations, which express the exact steps of computation. However, they can be slow. With fusion and RTC, we can retain the expressiveness while achieving the same level of efficiency of a big op. Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients, Adam also keeps an exponentially decaying average of past gradients, similar to momentum, and the update rule for variable with gradient is described as below: As you can see, there many element-wise operations in the update rules that can be fused as one big operator easily, and we expected that the performance will gain a boost by our optimization. we have done some benchmark tests of the training performance on LeNet and ResNet. We compared the training speed between CPU, GPU and GPU with NNVM-Fusion. It demonstrates that NNVM-Fusion can improve the GPU training performance by 1.4x-1.5x on LeNet and 1.1x-1.3x on ResNet with medium batch size. We also compared the training speed with the same model on TensorFlow. With NNVM-Fusion, TinyFlow's performance is on par with TensorFlow on ResNet, and better on LeNet. There still are lots of work to do in the future, like the AST class should be enriched to express more structure, and more types of fusion pattern can be explored, like combination of , and . Also, it\u2019s important to reduce the analysis overhead during detect fusible patterns. Can we design a caching mechanism to store the pattern or subgraph we have seen before? And so on. As we have said before, due to the fact that this optimization is built upon NNVM, it should be easily reusable on many platforms. In order to prove this, we will apply this module into MXNet as a plugin of NNVM next, and we also believe that we will discover more interesting ways to improve and extend this module during this procedure. The author has many thanks to Tianqi Chen, Mu Li, Minjie Wang and Prof. Zheng Zhang for their helpful advices on the implementation and documentation of NNVM-Fusion. Ziheng is an undergraduate student in Computer Science at Fudan NLP lab, this work is done when he works as a research intern at New York University Shanghai.","flair":"four\tProject"}
{"author":"mateja","created":"Tue Sep 27 20:12:27 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1608.06581 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1608 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Robert Adolf Saketh Rama Brandon Reagen Gu-Yeon Wei David M. Brooks Bookmark (what is this?) Computer Science > Learning Title: Fathom: Reference Workloads for Modern Deep Learning Methods Authors: Robert Adolf, Saketh Rama, Brandon Reagen, Gu-Yeon Wei, David Brooks (Submitted on 23 Aug 2016) Abstract: Deep learning has been popularized by its recent successes on challenging artificial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains. While their specificity does not diminish these approaches, there is a clear need for more flexible solutions. We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community. Consequently, we have assembled Fathom: a collection of eight archetypal deep learning workloads for study. Each of these models comes from a seminal work in the deep learning community, ranging from the familiar deep convolutional neural network of Krizhevsky et al., to the more exotic memory networks from Facebook's AI research group. Fathom has been released online, and this paper focuses on understanding the fundamental performance characteristics of each model. We use a set of application-level modeling tools built around the TensorFlow deep learning framework in order to analyze the behavior of the Fathom workloads. We present a breakdown of where time is spent, the similarities between the performance profiles of our models, an analysis of behavior in inference and training, and the effects of parallelism on scaling. Comments: Proceedings of the IEEE International Symposium on Workload Characterization, 2016 Subjects: Learning (cs.LG) DOI: 10.1109\/IISWC.2016.7581275 Cite as: arXiv:1608.06581 [cs.LG]   (or arXiv:1608.06581v1 [cs.LG] for this version) Submission history From: Robert Adolf [view email] [v1] Tue, 23 Aug 2016 17:11:07 GMT (688kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"nvrslnc","created":"Wed Nov 02 02:19:36 EDT 2016","text":"I am currently trying to work out a way to accurately classify documents into 3 different categories. The documents are rather lengthy, usually several thousands of words, unstructured and pretty much entirely full sentences. There are some keywords that increases the probability of the document belonging to one particular category, but not all of them are known.\n\nUntil now I have tried to clean the documents by getting rid of punctuation, common stop words and non-alphabetical strings. Since only a small part of the text is relevant, I was planning to try a tf-idf process to identify significant words within the documents.\n\nRight now I am coding this in python with a combination of scikit-learn and nltk. Someone suggested me to construct a NN since I will be doing this same type of classification a bit longer.\n\nDo you have some other suggestions what I could try to increase the accuracy and efficiency of this classification? Also if you have some good resources for these kind of projects, I would be thrilled :)","flair":"four\tProject"}
{"author":"iev6","created":"Mon Oct 10 01:17:09 EDT 2016","text":"Hello, Has anyone here tried using convolutional neural nets for speech signals? I need to check if CNNs work well for speaker identification, in comparison to the old fashioned feature extraction techniques. ","flair":"one\tDiscussion"}
{"author":"ShakespearePoop","created":"Wed Oct 19 03:53:55 EDT 2016","text":"Hi all,\n\nLooking to get into tensorflow by porting a [model](https:\/\/github.com\/harvardnlp\/im2markup\/). The gist of the model is a CNN followed by a bi-directional RNN which encodes input images with sizes that fall into 9 buckets\/bins, and an attention-based decoder which computes a context vector of the same size (regardless of input size) and runs for varying timesteps (one timestep per token in the output). I'm not sure how to go about organizing the data for efficient computation. \n\nFor the encoder, would I create a different model for each input size bucket and share variables? Is it easier to just pad all images to a uniform size? \n\nFor the decoder, padding would seem pretty inefficient as the mean output sequence length seems to be somewhere around 150-200, but a decent number of sequences go from several hundred all the way to ~1050.\n\n","flair":"four\tProject"}
{"author":"acanai","created":"Tue Nov 15 09:30:28 EST 2016","text":"Stochastic Variational Deep Kernel LearningNIPS 2016Paper: https:\/\/arxiv.org\/abs\/1611.00336Code: https:\/\/people.orie.cornell.edu\/andre...Authors: Andrew Gordon Wilson*, Zhiting Hu*, Ruslan Salakhutdinov, Eric P. XingThis work can be used as a plug-in to stand-alone deep networks, with minor additional runtime overhead, in exchange for improved predictive performance, interpretability, and full predictive distributions.SV-DKL exploits algebraic structure in deep kernels formed from (e.g. convolutional) deep neural networks in conjunction with stochastic variational inference. The resulting approach is an extremely scalable Gaussian process which can handle non-Gaussian likelihoods and stochastic mini-batch training, with very flexible kernel functions.We apply the method to many different classification problems, showing improvements in performance, but also steps towards more interpretable deep learning.Our poster is onMon Dec 5th 06:00 -- 09:30 PM @ Area 5+6+7+8 #100Longer talk about new directions for Gaussian processes and kernel methods: https:\/\/slideshot.epfl.ch\/play\/k5FuJc...\"Scalable Gaussian Processes for Scientific Discovery\".","flair":"three\tResearch"}
{"author":"vighneshbirodkar","created":"Tue Sep 27 19:03:37 EDT 2016","text":"Residual networks as shown here\nhttps:\/\/arxiv.org\/abs\/1512.03385\nare known to be easier to optimize and perform well. I was wondering if I could use them to build an image auto-encoder.\n\nDoes anyone know about a paper\/code which does the same ?","flair":"null\tnull"}
{"author":"zhongwenxu","created":"Wed Nov 02 22:37:02 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.00712 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables Authors: Chris J. Maddison, Andriy Mnih, Yee Whye Teh (Submitted on 2 Nov 2016 (v1), last revised 6 Nov 2016 (this version, v2)) Abstract: The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate effectiveness of concrete relaxations on density estimation and structured prediction tasks using neural networks. Subjects: Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1611.00712 [cs.LG]   (or arXiv:1611.00712v2 [cs.LG] for this version) Submission history From: Chris J. Maddison [view email] [v1] Wed, 2 Nov 2016 18:25:40 GMT (662kb,D) [v2] Sun, 6 Nov 2016 23:25:23 GMT (440kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"CireNeikual","created":"Mon Nov 21 14:14:43 EST 2016","text":" OgmaNeo Press Contact Landing page Ogma Intelligent Systems Corp Neuroscience based Machine Intelligence OgmaNeo Press Contact   OgmaNeo Press Contact OgmaNeo Overview and Video Prediction You are here: Home OgmaNeo OgmaNeo Overview and Video Prediction Here is an overview of the Feynman Machine architecture used in the OgmaNeo library, followed by an example for video prediction (recall). For the original Feynman Machine paper, see https:\/\/arxiv.org\/abs\/1609.03971. A High Level Look at the Feynman Machine The Feynman Machine is a hierarchical sequence prediction algorithm that functions on the basis of coupled dynamical systems. The Feynman Machine is implemented in the OgmaNeo C++ library, with bindings to Python and Java. Demos for the C++ version can be found here. What makes the Feynman Machine unique from other deep learning architectures is its focus on fully real-time spatio-temporal online learning, local computation (no backpropagation), and speed. Encoders and Decoders A Feynman Machine hierarchy can take many forms, but for now we will focus on a simple stack of 2D layers. Each layer consists of mainly two parts: An encoder and a decoder. The encoder attempts to model the underlying dynamical systems observed in the data. It produces a sparse code that represents the current spatio-temporal state. The decoder maps from the state of the encoder to either the state of the underlying dynamical system (input), or to the advanced state (next timestep) of the encoder. It does this by taking both the current layer encoder state and the next higher layer decoder state into account. This essentially forms a prediction of the next timestep, which can be used as the output of the model, or fed in to a lower layer to improve the predictions of that layer. Information flows in two directions: Up and down. We therefore separate processing into an up pass and a down pass. In the up pass (encoders pass), we attempt to model the inputs by extracting sparse spatio-temporal features. Each encoder extracts features of the encoder below it:   Where the red states are active units in the encoder (binary and sparse). In the down (decoding) pass, we then combine higher layer predictions with current layer state to predict the next state of the encoder (or the input of the encoder, depending on the setup). This allows us to avoid backpropagation of errors, since we know the targets of predictions at each layer are simply the next timestep (t + 1) of the state of the encoder. In most of our experiments, a simple linear combination will suffice for the decoder, trained with the perceptron delta rule. The encoder, however, is a bit trickier. We have experimented with many encoder types and architectures, and have included some of the best so far in the OgmaNeo release. For this article we will focus on the Chunk Encoder, a particularly general-purpose encoder with desirable properties. The Chunk Encoder The Chunk Encoder is essentially a grid of tiled (chunked) self-organizing maps (SOMs) with temporal modeling capabilities. This means that each chunk (tile) is self-contained, and produces a single active bit in its representation (the best matching unit in the SOM). Each SOM looks sort of like this: Where the green and grey units are the hidden units, and the blue units are the inputs. The network is fully connected within a single chunk, but sparsely connected (local radii) outside of a chunk. The different shades of green represent the influence the winning (red, best matching unit (BMU)) neuron has on its neighbors. When influenced, a neuron drives its synapses towards the currently observed pattern. The competitive-collaborative nature of the SOM ensures that the resulting representation is both sparse and does not contain \u201Cdead\u201D (unused) units. To represent the evolution of its inputs over time, each chunk has two mechanisms: A second recurrent input layer, and per-input traces. Given enough time and units, the recurrent connections are sufficient to produce a good spatiotemporal code of the input. However, it doesn\u2019t have \u201Cmulti-timestep credit assignment\u201D \u2013 it needs to propagate events back in time one step at a time. To address this problem, we augment the chunk with per-input memory traces. The per-input memory traces are simply running averages of the inputs at that time. Generally, different input sequences will usually produce unique running averages at each timestep, so these traces form a simple form of one-shot recurrent memory. These traces store a complete history in compressed form, and can be exploited without propagating credit back in time. Chunks are put together in 2D layers in the OgmaNeo library, however different dimensions are also possible. Encoder and Decoder Combination When combining encoders and decoders, we can use some additional trickery to improve performance of the system. One trick in particular relates to how information is passed upwards through the encoders. When a lower layer is able to predict the sequence it sees below it reliably, we don\u2019t really need to send any information upwards, since in the downwards pass the state of the current layer is enough to fully predict the input. On the other hand, if the layer is not able to fully predict its input, it makes sense to only propagate the errors (mistakes) the layer made to the next higher layer. This results in a form of predictive coding. Each encoder no longer receives the raw state of the encoder below it as input, but rather the difference in prediction and outcome. This means that when a prediction is correct, the encoder will receive only 0\u2019s. Otherwise, it will receive an error signal. Predictive coding allows us to not only reduce the amount of information flowing up the hierarchy, but also allows us to make sure that each layer only learns to predict what is necessary for an optimal prediction. Video Prediction Example In this portion of the article, we will go over the video prediction example available in the OgmaNeo demos repository. This is one of the simplest demos, requiring only that the hierarchy effectively recall the input sequence. Nevertheless, it has been a vital demo for the development of the algorithm. Future demos will be added that better showcase the generalization characteristics of the Feynman Machine, but for a simple tutorial the video prediction will suffice. To recall a sequence of video frames, we want to be able to predict each frame given the previous frame. This way, we can feed predictions back into the hierarchy as assumed correct input, allowing us to \u201Creplay\u201D what the hierarchy has seen. We will now go over the hierarchy creation code, in C++. The rest of the code is beyond the scope of this example, as it relies heavily on the SFML and OpenCV libraries to load and display videos. We assume OgmaNeo has been installed by following the instructions in the included README. \/\/ \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Create the Hierarchy \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 std::shared_ptr<ogmaneo::Resources> res = std::make_shared<ogmaneo::Resources>(); res->create(ogmaneo::ComputeSystem::_gpu); ogmaneo::Architect arch; arch.initialize(1234, res); Here we first acquire the resources necessary to create a Feynman Machine (OpenCL context and kernels), followed by the creation of an Architect. The Architect provides a simple interface to creating the most common forms of hierarchies. A lower-level interface is also available for finer control, but we will use the Architect interface for now. The Architect is initialized with the resources and a seed. We can then start adding layers using the Architect. Layers are 2D, and we want color video, so we will add 3 input layers for RGB components. \/\/ 3 input layers for RGB arch.addInputLayer(ogmaneo::Vec2i(width, height)) .setValue(\"in_p_alpha\", 0.02f) .setValue(\"in_p_radius\", 8); arch.addInputLayer(ogmaneo::Vec2i(width, height))) .setValue(\"in_p_alpha\", 0.02f) .setValue(\"in_p_radius\", 8); arch.addInputLayer(ogmaneo::Vec2i(width, height))) .setValue(\"in_p_alpha\", 0.02f) .setValue(\"in_p_radius\", 8); Each layer adding call returns a ParameterModifier that allows one to modify properties of that layer. Here we set two properties per layer, the input prediction alpha (learning rate) and the input prediction radius (receptive field size). For a full list of parameters, please refer to the README included in OgmaNeo. We can then add higher layers (encoder-decoder pairs) like so: for (int l = 0; l < 4; l++) arch.addHigherLayer(ogmaneo::Vec2i(60, 60), ogmaneo::_chunk) .setValue(\"sfc_chunkSize\", ogmaneo::Vec2i(6, 6)) .setValue(\"sfc_ff_radius\", 8) .setValue(\"hl_poolSteps\", 2) .setValue(\"sfc_numSamples\", 2) .setValue(\"sfc_weightAlpha\", 0.01f) .setValue(\"sfc_biasAlpha\", 0.1f) .setValue(\"sfc_gamma\", 0.92f) .setValue(\"p_alpha\", 0.04f) .setValue(\"p_beta\", 0.08f) .setValue(\"p_radius\", 8); Most of these parameters can be left at their defaults, but we set them here for clarity. In the above, we created 4 layers using chunk encoders. Each layer is 60×60 units, and each chunk is 6×6, meaning there are 10×10 chunks. We then generate the hierarchy: \/\/ Generate the hierarchy std::shared_ptr<ogmaneo::Hierarchy> h = arch.generateHierarchy(); \/\/ Input and prediction fields for color components ValueField2D inputFieldR(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); ValueField2D inputFieldG(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); ValueField2D inputFieldB(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); ValueField2D predFieldR(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); ValueField2D predFieldG(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); ValueField2D predFieldB(ogmaneo::Vec2i(rescaleRT.getSize().x, rescaleRT.getSize().y), 0.0f); We also created several value fields. These are used as temporary buffers to supply the hierarchy with input and to obtain predictions. Assuming we can obtain the video frames (see OgmaNeoDemos for how we did it using SFML and OpenCV), we can then train on the video in an online fashion: std::vector<ogmaneo::ValueField2D> inputVector = { inputFieldR, inputFieldG, inputFieldB }; h->simStep(inputVector, true); predFieldR = h->getPredictions()[0]; predFieldG = h->getPredictions()[1]; predFieldB = h->getPredictions()[2]; Here, we provided the RGB inputs, and then obtained the RGB next-timestep predictions. Finally, to recall the video, we simply need to feed the predictions back in to the hierarchy as inputs: std::vector<ogmaneo::ValueField2D> inputVector = { predFieldR, predFieldG, predFieldB }; h->simStep(inputVector, false); predFieldR = h->getPredictions()[0]; predFieldG = h->getPredictions()[1]; predFieldB = h->getPredictions()[2]; It is important to note that during training, frames were presented sequentially. Most deep learning systems are based on backpropagation and therefore require i.i.d. samples. However, due to the online learning nature of our system, we can learn off of inputs sequentially without any sort of history buffer. Below is a video of the system recalling various video sequences: For the full code, please visit the OgmaNeoDemos repository linked at the beginning of this article. Conclusion We hope that this new architecture will find uses in various fields. It is not intended to replace current deep learning systems on tasks where they excel, but rather to operate on a different problem field. We tackle online learning problems where backpropagation-based methods are either too slow or are not flexible enough. Our system typically takes in the order of minutes to train and can continue training while receive data. We encourage users to experiment and hopefully contribute to the development of Feynman Machines, as we believe that this problem domain has a lot of potential use. We will continue developing the Feynman Machine theory and the OgmaNeo software to support users. Share this: Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Related About the author Eric Laukien Related posts Introducing OgmaNeo \u2013 Machine Learning based on Neuroscience November 21, 2016 Leave Comment Cancel reply Your email address will not be published. Required fields are marked * Comment Name * Email * Website clear formSubmit Notify me of follow-up comments by email. Notify me of new posts by email. Recent Posts OgmaNeo Overview and Video Prediction Introducing OgmaNeo \u2013 Machine Learning based on Neuroscience Recent Comments Archives November 2016 Categories OgmaNeo Press Meta Log in Entries RSS Comments RSS WordPress.org Copyright © 2016 Ogma Intelligent Systems Corp. All Rights Reserved. ","flair":"three\tResearch"}
{"author":"NichG","created":"Sat Nov 12 22:49:36 EST 2016","text":"This post summarizes a bunch of connected tricks and methods I explored with the help of my co-authors. Following the previous post, above the stability properties of GANs, the overall aim was to improve our ability to train generative models stably and accurately, but we went through a lot of variations and experiments with different methods on the way. I\u2019ll try to explain why I think these things worked, but we\u2019re still exploring it ourselves as well. The basic problem is that generative neural network models seem to either be stable but fail to properly capture higher-order correlations in the data distribution (which manifests as blurriness in the image domain), or they are very unstable to train due to having to learn both the distribution and the loss function at the same time, leading to issues like non-stationarity and positive feedbacks. The way GANs capture higher order correlations is to say \u2018if there\u2019s any distinguishable statistic from real examples, the discriminator will exploit that\u2019. That is, they try to make things individually indistinguishable from real examples, rather than in the aggregate. The cost of that is the instability arising from not having a joint loss function \u2013 the discriminator can make a move that disproportionately harms the generator, and vice versa. Other methods are stable, but have difficulty in data spaces dominated by higher-order correlations. Variational autoencoders are an example of this. By imposing a certain distribution on the latent space, they provide a principled way to generate new samples from the data distribution which occur at the correct relative probabilities. VAEs tend to suffer blurriness in the output, however, especially when the entropy is high. This is because the user must specify a similarity metric in the data space by hand. That is to say, when training a variational autoencoder, one must say \u2018this image is closer to the target than that one\u2019, and that definition of closeness is usually taken to be per-pixel mean-squared-error (MSE). The problem with this is that it implies that the distribution of images should be modeled by a set of independent gaussian distributions for each pixel, whereas perceptually it\u2019s the higher-order correlation between pixels that is more important to a human observer. For example, an image of a circle can be shifted by one pixel to the left and still appear as the same circle, but in terms of per-pixel MSE, this corresponds to a huge fluctuation in the pixels at the edge. Because of this, methods using functional or perceptual information in place of MSE can be used to improve the quality of images generated by a non-adversarial setup (paper). Alex Champandard (@alexjc) has made a number of investigations along these lines as well. We poked around with these methods, but when used on any long-term or detail-sensitive task, it seemed like the GANs would work well for a bit and then crash or become unstable, while the variational autoencoder would train in a stable manner but wouldn\u2019t converge to the level of accuracy in the generative model to allow us to use the output for followup purposes. We started thinking about other methods outside of modern neural networks and how they worked, and that investigation, combined with a recent idea, led us to something that seemed to work for our purposes. One of the ingredients is described in a recent paper on Monte-Carlo sampling on autoencoders (code). This paper discusses the way in which the actual distribution of latent space encodings for auto-encoders and variational auto-encoders tends to have gaps and other structure, associated with features of the generated images that do not actually vary smoothly (for example, glasses versus no-glasses). The idea in this paper was that one could effectively perform a Monte Carlo sampling procedure over the latent space just by repeatedly decoding and encoding a given example. The result would be that the generated example would tend to slide towards areas of higher probability density, and thus would give more self-consistent and realistic results. In that paper, the autoencoder is trained in the standard way (as a single encoding and decoding pass), but resampling is used to improve the quality of the generated output. The second ingredient has to do with energy-based methods. In energy-based generative models, there is some network of links between features in the data which describe for each pair what their average correlation over the dataset is (this can also be modelled conditionally, hidden variables can be used, etc). To generate a sample, one starts with some arbitrary pattern and then performs a Monte-Carlo sampling process to try to relax the mismatch between the expected correlations and the ones in the pattern. It turns out there\u2019s a relationship between these kinds of model and neural networks, going all the way back to Hopfield networks. In a Hopfield network, a process akin to Hebbian learning is used to update the model, with the result that the correlation structure ends up being learned. Furthermore, rather than doing Monte-Carlo sampling to minimize the energy, it turns out you can just repeatedly run a pattern through the network and it will tend to converge on the equivalent result. The Hopfield network is basically just a single layer with a threshold nonlinearity, so there\u2019s a limit to what kinds of generalizations it will learn. To extend past that, Restricted Boltzmann Machines operate under a similar idea but now introduce a separation between a set of hidden variables (the same as the latent space of an autoencoder) and the variables describing the data. Gibbs sampling is used to converge on the equilibrium distribution rather than iterating the state through the network, though. An extra modification is added to make the learning stable \u2013 the network is adjusted to move towards correlations in the data, but to move away from what the network generates spontaneously. This prevents the hidden variables from developing their own strong, but meaningless correlations. The pattern we want to make note of here is that there seems to be a very general equivalency between Monte Carlo sampling and recurrently cycling data through a network. This was used in another paper (Conditional Random Fields as Recurrent Neural Networks) to make a neural network version of an algorithm used to learn Conditional Random Fields (yet another kind of energy-based model). Specifically, by replacing the sampling procedure with looping through a neural network, the entire thing can be attached to a bigger neural network pipeline and trained end-to-end with gradient descent. In that sense, an energy-based model ends up being just another kind of layer. The nice thing about energy-based models is they have a single, shared loss function in the form of the energy function, which makes them tend to be very stable to train. In a GAN, the mismatch between generator and discriminator can create oscillatory, chaotic, or crashing behaviors, but in an energy-based model that mismatch is guaranteed to be exactly zero. The problem is that they usually involve Monte Carlo sampling, which at best only approximately differentiable (using something like the re-parameterization trick of VAEs), could take a long time to converge depending on the type of step used, etc. So this equivalency between recurrent networks and Monte Carlo sampling is really cool \u2013 it means that we can basically get a differentiable, energy-like model for free, maybe gaining its nice stability properties without the hard work. So why not try that with the Monte Carlo sampling on autoencoders paper, but now instead of just doing a sampling procedure, we actually make the entire auto-encoder into a recurrent network and train not on the single-pass decoding accuracy, but on the N-pass decoding accuracy? The idea is, take the data as input, encode to the latent space, add some noise, decode, encode, add noise, \u2026 The loss function is then the mean-squared-error between the input and every subsequent decoded output (1-pass, 2-pass, etc). In addition, rather than do a full-on variational autoencoder, I can just add a penalty term proportional to the norm of the encoding vector. In this case, it seems we can even get away with not having the variational element but still obtain a generative model because of the repeated re-sampling. Normally, we wouldn\u2019t be able to say how to generate random latent variable values to obtain the correct probability distribution in the output as to match the data distribution. However, with the repeated re-sampling and addition of noise, any point we start at in the latent space eventually will converge to the equilibrium distribution over the latent space as defined by the network itself. So we just need to pick a point and iterate long enough, and we\u2019ll get something self-consistent. This is a nice hack in terms of the simplicity of the approach, since explaining and understanding variational autoencoders tends to be\u2026 challenging. We gave it a try, and it seems to work. One the one hand, it doesn\u2019t completely solve the blur problem like GANs do. But on the other hand, it seems to learn correlations in the data space better than variational autoencoders. For a test case, we made a simple set of 3D data where the first two dimensions were sampled from IID unit-norm Gaussian distributions, but the third dimension was equal to . The network architecture in each case uses a 6D latent space and has one extra 128-neuron hidden layer before and after the encoding layer (in other words, 3->128->6->128->3). No nonlinearity is applied to the encoding or output layer, though a weak (0.03) penalty is applied to keep the code vectors from becoming arbitrarily large. The networks were trained on 10000 samples, for 400 epochs. A pure autoencoder encodes these samples quite accurately, but if we pick random points in the latent space they seem to have nothing to do with the distribution at all. On the other hand, the reconstruction error isn\u2019t great with the variational autoencoder and while it has some sense of the distribution, its still not quite right. There\u2019s too much weight centered at the vertex, and furthermore the distribution seems to be biased in (probably being pulled towards the mean value of ). A few rounds of Monte-Carlo resampling of the plain autoencoder does seem to make it behave much better as a generative model, but it still isn\u2019t converging to the actual distribution. On the other hand, the recurrent autoencoder ends up learning a very sharp model of the correct distribution. The only differences between the Recurrent Auto-encoder and the Monte Carlo resampled autoencoder is that gradients were propagated across the resampling procedure in the RAE during training and that noise was added to the latent space each time, but it makes a huge difference. We also tested on an artificial multi-modal 2D data set, to see if we could generate complex and highly non-Gaussian output distributions. Again, the recurrent auto-encoder seemed to be able to learn to generate points belonging to each of the clusters, without just collapsing to the mean. We\u2019ve only done some minor tests with image data at this point, but the results were less impressive and still seemed to be blurry. The data set was quite high-entropy though, so a more thorough test using a faces dataset is probably worth doing in the future. Okay, great, so what? Well, it might help to explain why all the fuss about generative models. A few weeks ago at a workshop on Karl Friston\u2019s Free Energy ideas in neuroscience. If you aren\u2019t familiar with them, basically the idea boils down to a hypothesis that \u2018brains arrange themselves and take actions to try to minimize their surprise\u2019. However, this is expressed in terms of the joint probability distribution between all the various internal parameters of the brain, sensory states, even actions taken. This means you can write down some potentially interesting objective functions to optimize. For instance, you can ask for things like \u2018take actions that minimize the uncertainty of my future predictions of my sensory state\u2019 as easily as you can ask for things like \u2018take actions to end up near the goal\u2019 \u2013 at least, you can write it down as easily. The problem is, working with the full joint distribution is tough. The way people go about it in neuroscience makes it even tougher \u2013 they generally start with an explicit, parameterized model of reality and then do formally correct differential Bayesian inference on everything. This, if I may say so, causes a wealth of horrible implementation difficulties, practical problems, etc: you need to already have an explicit model, you need to keep track of a lot of high-dimensional distributions, or you have to make local Gaussian approximations which at best leave you needing to explicitly calculate Hessians between parameters. This means people generally use these methods on problems with a very, very small number of parameters and a very low dimensional input\/output space compared to machine learning. One thing that came out of the workshop was the idea that most of those technical difficulties can be answered in large part by just skipping the formal Bayesian inference procedure and instead heuristically estimating that with a generative neural network to learn the joint distribution over sensory states and action variables and so on, since the energy function can be directly computed from that joint distribution. This is something I\u2019ve been working on with Yen Yu as a way to make Free Energy-based methods scale to much larger systems. But to do this, we needed a good, solid base on the machine learning side to build on as well \u2013 that\u2019s going to be our recurrent auto-encoder. If you aren\u2019t into the neuroscience side of it, why care about this Friston Free Energy stuff? Well, being able to use things like your model\u2019s uncertainties in differentiable loss functions and objectives lets you do some cool stuff. As an example, I\u2019m going to talk about the problem of novelty search. One of the fundamentally troubling things about Friston\u2019s surprisal minimization idea is that surprise-minimizing actions could consist of turning off all the lights and shutting down \u2013 if you blind yourself, you won\u2019t see such surprising things. The discussion over that (the so-called Dark Room Problem) is pretty involved, and there are a number of ad hoc techniques for addressing it, but it was a bit unsatisfying. Following the workshop, we had an email exchange about this issue, and Lana Sinapayen proposed an \u2018Adversarial Free Energy\u2019 which would see the cognitive model trying to minimize its surprisal in terms of the parameters of its predictive processes, but maximize its surprisal in terms of the actions it took. The result would be a sort of proactive explorer \u2013 an agent that would try to prove itself wrong at every junction, but would still try to learn the most accurate model of the world that it could despite that. Just as its easy to write down ways to minimize Friston Free Energy given a nicely differentiable, generative model, it\u2019s possible to do the same for Sinapayan\u2019s Adversarial Free Energy. Putting aside issues of action selection and the response of an environment (which we\u2019ll write about shortly), we can also use this as a form of combinatoric novelty search on a static data set. That is to say, we learn a way to efficiently encode the data, and then intentionally go looking for examples that are hard, but not impossible, to code using our learned scheme. The result is a sort of local search in the space of descriptions \u2013 a network that has learned to code sine waves efficiently won\u2019t jump immediately to noise patterns because it doesn\u2019t know how to code for them, but instead finds new things which are hard but possible to code for given what the network has already seen. It won\u2019t expand randomly, but instead will stretch the limits of what the network currently understands. Explicitly, the algorithm is as follows: 1) Start with data set 2) Learn autoencoder weights to minimize , where is the input data, is the decoding after the nth pass, and is the latent representation in the nth pass. 3) Pick a set of random points in the latent space, corresponding to some small percentage of the data set size (say, 10%) 4) Compute the gradient where is the decoding of and is the encoding of back in the latent space. Normalize\/scale\/etc the gradient, and add to 5) Repeat step 4 until convergence\/as many times as you want. 6) Replace some of the original data with , and repeat from step 2 Last Friday, Nathaniel Virgo and I sat down and gave this a shot, starting with a data set composed of sine waves and square waves belonging to a narrow band of frequencies. The network is a bit bigger than the one used in the simple parabolic example (a few more layers, 384 hidden layer neurons, 64 dimensional coding space), but the idea is the same. We first trained the autoencoder a bit to get the initial structure of the coding space solidified, then alternated 50 epochs of training with a replacement of 2.5% of the data with novel samples generated according to the above gradient descent procedure. At first, the results aren\u2019t very interesting \u2013 the autoencoder is struggling to represent the square waves accurately, etc. Maybe the novel samples are slightly different amplitude, but generally have the same frequencies, shapes, and phase angles as the source data. Then, however, you slowly start to see the introduction of new features. At some point, the process discovers how creates a higher-frequency sinusoid. At some point it discovers shapes that combine a slow square wave and a fast sine wave, things with one frequency over half of the waveform and another over the other half, etc. In each case, the discovered forms have some relationship to what the network has learned to code for previously \u2013 jumping straight to raw noise doesn\u2019t seem to happen, because after all it\u2019s very hard for the network to accurately encode and decode the space of noise patterns. So the network finds some balance between novelty and structure. Watching the encoding space (or at least, a 2D projection of it) during training gives an idea of what this kind of exploration looks like. The encoding space of the recurrent auto-encoder has a sort of fixed-point structure, with a number of points acting as attractors for nearby codes. The gradient descent pushes out to the edges of those basins of attraction, but can\u2019t actually push the network very far outside of the data distribution. Those edges get explored, data is added, and the next training pass pulls those higher-density regions inwards to minimize the average norm of the latent code words over the data set. However, the network still has to code for these new patterns accurately, or it will lose out on the MSE part of the loss term, and so these new branches are preserved and explored further during the next novelty pass. In this case, there isn\u2019t really an external reference to ground this novelty aside from the network architecture itself \u2013 that is to say, the network is just learning what \u2018easy to code\u2019 and \u2018hard to code\u2019 mean given the context of how its own mind works. But in general, this kind of novelty search can just as well be tied to processes that pass through some kind of black box environment, leading to the discovery of action patterns that move the agent into a rarely or unexplored region of the problem space. There\u2019s a direct map between this kind of technique (seeking rare points in the latent space) and a technique called umbrella sampling, used for example in computational chemistry to sample rare chemical transitions in molecular dynamics simulations. In both cases, given some kind of representation on which relative probabilities of states can be evaluated, biasing generation towards lower probability states gives a way to efficiently seek out hard-to-find (but still feasible and realistic) configurations and transitions. The sourcecode for these experiments is available here.","flair":"three\tResearch"}
{"author":"FR_STARMER","created":"Sat Nov 26 12:31:18 EST 2016","text":"This makes compiling and running models take 5 minutes each, which means I spend all day tuning hyperparameters and trying new things out because most the time I'm waiting for the damn thing to link up to my GPU or to initialize the Theano backend \/ compile LSTM layers, yada yada. Why.","flair":"one\tDiscussion"}
{"author":"Agagla","created":"Mon Oct 10 07:33:27 EDT 2016","text":" Skip navigation Sign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose Dictionary Learning for Massive Matrix Factorization - RecsysFR RecsysFR SubscribeSubscribedUnsubscribe1919 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 223 views 3 Like this video? Sign in to make your opinion count. Sign in 4 0 Don't like this video? Sign in to make your opinion count. Sign in 1 Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Oct 7, 2016 Talk given by Arthur Mensch, Inria Parietal during the RecsysFR meetup on October 6th, 2016. Slides are on : http:\/\/www.slideshare.net\/recsysfr\/di... Category Science & Technology License Standard YouTube License Created using YouTube Video Editor Source videos View attributions Show more Show less Loading... Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Recommendation @Deezer - Benoit Mathieu - RecsysFR - Duration: 10:32. RecsysFR 84 views 10:32 What can bring library metadata to the web? Trust, links and love - Duration: 23:43. RecsysFR 54 views 23:43 RecsysFR: Criteo introduction word - Duration: 7:07. RecsysFR 36 views 7:07 Netflix Prize Winner, Principled Regularization for Matrix Factorization - Duration: 29:21. SAHD Workshop 75 views 29:21 Meta-Prod2Vec: Simple Product Embeddings with Side-Information - RecsysFR - Duration: 18:33. RecsysFR 45 views 18:33 Local Low-Rank Matrix Approximation - Duration: 1:05:07. Microsoft Research 140 views 1:05:07 Digital image processing: p067- Dictionary Learning - Duration: 17:14. Alireza Saberi 9,833 views 17:14 NIPS 2015 Workshop (Arora) 15601 Non-convex Optimization for Machine Learning: Theory and Practice - Duration: 27:38. NIPS 222 views 27:38 16 5 Vectorization Low Rank Matrix Factorization 8 min - Duration: 8:29. Rafael Merino GarcÃ­a 260 views 8:29 Sparse and large-scale learning with heterogeneous data - Duration: 54:58. GoogleTechTalks 2,733 views 54:58 Flexible Recommender System Based On Graphs - Kernix - RecsysFR - Duration: 15:49. RecsysFR 46 views 15:49 NIPS 2011 Sparse Representation & Low-rank Approximation Workshop: Automatic Relevance ... - Duration: 8:37. GoogleTechTalks 511 views 8:37 Understanding non-convex optimization for sparse coding - Duration: 1:25:52. Microsoft Research 143 views 1:25:52 Christian Thurau - Low-rank matrix approximations in Python - Duration: 35:38. PyData 1,330 views 35:38 Learning human motion primitives with multimodal non-negative matrix factorization - Duration: 13:57. InriaFlowers 1,379 views 13:57 The K-SVD algorithm explained visually - Duration: 1:17:01. CS Simplified 3,033 views 1:17:01 IEEE PATTERN ANALYSIS AND MACHINE INTELLIGENCE - FINAL YEAR IEEE COMPUTER SCIENCE PROJECTS - Duration: 8:39. Tsys Globalsolutions 36 views 8:39 Supervised Dictionary Learning for Action Localization - Duration: 1:30. Vijay Kumar B G 280 views 1:30 Neural networks [8.5] : Sparse coding - dictionary learning algorithm - Duration: 5:31. Hugo Larochelle 4,344 views 5:31 Simple, Efficient and Neural Algorithms for Sparse Coding - Duration: 24:45. Simons Institute 3,149 views 24:45 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"null\tnull"}
{"author":"hackpert","created":"Fri Sep 30 13:15:52 EDT 2016","text":" Google Research Blog The latest news from Research at Google Introducing the Open Images Dataset Friday, September 30, 2016 Posted by Ivan Krasin and Tom Duerig, Software Engineers In the last few years, advances in machine learning have enabled Computer Vision to progress rapidly, allowing for systems that can automatically caption images to apps that can create natural language replies in response to shared photos. Much of this progress can be attributed to publicly available image datasets, such as ImageNet and COCO for supervised learning, and YFCC100M for unsupervised learning. Today, we introduce Open Images, a dataset consisting of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories. We tried to make the dataset as practical as possible: the labels cover more real-life entities than the 1000 ImageNet classes, there are enough images to train a deep neural network from scratch and the images are listed as having a Creative Commons Attribution license*. The image-level annotations have been populated automatically with a vision model similar to Google Cloud Vision API. For the validation set, we had human raters verify these automated labels to find and remove false positives. On average, each image has about 8 labels assigned. Here are some examples: Annotated images form the Open Images dataset. Left: Ghost Arches by Kevin Krejci. Right: Some Silverware by J B. Both images used under CC BY 2.0 license We have trained an Inception v3 model based on Open Images annotations alone, and the model is good enough to be used for fine-tuning applications as well as for other things, like DeepDream or artistic style transfer which require a well developed hierarchy of filters. We hope to improve the quality of the annotations in Open Images the coming months, and therefore the quality of models which can be trained. The dataset is a product of a collaboration between Google, CMU and Cornell universities, and there are a number of research papers built on top of the Open Images dataset in the works. It is our hope that datasets like Open Images and the recently released YouTube-8M will be useful tools for the machine learning community. * While we tried to identify images that are licensed under a Creative Commons Attribution license, we make no representations or warranties regarding the license status of each image and you should verify the license for each image yourself.↩ Google Labels: Computer Vision , datasets , Machine Learning    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"gwern","created":"Tue Nov 08 15:54:05 EST 2016","text":" Under review as a conference paper at ICLR 2017 LEARNING CURVE PREDICTION WITH BAYESIAN NEU- RAL NETWORKS Aaron Klein, Stefan Falkner, Jost Tobias Springenberg & Frank Hutter Department of Computer Science University of Freiburg {kleinaa,sfalkner,springj,fh}@cs.uni-freiburg.de ABSTRACT Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. Exploiting the same information in automatic Bayesian hyperparameter optimiza- tion requires a probabilistic model of learning curves across hyperparameter set- tings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer. 1 INTRODUCTION Deep learning has celebrated many successes, but its performance relies crucially on good hyper- parameter settings. Bayesian optimization (e.g, Brochu et al. (2010); Snoek et al. (2012); Shahriari et al. (2016)) is a powerful method for optimizing the hyperparameters of such deep neural networks (DNNs). However, its traditional treatment of DNN performance as a black box poses fundamental limitations for large and computationally expensive data sets, for which training a single model can take weeks. Human experts go beyond this blackbox notion in their manual tuning and exploit cheaper signals about which hyperparameter settings work well: they estimate overall performance based on runs using subsets of the data and based on initial short runs to weed out bad parameter settings; armed with these tricks, human experts can often outperform Bayesian optimization. Recent extensions of Bayesian optimization and mulit-armed bandits therefore also drop the limiting blackbox assumption and exploit the performance of short runs Swersky et al. (2014a); Domhan et al. (2015); Li et al. (2016), performance on small subsets of the data Klein et al. (2016), and performance on other, related data sets Swersky et al. (2013); Feurer et al. (2015). While traditional solutions for scalable Bayesian optimization include approximate Gaussian process models (e.g., Hutter et al.; Swersky et al. (2014a)) and random forests Hutter et al. (2011), a recent trend is to exploit the flexible model class of neural networks for this purpose Snoek et al. (2015); Springenberg et al. (2016). In this paper, we study this model class for the prediction of learning curves. Our contributions in this paper are: 1. We study how well Bayesian neural networks can fit learning curves for various architectures and hyperparameter settings, and how reliable their uncertainty estimates are. 2. Building on the parametric learning curve models of Domhan et al. (2015), we develop a specialized neural network architecture with a learning curve layer that improves learning curve predictions. 3. We compare two different stochastic gradient based Markov Chain Monte Carlo (MCMC) methods \u2013 stochastic gradient Langevin dynamics (SGLD (Welling and Teh, 2011)) and stochastic gradient Hamiltonian MCMC (SGHMC (Chen et al., 2014)) \u2013 for standard Bayesian neural networks and our specialized architecture and show that SGHMC yields better uncertainty estimates. 4. We evaluate predictive quality for both completely new learning curves and for extrapolating partially-observed curves, showing better performance than the parametric function approach 1 Under review as a conference paper at ICLR 2017 by Domhan et al. (2015) if learning curves at stages were learning curves have not fully converged. Figure 1: Example learning curves of random hyperparameter configurations of 4 different iterative machine learning methods: convolutional neural network, fully connected neural network, variational auto-encoder, and logistic regression. Although different configurations can lead to different learning curves, they usually share some characteristics for a certain algorithm and dataset, but vary across methods. 2 PROBABILISTIC PREDICTION OF LEARNING CURVE In this Section, we describe a general framework to model learning curves of iterative machine learning methods. We first describe the approach by Domhan et al. (2015) which we will dub LC-Extrapolation from here on. Afterwards, we discuss a more general joint model between time steps and hyperparameter values that can exploit similarities between hyperparameter configurations and predict for unobserved learning curves. We also give some insights about the observation noise of the evaluation of hyperparameter configuration and how we can adapt our model to capture this noise. 2.1 LEARNING CURVE PREDICTION WITH BASIS FUNCTION An intuitive model for learning curves proposed by Domhan et al. (2015) uses a set of k different parametric functions φi(θi, t) ∈ {φ1(θ1, t), ...φk(θk, t)} to extrapolate learning curves (y1, . . . , yn) from the first n time steps. Each parametric function φi depends on a time step t ∈ [1, T ] and on a parameter vector θi. The individual functions are combined into a single model by a weighted linear combination f̂(t,Θ,w) = k∑ i=1 wiφi(t,θi) , (1) where Θ = (θ1, . . . ,θk) denotes the combined vector of all parameters θ1, . . . ,θk, and w = (w1, . . . , wk) is the concateneted vector of the respective weights of each function. Assuming observational noise around the true but unknown value f(t|θ,w), i.e. yt ∼ N (f(t|θ,w), σ2), 2 Under review as a conference paper at ICLR 2017 Domhan et al. (2015) define a prior for all parameters P (Θ,w, σ2) and use MCMC to obtain S samples, (Θ1,w1, σ21), . . . , (ΘS ,wS , σ 2 S) from the posterior P (Θ,w, σ2) ∝ P (y1, . . . , yn|Θ,w, σ2)P (Θ,w, σ2) (2) using the likelihood P (y1, . . . , yn|Θ,w, σ2) = n∏ t=1 N (yt; f̂(t,Θ,w), σ2) . (3) These samples then yield probabilistic extrapolations of the learning curve for future time steps m, with mean and variance predictions ŷm = E[ym|y1, . . . yn] ≈ 1 S S∑ s=1 f̂(m,Θs,ws) , and var(ŷm) ≈ 1 S S∑ s=1 (f̂(m,Θs,ws)− ŷm)2 + S∑ s=1 σ2s . (4) For our experiments, we use the original implementation by Domhan et al. (2015) with one mod- ification: the original code included a term in the likelihood that enforced the prediction at t = T to be strictly smaller than the last value of that particular curve. This biases the estimation to never overestimated the error at the asymptote. We found that in some of our benchmarks, this led to instabilities, especially with very noisy learning curves. Removing it cured that problem, and we did not observe any performance degradation on any of the other benchmarks. The ability to include arbitrary parametric functions make this model very flexible, and Domhan et al. (2015) used it successfully to terminate evaluations of poorly-performing hyperparameters early for various different architectures of neural networks (thereby speeding up Bayesian optimization by a factor of two). However, the model\u2019s major disadvantage is that it does not use previously evaluated hyperparameters at all and therefore can only make useful predictions after observing a substantial initial fraction of the learning curve. 2.2 LEARNING CURVE PREDICTION WITH BAYESIAN NEURAL NETWORKS In practice, similar hyperparameter configurations often lead to similar learning curves, and modelling this dependence would allow predicting learning curves for new configurations without the need to observe their initial performance. Swersky et al. (2014a) followed this approach based on an approximate Gaussian process model. Their Freeze-Thaw method showed promising results for finding good hyperparameters of iterative machine learning algorithms using learning curve prediction to allocate most resources for well-performing configurations during the optimization. The method introduces a special covariance function corresponding to exponentially decaying functions to model the learning curves. This results in a analytically tractable model, but using different function accounting for cases where the learning curves do not converge exponentially is not trivial. Here, we formulate the problem using Bayesian neural networks. We aim to model the validation loss f(x, t) of a configuration x ∈ X ⊂ Rd at time step t ∈ [1, T ] based on noisy observations y(x, t) ∼ N (f(x, t), σ2). For each configuration x trained for Tx time steps, we obtain Tx data points for our model; denoting the combined data by D = {(x1, t1, y11)), (x1, t2, y12), . . . , (xn, tT , ynT )} we can then write the joint probability of the data and the model parameters as P (D,W ) = P (W )P (σ2) |D|∏ i=1 N (yi|f̂(xi, ti,W ), σ2) . (5) It is intractable to compute the posterior weight distribution p(W |D), but we can use MCMC to sample it, in particular stochastic gradient MCMC methods, such as SGLD (Welling and Teh, 2011) or SGHMC (Chen et al., 2014). Given M samples W 1, . . . ,WM , we can then obtain the mean and 3 Under review as a conference paper at ICLR 2017 variance of the predictive distribution as µ(ŷ(x, t)|D) = 1 M M∑ i=1 ŷ(x, t;W i) , and σ2(ŷ(x, t)|D) = 1 M M∑ i=1 ( ŷ(x, t;W i)− µ(ŷ(x, t)|D) )2 (6) , respectively. This is similar to Eqs. 4 and exactly the model that Springenberg et al. (2016) used for (blackbox) Bayesian optimization with Bayesian neural networks; the only difference is in the input to the model: here, there is a data point for every time step of the curve, whereas Springenberg et al. (2016) only used a single data point per curve (for its final time step). 2.3 HETEROSCEDASTIC NOISE OF HYPERPARAMETER CONFIGURATION In the model described above we assume homoscedastic noise across hyperparameter configurations. To evaluate how realistic this assumption is we sampled 40 configurations of a fully connected network (see Section 3.1 for a more detailed description how the data was generated and Table 2 for the list of hyperparameters) and evaluated each configuration R = 10 times with different pseudorandom number seeds. Figure 4 (left) shows on the vertical axis the noise σ2(x, t) = 1 R ∑ r=1R(y(x, t)− µ(x, t))2 and on the horizontal axis the rank of each configuration based on their mean performance µ(x, t) = 1R ∑R r=1 y(x, t). Figure 4 (right) shows that the signal to noise ratio SNR = µσ is almost constant across configurations. Maybe not surprisingly, the noise seems to correlate with the asymptotic performance of a configura- tion. More interesting is that the noise between different configurations varies on different orders of magnitudes and is thus heteroscedastic. We can incorporate this observation by making the noise dependent on the input data to allow to predict different noise levels for different hyperparameters. 2.4 NEW BASIS FUNCTION LAYER FOR LEARNING CURVE PREDICTION WITH BAYESIAN NEURAL NETWORKS x1 x2 x3 x4 xd· · · hidden layer(s) ŷ∞ φ1 . . . φk basis function layer θ1 · · · θkt w1 · · ·wk ŷ = ŷ∞ + ∑k i=0 wiφi σ 2 Figure 2: Our neural network architecture to model learning curves. A common hidden layer is used to simultaneously model ŷ(x, T ), the parameters of the basis functions, their respective weights, and the noise. We now combine Bayesian neural networks with the parametric functions to incorporate more knowledge about learning curves into the net- work itself. Instead of obtaining the parameters Θ and w by sampling from the posterior, we use a Bayesian neural network to learn several map- pings simultaneously: 1. ŷ∞: X → R, the asymptotic value of the learning curve 2. Θ: X → RK , the parameters of a para- metric function model (see Figure 3 for some example curves from our basis functions) 3. w: X → Rk, the corresponding weights for each function in the model 4. σ2: X → R+, the observational noise for this hyperparameter configuration With these quantities, we can compute the likeli- hood in (3) which allows training the network. A schematic of this is shown in Fig. 2. For training, we will use the same MCMC methods, namely SGLD and SGHMC described above. 4 Under review as a conference paper at ICLR 2017 Figure 3: Example functions generated with our k = 5 basis functions (formulas for which are given in Appendix B). For each function, we drew 50 different parameters θi uniformly at random in the output domain of the hidden layer(s) of our model. This illustrates the type of functions used to model the learning curves. Figure 4: On the left we plot the noise estimated of 40 different configurations on the FCNet benchmark sorted by their mean performance at t = T . The color indicates the time step t, darker meaning a larger value. On the right we show the signal to noise ratio for the same configurations. 3 EXPERIMENTS We now empirically evaluate the predictive performance of Bayesian neural networks, with and without our special learning curve layer. For both networks we used a 3-layer architecture with tanh activations and 50 units per layer. We also evaluate two different sampling methods for both types of networks: stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian MCMC (SGHMC), following the approach of Springenberg et al. (2016) to automatically adapt the noise estimate and the preconditoning of the gradients. 3.1 DATASETS For our empirical evaluation we generated the following four datasets of learning curves, in each case sampling hyperparameter configurations at random from the hyperparameter spaces detailed in Table 2 in the appendix: \u2022 CNN: We sampled 256 configurations of 5 different hyperparameters of a 3-layer convo- lutional neural network (CNN) and trained each of them for 40 epochs on the CIFAR10 (Krizhevsky, 2009) benchmark. \u2022 FCNet: We sampled 4096 configurations of 10 hyperparameters of a 2-layer feed forward neural network (FCNet) on MNIST (LeCun et al., 2001), with batch normalization, dropout and ReLU activation functions, annealing the learning rate over time according to a power function. We trained the neural network for 100 epochs. \u2022 LR: We sampled 1024 configurations of the 4 hyperparameters of logistic regression (LR) and also trained it for 100 epochs on MNIST. \u2022 VAE: We sampled 1024 configuration of the 4 hyperparameters of a variational auto- encoder (VAE) (Kingma and Welling, 2014). We trained the VAE on MNIST, optimizing the approximation of the lower bound for 300 epochs. 5 Under review as a conference paper at ICLR 2017 Figure 5: Qualitative comparison of the different models. The left panel shows learning on the CNN benchmark. All models observed the validation error of the first 4 epochs of the true learning curve (black). We plot the mean predictions and a one σ confidence interval around it. On the right, the posterior distributions over the value at 40 epochs is plotted. 3.2 PREDICTING ASYMPTOTIC VALUES OF PARTIALLY OBSERVED CURVES We first study the problem of predicting the asymptotic values of partially-observed learning curves tackled by Domhan et al. (2015). The method by Domhan et al. (2015) (which we dub LC- Extrapolation) works on one individual learning curve at a time and does not allow to model performance across hyperparameter configurations. Thus, we trained it separately on single partial learning curves. Our Bayesian neural network models, on the other hand, can use training data from different hyperparameter configurations. Here, for a simple comparison, we used training data with the same number of epochs for every partial learning curve.1 Figure 5 (left) visualizes the extrapolation task, showing a learning curve from the CNN dataset and the prediction of the various models trained only using the first 12 of 40 epochs of the learning curve(s). Figure 5 (right) shows the corresponding predictive distributions obtained with all models. For a more quantitative evaluation we used all models to predict the asymptotic value of all learning curves, evaluating predictions based on observing between 10% and 90% of the learning curves. Figure 6 shows the mean squared error between true and predicted asymptotic value as a function of how much of the learning curves has been observed. We notice several patterns. Firstly, throughout, our specialized network architecture performed better than the standard Bayesian neural networks. Secondly, throughout, SGHMC outperformed SGLD. Finally, comparing LC-Extrapolation and the Bayesian neural network (BNN) approach, we notice that BNNs work better when only short parts of the learning curve have been observed, but that LC-Extrapolation in some cases works better for almost completely-observed learning curves. Specifically, this is the case for the datasets FCNet and VAE, for which most learning curves had already converged after about a third of the maximum number of epochs (compare Figure 1). In those cases, LC-Extrapolation can basically just predict the last observed performance value, which yields very strong performance. Our BNN approach, on the other hand, was trained on many configurations and needs to generalize across these, yielding somewhat worse performance for this (arguably easy) task.2 3.3 PREDICTING UNOBSERVED LEARNING CURVES As mentioned before, training a joint model across hyperparameters and time steps allows us to make predictions for completely unobserved learning curves of new configurations. To estimate how well Bayesian neural networks perform in this task, we used the datasets from Section 3.1 and split all of them into 16 folds, allowing us to perform cross-validation of the predictive performance. For each 1We note that when used inside Bayesian optimization, we would have access to a mix of fully-converged and partially-converged learning curves as training data, and could therefore expect better extrapolation performance. 2In the future, we aim to improve our BNN architecture for this case of partially-observed learning curves by also giving the network access to the partial learning curve it should extrapolate. 6 Under review as a conference paper at ICLR 2017 Figure 6: Assessment of the predictive quality based on partially observed learning curves. The panels on the left show the mean squared error of the prediction after the final epoch (y-axis) after observing a fraction of all learning curves (x-axis). 7 Under review as a conference paper at ICLR 2017 Figure 7: On the horizontal axis we plot the true value and on the vertical axis the predicted values. Each point is colored by its log-likelihood (the brighter the higher). Our learning curve BNN trained with SGHMC leads to the best mean predictions and assigns the highest likelihood to the test points. Method CNN FCNet LR VAE MSE ·102 ALL MSE·102 ALL MSE·102 ALL MSE ·104 ALL SGLD 1.8± 0.9 0.60± 0.25 4.5± 0.6 0.13± 0.05 1.1± 0.3 0.77± 0.08 4.7± 1.7 2.16± 0.07 SGLD-LC 1.4± 0.9 1.09± 0.22 3.3± 0.7 0.54± 0.06 1.3± 0.7 0.94± 0.09 3.1± 1.1 1.55± 0.01 SGHMC 0.8± 0.6 0.96± 0.15 1.9± 0.3 0.50± 0.04 0.5± 0.2 1.08± 0.05 3.6± 1.5 2.34± 0.07 SGHMC-LC 0.7± 0.6 1.15± 0.34 2.0± 0.3 0.80± 0.05 0.5± 0.2 1.17± 0.08 1.9± 1.1 1.39± 0.65 Table 1: In each column we report the mean squared error (MSE) and the average log-likelihood (ALL) of the 16 fold CV learning curve prediction for a neural network without learning curve prediction layer (SGLD and SGHMC) and with our new layer (SGLD-LC and SGHMC-LC). fold we trained all models on the full learning curves in the training set and let them predict for the held-out learning curves. Table 1 shows the mean squared error and the average log-likelihood (both computed for all points in each learning curve) across the 16 folds. We make two observations: firstly, both neural network architectures lead to a reasonable mean squared error and average log-likelihood, for both SGLD and SGHMC. Except on the VAE dataset our learning curve layer seems to improve mean squared error and average log likelihood. Secondly, SGHMC performed better than SGLD, with the latter resulting in predictions with too small variances. Figure 7 visualizes the results for our CNN dataset in more detail, showing that true and predicted errors correlate quite closely. 4 CONCLUSION We studied Bayesian neural networks for modelling the learning curves of iterative machine learning methods, such as stochastic gradient descent for convolutional neural networks. Based on the parametric learning curve models of Domhan et al. (2015), we also developed a specialized neural network architecture with a learning curve layer that improves learning curve predictions. In future work, we aim to study recurrent neural networks for predicting learning curves and will extend Bayesian optimization methods with Bayesian neural networks Springenberg et al. (2016) based on our learning curve models. ACKNOWLEDGMENT This work has partly been supported by the European Commission under Grant no. H2020-ICT- 645403-ROBDREAM, and by the German Research Foundation (DFG), under Priority Programme Autonomous Learning (SPP 1527, grant HU 1900\/3-1) and under the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086). REFERENCES E. Brochu, V. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, 2010. 8 Under review as a conference paper at ICLR 2017 J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Proc. of NIPS\u201912, 2012. B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas. Taking the human out of the loop: A Review of Bayesian Optimization. Proc. of the IEEE, (1), 12\/2015 2016. K. Swersky, J. Snoek, and R. Adams. Freeze-thaw Bayesian optimization. CoRR, 2014a. T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Q. Yang and M. Wooldridge, editors, Proc. of IJCAI\u201915, 2015. L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Efficient hyperparameter optimization and infinitely many armed bandits. 2016. A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. CoRR, 2016. K. Swersky, J. Snoek, and R. Adams. Multi-task Bayesian optimization. In Proc. of NIPS\u201913, 2013. M. Feurer, T. Springenberg, and F. Hutter. Initializing Bayesian hyperparameter optimization via meta-learning. In Proc. of AAAI\u201915, 2015. F. Hutter, H. Hoos, K. Leyton-Brown, and K. Murphy. Time-bounded sequential parameter optimiza- tion. F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In LION\u201911, 2011. J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary, Prabhat, and R. P. Adams. Scalable Bayesian optimization using deep neural networks. In Proc. of ICML\u201915, 2015. J. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust bayesian neural networks. In Proc. of NIPS\u201916, 2016. M. Welling and Y. Teh. Bayesian learning via stochastic gradient Langevin dynamics. 2011. T. Chen, E.B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proc. of ICML\u201914, 2014. A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In S. Haykin and B. Kosko, editors, Intelligent Signal Processing. IEEE Press, 2001. URL http:\/\/www.iro.umontreal.ca\/~lisa\/pointeurs\/lecun-01a.pdf. D. Kingma and M. Welling. Auto-encoding variational bayes. In Proc. of ICLR\u201914, 2014. K. Swersky, J. Snoek, and R. Adams. Freeze-thaw Bayesian optimization. CoRR, 2014b. 9 http:\/\/www.iro.umontreal.ca\/~lisa\/pointeurs\/lecun-01a.pdf Under review as a conference paper at ICLR 2017 A EXPERIMENTAL SETUP \u2013 DETAILS Name Range log scale CNN batch size [32, 512] - number of units layer 1 [4, 10] X number of units layer 2 [4, 10] X number of units layer 3 [4, 10] X learning rate [10−6, 10−0] X FCNet inital learning rate [10−6, 100] X L2 regularization [10−8, 10−1] X batch size [32, 512] - γ [−3,−1] - κ [0, 1] - momentum [0.3, 0.999] - number units 1 [5, 12] X number units 2 [5, 12] X dropout rate layer 1 [0.0, 0.99] - dropout rate layer 2 [0.0, 0.99] - LR learning rate [10−6, 100] X L2 [0.0, 1.0] X batch size [20, 2000] - dropout rate on inputs [0.0, 0.75] - VAE L [1, 3] - number of hidden units [32, 2048] - batch size [16, 512] - z dimension [2, 200] - Table 2: Hyperparameter configuration space of the four different iterative methods. For the FCNet we decayed the learning rate by a αdecay = (1 + γ ∗ t)−κ and also sampled different values for γ and κ. B DESCRIPTION OF THE BASIS FUNCTIONS To reduce complexity, we just used a subset of basis function from Domhan et al. (2015) which we found to be sufficient for learning curve prediction. We slightly changed these function such that their parameters are between [0, 1]. Name Formula vapor pressure θ(t, a, b, c) = exp(−a− b\/10− c\/10 log(t))− exp(−a− b\/10) pow θ(t, a, b) = a(tb − 1) log power θ(t, a, b, c) = 2a 11+exp(−cb\/10) − 11+( t exp(b\/10) )c exponential θ(t, a, b) = b(− exp(−10at) + exp(−10a)) hill-3 θ(t, a, b, c) = a( 1 (c\/t)b+1 − 1 cb+1 ) Table 3: The formulas of our 5 basis functions. 10 Introduction Probabilistic Prediction of Learning Curve Learning Curve Prediction with Basis Function Learning Curve Prediction with Bayesian Neural Networks Heteroscedastic Noise of Hyperparameter Configuration New Basis Function Layer for Learning Curve Prediction with Bayesian neural networks Experiments Datasets Predicting asymptotic values of partially observed curves Predicting unobserved learning curves Conclusion Experimental Setup \u2013 Details Description of the Basis Functions ","flair":"three\tResearch"}
{"author":"jast","created":"Fri Nov 18 07:51:08 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.05763 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cs cs.AI stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Learning to reinforcement learn Authors: Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick (Submitted on 17 Nov 2016 (v1), last revised 24 Nov 2016 (this version, v2)) Abstract: In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience. Comments: 17 pages, 7 figures, 1 table Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:1611.05763 [cs.LG]   (or arXiv:1611.05763v2 [cs.LG] for this version) Submission history From: Jane Wang [view email] [v1] Thu, 17 Nov 2016 16:29:11 GMT (3617kb,D) [v2] Thu, 24 Nov 2016 15:35:02 GMT (3268kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"flusterer","created":"Tue Nov 15 03:14:53 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.01843 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.AI cs.CV cs.LG cs.NE physics physics.soc-ph stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Learning to Perform Physics Experiments via Deep Reinforcement Learning Authors: Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas (Submitted on 6 Nov 2016 (v1), last revised 14 Nov 2016 (this version, v2)) Abstract: When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. Subjects: Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Physics and Society (physics.soc-ph) Cite as: arXiv:1611.01843 [stat.ML]   (or arXiv:1611.01843v2 [stat.ML] for this version) Submission history From: Misha Denil [view email] [v1] Sun, 6 Nov 2016 20:55:19 GMT (1060kb,D) [v2] Mon, 14 Nov 2016 16:40:58 GMT (1328kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Megatron_McLargeHuge","created":"Tue Oct 11 07:41:12 EDT 2016","text":"Suppose you have image metadata you want to incorporate into a classifier, want to train an RNN over categorical data, or simply want a reference result for a problem where you expect SVM or RF to do best.  \n\nHow do you preprocess the data and what transforms and architectures are known to work well?\n\nIf you're incorporating high information features with image\/audio data where you need the network to learn to extract features, how would you structure the model?","flair":"one\tDiscussion"}
{"author":"tezcaML","created":"Wed Oct 05 19:37:27 EDT 2016","text":"If, and only if, you **MUST** run Windows 10 and still want decent performance running Theano &amp; Keras, the following guide may help. It describes GPU acceleration on Windows 10 in **native mode** -- no VMs, no Docker:\n\nhttps:\/\/github.com\/philferriere\/dlwin\n\nWe tested it on the following hardware:\n\n- Dell Precision T7900, 64GB RAM [Intel Xeon E5-2630 v4 @ 2.20 GHz (1 processor, 10 cores total, 20 logical processors)]\n- NVIDIA GeForce Titan X, 12GB RAM [Driver version: 372.90 \/ Win 10 64]\n\nWe used the following tools\/libraries:\n\n- Visual Studio 2015 Community Edition Update 3 w. Windows Kit 10.0.10240.0 [Used for its C\/C++ compiler (not its IDE) and SDK]\n- CUDA 8.0.44 (64-bit) [Used for its GPU math libraries, card driver, and CUDA compiler]\n- MinGW-w64 (5.4.0) [Used for its Unix-like compiler and build tools (g++\/gcc, make...) for Windows]\n- Anaconda (64-bit) w. Python 2.7 (Anaconda2-4.2.0) [A Python distro that gives us NumPy, SciPy, and other scientific libraries]\n- Theano 0.8.2 [Used to evaluate mathematical expressions on multi-dimensional arrays]\n- Keras 1.1.0 [Used for deep learning on top of Theano]\n- OpenBLAS 0.2.14 (Optional) [Used for its CPU-optimized implementation of many linear algebra operations]\n- cuDNN v5.1 (August 10, 2016) for CUDA 8.0 (Recommended) [Used to run vastly faster convolution neural networks]\n\nHope this helps!","flair":"null\tnull"}
{"author":"downtownslim","created":"Mon Nov 14 21:39:21 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.04558 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1611 Change to browse by: cs cs.AI References & Citations NASA ADS 1 blog link (what is this?) Bookmark (what is this?) Computer Science > Computation and Language Title: Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation Authors: Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean (Submitted on 14 Nov 2016) Abstract: We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI) Cite as: arXiv:1611.04558 [cs.CL]   (or arXiv:1611.04558v1 [cs.CL] for this version) Submission history From: Mike Schuster [view email] [v1] Mon, 14 Nov 2016 20:24:39 GMT (2497kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Mathriddle","created":"Tue Oct 11 17:36:26 EDT 2016","text":"Each month we will hold one Kaggle focused meetup. The group will vote on the competition topic and break up into teams. Each team will consist of a Team Leader, Algorithm experts, Programmers, and Data mungers(this can change depending on the category). We will try to rank as high as possible on the leader-boards. Once the competition is complete, we will hold a debriefing for all participating teams to discuss best practices, suggestions for future competitions and an analysis of our performance. If we win a competition, then the proceeds will be distributed equally amongst each member of the winning team.","flair":"four\tProject"}
{"author":"michal_sustr","created":"Thu Sep 29 14:08:52 EDT 2016","text":"I didn't find anything. I wonder why wouldn't they, when Google's policy is to release other networks and accelerate research. It might be interesting to compare it with other approaches, which might not rely on studying human play for pretraining, to achieve a similar result as TD-gammon - learn purely from self-play.","flair":"null\tnull"}
{"author":"Indy20161","created":"Tue Nov 01 21:42:20 EDT 2016","text":"I dropped out from CS school(3rd year) because I didn't like studying there and then I studied 2 years of finance(associate's degree) and then I decided to give machine learning Nanodegree from udacity a shot, I completed it and loved it. Now I 'm very sure that I want to do ML. What should I do next to make a good career in ML?\n\nAnd I'm reading Kevin murphy's book and absolutely love it.","flair":"one\tDiscussion"}
{"author":"evc123","created":"Thu Sep 29 23:07:24 EDT 2016","text":"Scared of superintelligent AI? You should be, says neuroscientist and philosopher Sam Harris \u2014 and not just in some theoretical way. We're going to build superhuman machines, says Harris, but we haven't yet grappled with the problems associated with creating something that may treat us the way we treat ants. This talk was presented at an official TED conference, and was featured by our editors on the home page.","flair":"one\tDiscusssion"}
{"author":"improbabble","created":"Wed Sep 28 14:43:42 EDT 2016","text":"TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. The TensorFlow API is composed of a set of Python modules that enable constructing and executing TensorFlow graphs. The tensorflow package provides access to the complete TensorFlow API from within R. You can install the main TensorFlow distribution from here: NOTE: You should NOT install TensorFlow with Anaconda as there are issues with the way Anaconda builds the python shared library that prevent dynamic linking from R. If you install TensorFlow within a Virtualenv environment you'll need to be sure to use that same environment when installing the tensorflow R package (see below for details). If you installed TensorFlow via pip with your system default version of python then you can install the tensorflow R package as follows: If you are using a different version of python for TensorFlow, you should set the environment variable to the full path of the python binary before installing, for example: If you only need to customize the version of python used (for example specifing python 3 on an Ubuntu system), you can set the environment variable before installation: You can verify that your installation is working correctly by running this script: See the package website for additional details on using the TensorFlow API from R: https:\/\/rstudio.github.io\/tensorflow See the TensorFlow API reference for details on all of the modules, classes, and functions within the API: https:\/\/www.tensorflow.org\/api_docs\/python\/index.html The tensorflow package provides code completion and inline help for the TensorFlow API when running within the RStudio IDE. In order to take advantage of these features you should also install the current Preview Release of RStudio.","flair":"null\tnull"}
{"author":"timburg","created":"Thu Oct 06 12:29:26 EDT 2016","text":" Toggle navigation Tim Sainburg Spectrograms, MFCCs, and Inversion in Python Posted by Tim Sainburg on Thu 06 October 2016 Blog powered by Pelican, which takes great advantage of Python. ","flair":"null\tnull"}
{"author":"TheFlyingDrildo","created":"Sun Nov 27 20:28:15 EST 2016","text":" Learning Multiple Views with Orthogonal Denoising Autoencoders TengQi Ye1(B), Tianchun Wang2, Kevin McGuinness1, Yu Guo3, and Cathal Gurrin1 1 Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland {yetengqi,kevin.mcguinness}@gmail.com 2 School of Software, TNList, Tsinghua University, Beijing, China wtc13@mails.tsinghua.edu.cn 3 Department of Computer Science, City University of Hong Kong, Hong Kong, China Abstract. Multi-view learning techniques are necessary when data is described by multiple distinct feature sets because single-view learning algorithms tend to overfit on these high-dimensional data. Prior success- ful approaches followed either consensus or complementary principles. Recent work has focused on learning both the shared and private latent spaces of views in order to take advantage of both principles. However, these methods can not ensure that the latent spaces are strictly indepen- dent through encouraging the orthogonality in their objective functions. Also little work has explored representation learning techniques for multi- view learning. In this paper, we use the denoising autoencoder to learn shared and private latent spaces, with orthogonal constraints \u2014 discon- necting every private latent space from the remaining views. Instead of computationally expensive optimization, we adapt the backpropagation algorithm to train our model. Keywords: Denoising autoencoder · Autoencoder · Representation learning · Multi-view learning · Multimedia fusion 1 Introduction In many machine learning problems, data samples are collected from diverse sen- sors or described by various features and inherently have multiple disjoint feature sets (conventionally referred as views) [1,2]. For example, in video classification, the videos can be characterized with respect to vision, audio and even attached comments; most article search engines take title, keywords, author, publisher, date and content into consideration; images have different forms of descriptors: color descriptors, local binary patterns, local shape descriptors, etc. The last example reveals the noteworthy case of views obtained from manual descrip- tors instead of natural splits. With multiple descriptors, important information concerning the task, which may be discarded by single descriptor, is hopefully retained by others [3]. c© Springer International Publishing Switzerland 2016 Q. Tian et al. (Eds.): MMM 2016, Part I, LNCS 9516, pp. 313\u2013324, 2016. DOI: 10.1007\/978-3-319-27671-7 26 314 T. Ye et al. Traditional machine learning methods may fail when concatenating all views into one for learning because the concatenation can cause over-fitting due to the high-dimensionality of the features (the curse of dimensionality [4]) and also ignores the specific properties of each view [2]. Compared with traditional single-view data, the multi-view data contains significant redundancy shared by its views. Previous successful multi-view learning algorithms follow two princi- ples: consensus and complementary principles [2]. The consensus principle aims to maximize the agreement or shared knowledge between views. The comple- mentary principle asserts each view may contain useful knowledge that the rest do not have, and errors can be corrected by this private knowledge. To generalize over previous multi-view learning approaches, recent works focused on explicitly accounting for the dependencies and independencies of views, i.e., decompose the latent space into a shared common one (of all views) and several private spaces (of each view) [5,6]. The intuition is that each view is generated from the combination of the shared latent space and a corresponding private latent space. Although these methods benefit from the idea, they embed the orthogonality requirement into the objective function with a weight to set its relative influence, i.e., they encourage rather than restrict the orthogonality. The main reason is that it is hard to optimize an objective function with complex constraints. However, in this case, orthogonality may not be satisfied and extra costly computation effort is needed. Representation learning, which seeks good representations or features that facilitate learning functions of interest, has promising performance in single-view learning [7]. From the perspective of representation learning, multi-view learning can be viewed as learning several latent spaces or features with orthogonality constraints. Only very few works have discussed the similarity between repre- sentation learning, in the form of sparse learning, and multi-view learning [5,8]. In this paper, we propose using the denoising autoencoder to learn the shared and private latent spaces with orthogonality constraints. In our approach, the constraints are satisfied by disconnecting every private latent space from the remaining views. The advantages of our proposed method are: (i) By discon- necting the irrelevant latent spaces and views, the orthogonality constraints are enforced. (ii) Such constraints keep the chain rule almost the same, thus sim- plify training the model, i.e., no extra effort is needed for tuning weights or complex optimization. (iii) No preprocessing is required for denoising because the denoising autoencoder learns robust features. 2 Related Work Existing multi-view learning algorithms can be classified into three groups: co- training, multiple kernel learning, and subspace learning [2]. Co-training [9] was the first formalized learning method in the multi-view framework. It trains repeatedly on labeled and unlabeled data until the mutual agreement on two dis- tinct views is maximized. Further extensions include: an improved version with the Bayesian undirected graphical model [10], and application in active multi- view learning [11]. Multiple Kernel Learning naturally corresponds to different Learning Multiple Views with Denoising Autoencoder 315 modalities (views) and combining kernels either linearly or non-linearly improves learning performance [12]. Multiple Kernel Learning always comes with diverse linear or nonlinear constraints, which makes the objective function complex [13\u201315]. Subspace learning-based approaches aim to obtain latent subspaces that have lower dimensions than input views, thus effective information is learned and redundancy is discarded from views. As those latent spaces can be regarded as effective features, subspace learning-based algorithms allow single-view learning algorithms to be capable for learning on multi-view data. Subspace learning-based algorithms initially targeted on conducting mean- ingful dimensional reduction for multi-view data [2]. Canonical correlation analy- sis based approaches, following the consensus principle, linearly or non-linearly project two different views into the same space where the correlation between views is maximized [16,17]. Because the dimension of the space to be projected on equals to the smaller one of the two views, the dimension is reduced by at least half. Other similar approaches include Multi-view Fisher Discriminant Analysis, Multi-view Embedding and Multi-view Metric Learning [2]. Unlike other types of subspace learning-based algorithms, latent subspace learning models resort to explicitly building a shared latent space and several private latent spaces (a private latent space corresponds to a view). The Factor- ized Orthogonal Latent Space model proposes to factorize the latent space into shared and private latent spaces by encouraging these spaces to be orthogonal [6]. In addition, it penalized the dimensions of latent spaces to reduce redundancy. Similarly, a more advanced version is employed with sparse coding of structured sparsity for the same purpose [5]. Nevertheless, in their approaches, orthogonal- ity is encouraged by penalizing inner products of latent spaces or encouraging the structured sparsity in the objective function. These approaches to not guarantee orthogonality. Representation learning focuses on learning good representations or extract- ing useful information from data that simplifies further learning tasks [18]. A well-known successful example of representation learning is deep learning. By stacking autoencoders to learn better representations for each layer, deep learn- ing drastically improves the performance of neural networks in tasks such as image classification [19], speech recognition [20], and natural language process- ing [21]. An autoencoder is simply a neural network that tries to copy its input to its output [7]. Multi-view feature learning was first analyized from the perspective of rep- resentation learning through sparse coding in [8], but little work has employed representation learning for studying multi-view data so far, thus far only sparse coding has been used [5,22]. Since the autoencoder is the most prevalent repre- sentation learning algorithm, our model enables various existing work on autoen- coder to inherently extend it to multi-view settings. 316 T. Ye et al. 3 Approach 3.1 Problem Formulation Let X = {X(1),X(2), · · · ,X(V )} be a data set of N observations from V views and X(v) be the vth view of data, where X(v) ∈ RN×P (X(v)). P (·) is the number of columns of the matrix; and D(·) is the number of features that the space or matrix has. Additionally, Y ∈ RN×P (Y ) is the shared latent space across all views; Z = {Z(1), Z(2), · · · , Z(V )} is the set of private latent spaces of each individual view, where Z(v) ∈ RN×P (Z(V )). Because the latent spaces are required to have no redundancy, then P (Y ) = D(Y ) and P (Z(v)) = D(Z(v)). Moreover, X(v) is expected to be linearly represented by Y and Z(v), D(X(v)) = D(Y ) + D(Z(v)). Our goal is to learn Y and the Z from X where Y is independent of Z and the arbitrary two private latent spaces Z(vi), Z(vj) are orthogonal. 3.2 Basic Autoencoder A standard autoencoder takes an input vector x and initially transforms it to a hidden representation vector y = s(Wx + b) through an activation function s and weights W . Note the activation function can be linear or nonlinear. The latent representation y is subsequently mapped back to a reconstructed vector z = s(W \u2032y + b\u2032). The objective is that the output z is as close as possible to x, i.e., the parameters are optimized to minimize the average reconstruction error: W �, b�,W \u2032�, b\u2032� = argmin W,b,W \u2032,b\u2032 L(x, z) (1) where L is a loss function to measure how good the reconstruction is, and often least squares L = \u2016(x− z)\u20162 is used. The expectation is that the hidden repre- sentation y could capture the main factors of data x [23]. 3.3 Orthogonal Autoencoder for Multi-view Learning In the scenarios of multi-view learning, the hidden or latent representation (neu- ron) is expected to be consist of shared and private spaces. To this end, we modify the autoencoder such that every private latent space only connects to its own view. Figure 1 depicts the graphical model of such improved autoencoder given that the first view has two original features while the second one has three. Because the first view is disconnected from the second private latent space (the third hidden neuron), the second private latent space is strictly independent of the first view. Similarly, the first private latent space is independent of the second view. In order to maintain orthogonality of private latent spaces, the bias is disconnected from private latent spaces (proof is given below). In addition, in order to retain that views are linearly dependent of latent spaces, the hidden representation before the nonlinear mapping (Wx + b) is regarded as latent spaces. If the activation function is nonlinear, then y and Learning Multiple Views with Denoising Autoencoder 317 Fig. 1. Graphical model of an orthogonal autoencoder for multi-view learning with two views. W \u2032y+ b\u2032 can be considered as mappings of latent representation and reconstruc- tion of input in a nonlinear space. And the last activation function maps the W \u2032y + b\u2032 back to original space. Also note that the number of neurons representing the shared latent space equals the dimensions of its features D(Y ) and it is the same for private latent space, i.e., a neuron represents a feature in the space. Our model is inherently able to fit in any number of views and arbitrary numbers of features for each view and latent space. Following the aforementioned notations, we further define I(A|B) as the indices of columns of A in terms of B if matrix A is a submatrix of matrix B and they have same row numbers. The orthogonality constraints on weights can be formulated as: WI(Z(v2)|[Y,Z]),I(X(v1)|X) = 0 (v1 �= v2) (2) W \u2032I(X(v1)|X),I(Z(v2)|[Y,Z]) = 0 (v1 �= v2) (3) bI(Z|[Y,Z]) = 0 (4) For a matrix A, the symbol AI,· denotes a sub-matrix consisting of row vectors indexing by I (of A); similarly, A·,I is a sub-matrix from such column vectors. We provide the rigorous proof that arbitrary two private latent spaces (Z(v1) and Z(v2)) are orthogonal: 318 T. Ye et al. (Z(v1))T · Z(v2) = (WI(Z(v1)|[Y,Z]),· · x+ 0)T · (WI(Z(v2)|[Y,Z]),· · x+ 0) = xT · ((WI(Z(v1)|[Y,Z]),·)T ·WI(Z(v2)|[Y,Z]),·) · x = xT · 0 · x = 0 [(Z(v1))T ·Z(v2)]ij = 0 indicates that the component [Z(v1)]·,i is orthogonal to [Z(v2)]·,j . Because any two components of any two different private latent spaces are orthogonal, the private latent spaces are orthogonal to each other. Although we do not explicitly restrict Y to be orthogonal to Z, the objective function (Eq. 1) prefers that the shared latent space is orthogonal to the private latent spaces. Because if Y contains components from any views, i.e. not orthog- onal to private latent spaces, then the components of that view will introduce noise into the others during reconstruction. 3.4 Training of Orthogonal Autoencoder The autoencoder, intrinsically a neural network, is trained by the backpropa- gation algorithm, which propagates the derivation of the error from the output to the input, layer-by-layer [4]. As the 0 values automatically break gradients passing through that connection, we only need to set Eqs. 5, 6 and 7 after basic backpropagation as follows: ∂L WI(Z(v2)|[Y,Z]),I(X(v1)|X) = 0 (v1 �= v2) (5) ∂L W \u2032 I(X(v1)|X),I(Z(v2)|[Y,Z]) = 0 (v1 �= v2) (6) ∂L bI(Z|[Y,Z]) = 0 (7) 3.5 Orthogonal Denoising Autoencoder for Robust Latent Spaces The denoising autoencoder was formally proposed to enforce the autoencoder to learn robust features or latent spaces in our case [24]. The idea is based on the assumption that robust features can reconstruct or repair input which is partially destroyed or corrupted. A typical way of producing x̃, the corrupted version of initial input x, is randomly choosing a fixed number of components and forcing their values to be 0; while other components stays the same. As a consequence, the autoencdoer is encouraged to learn robust features which are most likely to recover the wiped information. Afterwards the x̃ is fed as input to the autoencoder, then mapped to the hidden representation y = s(Wx̃ + b) and finally transformed to output z = s(W \u2032y + b\u2032). In the process, the aforementioned orthogonality constraints (Eqs. (2) and (3)) remain the same. In the end, the objective function L enforces the output z to be as close as possible to the original, uncorrupted x, instead of input x̃. Learning Multiple Views with Denoising Autoencoder 319 4 Experiments In this section, we introduce two datasets to evaluate our method: the synthetic dataset is straightforward to demonstrate the ability of our method to learn shared and private latent spaces from data with noise; and the real-world dataset is employed to compare our approach with other state-of-the-art algorithms and display optimization on the number of neurons for vieww using random search. 4.1 Synthetic Dataset We evaluated our approach with a toy dataset similar to [6], which can be gen- erated in 10 lines of MATLAB code (listed in Algorithm1). Algorithm 1. Toy data generation in MATLAB notation t = -1:0.02:1; x = sin(2*pi*t); z1 = cos(pi*pi*t); z2 = cos(5*pi*t); v1 = (rand(20, size(x, 1)+1)) * [x;z1]; v1 = v1 + randn(size(v1))*0.01; v2 = (rand(20, size(x, 1)+1)) * [x;z2]; v2 = v2 + randn(size(v2))*0.01; v1 = [v1; 0.02*sin(3.6*pi*t)]; v2 = [v2; 0.02*sin(3.6*pi*t)]; In words, v1 and v2 (first and second views in Fig. 2b respectively) are two views generated by randomly projecting two ground truths, [x;z1] and [x;z2], to 20 dimensions spaces. The first component (blue curve in Fig. 2a) of the two ground truths are shared latent space and their second components (green curves in first and second ground truth of Fig. 2a respectively) are individual private latent spaces. The two views are then added to Gaussian noise with standard deviation 0.01 and correlated noise on their last dimension (both views have 21 dimensions now). In the experiment, we adopt the Hyperbolic Tangent ( tanh(x) = 1−e −2x 1+e−2x ) as the activation function. Three hidden neurons are used as latent spaces, one represents the shared space and the other two represent the private ones. Features of the original data are evenly corrupted for denoising. The result of our method is depicted in Fig. 2d, where the first graph employs modified autoencoder while the second one is modified denoising autoencoder. The shared latent factor is in blue, the private latent factor of first view is in green and private latent factor of second view is in red. The denoising autoencoder generates more robust latent spaces than those from the autoencoder. As expected, Canonical Correlation Analysis (Fig. 2c) extracts the true shared signal (in blue) and the correlated noise (in red), while fails to discover the two private signals. Notice that both true recovered signals are scaled and the correlated noise is even inverted. The reason is that we can multiply a number 320 T. Ye et al. −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1 −0.5 0 0.5 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1 −0.5 0 0.5 1 (a) Two ground truths (shared signals in blue, private signals in green, correlated noise in red) −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1.5 −1 −0.5 0 0.5 1 1.5 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −2 −1 0 1 2 (b) Two 21D views −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −2.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5 (c) Result of CCA −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1 −0.5 0 0.5 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1.5 −1 −0.5 0 0.5 1 1.5 (d) Result of our approach (orthogonal non- denoising and denoising autoencoder) Fig. 2. Latent spaces recovered on synthetic data (Color figure online). and its multiplicative inverse respectively to the latent spaces and corresponding coefficient to attain same product. The experiment confirms our approach is able to effectively learn robust shared and private latent spaces, while CCA is sensitive to noise. Moreover, we do not need to use PCA like previous work [5]1. Because the local minimum differs due to the random initialization of weights, methods of optimization, etc., Fig. 2d may vary slightly in repeat experiments [25]. 4.2 Real-World Dataset We applied our model to the PASCAL VOC\u201907 data set [26] for multi-label object classification, i.e., an image may contain more than one object label. Through the experiment, we compare the results from using only images, tags, and their combinations to demonstrate multi-view learning algorithms are capable of using 1 With PCA, CCA can also perform well. Learning Multiple Views with Denoising Autoencoder 321 Table 1. Mean and variance of AP of different approaches (best results of classification for each class in bold). aeroplane bicycle bird boat bottle bus car Images 0.596± 0.012 0.367± 0.028 0.277± 0.044 0.568± 0.018 0.127± 0.021 0.319± 0.023 0.613± 0.011 Tags 0.734± 0.016 0.539± 0.012 0.688± 0.003 0.495± 0.016 0.237± 0.017 0.430± 0.016 0.591± 0.003 MKL 0.847± 0.031 0.532± 0.030 0.711± 0.027 0.596± 0.044 0.210± 0.032 0.569± 0.026 0.694± 0.025 SVM2K(0) 0.584± 0.157 0.360± 0.271 0.576± 0.356 0.290± 0.290 0.117± 0.098 0.116± 0.033 0.431± 0.115 SVM2K(1) 0.682± 0.132 0.385± 0.377 0.113± 0.356 0.796± 0.122 0.080± 0.038 0.133± 0.098 0.579± 0.292 ODAE 0.837± 0.009 0.548± 0.008 0.725± 0.005 0.682± 0.020 0.205± 0.024 0.588± 0.029 0.714± 0.013 cat chair cow diningtable dog horse motorbike Images 0.369± 0.010 0.364± 0.006 0.225± 0.079 0.291± 0.102 0.220± 0.019 0.607± 0.040 0.394± 0.039 Tags 0.715± 0.009 0.174± 0.004 0.471± 0.014 0.106± 0.012 0.678± 0.006 0.775± 0.004 0.607± 0.006 MKL 0.722± 0.018 0.296± 0.018 0.513± 0.037 0.193± 0.017 0.689± 0.022 0.815± 0.014 0.660± 0.032 SVM2K(0) 0.231± 0.307 0.095± 0.013 0.089± 0.011 0.074± 0.034 0.137± 0.056 0.068± 0.013 0.521± 0.305 SVM2K(1) 0.366± 0.199 0.311± 0.164 0.187± 0.170 0.070± 0.028 0.486± 0.293 0.516± 0.353 0.258± 0.222 ODAE 0.725± 0.012 0.347± 0.011 0.521± 0.015 0.219± 0.014 0.665± 0.009 0.760± 0.015 0.624± 0.021 person pottedplant sheep sofa train tvmonitor Images 0.689± 0.016 0.115± 0.018 0.163± 0.026 0.220± 0.016 0.589± 0.035 0.230± 0.014 Tags 0.686± 0.002 0.329± 0.012 0.592± 0.031 0.180± 0.006 0.811± 0.005 0.378± 0.013 MKL 0.782± 0.028 0.388± 0.047 0.584± 0.043 0.225± 0.026 0.845± 0.017 0.395± 0.022 SVM2K(0) 0.643± 0.227 0.053± 0.003 0.599± 0.329 0.510± 0.404 0.188± 0.156 0.711± 0.299 SVM2K(1) 0.739± 0.038 0.256± 0.308 0.126± 0.115 0.151± 0.064 0.290± 0.214 0.197± 0.115 ODAE 0.749± 0.015 0.350± 0.012 0.644± 0.008 0.244± 0.026 0.838± 0.002 0.435± 0.006 information from different views. We also provide a comparison of our method to other methods, and test the sensitivity of our model. The data set contains around 10,000 images of 20 different categories of object with standard train\/test sets provided. The dataset also provides tags for each image: textual descriptions of the images. A total of 804 tags, which appear at least 8 times, form the bag-of-words vectors (bit-based) of a view. 15 image features are chosen from [15] as visual descriptors: local SIFT fea- tures [27]; local hue histograms [28] (both were computed on a dense multi-scale grid and on regions found with a Harris interest-point detector); global color histograms over RGB, HSV, and LAB color spaces; the above histogram image representations computed over a 3 × 1 horizontal decomposition of the image; and GIST descriptor [29], which roughly encodes the image layout. This pro- duces two views: a visual modality with 37,152 features; and a textual modality with 804 features. The default experiment setup used 6 SVMs (sigmoid kernel) with different C parameters2 for classification. Table 1 reports the mean and variance of the average precision (AP) values from these SVMs. For the single modality image classification (\u201CImages\u201D in Table 1), we report AP using the single visual feature that performed best for each class. In Multiple Kernel Learning (MKL), we computed textual kernel kt(·, ·) and visual kernels kv(·, ·) following [15]. Because the final kernel kf (·, ·) = dvkv(·, ·)+ dtkt(·, ·), where dv, dt > 0 and dv+dt = 1, is a convex combination, the dv and dt 2 C from {10−3, 10−2, 10−1, 100, 101, 102}. 322 T. Ye et al. were chosen by grid search with step 0.1 and C = 1. Instead of original features, kernels kf (·, ·) were then used in place of the original features. We tested two variants of SVM2K, both containing two views, since SVM2K [17] only supports two-view learning. SVM2K(0) concatenates all visual features into one view, with the other view comprising the textual features; SVM2K(1) uses only the 2 SIFT-based features. SVM2K contains 4 penalty parameters: we set the penalty value for the first SVM to 0.2; for the second SVM to 0.1; tolerance for the synthesis to 0.01; and varied the penalty value for synthesis in the same way as previously described for linear SVM parameter C. Our orthogonal denoising autoencoder for multi-view learning (ODAE) uses the sigmoid function (y = 11+e−x ) as the activation function. We reduplicate the data 10 times and randomly corrupted 10% of total features each time for denoising. Random search was used to find the optimal numbers of neurons for each hidden views [30]. Generally speaking, multi-view learning approaches for object classification outperform those using only the visual or textual view. SVM2K methods have the worst performance and are highly unstable among all the multi-view learning approaches, since they can only accept 2 views. Specifically, the visual view of SVM2K(0) tends to overfit while that of SVM2K(1) tends to underfit. ODAE performs best for 7 classes and second best for another 10 classes (slightly worse). 0 2 4 6 8 10 12 14 16 18 20 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58 0.6 A P Index of experiment Fig. 3. AP values for 20 experiments with random hyperparameters (max 300 SGD iterations). The number of hidden nodes in each experiment are uniformly drawn from:[ 3, 40 ] for the shared view, [ 500, 700 ] for the textual view, and [ 50, 200 ] for all visual views with the exception of Harris hue ranges, which were drawn from [ 50, 100 ] . We used random search [30] to select reasonable values for the number of hidden neurons for each view3. Figure 3 plots AP values for different parameter configurations of an orthogonal autoencoder, demonstrating the relative robust- ness of the method with respect to the hyperparameters. The best configuration found was: 29 nodes of hidden layer for the shared view, 672 for the textual view, and [ 99, 148, 149, 143, 74, 51, 113, 121, 96, 164, 113, 91, 148, 68, 56 ] for the visual views. 3 Because of pages limit, we can not provide more details. Learning Multiple Views with Denoising Autoencoder 323 5 Conclusions In this paper, we modify the basic autoencoder for multi-view learning to enable private latent spaces to be absolutely orthogonal to each other while simulta- neously encouraging the shared latent space to be orthogonal to private latent spaces as well. Inheriting from the denoising autoencoder, our model is able to learn robust features, i.e., features that are strongly resistant to noise. We also extend back-propagation algorithm elegantly to train the model, which means our model is exempt from extra complex optimization tricks. Acknowledgement. The research was supported by the Irish Research Council (IRC- SET) under Grant Number GOIPG\/2013\/330. The authors wish to acknowledge the DJEI\/DES\/SFI\/HEA Irish Centre for High-End Computing (ICHEC) for the provision of computational facilities and support. Amen. References 1. Sun, S.: A survey of multi-view machine learning. Neural Comput. Appl. 23(7\u20138), 2031\u20132038 (2013) 2. Xu, C., Tao, D., Xu, C.: A survey on multi-view learning. arXiv preprint arXiv:1304.5634 (2013) 3. Dasgupta, S., Littman, M.L., McAllester, D.: Pac generalization bounds for co- training. Adv. Neural Inf. Process. Syst. 1, 375\u2013382 (2002) 4. Bishop, C.M.: Pattern Recognition and Machine Learning. Springer, New York (2006) 5. Jia, Y., Salzmann, M., Darrell, T.: Factorized latent spaces with structured spar- sity. In: Advances in Neural Information Processing Systems, pp. 982\u2013990 (2010) 6. Salzmann, M., Ek, C.H., Urtasun, R., Darrell, T.: Factorized orthogonal latent spaces. In: International Conference on Artificial Intelligence and Statistics, pp. 701\u2013708 (2010) 7. Bengio, Y., Goodfellow, I.J., Courville, A.: Deep learning. Book in preparation for MIT Press (2015) 8. Memisevic, R.: On multi-view feature learning. arXiv preprint arXiv:1206.4609 (2012) 9. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: Proceedings of the Eleventh Annual Conference On Computational Learning Theory, pp. 92\u2013100. ACM (1998) 10. Yu, S., Krishnapuram, B., Rosales, R., Rao, R.B.: Bayesian co-training. J. Mach. Learn. Res. 12, 2649\u20132680 (2011) 11. Muslea, I., Minton, S., Knoblock, C.A.: Active learning with multiple views. J. Artif. Intell. Res. 27, 203\u2013233 (2006) 12. Gönen, M., Alpaydın, E.: Multiple kernel learning algorithms. J. Mach. Learn. Res. 12, 2211\u20132268 (2011) 13. Rakotomamonjy, A., Bach, F., Canu, S., Grandvalet, Y.: More efficiency in multiple kernel learning. In: Proceedings of the 24th International Conference On Machine Learning, pp. 775\u2013782. ACM (2007) 14. Akaho, S.: A kernel method for canonical correlation analysis. arXiv preprint cs\/0609071 (2006) http:\/\/arxiv.org\/abs\/1304.5634 http:\/\/arxiv.org\/abs\/1206.4609 http:\/\/arxiv.org\/abs\/cs\/0609071 324 T. Ye et al. 15. Guillaumin, M., Verbeek, J., Schmid, C.: Multimodal semi-supervised learning for image classification. In: 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 902\u2013909. IEEE (2010) 16. Sun, S., Hardoon, D.R.: Active learning with extremely sparse labeled examples. Neurocomputing 73(16), 2980\u20132988 (2010) 17. Farquhar, J., Hardoon, D., Meng, H., Shawe-taylor, J.S., Szedmak, S.: Two view learning: Svm-2k, theory and practice. In: Advances in Neural Information Process- ing Systems, pp. 355\u2013362 (2005) 18. Bengio, Y., Courville, A., Vincent, P.: Representation learning: a review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 35(8), 1798\u20131828 (2013) 19. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: Advances in Neural Information Processing Sys- tems, pp. 1097\u20131105 (2012) 20. Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N., et al.: Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Signal Process. Mag. 29(6), 82\u201397 (2012) 21. Collobert, R., Weston, J.: A unified architecture for natural language processing: deep neural networks with multitask learning. In: Proceedings of the 25th Inter- national Conference On Machine Learning, pp. 160\u2013167. ACM (2008) 22. Liu, W., Tao, D., Cheng, J., Tang, Y.: Multiview hessian discriminative sparse coding for image annotation. Comput. Vis. Image Underst. 118, 50\u201360 (2014) 23. Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al.: Greedy layer-wise training of deep networks. Adv. Neural Inf. Process. Syst. 19, 153 (2007) 24. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th Interna- tional Conference on Machine Learning, pp. 1096\u20131103. ACM (2008) 25. LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.-R.: Efficient BackProp. In: Mon- tavon, G., Orr, G.B., Müller, K.-R. (eds.) Neural Networks: Tricks of the Trade, 2nd edn. LNCS, vol. 7700, pp. 9\u201348. Springer, Heidelberg (2012) 26. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http:\/\/www. pascal-network.org\/challenges\/VOC\/voc2007\/workshop\/index.html 27. Lowe, D.G.: Object recognition from local scale-invariant features. In: The pro- ceedings of the Seventh IEEE International Conference on Computer Vision, vol. 2, pp. 1150\u20131157. IEEE (1999) 28. van de Weijer, J., Schmid, C.: Coloring local feature extraction. In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006. LNCS, vol. 3952, pp. 334\u2013348. Springer, Heidelberg (2006) 29. Oliva, A., Torralba, A.: Modeling the shape of the scene: a holistic representation of the spatial envelope. Int. J. Comput. Vis. 42(3), 145\u2013175 (2001) 30. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. J. Mach. Learn. Res. 13(1), 281\u2013305 (2012) http:\/\/www.pascal-network.org\/challenges\/VOC\/voc2007\/workshop\/index.html http:\/\/www.pascal-network.org\/challenges\/VOC\/voc2007\/workshop\/index.html Learning Multiple Views with Orthogonal Denoising Autoencoders 1 Introduction 2 Related Work 3 Approach 3.1 Problem Formulation 3.2 Basic Autoencoder 3.3 Orthogonal Autoencoder for Multi-view Learning 3.4 Training of Orthogonal Autoencoder 3.5 Orthogonal Denoising Autoencoder for Robust Latent Spaces 4 Experiments 4.1 Synthetic Dataset 4.2 Real-World Dataset 5 Conclusions References ","flair":"three\tResearch"}
{"author":"marmle","created":"Tue Nov 01 21:44:01 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.00328 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.LG stat stat.CO stat.ME References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: The $χ$-Divergence for Approximate Inference Authors: Adji B. Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David M. Blei (Submitted on 1 Nov 2016) Abstract: Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ || $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ || $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty. Subjects: Machine Learning (stat.ML); Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME) Cite as: arXiv:1611.00328 [stat.ML]   (or arXiv:1611.00328v1 [stat.ML] for this version) Submission history From: Adji Bousso Dieng [view email] [v1] Tue, 1 Nov 2016 18:40:23 GMT (6045kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"hardmaru","created":"Thu Oct 20 03:22:35 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.05683 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: stat stat.ME References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Rejection Sampling Variational Inference Authors: Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, David M. Blei (Submitted on 18 Oct 2016) Abstract: Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a (differentiable) deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on rejection sampling. The discontinuity introduced by the accept--reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic optimization variational inference. Subjects: Machine Learning (stat.ML); Methodology (stat.ME) Cite as: arXiv:1610.05683 [stat.ML]   (or arXiv:1610.05683v1 [stat.ML] for this version) Submission history From: Christian A. Naesseth [view email] [v1] Tue, 18 Oct 2016 15:55:08 GMT (778kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"singham","created":"Sun Nov 06 01:32:09 EST 2016","text":"I was looking for a way to automatically create interesting puzzle\nlevels for games through machine learning.\n\nLet's say I have 500 level valid gameplays (+ invalid gameplays, more can be generated if required) for a grid based puzzle game. Gameplays are essentially path from Initial State to Goal State through Intermediate States using various directional moves like move up, down, left or right.\nMy question is \"**Is it possible to generate interesting, complex initial game state so that it would be hard\/fun to solve?**\" \n\nMany papers deal with learning to play a game(q-learning) , or learning a policy, etc. But I haven't come across a paper which talks about making a initial game state difficult so that its metrics are maximized like no. of moves required to solve are high, sequences of moves are distinct, etc.\n\nA general solution to such problems would be highly advantageous to game developers. Using other techniques like Monte Carlo tree search etc might be possible, but I guess it wouldn't be general enough or would be supervised in nature. *Here, all that is provided are gameplays.*\n\n\nThe game I have in mind is this grid based puzzle game. \n\nOrangle : https:\/\/play.google.com\/store\/apps\/details?id=hi.brett.orangle&amp;hl=en \n\nYoutube Trailer : https:\/\/www.youtube.com\/watch?v=dTsSh5fTItw\n\nThere is initial state. Direction moves like left, right, up and down which changes game state. Final goal state. Other states where no moves are possible (the game is stuck ) or the game goes into invalid state.\n\nDifficulty of puzzle can be approximately based on how many moves it requires to reach the goal state.\n \nIt would be good if the puzzles generated are as varied as possible. \nThis could be measured by sequence of game moves required to solve the level. We can have stats on the number of different blue boxes and their sizes.\n\nValid generated puzzles could be added back to the training data.\n\n\nSome questions that I have: \n\n1] Since plays of the game would vary in number of steps (lets say max moves 20), how do I take the input. What should be my input neural network? Would providing the initial state be enough and proceed from that? Or having a complicated network learn the gameplays will be more intelligent.\n\n2] Do I have to specify goal state\/invalid state\/stuck state differently or will the network figure it out from the sample gameplays?\n\n3] Would a structure similar to Generative Adversarial Networks be advantageous here? Instead of discriminator network, we would have a module which exhaustively plays the game to reach goal state and then determines the label. \n\n4] How do we make sure the initial states produced by the network do well\non the metrics i.e. the game is interesting, varied, requires some clever thinking?\n\nI am not sure if people have already looked at this problem. I don't know what terms\/keywords to search for. Any help would be highly appreciated.","flair":"four\tProject"}
{"author":"rd11235","created":"Fri Nov 11 03:45:41 EST 2016","text":"The Recurrent Batch Norm paper (https:\/\/arxiv.org\/abs\/1603.09025) reports 99.0% accuracy on flattened MNIST and 95.4% accuracy on flattened pMNIST. Does anyone know of other papers that do better? There's the Layer Norm paper (https:\/\/arxiv.org\/pdf\/1607.06450), but they only report results for non-flattened pMNIST using a CNN (and that layer norm doesn't help in that case).","flair":"one\tDiscussion"}
{"author":"ExUtumno","created":"Fri Sep 30 09:28:00 EDT 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 198 Star 6,339 Fork 237 mxgmn\/WaveFunctionCollapse Code Issues 1 Pull requests 1 Projects 0 Pulse Graphs Bitmap & tilemap generation from a single example with the help of ideas from quantum mechanics. 45 commits 1 branch 0 releases 7 contributors C# 100.0% C# Clone or download Clone with HTTPS Use Git or checkout with SVN using the web URL. Download ZIP Find file Branch: master Switch branches\/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit dd9f359 Nov 10, 2016 mxgmn sparse array optimization for the tiled model Permalink Failed to load latest commit information. samples better subset names, some optimizations Nov 7, 2016 .gitattributes 👾 Added .gitattributes & .gitignore files Sep 30, 2016 .gitignore 👾 Added .gitattributes & .gitignore files Sep 30, 2016 LICENSE.md added license file Oct 4, 2016 Main.cs text output option for the tiled model Oct 11, 2016 Model.cs better subset names, some optimizations Nov 7, 2016 OverlappingModel.cs better subset names, some optimizations Nov 7, 2016 README.md added a (link to a) very basic 3d version Nov 3, 2016 SimpleTiledModel.cs sparse array optimization for the tiled model Nov 10, 2016 Stuff.cs long index, overlapping model works with bigger N now Oct 7, 2016 samples.xml better subset names, some optimizations Nov 7, 2016 README.md WaveFunctionCollapse This program generates bitmaps that are locally similar to the input bitmap. Local similarity means that (C1) Each NxN pattern of pixels in the output should occur at least once in the input. (Weak C2) Distribution of NxN patterns in the input should be similar to the distribution of NxN patterns over a sufficiently large number of outputs. In other words, probability to meet a particular pattern in the output should be close to the density of such patterns in the input. In our examples typical value of N is 3. WFC initializes output bitmap in a completely unobserved state, where each pixel value is in superposition of colors of the input bitmap (so if the input was black & white then the unobserved states are shown in different shades of grey). The coefficients in these superpositions are real numbers, not complex numbers, so it doesn't do the actual quantum mechanics, but it was inspired by QM. Then the program goes into the observation-propagation cycle: On each observation step an NxN region is chosen among the unobserved which has the lowest Shannon entropy. This region's state then collapses into a definite state according to its coefficients and the distribution of NxN patterns in the input. On each propagation step new information gained from the collapse on the previous step propagates through the output. On each step the overall entropy decreases and in the end we have a completely observed state, the wave function has collapsed. It may happen that during propagation all the coefficients for a certain pixel become zero. That means that the algorithm have run into a contradiction and can not continue. The problem of determining whether a certain bitmap allows other nontrivial bitmaps satisfying condition (C1) is NP-hard, so it's impossible to create a fast solution that always finishes. In practice, however, our algorithm runs into contradictions surprisingly rarely. Watch a video demonstration of WFC algorithm on YouTube: https:\/\/youtu.be\/DOQTr2Xmlz0 Algorithm Read the input bitmap and count NxN patterns. (optional) Augment pattern data with rotations and reflections. Create an array with the dimensions of the output (called \"wave\" in the source). Each element of this array represents a state of an NxN region in the output. A state of an NxN region is a superpostion of NxN patterns of the input with boolean coefficients (so a state of a pixel in the output is a superposition of input colors with real coefficients). False coefficient means that the corresponding pattern is forbidden, true coefficient means that the corresponding pattern is not yet forbidden. Initialize the wave in the completely unobserved state, i.e. with all the boolean coefficients being true. Repeat the following steps: Observation: Find a wave element with the minimal nonzero entropy. If there is no such elements (if all elements have zero or undefined entropy) then break the cycle (4) and go to step (5). Collapse this element into a definite state according to its coefficients and the distribution of NxN patterns in the input. Propagation: propagate information gained on the previous observation step. By now all the wave elements are either in a completely observed state (all the coefficients except one being zero) or in the contradictive state (all the coefficients being zero). In the first case return the output. In the second case finish the work without returning anything. Tilemap generation The simplest nontrivial case of our algorithm is when NxN=1x2 (well, NxM). If we simplify it even further by storing not the probabilities of pairs of colors but the probabilities of colors themselves, we get what we call a \"simple tiled model\". The propagation phase in this model is just adjacency constraint propagation. It's convenient to initialize the simple tiled model with a list of tiles and their adjacency data (adjacency data can be viewed as a large set of very small samples) rather than a sample bitmap. GIF | GIFV Lists of all the possible pairs of adjacent tiles in practical tilesets can be quite long, so we implemented a symmetry system for tiles to shorten the enumeration. In that system each tile should be assigned with its symmetry type. Note that the tiles have the same symmetry type as their assigned letters (or, in other words, actions of the dihedral group D4 are isomorphic for tiles and their corresponding letters). With this system it's enough to enumerate pairs of adjacent tiles only up to symmetry, which makes lists of adjacencies for tilesets with many symmetrical tiles (even the summer tileset, despite drawings not being symmetrical the system considers such tiles to be symmetrical) several times shorter. Note that the unrestrained knot tileset (with all 5 tiles being allowed) is not interesting for WFC, because you can't run into a situation where you can't place a tile. We call tilesets with this property \"easy\". For example, Wang tilesets are easy. Without special heuristics easy tilesets don't produce interesting global arrangements, because correlations of tiles in easy tilesets quickly fall off with a distance. Btw, a lot of cool Wang tilesets can be found on cr31's site. Consider the \"Dual\" 2-edge tileset there. How can it generate knots (without t-junctions, not easy) while being easy? The answer is, it can only generate a narrow class of knots, it can't produce an arbitrary knot. Higher dimensions WFC algorithm in higher dimensions works completely the same way as in dimension 2, though performance becomes a big issue. These voxel models were generated with N=2 overlapping tiled model using 5x5x5 and 5x5x2 blocks and additional heuristics (height, density, curvature, ...). Higher resolution screenshots: 1, 2, 3. Voxel models generated with WFC and other algorithms will be in a separate repo. Constrained synthesis WFC algorithm supports constraints. Therefore, it can be easely combined with other generative algorithms or with manual creation. Here is WFC autocompleting a level started by a human: GIF | GIFV ConvChain algorithm satisfies the strong version of the condition (C2): the limit distribution of NxN patterns in the outputs it is producing is exactly the same as the distributions of patterns in the input. However, ConvChain doesn't satisfy (C1): it often produces noticeable artefacts. It makes sense to run ConvChain first to get a well-sampled configuration and then run WFC to correct local artefacts. This is similar to a common strategy in optimization: first run a Monte-Carlo method to find a point close to a global optimum and then run a gradient descent from that point for greater accuracy. P. F. Harrison's texture synthesis algorithm is significantly faster than WFC, but it has trouble with long correlations (for example, it's difficult for this algorithm to synthesize brick wall textures with correctly aligned bricks). But this is exactly where WFC shines, and Harrison's algorithm supports constraints. It makes sense first to generate a perfect brick wall blueprint with WFC and then run a constrained texture synthesis algorithm on that blueprint. Comments Why the minimal entropy heuristic? I noticed that when humans draw something they often follow the minimal entropy heuristic themselves. That's why the algorithm is so enjoyable to watch. The overlapping model relates to the simple tiled model the same way higher order Markov chains relate to order one Markov chains. Note that the entropy of any node can't increase during the propagation phase, i.e. possibilities are not arising, but can be canceled. When propagation step can not decrease entropy further, we activate observation step. If the observation step can not decrease entropy, that means that the algorithm has finished working. WFC's propagation phase is very similar to the loopy belief propagation algorithm. In fact, I first programmed belief propagation, but then switched to constraint propagation with a saved stationary distribution, because BP is significantly slower without a massive parallelization (on a CPU) and didn't produce significantly better results in my problems. Note that the \"Simple Knot\" and \"Trick Knot\" samples have 3 colors, not 2. One of the dimensions can be time. In particular, d-dimensional WFC captures the behaviour of any (d-1)-dimensional cellular automata. References This project builds upon Paul Merrell's work on model synthesis, in particular discrete model synthesis chapter of his dissertation. Paul propagates adjacency constraints in what we call a simple tiled model with a heuristic that tries to complete propagation in a small moving region. It was also heavily influenced by declarative texture synthesis chapter of Paul F. Harrison's dissertation. Paul defines adjacency data of tiles by labeling their borders and uses backtracking search to fill the tilemap. Ports, notable forks and other projects based on this work Emil Ernerfeldt made a C++ port. Max Aller is making a Kotlin (JVM) library, Kollapse. Kevin Chapelier made a JavaScript port. Oskar Stalberg programmed a 3d tiled model, a 2d tiled model for irregular grids on a sphere and is building beautiful 3d tilesets for them: 1, 2, 3, 4, 5, 6. Joseph Parker adapted WFC to Unity. Martin O'Leary applied a WFC-like algorithm to poetry generation: 1, 2, 3, 4. Nick Nenov made a 3d voxel tileset based on my Castle tileset. Nick uses text output option in the tiled model to reconstruct 3d models in Cinema 4D. Sean Leffler implemented the overlapping model in Rust. rid5x is making an OCaml version of WFC. I published a very basic 3d tiled model so people could make their own 3d tilesets without waiting for the full 3d repository. How to build WFC is a console application that depends only on the standard library. Build instructions from the community for various platforms can be found in the relevant issue. Casey Marshall made a pull request that makes using the program with the command line more convenient and includes snap packaging. Credits Some samples are taken from the games Ultima IV and Dungeon Crawl. Circles tileset is taken from Mario Klingemann. Idea of generating integrated circuits was suggested to me by Moonasaur and their style was taken from Zachtronics' Ruckingenur II. Summer tileset was made by Hermann Hillmann. Voxel models were rendered in MagicaVoxel. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"null\tnull"}
{"author":"techrat_reddit","created":"Wed Oct 12 14:10:36 EDT 2016","text":"Hello, fellow aficionados of machine learning!\n\n\/r\/LearnMachineLearning aims to provide a friendly environment to learn machine learning for redditors of various machine learning backgrounds. Whether you are a complete beginner who would like to begin your first machine learning project or a machine learning expert who wants to expand your boundary, anyone who wishes to *learn* machine learning is welcome. \n\nFeel free to share any educational resources of machine learning. An educational resource could be anything from a professional blog article to tips you would like to share to the fellow redditors. \n\nAlso, we are a beginner-friendly subreddit, so don't be afraid to ask questions! This can include questions that are non-technical such as a systematic approach to a machine learning problem. \n\nIf you have any questions or suggestions, please don\u2019t hesitate to comment on this thread or [the welcoming page](https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/56vloc\/welcome_to_rlearnmachinelearning\/) at \/r\/LearnMachineLearning","flair":"one\tDiscussion"}
{"author":"hardmaru","created":"Mon Oct 31 22:20:59 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.10099 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: Neural Machine Translation in Linear Time Authors: Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu (Submitted on 31 Oct 2016) Abstract: We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences. Comments: 11 pages Subjects: Computation and Language (cs.CL); Learning (cs.LG) Cite as: arXiv:1610.10099 [cs.CL]   (or arXiv:1610.10099v1 [cs.CL] for this version) Submission history From: Nal Kalchbrenner [view email] [v1] Mon, 31 Oct 2016 19:56:39 GMT (5893kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"dauntless26","created":"Sat Oct 22 14:42:06 EDT 2016","text":"I am using the blood transfusion dataset from UCI: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Blood+Transfusion+Service+Center\n\nNo matter what features I remove (or even just leaving in one random feature) I get about the same results: 77% accuracy. Why is this happening?","flair":"one\tDiscussion"}
{"author":"icancto","created":"Wed Oct 05 04:24:03 EDT 2016","text":"Hello ML. I am struggling. I have been learning about DNNs, supervised and unsupervised learning, RL, etc. for the past 3 months now. Whenever I start reading a paper or a book or any kind of lengthy text I get sidetracked and start thinking about something else. I can't focus. The stuff I'm reading is pretty difficult. The mathematical formulas, the fancy words, they overwhelm me. Also, I don't even memorize that much after a read. I don't feel I'm learning much.\n\nIt's rather peculiar, because no matter what area I studied in the past I pretty much excelled at everything.\n\nIs it normal to struggle at ML? Is it really this difficult at the beginning?","flair":"null\tnull"}
{"author":"__lava__","created":"Sun Nov 13 19:30:22 EST 2016","text":"Recently I decided to try my hand at the Extraction of product attribute values competition hosted on CrowdAnalytix, a website that allows companies to outsource data science problems to people with the skills to solve them. I usually work with image or video data, so this was a refreshing exercise working with text data. The challenge was to extract the Manufacturer Part Number (MPN) from provided product titles and descriptions that were of varying length \u2013 a standard RegEx problem. After a cursory look at the data, I saw that there were ~54,000 training examples so I decided to give Deep Learning a chance. Here I describe my solution that landed me a 4th place position on the public leaderboard. Because this was a winning submission, I cannot share code as per CrowdAnalytix\u2019s Solver\u2019s Agreement. Permission is however given to share the approach to the solution. From the competition website, \u201CThe objective of this contest is to extract the MPN for a given product from its Title\/Description using regex patterns.\u201D Now, I didn\u2019t know what RegEx patterns were, but I could understand the problem of extracting text from a larger text. For my purposes, given that I wanted to learn representations, it was enough for me to understand that if I had the following: Then I just wanted to extract the MPN \u201C08GP46180KR\u201D using some representations that learned to distinguish MPNs from other text making up the product title and description. Here\u2019s the basic gist of approaching this problem using RegEx: you hard-code some rules for patterns that you are interested in finding. Here\u2019s an example for finding e-mail addresses: Here, this RegEx looks for pre-defined characters in fields surrounding the \u201C@\u201D and \u201C.\u201D characters. The power of Deep Learning is that, provided enough training examples, we can learn these RegEx patterns from the data directly instead of hard-coding them. This is the approach that I took. The training data consisted of ~54,000 examples with the following four entries: [id, product_title, product_description, mpn]; test data was the same except for the omission of MPN field. Upon inspection of the data, I found that the MPN was, in almost all cases, present in either the product title or description, if not both. It also became evident to me that this was a hard problem as there were many other \u201Cdistractors\u201D that looked very similar to MPNs but were not marked as the target (for example, in the above Graphics Processing Unit product, \u201CGddr5x\u201D looks a lot like other MPNs that existed in the training set). Given that the problem was to extract the MPN from the other fields, I set the input as a concatenation of the product title and description and set the target (or output) as the MPN. Now that I had determined what my inputs and outputs were, I needed to determine some sort of embedding so that I could use a neural network. Because this was not a usual Natural Language Processing problem do the presence of MPN codes, HTML snippets and other odd characters, common choices such as word2vec were not going to be suitable (correct me if I\u2019m wrong here). I fortunately had a rock-climbing buddy, Joseph Prusa, that had been working with character-wise embeddings for sentiment analysis (Prusa & Khoshgoftaar, 2014). He very kindly shared his embedding code, and after some custom-tailoring to my problem, I had an embedding solution. The embedding procedure takes each character and embeds it as an 8-bit binary vector. For example the string \u201CEVGA NVIDIA GeForce GTX 1080 Founders Edition 8gb Gddr5x 08GP46180KR\u201D from the above example would be represented like such: The next problem was that inputs (i.e., the concatenated product title and description string) were of varying length. Thus, I figured that I needed to settle on some way to make them all the same length to feed to the network. My first step was to visualize the distribution of all the input lengths. Based on this distribution, I chose to set the max length to 2000 as it included most examples and avoided very long inputs to only include a couple outliers. With this max length set, I first clipped each string input and then embedded it using the procedure above. In the case that an input was shorter than the max length, it was padded with zeros. In the case that it was longer, if the MPN code was within the range of the max length, then no problem, if it was, then it was just another case where the MPN code was absent (which was very infrequent as well). The result of all this is a 8 x 2000 \u201Cimage\u201D that can now be fed to the model. Assuming that we want to build some neural network that we can train using back-propagation, the next question is what is the appropriate output and loss function. The most natural choice seemed to be that the output would be just the MPN in the embedded vectorial format. This, in combination with a loss like Mean-Squared Error that is common of generative models in unsupervised learning just did not do the trick due to technical reasons. Eventually I converged on the following solution that was sufficient to get some reasonable results. Namely, I defined the output of the network to be two one-hot binary vectors with a length equal to the max length (set to 2000 here), where the first vector indicated the starting index of the MPN and the second vector indicated the ending index of the MPN. Then the loss was simply the summed categorical crossentropy for both vectors. Given this output, an auxiliary function was then created on the backend to extract the MPN vectorial representation from the input given the two indices and then convert the embedded MPN back to a string representation as the final output. In the cases where no MPN was present, the target was defined as ones at the end of both vectors. Ok, so now that the data has been embedded, and our target has been formulated, the next step was to build a model that would perform the above task well. I tried a bunch of different neural network models, including deep convolutional neural networks with standard architectures (e.g., 2D conv-net with max-pooling layers). These produced good but unsatisfactory results \u2013 nothing that was going to get me a winning spot. Fortunately, Google DeepMind had just put out a paper on their new model WaveNet that used causal, dilated convolutions that served as the seed of my idea. WaveNet, and other similar models, were very intriguing because they used multiple layers of convolutional operators with no pooling that were able to obtain filters with very large receptive fields while keeping the number of parameters within a reasonable range because of the dilations used at each subsequent layer (see the red portion of the figure below; image source \u2013 Neural Machine Translation in Linear Time). The final model idea that I converged on was to extract a set of basic features from the input, feed them through a series of dilated convolutions and then branch off two convolutional filters with softmax activation functions to predict start and end indices. In more detail, the model was as follows: The model architecture is represented graphically below, showing the major features of the model. After training, I observed that the model was close to perfect on the training set, hovered around ~90% accuracy for the validation set, and obtained ~84% on the public leaderboard. Not bad! One thing that I noticed as I was scrambling to make submissions was that the model overfit the data very quickly due to the relatively small number of samples. I know that with only ~54,000 training examples, learning representations directly from the data was a bit risky, but I believe with a couple hundred thousand, my solution might have placed higher. Because I was late to the competition, I just chose to lower the learning rate and only train for a couple of epochs, which in the end worked out for me. However, provided that there was more time, I would have liked to explore some data augmentation techniques and model regularization which would have helped made the model more expressive and prevented overfitting. Additionally, pretraining on other text might have been a successful strategy. A brute-force effort would have also been increasing the max length parameter slightly, that may have given me some marginal improvements, but at a very high computational cost. This was a fun challenge for me and I found it satisfying to place especially given that I had not really worked on this type of problem before. Sorry in advance for adding to the Deep Learning hype, but I found this to be another interesting application of said methods to a domain that probably doesn\u2019t see much of these techniques used, again showing the general abilities of Deep Learning. Hope this helps someone with a similar problem.","flair":"three\tResearch"}
{"author":"Kiuhnm","created":"Wed Oct 26 10:53:41 EDT 2016","text":"I'm trying to read [Understanding Deep Convolutional Networks](https:\/\/arxiv.org\/abs\/1601.04920), but I'm struggling because I'm completely new to concepts such as lie groups, diffeomorphisms, wavelets and scattering transforms.\n\nI don't have the time to study all that math properly and I don't think it'd be worth it. I have no problems with delta-epsilon definitions and abstractions like groups, rings, etc..., but I don't want to learn theorems and concepts which aren't strictly relevant.\n\nIs there something like an *extended* version of that paper or a book which gives you just enough to understand all those concepts?\n\n---\n\nedit: The book [Functions, Spaces, and Expansions](https:\/\/www.amazon.com\/Functions-Spaces-Expansions-Mathematical-Engineering-ebook\/dp\/B00FBG7A8I\/keywords=wavelets+linear) seems like a good choice since the prerequisites are modest enough.","flair":"one\tDiscussion"}
{"author":"bjornsing","created":"Sat Oct 29 16:34:11 EDT 2016","text":"First of all, a big thanks for the great response to the previous post in this series. For a brief time it occupied the top spot in Reddit\u2019s \/r\/Statistics and \/r\/MachineLearning. The last post ended with a speculation on next steps and a request for collaborators. Since most of the interest has come from academics I\u2019m leaning towards a paper as the primary deliverable, with an implementation submitted to the Linux kernel coming in a strong second place. The other day I went out and bought a TP\u2011Link TL\u2011WDN4800 Wi\u2011Fi card for my desktop and a TP\u2011Link TL\u2011WA901ND v4 wireless access point. Luckily it was very straightforward to install LEDE (a fork of OpenWrt) on the access point, and the Wi\u2011Fi card just worked instantly with Ubuntu 16.04. I\u2019ve also set up a GitHub repository for the project. So far it contains a Makefile to make it easy to clone the LEDE git repository, apply some patches and build firmware for a number of consumer Wi\u2011Fi routers, among them my TP\u2011Link TL\u2011WA901ND v4. The patches make Minstrel sample a lot more and log the result of transmissions. A very simple script called log2csv.py takes log prints and turns them into comma-separated values, fit for consumption in a Jupyter notebook. In order to make it easy to jump in without buying gear I\u2019ve added a couple of datasets to the git repo, so you can just and start hacking away. :) So what should be the aim of the explorative data analysis? Well, my first hunch is that there\u2019s probably a significant difference between the probability of a transmission with a particular set of radio parameters succeeding, and the conditional probability of that same transmission succeeding given that another transmission has just failed. Since Minstrel does not exploit this pattern in the data it could be \u201Clow hanging fruit\u201D for a new rate control algorithm. If you want to explore the above hypothesis, or if you have one of your own, feel free to and send me a pull request! Want to join in the fun, and get your name on a research paper? Shoot me an email at bjorn@openias.org or fill in the form below!","flair":"four\tProject"}
{"author":"downtownslim","created":"Sun Oct 09 15:11:53 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.01685 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.RO < prev | next > new | recent | 1610 Change to browse by: cs cs.CV cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Robotics Title: Supervision via Competition: Robot Adversaries for Learning Tasks Authors: Lerrel Pinto, James Davidson, Abhinav Gupta (Submitted on 5 Oct 2016) Abstract: There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success\/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots. Comments: Submission to ICRA 2017 Subjects: Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG) Cite as: arXiv:1610.01685 [cs.RO]   (or arXiv:1610.01685v1 [cs.RO] for this version) Submission history From: Lerrel Pinto Mr [view email] [v1] Wed, 5 Oct 2016 23:28:12 GMT (7292kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"ijenab","created":"Fri Oct 21 22:13:45 EDT 2016","text":"Hey reddit! I was reading the proof of ADAM (link to paper: https:\/\/arxiv.org\/pdf\/1412.6980v8.pdf). Regret seems to shrink as beta1 gets smaller and gets closer to 0. So, in the proof we can just put beta1 to zero and everything still works fine, right? Am I missing something? Thanks :)   \n","flair":"one\tDiscussion"}
{"author":"liyi14","created":"Fri Nov 25 00:23:39 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.07715 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Deep Feature Flow for Video Recognition Authors: Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei (Submitted on 23 Nov 2016) Abstract: Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1611.07715 [cs.CV]   (or arXiv:1611.07715v1 [cs.CV] for this version) Submission history From: Jifeng Dai [view email] [v1] Wed, 23 Nov 2016 10:06:30 GMT (3437kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"kh40tika","created":"Fri Nov 11 05:17:50 EST 2016","text":"This is a naive implementation of DeepMind's Differentiable Neural Computer model. Clone this repository first. Then run script in interactive mode. NOTE: If you don't run in interactive mode it will just quit silently. Compilation in theano is a bit slow so it may take a while. Once compiled, you can just call python functions defined in . The default training task is the conditional copy task, the network must produce input signal in same or reversed order, depending on the first \"condition\" signal. You may want to modify the code to introduce new tasks. Most useful functionalities are shown below:","flair":"four\tProject"}
{"author":"spoodmon97","created":"Fri Nov 04 17:45:32 EDT 2016","text":"#Stylit allows you to transfer your personal artistic style from paper to screen, making your digital art appear like it was colored by hand. Presented live during the Adobe MAX 2016 Sneak Peeks, co-hosted by Jordan Peele. Learn more about this year's Sneak Peeks here: http:\/\/adobe.ly\/2ffyderSubscribe: https:\/\/www.youtube.com\/user\/adobecre... LET\u2019S CONNECTFacebook: http:\/\/facebook.com\/adobecreativecloud Twitter: http:\/\/twitter.com\/creativecloud Instagram: http:\/\/www.instagram.com\/adobecreativ...Adobe Creative Cloud gives you the world's best creative apps so you can turn your brightest ideas into your greatest work across your desktop and mobile devices.","flair":"two\tNews"}
{"author":"gabrielgoh","created":"Fri Nov 18 15:25:27 EST 2016","text":"The Google Neural Machine Translation paper (GNMT) describes an interesting approach towards deep learning in production. The paper and architecture are non-standard, in many cases deviating far from what you might expect from an architecture you'd find in an academic paper. Emphasis is placed on ensuring the system remains practical rather than chasing the state of the art through typical but computationally intensive tweaks. To understand the model used in GNMT we'll start with a traditional encoder decoder machine translation model and keep evolving it until it matches GNMT. The GNMT evolution seems primarily motivated by improving accuracy while maintaining practical production speeds for both training and prediction. The encoder decoder architecture started the recent neural machine translation trend. I'd refer to it as old school if it were more than a few years old. Shockingly, as the name implies, there are two components - an encoder and a decoder. For a word level encoder-decoder machine translation system, such as the depiction below: The encoder decoder showcased the potential that neural based machine translation may provide. Even with modern complex neural machine translation architectures, the majority of them can still be decomposed in terms of the encoder-decoder architecture. There are two primary drawbacks to this architecture, both related to length. First, as with humans, this architecture has very limited memory. That final hidden state of the LSTM, which we call S, is where you're trying to cram the entirety of the sentence you have to translate. S is usually only a few hundred units (read: floating point numbers) long - the more you try to force into this fixed dimensionality vector, the more lossy the neural network is forced to be. Thinking of neural networks in terms of the \"lossy compression\" they're required to perform is sometimes quite useful. Second, as a general rule of thumb, the deeper a neural network is, the harder it is to train. For recurrent neural networks, the longer the sequence is, the deeper the neural network is along the time dimension. This results in vanishing gradients, where the gradient signal from the objective that the recurrent neural network learns from disappears as it travels backwards. Even with RNNs specifically made to help prevent vanishing gradients, such as the LSTM, this is still a fundamental problem. Caption: Image from Bahdanau et al.'s \"Neural Machine Translation by Jointly Learning to Align and Translate\" (2014) showing the impact on translation scores (in the form of BLEU scores) as sentences get longer. While it may work for our relatively short example above, it's likely to begin failing as the sentence gets longer. One obvious solution for the memory problem would be increasing the hidden size of the LSTM. Unfortunately, training rapidly becomes impractical if we do this. As you increase the hidden size h of the LSTM, the number of parameters increases quadratically! You'll either start running out of memory on your GPU or have training take an eternity. This also doesn't solve the vanishing gradient problem... How do we solve these problems? We might try turning to a technique humans naturally perform - iterative attention to relevant parts of the source sentence. If you were translating a long sentence, you'd likely glance back at the source sentence to ensure you're capturing all the details. We can allow neural networks to do exactly the same. By storing and referring to the previous outputs of the LSTM we can increase the storage of our neural network without changing the operation of the LSTM. Additionally, if we know we're interested in a particular part of the sentence, the attention mechanism acts as a \"shortcut\" so we can provide a supervision signal that doesn't have to traverse the large number of timesteps that would result in a vanishing gradient. This is akin to how a human might answer a question after having just finished reading all of Lord of the Rings. If I was to ask a specific question about the start of the story, at Bag End, a human would know where to flick open the first book and retrieve the answer from. The length of the book series doesn't impair your ability to perform that lookup. Briefly (as attention mechanisms are already well covered elsewhere) the idea is, once you have the LSTM outputs from the encoder stored, you query each output asking how relevant they are to the current computation on the decoder side. Each output from the encoder then gets a score of relevance which we can turn into a probability distribution that sums up to one via the softmax activation. We can then extract a context vector that's a weighted summation of the encoder outputs depending on how relevant we think they are. A drawback to the attention mechanism is that we now have to perform a calculation over all of the encoded source sentence for each and every output of the decoder. While this is likely fine for translating between sentences, it can become problematic for long inputs. In computing terminology, if your source sentence is of length \\(N\\) and your target sentence of length \\(M\\), we've just take the decoder from \\(O(M)\\) in the encoder-decoder architecture to \\(O(MN)\\) in the attention architecture. While not optimal the advantages of the attention mechanism far outweigh the disadvantages, at least for this task. Note: You may notice that the direct connection between the encoder and decoder (S in our previous architecture) has disappeared. While many standard encoder-decoder architectures maintain this direct connection, the GNMT architecture decides to remove it. The GNMT architectures decides to make the attention mechanism the only way that information can be shifted from the encoder side to the decoder side. Caption: Image from Bahdanau et al.'s \"Neural Machine Translation by Jointly Learning to Align and Translate\" (2014) showing the impact on translation scores (in the form of BLEU scores) when using attention. RNNsearch is the architecture with attention and the number afterwards indicates how long the training examples were. While the attention mechanism allows for retrieving different parts of the sentence depending on the decoding context, there's still an issue. The attention mechanism is essentially asking the stored outputs of the encoder \"are you relevant to this?\" and using that to determine what information to extract. If the encoder outputs don't have sufficient context themselves, they're unlikely able to provide a good answer. Adding information about future words, such that the encoder output is determined by words on both the left and the right, is clearly beneficial. If a full sentence is available, humans would almost certainly use the full context to determine the meaning and context of a word. Why would we force computers, who are already at a substantial handicap, to not use all available information? The easiest way to add this bi-directionality is to run two RNNs - one that goes forward over the sentence and another that goes backwards. Then, for each word, we can either concatenate or add the resulting vector outputs, producing a vector with context from both sides. A bi-directional encoder becomes even more important if you're translating to or from a language that has a different ordering (i.e. right-to-left). The GNMT architecture concatenates them - potentially advantageous as that results in the both the forward and backward RNN being only half the size. Given that the bi-directional layer ends up being a bottleneck in GNMT, and the number of parameters in an RNN scale quadratically, this is not an insignificant saving. For many architectures in neural machine translation, increased depth is a key component in accurate models. The GNMT architecture trends this direction too by adding a large number of layers. A pretty darn crazy large number of layers - 8 on the encoder and 8 on the decoder for 16 total. Many state of the art machine translation systems use far less than this. For the encoder, their model has one bi-directional RNN layer followed by seven uni-directional RNN layers. The decoder has eight uni-directional RNN layers. In most papers I'd have instead expected all of the layers to be bi-directional for improved accuracy. A model with all bi-directional layers would be expected to get the same or higher results. The next section explains why the GNMT model variant strayed away from this. Recurrent neural networks of this depth are highly problematic to train. Indeed, standard neural networks of this depth are not trivial to train either, and those are relatively simple compared to what happens over the many timesteps. We'll look to solve this problem in a later variant. A primary motivator for the GNMT architecture was practicality. This forces some limitations and odd choices when compared to standard encoder-decoder architectures. For launching a system like GNMT into production, being parallelizable is a requirement. It makes not only training faster, allowing more experiments, but also makes production deployments faster too. The graph we've been looking at represents not only the architecture of the machine translation model but also a dependency graph. To begin computation at one of the nodes, all of the nodes pointing toward you must already have been computed. Even if you had an infinite amount of computation, you still need to follow the flow of the dependency graph. As such, you want to minimize any dependencies that may take far more computation than others at a similar level. Note: Unshaded nodes have not yet been completed and nodes shaded white have finished their computation. Layers shaded blue are in the process of being computed or have finished being computed. This is the reason that only a single bi-directional RNN layer is used. A bi-directional layer has to run two RNNs - one from left to right and the other from right to left. This means to compute the first output you have to wait for computations from the far right hand side to reach you (where is the length of sequence). If all layers were bi-directional, the entirety of that layer would have to finish before any later dependencies could start computing. If we use uni-directional layers however, our computations can be more flexible and more parallel. In the example above, focusing just on the encoding side, the first four layers are all computing nodes at the same time. This is only possible as the layer above doesn't rely on all of the nodes in the layer below, only those directly below it. This would not be possible if these were bi-directional layers. Parallelization (decoder side): Due to the attention mechanism using all the outputs of the encoder side, the decoder side must wait until all of the encoder has finished running. When the encoder has finished however, the decoder can perform its operations in the same parallel way as the encoder side. Usually the attention mechanism would use the top most output of the decoder to query the attention mechanism but the GNMT architecture uses the lowest decoder layer for querying the attention mechanism to minimize the dependency graph and allow for maximal parallelization. In the example above, if we used the topmost decoder layer for attention, we would not have been able to start computing the second output of the decoder yet, as the first output of the decoder is still in progress. Side note (teacher forcing and training vs production): During training, we already known what the English sentence should translate to. This allows us to have higher levels of parallelism than at prediction time. As we already have all of the correct words (the \"known\" translation at the bottom right), we can use it in teacher forcing. Teacher forcing is where you give the neural network the correct answer for the next word even if it would have actually guessed the wrong word. This makes sense as training will keep forcing the network towards outputting the correct word, so eventually our assumption should hopefully be correct. This allows you to \"cheat\" and be computing the second output word while still in the process of computing the first output word. In the real word, we need to wait to produce each word one by one, as there's no \"known\" translation. Side note (multiple GPUs): This parallelization really only strongly makes sense with multiple GPUs. In the GNMT paper, their diagrams actually label each of the layers according to the GPU they use. For the encoder and decoder, they use eight GPUs - essentially one for each layer. Why doesn't this make as much sense for a single GPU? Generally a GPU should be hitting high utilization when computing a given layer, assuming you're able to set reasonable batch sizes, network sizes, and sequence lengths. This work is primarily about re-ordering dependencies such that more computation can be done at once, allowing for better utilization of more devices. You're hopefully already fully utilizing your single GPU well, so being able to compute more at the same time won't help you if you don't have that computation spare. If the production deployment was on CPUs, this re-ordering may still be quite beneficial. Eight layers is pretty darn deep - at least for recurrent neural networks. Returning to our general rule of thumb - barring some exceptions or specific constructions - the deeper a network is, the harder it is to train. While this is for a variety of reasons, the most intuitive is that the further a gradient has to travel, the more it risks vanishing or exploding. Luckily there are many potential ways to tackle this problem. One solution for vanishing gradients is residual networks, which has been applied most famously to CNNs such that training neural networks hundreds of layers deep remains feasible. The idea is relatively simple. By default, we like the idea of a layer computing an identity function. This makes sense. If you do well with one layer, you don't expect a two or three to do worse. At worst, the second and third layer should just learn to \"copy\" the output of the first layer - no modifications. Hence, they just need to learn an identity function. Unfortunately, learning the identity function seems non-trivial for most networks. Even worse, later layers confuse training of earlier layers as the supervision signal - the direction it's meant to go - keeps shifting. As such, the first layer may fail to train well at all if there are more layers below it. To solve this, we bias the architecture of each of these layers towards performing the identity function. We can do this by only allowing the later layers to add deltas (updates) to the existing vector. Now, if the next layer is lazy and outputs nothing but zeroes, that's fine, as you'll still have the original vector. $$ \\text{Starting by processing } x \\\\ h_1 = f_1(x) + x \\\\ h_2 = f_2(h_1) + h_1 = f_2(h_1) + f_1(x) + x \\\\ h_3 = f_3(h_2) + h_2 = f_3(h_2) + f_2(h_1) + h_1 = f_3(h_2) + f_2(h_1) + f_1(x) + x \\\\ h_3 = x + \\delta_1 + \\delta_2 + \\delta_3 \\\\ \\text{(if all values for }\\delta\\text{ are zero, we still end up with } x \\text{)} $$ All of these changes build upon their previous iteration to result in the full architecture described in the GNMT paper. Notice that, while far more complex, it still has the same components of the original encoder-decoder architecture. It may look scary, but each change is motivated by a relatively simple idea. The Google Neural Machine Translation architecture is an interesting beast. While there's nothing tremendously novel in it, the real innovation is the care to which the architecture has been engineered. If this was a boat, it'd be a chrome speed boat that slices through rough waters with near zero drag. We're already seeing the architecture used in a variety of contexts, both in translating and generating natural language, likely due to the ability to scale to computationally large and intensive tasks cleanly. For that reason, I expect to see it pop up in far more places. This article also contains only a small portion of the paper. Not discussed here is what BLEU is, how wordpiece level granularity improves translation over word level, advantages\/disadvantages of BLEU, quantization of models for faster models during deployment, or jumping between optimization algorithms for better convergence, or that their datasets are so large they don't use dropout! All of this without mentioning the zero-shot GNMT paper or using the GNMT architecture for conversation models... Gosh there's a lot to write about! If you like this content, I share similar things on Twitter a few hundred characters at a time ;) My sincere thanks to:","flair":"three\tResearch"}
{"author":"antirabbit","created":"Sat Nov 05 11:18:35 EDT 2016","text":"I am currently using a pretty weak GPU (1 GB VRAM) and a low sample size (57 image classes, ~ 20-100 samples per class) for an image sketching application on my website. **[Link to application](http:\/\/maxcandocia.com\/app\/sketch-and-guess\/)**. All of the training data is crowd-sourced from submissions within the application.\n\nThe images are 400x400, and they are shrunk down to 200x200. For some reason, the best performance I have gotten is starting with a 15x15 convolution with a 6x6 pooling layer with a 4x4 stride.\n\n[Layer 1](http:\/\/imgur.com\/EV4YILx) has an interesting pattern where every other column and row tends to be zero in the convolution (probably due to redundant information and regularization).\n\nHere is [layer 2](http:\/\/imgur.com\/eQA3enk) and [layer 3](http:\/\/imgur.com\/ernf3yi).\n\nI currently have a series of linear and nonlinear distortions for input images to artificially increase the sample size, and I've combined dropout, momentum, stochastic gradient descent, and a small L2 regularization in the network. All of the layers use tanh activation.\n\nI am planning on purchasing better hardware in the near future, but smaller networks are somewhat ideal because the application on my website uses a CPU and is not extremely powerful.\n\n**[Source code](https:\/\/github.com\/mcandocia\/SketchClassification)**\n\nDoes anyone have any feedback or questions about the network architecture or the application?","flair":"four\tProject"}
{"author":"DanielleMolloy","created":"Wed Sep 28 11:20:52 EDT 2016","text":"We explore the following crucial question: how could brains potentially perform the kind of powerful credit assignment that allows hidden layers of a very deep network to be trained and that has been so successful with deep learning recently? Global reinforcement learning signals have too much variance (scaling with the number of neurons or synapses) to be credible (by themselves) from a machine learning point of view. Concerns have been raised about how something like back-propagation could be implemented in brains. We present several intriguing results all aimed at answering this question and possibly providing pieces of this puzzle. We start with an update rule that yields updates similar to STDP but that is anchored in quantities such as pre-synaptic and post-synaptic firing rates and temporal rates of change. We then show that if neurons are connected symmetrically and thus define an energy function, (a) their behaviour corresponds to inference, i.e., going down the energy, and (b) after a prediction is made on a sensor and an actual value is observed, the early phases of inference in this network actually propagate prediction error gradients, and (c) using the above STDP-inspired rule yields a gradient descent step on prediction error for feedforward weights. This is based on a new mathematical result which provides a more general framework for machine learning to train dynamical systems at equilibrium. Finally, we discuss some of the limitations of the current model (such as the forced symmetry of synaptic weights and the question of learning the full joint distribution and not just a point prediction, and how to train dynamical systems which are generally not near their equilibrium points) as well as ideas around them. You can subscribe to our weekly seminar email list by sending an email to majordomo@lists.berkeley.edu that contains the words subscribe redwood in the body of the message. (Note: The subject line can be arbitrary and will be ignored)","flair":"null\tnull"}
{"author":"minimum_liklihood","created":"Thu Nov 17 09:14:19 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1611.05209 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1611 Change to browse by: cs cs.CV cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Deep Variational Inference Without Pixel-Wise Reconstruction Authors: Siddharth Agrawal, Ambedkar Dukkipati (Submitted on 16 Nov 2016) Abstract: Variational autoencoders (VAEs), that are built upon deep neural networks have emerged as popular generative models in computer vision. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate, leading to tremendous progress. However, there have been limited efforts to replace pixel-wise reconstruction, which have known shortcomings. In this work, we use real-valued non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the data given the latent distribution. We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures, on image modeling tasks. As part of our model, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers. Subjects: Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG) Cite as: arXiv:1611.05209 [stat.ML]   (or arXiv:1611.05209v1 [stat.ML] for this version) Submission history From: Siddharth Agrawal [view email] [v1] Wed, 16 Nov 2016 10:20:10 GMT (2149kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"ResNets","created":"Wed Oct 12 11:21:23 EDT 2016","text":"cs294-dl-f16@googlegroups.com Please sign up for the course mailing list for future updates. If you do not plan to take the class, but are interested in getting announcements about guest speakers in class, and more generally, deep learning talks at Berkeley, please sign up for the talk announcement mailing list for future announcements. Please also sign up for our Piazza.","flair":"three\tResearch"}
{"author":"AutoModerator","created":"Wed Oct 12 11:52:22 EDT 2016","text":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n","flair":"null\tnull"}
{"author":"TotemCaster","created":"Fri Nov 25 23:24:55 EST 2016","text":"Torch implementation for learning a mapping from input images to output images, for example: On some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). The test results will be saved to an html file here: . Switch to to train translation in opposite direction. Models are saved to (can be changed by passing in train.lua). See in train.lua for additional training options. This will run the model named in direction on all images in . Result images, and a webpage to view them, are saved to (can be changed by passing in test.lua). See in test.lua for additional testing options. Download the datasets using the following script (more datasets are coming soon!): We require training data in the form of pairs of images {A,B}, where A and B are two different depicitions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A: Create folder with subfolders and . and should each have their own subfolders , , , etc. In , put training images in style A. In , put the corresponding images in style B. Repeat same for other data splits (, , etc). Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g. is considered to correspond to . Once the data is formatted this way, call: This will combine each pair of images (A,B) into a single image file, ready for training. Optionally, for displaying images during training and test, use the display package. By default, the server listens on localhost. Pass to allow external connections on any interface: Then open in your browser to load the remote desktop. If you use this code for your research, please cite our paper Image-to-Image Translation Using Conditional Adversarial Networks: Code borrows heavily from DCGAN. The data loader is modified from DCGAN and Context-Encoder.","flair":"four\tProject"}
{"author":"benjaminwilson","created":"Wed Oct 19 10:27:49 EDT 2016","text":"Having trained a model, it is natural to want to understand how it works. An intuitively appealing approach is to consider data samples that maximise the activation of a hidden unit, and to take the common input features of these samples as an indication of what that unit has learned to recognise. However, as we\u2019ll see below, it is a misconception to speak of hidden units if: In such a scenario, the hidden feature space must instead be considered as a whole. Consider the task of factorising a matrix as a product of matrices with some fixed inner dimension . The model parameters are pairs of matrices with the appropriate dimensions, and the image of an input vector on the hidden layer is given by . To consider this vector in terms of hidden unit activations is to fix a co-ordinate system in the hidden feature space, and to measure the displacement of the vector along each co-ordinate axis. If denote the unit vectors corresponding to the chosen co-ordinate system, then the displacements are given by the inner products We show below that if is any rotation of the hidden feature space, then the model parameters are just as likely as to result in the factorisation of a fixed matrix and that which of these occurs depends only on the random initialisation of gradient descent. Thus the hidden unit activations might just as likely have been given by The hidden unit activations given by 1 and 2 can be very different indeed. In fact, since is an orthogonal transformation, we have (see e.g. here). Thus the indeterminacy of the model parameters, i.e. vs. , might equivalently be thought of as an indeterminacy in the orientation of the co-ordinate system, i.e. the vs. the . The choice of orientation of co-ordinate basis is completely arbitrary, so speaking of hidden unit activations makes no sense at all. The above holds more generally for an orthogonal transformation of the hidden feature space, i.e. for a composition of rotations and reflections. None of the above is new. For example, it was stated by Szegedy et al. in an empirical study of the interpretability of hidden units. We are demonstrating, step-by-step, a statement of theirs (which was about word2vec): Given a matrix and an inner dimension , the task of matrix factorisation is to learn two matrices and whose product approximates : The parameter space consists of the entries of the matrices and . The hidden feature space, on the other hand, is the k-dimensional space containing the columns of and . To train a matrix factorisation model using gradient descent, the model parameters are repeatedly updated using the gradient vector of the error function. An example error function could be Notice that this choice of error function doesn\u2019t depend directly on the pair of matrices , but rather only on their product , i.e. only on the approximation of . This is true of any error function , because error functions depend only on inputs and outputs. Recall that orthogonal transformations of a space are just compositions of rotations and reflections about hyperplanes passing through the origin. Considered as matrices, orthogonal transformations are defined by the property that their product with their transpose gives the identity matrix. Using this property, it can be seen that an orthogonal transformation of the hidden feature space defines an orthogonal transformation of the parameter space by acting simultaneously on the column vectors of the matrices. If and denote the groups of orthogonal transformations on the hidden feature space and the parameter space, respectively, then: The effect of this block-diagonal orthogonal transformation on the parameter space corresponds to multiplying the matrices and on the left by the orthogonal transformation of the feature space, i.e. it effects . Notice that and yield the same approximation to the original matrix , since: Thus , so the orthogonal transformations of the hidden feature space trace out contour lines of in the parameter space. Now the gradient vector is always perpendicular to the contour line, so the sequence of points in the parameter space visited during gradient descent preserve the orientation of the hidden feature space set at initialisation (see here, for example). So if gradient descent of starting at the initial parameters converges to the parameters , and you\u2019d prefer that it instead converged to , then all you need to do is start the gradient descent over again, but this time with the initial parameters . We thus see that the matrices that our matrix factorisation model has learned are only determined up to an orthogonal transformation of the hidden feature space, i.e. up to a simultaneous transformation of their columns. The above statements continue to hold in the case of stochastic gradient descent, where the error function is not fixed but rather defined by varying mini-tasks (an instance being e.g. word2vec). Such error functions still don\u2019t depend upon hidden layer values, so as above their gradient vectors are perpendicular to the contour lines traced out by the orthogonal transformations of the hidden layer. Thus the updates performed in stochastic gradient descent also preserve the original orientation of the feature space. How likely is it that initial parameters, transformed via an orthogonal transformation as above, ever occur themselves as initial parameters? In order to conclude that the orientation of the co-ordinate system on the hidden layer is completely arbitrary, we need it to be precisely as likely. Thus if denotes the probability distribution on the parameter space from which the initial parameters are drawn, we require for any initial parameters and any orthogonal transformation of the hidden feature space. This is not the case with word2vec, where each parameter is drawn independently from a uniform distribution. However, it remains true that for any choice of initial parameters, there will still be any number of possible orientations of the co-ordinate system, but for some choices of initial parameters there is less freedom than for others. GloVe performs weighted matrix factorisation with bias terms, so the above should apply. The weighting is just a modified error function, and the bias terms are not hidden features and so are left unmodified by its orthogonal transformations. Like word2vec, GloVe initialises each parameter with independent samples from uniform distribution, so there are no new problems there. The real problem with applying the above analysis to GloVe is that the implementation of Adagrad used makes the learning regime dependent on the choice of basis of the hidden feature space (see e.g. here). This doesn\u2019t mean that the hidden unit activations of GloVe make sense, it just means that GloVe is less amenable to theoretical arguments like those above and needs to be considered empirically e.g. in the manner of Szegedy et al.","flair":"null\tnull"}
{"author":"tuan3w","created":"Sun Nov 13 10:43:24 EST 2016","text":"Currently, I'm working on project that need to extract real estate information from various sources. My best way I can think of is using regexes. I love deep learning approaches and would like to give it a try but I haven't found any good approach yet. Can you give me some suggestions on this?\nThank you so much.\n","flair":"one\tDiscussion"}
{"author":"evc123","created":"Tue Nov 08 18:25:31 EST 2016","text":"Chainer is an open source framework designed for efficient research into and development of deep learning algorithms. In this post, we briefly introduce Chainer with a few examples and compare with other frameworks such as Caffe, Theano, Torch, and Tensorflow. Most existing frameworks construct a computational graph in advance of training. This approach is fairly straightforward, especially for implementing fixed and layer-wise neural networks like convolutional neural networks. However, state-of-the-art performance and new applications are now coming from more complex networks, such as recurrent or stochastic neural networks. Though existing frameworks can be used for these kinds of complex networks, it sometimes requires (dirty) hacks that can reduce development efficiency and maintainability of the code. Chainer\u2019s approach is unique: building the computational graph \"on-the-fly\" during training. This allows users to change the graph at each iteration or for each sample, depending on conditions. It is also easy to debug and refactor Chainer-based code with a standard debugger and profiler, since Chainer provides an imperative API in plain Python and NumPy. This gives much greater flexibility in the implementation of complex neural networks, which leads in turn to faster iteration, and greater ability to quickly realize cutting-edge deep learning algorithms. Below, I describe how Chainer actually works and what kind of benefits users can get from it. Unlike other frameworks with a Python interface such as Theano and TensorFlow, Chainer provides imperative ways of declaring neural networks by supporting Numpy-compatible operations between arrays. Chainer also includes a GPU-based numerical computation library named CuPy. A class represents the unit of computation by wrapping in it (). Users can define operations and functions (instances of ) directly on . Since remember what they are generated from, has the additive operation as its parent (). This mechanism makes backword computation possible by tracking back the entire path from the final loss function to the input, which is memorized through the execution of forward computation\u2014without defining the computational graph in advance. Many numerical operations and activation functions are given in . Standard neural network operations such as fully connected linear and convolutional layers are implemented in Chainer as an instance of . A can be thought of as a function together with its corresponding learnable parameters (such as weight and bias parameters, for example). It is also possible to create a that itself contains several other links. Such a container of links is called a . This allows Chainer to support modeling a neural network as a hierarchy of links and chains. Chainer also supports state-of-the-art optimization methods, serialization, and CUDA-powered faster computations with CuPy. To train a neural network, three steps are needed: (1) build a computational graph from network definition, (2) input training data and compute the loss function, and (3) update the parameters using an optimizer and repeat until convergence. Usually, DL frameworks complete step one in advance of step two. We call this approach define-and-run. This is straightforward but not optimal for complex neural networks since the graph must be fixed before training. Therefore, when implementing recurrent neural networks, for examples, users are forced to exploit special tricks (such as the function in Theano) which make it harder to debug and maintain the code. Instead, Chainer uses a unique approach called define-by-run, which combines steps one and two into a single step. The computational graph is not given before training but obtained in the course of training. Since forward computation directly corresponds to the computational graph and backpropagation through it, any modifications to the computational graph can be done in the forward computation at each iteration and even for each sample. As a simple example, let\u2019s see what happens using two-layer perceptron for MNIST digit classification. The following code shows the implementation of two-layer perceptron in Chainer: In the constructer (), we define two linear transformations from the input to hidden units, and hidden to output units, respectively. Note that no connection between these transformations is defined at this point, which means that the computation graph is not even generated, let alone fixed. Instead, their relationship will be later given in the forward computation (), by defining the activation function () between the layers. Once forward computation is finished for a minibatch on the MNIST training data set (784 dimensions), the following computational graph can be obtained on-the-fly by backtracking from the final node (the output of the loss function) to the input (note that is also introduced as the loss function): The point is that the network definition is simply represented in Python rather than a domain-specific language, so users can make changes to the network in each iteration (forward computation). This imperative declaration of neural networks allows users to use standard Python syntax for branching, without studying any domain specific language (DSL), which can be beneficial as compared to the symbolic approaches that TensorFlow and Theano utilize and also the text DSL that Caffe and CNTK rely on. In addition, a standard debugger and profiler can be used to find the bugs, refactor the code, and also tune the hyper-parameters. On the other hand, although Torch and MXNet also allow users to employ imperative modeling of neural networks, they still use the define-and-run approach for building a computational graph object, so debugging requires special care. The above is just an example of a simple and fixed neural network. Next, let\u2019s look at how complex neural networks can be implemented in Chainer. A recurrent neural network is a type of neural network that takes sequence as input, so it is frequently used for tasks in natural language processing such as sequence-to-sequence translation and question answering systems. It updates the internal state depending not only on each tuple from the input sequence, but also on its previous state so it can take into account dependencies across the sequence of tuples. Since the computational graph of a recurrent neural network contains directed edges between previous and current time steps, its construction and backpropagation are different from those for fixed neural networks, such as convolutional neural networks. In current practice, such cyclic computational graphs are unfolded into a directed acyclic graph each time for model update by a method called truncated backpropagation through time. For this example, the target task is to predict the next word given a part of sentence. A successfully trained neural network is expected to generate syntactically correct words rather than random words, even if the entire sentence does not make sense to humans. The following example shows a simple recurrent neural network with one recurrent hidden unit: Only the types and size of layers are defined in the constructor as well as on the multi-layer perceptron. Given input word and current state as arguments, method returns output word and new state. In the forward computation (), is called for each step and updates the hidden recurrent state with a new one. By using the popular text data set Penn Treebank (PTB), we trained a model to predict the next word from probable vocabularies. Then the trained model is used to predict subsequent words using weighted sampling. This model has learned\u2014and then produced\u2014many repeated pairs of \u201Ca\u201D and a noun or an adjective. Which means \u201Ca\u201D is one of the most probable words, and a noun or adjective tend to follow after \u201Ca.\u201D To humans, the results look almost the same, being syntactically wrong and meaningless, even when using different inputs. However, these are definitely inferred based on the real sentences in the data set by training the type of words and relationship between them. Though this is inevitable due to the lack of expressiveness in the SimpleRNN model, the point here is that users can implement any kinds of recurrent neural networks just like SimpleRNN. Just for comparison, by using off-the-shelf mode of recurrent neural network called Long Short Term Memory (LSTM), the generated texts become more syntactically correct. Since popular RNN components such as LSTM and gated recurrent unit (GRU) have already been implemented in most of the frameworks, users do not need to care about the underlying implementations. However, if you want to significantly modify them or make a completely new algorithm and components, the flexibility of Chainer makes a great difference compared to other frameworks. In the same way, it is very easy to implement stochastically changing neural networks with Chainer. The following is mock code to implement Stochastic ResNet. In , just flip a skewed coin with probability p, and change the forward path by having or not having unit f. This is done at each iteration for each minibatch, and the memorized computational graph is different each time but updated accordingly with backpropagation after computing the loss function. In addition to the above, Chainer has many features to help users to realize neural networks for their tasks as easily and efficiently as possible. CuPy is a NumPy-equivalent array backend for GPUs included in Chainer, which enables CPU\/GPU-agnostic coding, just like NumPy-based operations. The training loop and data set handling can be abstracted by Trainer, which keeps users away from writing such routines every time, and allows them to focus on writing innovative algorithms. Though scalability and performance are not the main focus of Chainer, it is still competitive with other frameworks, as shown in the public benchmark results, by making full use of NVIDIA's CUDA and cuDNN. Chainer has been used in many academic papers not only for computer vision, but also speech processing, natural language processing, and robotics. Moreover, Chainer is gaining popularity in many industries since it is good for research and development of new products and services. Toyota motors, Panasonic, and FANUC are among the companies that use Chainer extensively and have shown some demonstrations, in partnership with the original Chainer developement team at Preferred Networks. Interested readers are encouraged to visit the Chainer website for further details. I hope Chainer will make a difference for many cutting-edge research and real-world products based on deep learning!","flair":"one\tDiscussion"}
{"author":"a_endurance","created":"Sat Nov 05 07:26:16 EDT 2016","text":"In the recent years, we have seen a rapid increase in smartphones usage which are equipped with sophisticated sensors such as accelerometer and gyroscope etc. These devices provide the opportunity for continuous collection and monitoring of data for various purposes. One such application is human activity recognition (HAR) using data collected from smartphone\u2019s accelerometer. There are several techniques proposed in the literature for HAR using machine learning (see [1]) The performance (accuracy) of such methods largely depends on good feature extraction methods. Hand-crafting features in a specific application area require very good domain knowledge. Neural networks especially deep learning methods are applied successfully to solve very difficult problems such as object recognition, machine translation, audio generation etc. In literature, similar work has also been done for HAR using deep learning techniques (see [2]). In this post, we will see how to employ Convolutional Neural Network (CNN) for HAR, that will learn complex features automatically from the raw accelerometer signal to differentiate between different activities of daily life. We will use Actitracker data set released by Wireless Sensor Data Mining (WISDM) lab. This dataset contains six daily activities collected in a controlled laboratory environment. The activities include jogging, walking, ascending stairs, descending stairs, sitting and standing. The data is collected from 36 users using a smartphone in their pocket with the 20Hz sampling rate (20 values per second). The dataset distribution with respect to activities (class labels) is shown in the figure below. Let\u2019s get started by loading required libraries and defining some helper functions for reading, normalising and plotting dataset. import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy import stats import tensorflow as tf %matplotlib inline plt.style.use('ggplot') def read_data(file_path): column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis'] data = pd.read_csv(file_path,header = None, names = column_names) return data def feature_normalize(dataset): mu = np.mean(dataset,axis = 0) sigma = np.std(dataset,axis = 0) return (dataset - mu)\/sigma def plot_axis(ax, x, y, title): ax.plot(x, y) ax.set_title(title) ax.xaxis.set_visible(False) ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)]) ax.set_xlim([min(x), max(x)]) ax.grid(True) def plot_activity(activity,data): fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True) plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis') plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis') plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis') plt.subplots_adjust(hspace=0.2) fig.suptitle(activity) plt.subplots_adjust(top=0.90) plt.show() First, read the data set using function defined above which will return a Pandas data frame. After that, normalise each of the accelerometer component (i.e. x, y and z) using feature_normalize method. Now we can visualize each component of accelerometer for different activities using method. The code below will plot the 9 seconds signal for each human activity, which we can see in figures below. By visual inspection of the graphs, we can identify differences in each axis of the signal across different activities. Now we have to prepare the dataset in a format required by the CNN model. For doing this we define some helper functions to create fixed sized segments from the raw signal. The function will generate indexes as specified by the size parameter by moving over the signal by fixed step size. The window size used is 90, which equals to 4.5 seconds of data and as we are moving each time by 45 points the step size is equal to 2.25 seconds. The label (activity) for each segment will be selected by the most frequent class label presented in that window. The will generate fixed size segments and append each signal component along the third dimension so that the input dimension will be [total segments, input width and input channel]. We will reshape the generated segments to have a height of 1 as we are going to perform one-dimensional convolution (depth wise) over the signal. Moreover, labels will be one hot encoded using function available in Pandas package. def windows(data, size): start = 0 while start < data.count(): yield start, start + size start += (size \/ 2) def segment_signal(data,window_size = 90): segments = np.empty((0,window_size,3)) labels = np.empty((0)) for (start, end) in windows(data[\"timestamp\"], window_size): x = data[\"x-axis\"][start:end] y = data[\"y-axis\"][start:end] z = data[\"z-axis\"][start:end] if(len(dataset[\"timestamp\"][start:end]) == window_size): segments = np.vstack([segments,np.dstack([x,y,z])]) labels = np.append(labels,stats.mode(data[\"activity\"][start:end])[0][0]) return segments, labels Now we have our data set in the desired format, let\u2019s divide it into training and testing set (70\/30) randomly. The figure below provides the CNN model architecture that we are going to implement using Tensorflow. If you are comfortable with Keras or any other deep learning framework, feel free to use that. The model will consist of one convolution layer followed by max pooling and another convolution layer. After that, the model will have fully connected layer which is connected to Softmax layer. Remember that the convolution and max-pool layers will be 1D or temporal. First, let\u2019s define some helper functions and configuration variable for our CNN model. The helper functions will be wrapper around Tensorflow functions to increase reuse and readability. The and will initialize Tensorflow variables for our model layers. The (see Depthwise Convolution) will perform 1D convolution on each input channel separately and pass the output through ReLU activation function. Likewise, will perform 1D max pooling on the output of convolution layer. Tensorflow placeholders for input and output data are defined next. The first convolution layer has a filter size and depth of 60 (number of channels, we will get as output from convolution layer). The pooling layer\u2019s filter size is set to 20 and with a stride of 2. Next, the convolution layer takes an input of max-pooling layer apply the filter of size 6 and will have a tenth of depth as of max-pooling layer. After that, the output is flattened out for the fully connected layer input. There are 1000 neurones in the fully connected layer as defined by the above configuration. The tanh function is used as non-linearity in this layer. Lastly, the Softmax layer is defined to output probabilities of the class labels. The negative log-likelihood cost function will be minimised using stochastic gradient descent optimizer, the code provided below initialize cost function and optimizer. It also defines the code for accuracy calculation of the prediction by model. We have all the required pieces for CNN. Next, let\u2019s write code for training the model. The code provided below, will train the CNN model using a batch size of 10 for 5 training epochs. At each epoch, we will print out the model\u2019s loss and accuracy on the training set. At the end of training, the model will classify the testing set instances and will print out achieved accuracy. cost_history = np.empty(shape=[1],dtype=float) with tf.Session() as session: tf.initialize_all_variables().run() for epoch in range(training_epochs): for b in range(total_batchs): offset = (b * batch_size) % (train_y.shape[0] - batch_size) batch_x = train_x[offset:(offset + batch_size), :, :, :] batch_y = train_y[offset:(offset + batch_size), :] _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y}) cost_history = np.append(cost_history,c) print \"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", session.run(accuracy, feed_dict={X: train_x, Y: train_y}) print \"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y})","flair":"four\tProject"}
{"author":"kkastner","created":"Fri Oct 14 16:49:17 EDT 2016","text":"Now we need your help to evaluate our work! In particular, we need human ears to evaluate, in the first place, how \u201CBach-like\u201D our outputs are. Therefore, in the second part of  the experiment, we would like to ask you to discriminate between Bach and a computer. The results of the test as well as all technical information about the models will be disclosed in an upcoming paper. The test takes just a few minutes! You can play even if you don\u2019t listen to classical music (we just ask you to tell it to us). Please participate here:   http:\/\/flow-machines.com:3005","flair":"two\tNews"}
{"author":"spruceabtuse","created":"Wed Nov 23 10:52:59 EST 2016","text":" Menu Search Follow Us Facebook Instagram Twitter Youtube Flipboard LinkedIn Google+ RSS More Youtube Flipboard LinkedIn Google+ RSS Got a tip? Let us know. News Channels Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Topics All Galleries All Timelines Video Shows Apps News Crunch Report Disrupt SF 2016 Gadgets Reviews Interviews TC Features All Shows All Videos Events TechCrunch Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store News About Mobile World Congress CES All Events Crunchbase Trending Tesla Google Facebook News Startups Mobile Gadgets Enterprise Social Europe Message Us Search TechCrunch Search TechCrunch Search Search × Hi! You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you. Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot. Thanks, TC Team Cyber Monday SaleGet A $200 Holiday Gift Guide Item Free With Disrupt London Early Bird Ticket Purchase Today Only - Get Yours Now Artificial Intelligence Relax, artificial intelligence isn\u2019t coming for your job Japan looks to create a superfast supercomputer for deep learning Google\u2019s AI translation tool seems to have invented its own secret internal language Browse more... AI AI-powered virtual assistant, Mezi, pivots to focus on travel Adobe makes big bets on AI and the public cloud Zugata raises $7 million to make annual performance reviews obsolete Browse more... university-of-california-berkeley huawei Huawei goes over-the-top with a Porsche Design version of its Mate 9 Huawei\u2019s Mate 9 has a 5.9-inch display, a giant battery and is coming to the States, eventually Huawei\u2019s Fit is a no-frills activity tracker Browse more... Huawei puts $1M into a new AI research partnership with UC Berkeley Posted Oct 11, 2016 by Ingrid Lunden (@ingridlunden) 0 SHARES Next Story Samsung kills the Galaxy Note 7 for good Artificial intelligence continues to have its moment in the spotlight, with a surge of interest in startups and efforts from huge tech companies to push the boundaries of how we might best use machine learning, computer vision and other areas of AI in the future. The latest development on that front comes from China\u2019s Huawei, which today announced that it would form a research partnership with UC Berkeley focused on AI, and fund it to the initial tune of $1 million. The alliance, between Huawei\u2019s Noah\u2019s Ark Laboratory and Berkeley Artificial Intelligence Research (BAIR), is being billed as a \u201Cstrategic partnership into basic research\u201D, and it will cover areas like deep learning, reinforcement learning, machine learning, natural language processing and computer vision. \u201CThe two parties believe that this strategic partnership will fuel the advancement of AI technology and create completely new experiences for people, thus contributing greatly to society at large,\u201D Huawei notes. Some of these areas of AI you will have heard a lot about already. Machine learning has become a central part of a lot of basic large-scale computing projects, from bots to search engines and more. Computer vision is being applied in areas like facial recognition tech, AR, VR and self-driving applications. NLP is what makes services like Amazon\u2019s Alexa, Apple\u2019s Siri, and Microsoft\u2019s Cortana work. Others are still in an early phase: reinforcement learning, for example, sits on top of AI systems to help them learn to make better decisions. To date it is still only being worked on by a handful of companies, so that is one key area where Huawei could potentially get a jump by partnering with a university directly. AI has become a central part of how many tech companies are thinking about their next generation of devices and services. But one of the challenges for companies like Huawei \u2014 one of the world\u2019s biggest phone and telecoms equipment makers, and highly ambitious in the wider world of consumer electronics \u2014 is that it has become a race for talent. Competitors like Google, Apple, Amazon and Facebook have been working hard to not only develop a lot of the cutting-edge work in-house, but they have been working closely with universities, as well as snapping up companies and talent coming straight out of universities, to advance their in-house work. Google back in 2014, off the back of its acquisition of DeepMind, formed a partnership with Oxford University, contributing funding to AI research there, and acqui-hired two of the startups that had been incubated at Oxford as part of that effort. Apple has also been known to tap and buy startups that are fresh out of the research labs, such as its acquisition of Faceshift last year. And Amazon is running university competitions to lure out interesting ideas in AI. More recently, several large tech companies (pointedly not including Huawei) formed a non-profit group to explore the opportunities and pitfalls of AI. You could say that now Huawei is rising to challenge by working with researchers at Berkeley on their own AI projects. Berkeley has a history of extensive work and research in AI, and while the BAIR lab is a relatively new face for that effort, it already works with 11 other big companies as \u201Cindustrial partners\u201D, which means Huawei is not exactly getting free reign as a corporate partner. (The others are Facebook, Microsoft, Samsung, Sony, Adobe, Amazon, Yahoo Japan, Nvidia, Intel, Siemens \u2014 itself investing $1 billion into AI research \u2014 and Citris.) The lab also has 2 dozen faculty and 100 grad students. Huawei\u2019s expressively named Noah\u2019s Ark lab, meanwhile, has been around for about four years and is part of the company\u2019s effort to put some $8.1 billion annually into R&D. Huawei is very focused on advancing on this front, and has over 75,000 employees working solely on R&D \u2014 perhaps one reason why MIT Technology Review named it the 10th \u201Csmartest company\u201D in a recent survey. Maybe fittingly for an organization called Noah\u2019s Ark, expanding research to Berkeley appears to be part of a wider remit to plant seeds for Huawei research globally: another recent outpost was created in Paris. It\u2019s just early days, and so it remains to be seen whether Huawei can attract and bottle some of the more interesting AI tech that it will come across in this newest partnership with Berkeley. But at the very least, forging the alliance will put it in a position to try. 0 SHARES Advertisement Advertisement Crunchbase Huawei Technologies Founded 1987 Overview Huawei Technologies is a telecom solutions company that offers infrastructure application software and devices with wireline, wireless, and IP technologies. The company\u2019s products and solutions have been deployed in over 100 countries and have served 45 of the world's top 50 telecom operators as well as one third of the world's population. Huawei Technologies has 3 divisions in the United States: \u2026 Location Plano, TX Categories Security, Web Hosting, Wireless, Mobile Website http:\/\/huawei.com Full profile for Huawei Technologies U.C. Berkeley Founded 1868 Overview The University of California, Berkeley is a public research university located in Berkeley, California. It is the most selective \u2013 and highest ranked in U.S. News and ARWU \u2013 public university in the world for undergraduate education. Aside from its academic prestige, UC Berkeley is also well known for producing a high number of entrepreneurs. Full profile for U.C. Berkeley Newsletter Subscriptions The Daily Crunch Get the top tech stories of the day delivered to your inbox TC Weekly Roundup Get a weekly recap of the biggest tech stories Crunchbase Daily The latest startup funding announcements Enter Address Subscribe Latest Crunch Report Facebook Builds a Censorship Tool | Crunch Report Watch More Episodes AI university-of-california-berkeley huawei Artificial Intelligence Popular Posts Featured Stories Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 | Sarah Perez Siren Care makes a \u201Csmart\u201D sock to track diabetic health Nov 25, 2016 | Sarah Buhr Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 | Connie Loizos Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 | Ingrid Lunden Latest From Asia Uber rival Grab\u2019s first CFO is leaving the company after just seven months Nov 25, 2016 | Jon Russell China\u2019s Ctrip is buying flight search company Skyscanner for $1.74 billion Nov 23, 2016 | Jon Russell Facebook is unlikely to succeed in China, even if it compromises on free speech Nov 23, 2016 | Jon Russell Singapore\u2019s Spacemob lands $5.5M to expand its co-working spaces across Asia Pacific Nov 23, 2016 | Jon Russell Comment moderation powered by Up Next Samsung kills the Galaxy Note 7 for good Posted Oct 11, 2016 CrunchBoard Job Listings CRM Berater (m\/f) at eGym GmbH (München, Deutschland) C++ Developer - Qt Product Development (m\/f) at eGym GmbH (München, Deutschland) Full Stack Engineer at FactorChain at The Sourcery (Los Altos, CA, United States) UI Engineering @ FactorChain at The Sourcery (Los Altos, CA, United States) Senior Backend Engineer @ MeetMe at The Sourcery (San Francisco, CA, United States) More from CrunchBoard Advertisement TechCrunch News Video Events Crunchbase TechCrunch Store About Staff Contact Us Advertise With Us Send Us A Tip International China Europe Japan Follow TechCrunch Facebook Twitter Google+ LinkedIn Youtube Pinterest Tumblr Instagram StumbleUpon Feed TechCrunch Apps iOS Android Windows 8 Subscribe to The Daily Crunch Latest headlines delivered to you daily Subscribe to <span class=\"no-mobile\">Subscribe to <\/span>The Daily Crunch Enter Email Address Subscribe © 2013-2016 AOL Inc. All rights reserved. Aol Tech Privacy Policy About Our Ads Anti Harassment Policy Terms of Service Powered by WordPress.com VIP Fonts by News Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Galleries All Timelines Videos Apps News Crunch Report Disrupt SF 2016 All Shows All Videos Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store All Events Crunchbase Message Us Most Popular RAWR: Samsung Canada Wins The Internet With This Custom Galaxy S III Aug 28, 2012 by Matt Burns A Dongle Joke That Spiraled Way Out Of Control Mar 21, 2013 by Kim-Mai Cutler Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago by Ron Miller Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 by Sarah Perez UPDATED: Machine learning can fix Twitter, Facebook, and maybe even America yesterday by Chris Nicholson Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 by Ingrid Lunden Xiaomi admits it doesn\u2019t make money on smartphone hardware sales Nov 25, 2016 by Darrell Etherington Disrupting the world of science publishing 8 hours ago by Bérénice Magistretti How to recruit, hire and retain female engineers Nov 25, 2016 by Sharon Wienbar Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 by Connie Loizos ","flair":"two\tNews"}
{"author":"kh40tika","created":"Tue Oct 25 14:08:27 EDT 2016","text":"I built this bot to learn deep reinforcement learning. Currently the bot is quite weak, however it does seems to be able to learn concepts like board corner, X\/Y point, unbalanced edges. Still, the bot don't seem to hold a chance against search based EDAX program yet. Run to start training a model. Hit CTRL-C to stop training and the model will be saved at model.pkl. Run to play against the bot in a CLI, type \"h\" for help. Base algorithm is standard Q-learning with a learnable value function approximator. The output of value function is 8 by 8 matrix. Invalid moves were pruned as a post-processing step. Value function is trained with SGD with ADAM optimizer. During playing, OmegaOthello is augmented with a 3-moves minmax search. NN architechture is made of 8-step Convoluitonal-GRU followed by 3 full connected MLP layers. Note convolutional weight is shared in all 8 steps. NN takes 4 features as input: black pieces, white pieces, valid moves, constant 1(due to zero padding during convolution, this feature can help edge\/corner detection) Training is using pure self playing, coupled with epsilon-greedy exploration.","flair":"four\tProject"}
{"author":"whateverr123","created":"Wed Oct 05 21:22:49 EDT 2016","text":"This directory contains CMake files for building TensorFlow on Microsoft Windows. CMake is a cross-platform tool that can generate build scripts for multiple build systems, including Microsoft Visual Studio. N.B. We provide Linux build instructions primarily for the purpose of testing the build. We recommend using the standard Bazel-based build on Linux. The CMake files in this directory can build the core TensorFlow runtime, an example C++ binary, and a PIP package containing the runtime and Python bindings. Note: Windows support is in an alpha state, and we welcome your feedback. The Python package supports Python 3.5 only, because that is the only version for which standard Python binaries exist and those binaries are compatible with the TensorFlow runtime. (On Windows, the standard Python binaries for versions earlier than 3.5 were compiled with older compilers that do not have all of the features (e.g. C++11 support) needed to compile TensorFlow. We welcome patches for making TensorFlow work with Python 2.7 on Windows, but have not yet committed to supporting that configuration.) The following Python APIs are not currently implemented: Loading custom op libraries via . In order to use your custom op, please put the source code under the tensorflow\/core\/user_ops directory, and a shape function is required (not optional) for each op. Path manipulation functions (such as ) are not functional. The libraries are not currently included in the PIP package. The following operations are not currently implemented: Google Cloud Storage support is not currently implemented. The GCS library currently depends on and , and the Windows version could use standard Windows APIs for making HTTP requests and cryptography (for OAuth). Contributions are welcome for this feature. We are actively working on improving CMake and Windows support, and addressing these limitations. We would appreciate pull requests that implement missing ops or APIs. Install the pre-requisites detailed above, and set up your environment. The following commands assume that you are using the Windows Command Prompt (). You will need to set up your environment to use the appropriate toolchain, i.e. the 64-bit tools. (Some of the binary targets we will build are too large for the 32-bit tools, and they will fail with out-of-memory errors.) The typical command to do set up your environment is: When building with GPU support after installing the CUDNN zip file from NVidia, append its bin directory to your PATH environment variable. In case TensorFlow fails to find the CUDA dll's during initialization, check your PATH environment variable. It should contain the directory of the CUDA dlls and the directory of the CUDNN dll. For example: We assume that and are installed and in your . If for example is not in your path and it is installed in , you can add this directory to your as follows: Clone the TensorFlow repository and create a working directory for your build: N.B. This assumes that is in your environment variable. The other paths are for illustrative purposes only, and may be different on your platform. The character is a line continuation and must be the last character on each line. To build with GPU support add \"^\" at the end of the last line above following with: Note that the flag must match the build configuration that you choose when invoking . The known-good values are and . The build type is not currently supported, because it relies on a library for Python () that is not distributed by default. There are various options that can be specified when generating the solution and project files: : Note that the option must match the build configuration that you choose when invoking MSBuild in step 4. The known-good values are and . The build type is not currently supported, because it relies on a library for Python () that is not distributed by default. . Defaults to . You can build a small subset of the kernels for a faster build by setting this option to . . Defaults to . Generate project files for a simple C++ example training program. . Defaults to . Generate project files for building a PIP package containing the TensorFlow runtime and its Python bindings. . Defaults to . Include gRPC support and the distributed client and server code in the TensorFlow runtime. . Defaults to . Include SSL support (for making secure HTTP requests) in the TensorFlow runtime. This support is incomplete, and will be used for Google Cloud Storage support. . Defaults to . Include GPU support. If GPU is enabled you need to install the CUDA 8.0 Toolkit and CUDNN 5.1. CMake will expect the location of CUDNN in -DCUDNN_HOME=path_you_unziped_cudnn. . Defaults to . This builds cc unit tests. There are many of them and building will take a few hours. After cmake, build and execute the tests with . Defaults to . This enables python kernel tests. After building the python wheel, you need to install the new wheel before running the tests. To execute the tests, use This build requires Docker to be installed on the local machine.","flair":"null\tnull"}
{"author":"evc123","created":"Fri Oct 28 01:16:08 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.08613 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.CL References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Can Active Memory Replace Attention? Authors: Łukasz Kaiser, Samy Bengio (Submitted on 27 Oct 2016) Abstract: Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice. Subjects: Learning (cs.LG); Computation and Language (cs.CL) Cite as: arXiv:1610.08613 [cs.LG]   (or arXiv:1610.08613v1 [cs.LG] for this version) Submission history From: Łukasz Kaiser [view email] [v1] Thu, 27 Oct 2016 04:28:29 GMT (18kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Nazka231","created":"Sat Oct 15 17:07:25 EDT 2016","text":" Skip to content the morning paper an interesting\/influential\/important paper from the world of CS every weekday morning, as selected by Adrian Colyer Home About Subscribe Towards deep symbolic reinforcement learning October 12, 2016 Towards deep symbolic reinforcement learning Garnelo et al, 2016 Every now and then I read a paper that makes a really strong connection with me, one where I can\u2019t stop thinking about the implications and I can\u2019t wait to share it with all of you. For me, this is one such paper. In the great see-saw of popularity for artificial intelligence techniques, symbolic reasoning and neural networks have taken turns, each having their dominant decade(s). The popular wisdom is that data-driven learning techniques (machine learning) won. Symbolic reasoning systems were just too hard and fragile to be successful at scale. But what if we\u2019re throwing the baby out with the bath water? What if instead of having to choose between the two approaches, we could combine them: a system that can learn representations, and then perform higher-order reasoning about those representations? Such combinations could potentially bring to bear the fullness of AI research over the last decades. I love this idea because: (a) it feels intuitively right (we as humans learn to recognise types of things, and then form probability-based rules about their behaviour for example); (b) it is very data efficient, to appropriate a phrase:  \u201Ca rule can be worth 1000(+) data points!\u201D (c) it opens up the possibility to incorporate decades of research into modern learning systems, where you can\u2019t help but think there would be some quick wins. \u2026 we show that the resulting system \u2013 though just a prototype \u2013 learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural Deep Reinforcement Learning system on a stochastic variant of the game. In short, this feels to me like something that could represent a real breakthrough and a step-change in the power of learning systems. You should take that with a pinch of salt, because I\u2019m just an interested outsider following along. I hope that I\u2019m right though! Why combine symbolic reasoning and deep learning? Contemporary Deep Reinforcement Learning (DRL) systems achieve impressive results but still suffer from a number of drawbacks: They inherit from deep learning the need for very large training sets, so that they learn very slowly They are brittle in the sense that a trained network that performs well on one task often performs poorly on a new task, even if it is very similar to the original one. They do not use high-level processes such as planning, causal reasoning, or analogical reasoning to fully exploit the statistical regularities present in the training data. They are opaque \u2013 it is typically difficult to extract a human-comprehensible chain of reasons for the action choice the system makes . In contrast, classical AI uses language-like propositional representations to encode knowledge.   Thanks to their compositional structure, such representations are amenable to endless extension and recombination, an essential feature for the acquisition and deployment of high-level abstract concepts, which are key to general intelligence. Moreover, knowledge expressed in propositional form can be exploited by multiple high-level reasoning processes and has general-purpose application across multiple tasks and domains. Features such as these, derived from the benefits of human language, motivated several decades of research in symbolic AI. But as an approach to general intelligence, classical symbolic AI has been disappointing. A major obstacle here is the symbol grounding problem\u2026 The symbol grounding problem is this: where do the symbols come from? They are typically hand-crafted rather than grounded in data from the real world. This brings a number of problems: They cannot support support ongoing adaptation to a new environment They cannot capture the rich statistics of real-world perceptual data They create a barrier to full autonomy Machine learning has none of these problems! So what if we could take the good bits from machine learning, and combine them with the good bits from classical AI? Use machine learning to learn symbolic representations, and then use symbolic reasoning on top of those learned symbols for action selection (in the case of DRL). You end with a system architecture that looks like this: Four principles for building Deep Symbolic Reinforcement Learning (DSRL) systems Support conceptual abstraction by mapping high-dimensional raw input into a lower dimensional conceptual state space, and then using symbolic methods that operate at a higher level of abstraction.  \u201CThis facilitates both data efficient learning and transfer learning as well as providing a foundation for other high-level cognitive processes such as planning, innovative problem solving, and communication with other agents (including humans). Enable compositional structure that supports combining and recombining elements in an open-ended way.  \u201CTo handle uncertainty, we propose probabilistic first-order logic for the semantic underpinnings of the low-dimensional conceptual state space representation onto which the neural front end must map the system\u2019s high-dimensional raw input. Build on top of common sense priors \u2013 it is unrealistic to expect an end-to-end reinforcement learning system to succeed with no prior assumptions about the domain. For example: objects frequently move, and typically do so in continuous trajectories, there are stereotypical events such as beginning to move, stopping, coming into contact with other objects, and so on. Support causal reasoning through discovery of the causal structure of the domain and symbolic rules expressed in terms of both the domain and the common sense priors. To carry out analogical inference at a more abstract level, and thereby facilitate the transfer of expertise from one domain to another, the narrative structure of the ongoing situation needs to be mapped to the causal structure of a set of previously encountered situations. As well as maximising the benefit of past experience, this enables high-level causal reasoning processes to be deployed in action selection, such as planning, lookahead, and off-line exploration (imagination). A worked example Well it all sounds good, but does it actually work? The prototype system built by the authors learns to play four variations on a simple game. An agent moves around a square space populated by circles and crosses. It receives a positive reward for every cross it \u2018collects\u2019, and a negative reward for every circle. Four variations are used: the first has only circles in a fixed grid, the second both circles and crosses in a fixed grid, the third only circles but in a random grid, and the fourth both circles and crosses in a random grid. The system has three stages: low-level symbol generation, representation building, and reinforcement learning. The first stage use a convolutional neural network (autoencoder) trained on 5000 randomly generated images on game objects scattered across the screen. The activations in the middle layer of the CNN are used directly for detection of objects in the scene. The salient areas in an image result in higher activation throughout the layers of the convolutional network. Given the geometric simplicity of the games, this is enough to extract the individual objects from any given frame. The objects identified this way are then assigned a symbolic type according to the geometric properties computed by the autoencoder. This is done by comparing the activation spectra of the salient pixels across features. At this point we know the type and position of the objects. The second stage learns to track objects across frames in order to learn from their dynamics. The system is supported in this task by a common sense prior, \u201Cobject persistence over time\u201D. This is broken down into three measures ultimately combined into a single value: how close an object in one frame is to an object in another frame (spatial proximity is an indicator it may be the same object); how likely it is that an object transformed from one type into another (by learning a transition probability matrix); and what change there is in the neighbourhood  of an object. The representation learned at the end of the second stage differs from that of the first stage in two key ways: It is extended to understand changes over time, and Positions of objects are represented by relative coordinates to other objects rather than using absolute coordinates. \u201CThis approach is justified by the common sense prior that local relations between multiple objects are more relevant than the global properties of single objects.\u201D We  now have a concise spatio-temporal representation of the game situation, one that captures not only what objects are in the scene along with their locations and types, but also what they are doing. In particular, it represents frame-to-frame interactions between objects, and the changes in type and relative position that result. This is the input to reinforcement learning, the third and final stage of the pipeline. Finally we enter the reinforcement learning stage, where the relative location representation of objects greatly reduces the state space, on the assumption that things that are far apart (both in the game and in the real world) tend to have little influence on each other. In order to implement this independence we train a separate Q function for each interaction between two object types. The main idea is to learn several Q functions for the different interactions and query those that are relevant for the current situation. Given the simplicity of the game and the reduced state space that results from the sparse symbolic representation we can approximate the optimal policy using tabular Q-learning. Evaluation results You can use precision and recall metrics to evaluate agent performance \u2013 where precision is interpreted as the percentage of objects you collect that are positive (crosses), and recall is interpreted as the percentage of available positive objects that you actually do collect. The first results show that the agent improves with training to a precision of 70%. It\u2019s interesting to compare the performance of DQN and the DSRL system. DQN does better in the the grid scenario, but when objects are position at random the DQN agent struggles to learn an effective policy within 1000 epochs. The DSRL system performs much better on this variant: To evaluate transfer learning, both DQN and DSRL were trained on the grid variation, and then tested on the random variant.  DQN does no better than chance, whereas the DSRL system is able to approach 70% precision.   We conjecture that DQN struggles with this game because it has to form a statistical picture of all possible object placements, which would require a much larger number of games. In contrast, thanks to the conceptual abstraction made possible by its symbolic front end, our system very quickly \u201Cgets\u201D the game and forms a set of general rules that covers every possible initial configuration. This demonstration merely hints at the potential for a symbolic front end to promote data efficient learning, potential that we aim to exploit more fully in future work. Our proof-of-concept system also illustrates one aspect of the architecture\u2019s inherent capacity for transfer learning\u2026 Moreover, the DSRL system can explain its actions: every action choice can be analysed in terms of the Q functions involved in the decision. These Q functions describe the types of objects involved in the interaction as well as their relations, so we can track back to the reasons that led to a certain decision. And this is just the beginning\u2026 We could wire in a more sophisticated deep network capable of unsupervised learning to disentangle representations in the earlier stages. We can exploit many more of the achievements in classical AI, notably the incorporation of inductive logic programming, formal techniques for analogical reasoning, and building in a planning component that exploits the knowledge of the cause structure of the domain acquired during the learning process.   In domains with sparse reward, it\u2019s often possible for an agent to discover a sequence of actions leading to a reward state through off-line search rather than on-line exploration. Contemporary logic-based planning methods are capable of efficiently finding large plans in complex domains (eg:[37]), and it would be rash not to exploit the potential of these techniques. Finally, one day the symbolic components of the proposed architecture could well be one day using neurally-based implementations of symbolic reasoning functions. In the mean time, an architecture that combines deep neural networks with directly implemented symbolic reasoning seems like a promising research direction. Share this: Twitter LinkedIn Email Print Like this: Like Loading... Related from → Uncategorized ← Progressive neural networks Go to statement considered harmful → 15 Comments leave one → Mats Jonsson permalink October 12, 2016 6:56 am This is one of those \u2018send thrills down your spine\u2019 papers. Thanks! Reply ya permalink October 15, 2016 11:05 pm Its a shame the paper does not mention or reference existing work in this area. Reply ctaank permalink October 16, 2016 1:23 am I think the approach you\u2019ve outlined is definitely the way to go. Reply Sean Aubin permalink October 16, 2016 5:29 am Disclaimer: I work in the lab that created Spaun. This seems kind of similar to Spaun [0]. You use Deep Learning to get the features you want and then you do symbolic like operations on those feature vectors to accomplish reinforcement learning. At least Spaun used neurons for the rule manipulation too. I also don\u2019t believe the transfer learning claim at all, because the tasks are way too similar. Still, it\u2019s a step in the right direction as far as I\u2019m concerned [1]. That being said, good luck find a set of general prior rules and scaling them. [0] http:\/\/science.sciencemag.org\/content\/338\/6111\/1202 [1] https:\/\/medium.com\/@seanaubin\/why-does-ai-still-suck-and-can-it-suck-less-9db36be294dc#.akb82s8o9 Reply Kevin permalink October 17, 2016 3:51 pm Hi, Did you have a look at COQ program which is aimed to prove through computation Mathematical Theorem ? it seems sharing a lot of common points. Thank you Kind re Reply Tim Vergenz permalink November 22, 2016 2:31 am Amazingly exciting. I feel like this is starting to head towards the way the human brain works: blending association-based learning (i.e. artificially, neural networks; or biologically, the subconscious part of the human brain\u2013see \u201CBlink: The Power of Thinking Without Thinking\u201D [2007] for an interesting layman\u2019s discussion of the latter) with sequential rational processing. Reading this gave me chills. Aside: Could you set the og:image meta tag for the page to be the first figure with the general architecture?🙂 It does a better job inviting reader in IMO. Reply adriancolyer permalink* November 22, 2016 11:11 am Glad you enjoyed the paper, it\u2019s one of my recent favourites too. I\u2019ve set the architecture diagram as the featured image in WordPress \u2013 hopefully that will do the trick\u2026. Regards, Adrian. Reply Tim Vergenz permalink November 22, 2016 3:40 pm Thanks! Seems like it worked.🙂 This is so exciting. We\u2019re finally starting to make AI approaches that combine logical thinking with association-based learning\u2013much like how the human brain works! For the interested, here\u2019s a 3-minute excerpt from \u201CBlink\u201D (2007). Connecting the paper to this book was what made me so excited. \u2014 \u201CImagine that I were to ask you to play a very simple gambling game. In front of you are four decks of cards\u2014two of them red and the other two blue. Each card in those four decks either wins you a sum of money or costs you some money. \u2026 What you don\u2019t know at the beginning, however, is that the red decks are a minefield. The rewards are high, but when you lose on the red cards, you lose a lot. Actually, you can win only by taking cards from the blue decks, which offer a nice steady diet of $50 payouts and modest penalties. The question is how long will it take you to figure this out? A group of scientists at the University of Iowa did this experiment a few years ago, and what they found is that after we\u2019ve turned over about fifty cards, most of us start to develop a hunch about what\u2019s going on. We don\u2019t know why we prefer the blue decks, but we\u2019re pretty sure at that point that they are a better bet. After turning over about eighty cards, most of us have figured out the game and can explain exactly why the first two decks are such a bad idea. \u2026 But the Iowa scientists [also] did something else. \u2026 They hooked each gambler up to a machine that measured the activity of the sweat glands below the skin in the palms of their hands. [These glands respond to stress.] \u2026 What [they] found is that gamblers started generating stress responses to the red decks by the tenth card, forty cards before they were able to say that they had a hunch about what was wrong with those two decks. More importantly, right around the time their palms started sweating, their behavior began to change as well. They started favoring the blue cards and taking fewer and fewer cards from the red decks. In other words, the gamblers figured the game out before they realized they had figured the game out: they began making the necessary adjustments long before they were consciously aware of what adjustments they were supposed to be making. \u2026 [I]n those moments, our brain uses two very different strategies to make sense of the situation. The first is the one we\u2019re most familiar with. It\u2019s the conscious strategy. We think about what we\u2019ve learned, and eventually we come up with an answer. This strategy is logical and definitive. But it takes us eighty cards to get there. It\u2019s slow, and it needs a lot of information. There\u2019s a second strategy, though. It operates a lot more quickly. It starts to kick in after ten cards, and it\u2019s really smart, because it picks up the problem with the red decks almost immediately. It has the drawback, however, that it operates\u2014at least at first\u2014entirely below the surface of consciousness. It sends its messages through weirdly indirect channels, such as the sweat glands in the palms of our hands. It\u2019s a system in which our brain reaches conclusions without immediately telling us that it\u2019s reaching conclusions.\u201D Tim Vergenz permalink November 22, 2016 3:41 pm Thanks! Seems to have worked.🙂 For the interested, here\u2019s a 3-minute excerpt from Blink (2007). The obvious parallel is most of why I got so excited about this article. \u2014 \u201CImagine that I were to ask you to play a very simple gambling game. In front of you are four decks of cards\u2014two of them red and the other two blue. Each card in those four decks either wins you a sum of money or costs you some money. \u2026 What you don\u2019t know at the beginning, however, is that the red decks are a minefield. The rewards are high, but when you lose on the red cards, you lose a lot. Actually, you can win only by taking cards from the blue decks, which offer a nice steady diet of $50 payouts and modest penalties. The question is how long will it take you to figure this out? A group of scientists at the University of Iowa did this experiment a few years ago, and what they found is that after we\u2019ve turned over about fifty cards, most of us start to develop a hunch about what\u2019s going on. We don\u2019t know why we prefer the blue decks, but we\u2019re pretty sure at that point that they are a better bet. After turning over about eighty cards, most of us have figured out the game and can explain exactly why the first two decks are such a bad idea. \u2026 But the Iowa scientists [also] did something else. \u2026 They hooked each gambler up to a machine that measured the activity of the sweat glands below the skin in the palms of their hands. [These glands respond to stress.] \u2026 What [they] found is that gamblers started generating stress responses to the red decks by the tenth card, forty cards before they were able to say that they had a hunch about what was wrong with those two decks. More importantly, right around the time their palms started sweating, their behavior began to change as well. They started favoring the blue cards and taking fewer and fewer cards from the red decks. In other words, the gamblers figured the game out before they realized they had figured the game out: they began making the necessary adjustments long before they were consciously aware of what adjustments they were supposed to be making. \u2026 [I]n those moments, our brain uses two very different strategies to make sense of the situation. The first is the one we\u2019re most familiar with. It\u2019s the conscious strategy. We think about what we\u2019ve learned, and eventually we come up with an answer. This strategy is logical and definitive. But it takes us eighty cards to get there. It\u2019s slow, and it needs a lot of information. There\u2019s a second strategy, though. It operates a lot more quickly. It starts to kick in after ten cards, and it\u2019s really smart, because it picks up the problem with the red decks almost immediately. It has the drawback, however, that it operates\u2014at least at first\u2014entirely below the surface of consciousness. It sends its messages through weirdly indirect channels, such as the sweat glands in the palms of our hands. It\u2019s a system in which our brain reaches conclusions without immediately telling us that it\u2019s reaching conclusions.\u201D Reply Trackbacks Machine Learning Roundup 10\/13\/2016 | Big Analytics Bookmarks for October 14th through October 15th : Extenuating Circumstances Misc stuff: The verification gap, ML training and more \u2013 The Foretellix Blog Distilled News | Data Analytics & R Artificial Intelligence and life in 2030 | the morning paper Building machines that learn and think like people | the morning paper Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Subscribe never miss an issue! The Morning Paper delivered straight to your inbox. Search Archives Archives Select Month November 2016  (19) October 2016  (21) September 2016  (20) July 2016  (16) June 2016  (22) May 2016  (22) April 2016  (12) March 2016  (23) February 2016  (21) January 2016  (23) December 2015  (10) November 2015  (21) October 2015  (22) September 2015  (22) August 2015  (22) June 2015  (21) May 2015  (21) April 2015  (14) March 2015  (23) February 2015  (23) January 2015  (21) December 2014  (15) November 2014  (20) October 2014  (20) May 2011  (3) Most read in the last few days About Building machines that learn and think like people The amazing power of word vectors When CSI meets public wifi: Inferring your mobile phone password via wifi signals Artificial Intelligence and life in 2030 Smart Reply: Automated response suggestion for email Twice the bits, twice the trouble: vulnerabilities induced by migrating to 64-bit platforms Towards deep symbolic reinforcement learning The Linux Scheduler: a Decade of Wasted Cores Playing FPS games with deep reinforcement learning RSS RSS - Posts RSS - Comments Live on twitter My Tweets Blog at WordPress.com. Send to Email Address Your Name Your Email Address Cancel Post was not sent - check your email addresses! Email check failed, please try again Sorry, your blog cannot share posts by email. %d bloggers like this: ","flair":"one\tDiscussion"}
{"author":"Kaixhin","created":"Sun Oct 30 20:34:23 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.09296 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs cs.AI stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Improving Sampling from Generative Autoencoders with Markov Chains Authors: Kai Arulkumaran, Antonia Creswell, Anil Anthony Bharath (Submitted on 28 Oct 2016 (v1), last revised 7 Nov 2016 (this version, v2)) Abstract: We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. However, the inference model may not always map inputs to latent samples that are consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent distribution. Using this, we can improve the quality of samples drawn from the model, especially when the learned distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion. Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:1610.09296 [cs.LG]   (or arXiv:1610.09296v2 [cs.LG] for this version) Submission history From: Kai Arulkumaran [view email] [v1] Fri, 28 Oct 2016 16:17:03 GMT (12807kb,D) [v2] Mon, 7 Nov 2016 15:17:32 GMT (12807kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"liyi14","created":"Fri Nov 25 00:11:58 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.07709 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Fully Convolutional Instance-aware Semantic Segmentation Authors: Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei (Submitted on 23 Nov 2016) Abstract: We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at \\url{this https URL}. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1611.07709 [cs.CV]   (or arXiv:1611.07709v1 [cs.CV] for this version) Submission history From: Jifeng Dai [view email] [v1] Wed, 23 Nov 2016 09:53:57 GMT (4524kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"RobeDM","created":"Wed Oct 19 16:34:33 EDT 2016","text":"LIBIRWLS is an integrated library that makes use of a parallel implementation of the Iterative Re-Weighted Least Squares (IRWLS) procedure for solving the quadratic programmig (QP) problem that arises during the training of Support Vector Machines (SVMs). It implements the functions to run two different algorithms: Parallel Iterative Re-Weighted Least Squares: A Parallel SVM solver based on the IRWLS algorithm. Parallel Semi-parametric Iterative Re-Weighted Least Squares: A Parallel Semiparametric SVMs solver based on the IRWLS algorithm. For a detailed explanation of the algorithms take a look at the web page SVMs are a very popular machine learning technique because they can easily create non-linear solutions by transforming the input space onto a high dimensional one where a kernel function can compute the inner product of a pair vectors. Thanks to this ability, they offer a good compromise between complexity and performance in many applications. SVMs have two main limitations. The first problem is related to their non-parametric nature. The complexity of the classifier is not limited and depends on the number of Support Vectors (SVs) after training. If the number of SVs is very large we may obtain a very slow classifier when processing new samples. The second problem is the run time associated to the training procedure that may be excessive for large datasets. To face these problems, we can make use of parallel computing, thus reducing the run time of the training procedure or we can use semi-parametric approximations than can limit the complexity of the model in advance, which directly implies a faster classifier. The above situation motivated us to develop \"LIBIRWLS\", an integrated library based on a parallel implementation of the IRWLS procedure to solve non-linear SVMs and semi-parametric SVMs. This library is implemented in C, supports a wide range of platforms and also provides detailed information about its programming interface and dependencies. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. The structure of this library is as follows: You can find detailed information about the software and algorithms in its respective webpage: A documentation of the application programming interface (API) has been created in html format and it can be found in the folder docs\/html. This documentation is also available online: If you have any installation problem you can se an example of a correct installation step by step and a demonstration of running the algorithms using the command line interface for Linux and OSX here. If that doesn't solve you problem, please, report the issue here. This software is implemented in C and requires the following libraries: You need to run make in the library folder to build LIBIRWLS. If you have installed atlas using apt-get: If you have manually installed ATLAS, you must tell the installation directory. The default compiler in OS X is clang. It currently doesn't works with openmp. You can install gcc using Homebrew or Macports: OS X has its own accelerated algebra standard routines. The name of this library is veclib and it is composed by two files: These files are in the directory: This library will look for the library in that directory. Check that both files are there. If they are in a diferent path, look for them using the command \"find\" and note the folder for the next step: You must use the make command using the following parameters: LIBIRWLS contains windows executable files that were precompiled for 32 and 64 bits instancies. These executables are static so no extra packages are needed. If you want to obtain an optimized performance the software must be compiled and built in your system using tools like cygwin. This is because ATLAS fixes some parameters to optimize the run time attending to the microprocessor in the computer that builds it. For testing purposes, the folder demo contains a .bat windows demo script and a Unix .sh demo script that download a sample dataset from the libsvm repository and runs the executable files. The algorithm is described in this paper: To train the algorithm and create the model: training_set_file: Training set in LibSVM format model_file: File where the classifier will be stored The algorithm is described in this paper: Morales, R. D., & Vázquez, Á. N. (2016). Improving the efficiency of IRWLS SVMs using Parallel Cholesky Factorization. Pattern Recognition Letters. To train the algorithm and create the model: training_set_file: Training set in LibSVM format model_file: File where the classifier will be stored To make predictions with the model in a different dataset: The dataset must be provided in LibSVM format, labeled to train the model and labeled or unlabeled for predictions (using the -l option in the PIRWLS-predict command to tell if the file is labeled or unlabeled): Installation and running instructions are detailed in the README.md file of the folder python-module","flair":"four\tProject"}
{"author":"jun_wei_chen","created":"Mon Oct 31 07:55:49 EDT 2016","text":"recently i am working on the project to implement SSD (single shot multibox detector), and i have a few point still can't understand, hope someone could give me answer\n\n&amp;nbsp;\n\n**question 1 : how is the sample presented, do i need to assign a number \" 1 - iou \" for background ?**\n\n&amp;nbsp;\n\nfor example i got 3 classes , [ background, class a, class b ]\n\n&amp;nbsp;\n\nif the iou between box and groundtruth is 0.7 and is class b ,\nwhich sample shoud i set [ 0.3, 0.7, 0] , or just [ 0, 0.7, 0 ] ?\n\n&amp;nbsp;\n\nwhen iou = 0.1 , [0.9, 0.1, 0] or [0, 0.1, 0]\n\n&amp;nbsp;\n\ni train with \" 1 - iou \" , but when i test with the model, it seems the highest confidence box would like [ 0.4xxx, 0.5xxx , 0 ], even the bounding box match the groundtruth with high accuracy\n\n&amp;nbsp;\n\nand i try another method\n\n&amp;nbsp;\n\nif iou &gt; 0.5 set sample to 1 \niou &lt; 0.5 set sample to 0\n\n&amp;nbsp;\n\nfor example if iou = 0.7, 0.7 &gt; 0.5 , so sample = [0 , 1 , 0] ,\n\n&amp;nbsp;\n\nand if iou = 0.3, 0.3 &lt; 0.5 , so sample = [1 , 0 , 0]\nbut the result is that many box with high confidence is not that accuracy\ncan anyone explain how the negative sample presented\n\n&amp;nbsp;\n\n**question 2 : which confidence is the network output ?**\n\n&amp;nbsp;\n\nis the output of network a default box confidence or a predicted confidence ( default box after mapping with predicted x y w h ),\n\n&amp;nbsp;\n\nif the output is confidence of default box , it seem it won't know the confidence of the box after mapping","flair":"null\tnull"}
{"author":"nagasgura","created":"Thu Oct 27 22:37:57 EDT 2016","text":" Google Research Blog The latest news from Research at Google Supercharging Style Transfer Wednesday, October 26, 2016 Posted by Vincent Dumoulin*, Jonathon Shlens and Manjunath Kudlur, Google Brain Team Pastiche. A French word, it designates a work of art that imitates the style of another one (not to be confused with its more humorous Greek cousin, parody). Although it has been used for a long time in visual art, music and literature, pastiche has been getting mass attention lately with online forums dedicated to images that have been modified to be in the style of famous paintings. Using a technique known as style transfer, these images are generated by phone or web apps that allow a user to render their favorite picture in the style of a well known work of art. Although users have already produced gorgeous pastiches using the current technology, we feel that it could be made even more engaging. Right now, each painting is its own island, so to speak: the user provides a content image, selects an artistic style and gets a pastiche back. But what if one could combine many different styles, exploring unique mixtures of well known artists to create an entirely unique pastiche? Learning a representation for artistic style In our recent paper titled \u201CA Learned Representation for Artistic Style\u201D, we introduce a simple method to allow a single deep convolutional style transfer network to learn multiple styles at the same time. The network, having learned multiple styles, is able to do style interpolation, where the pastiche varies smoothly from one style to another. Our method enables style interpolation in real-time as well, allowing this to be applied not only to static images, but also videos. Credit: awesome dog role played by Google Brain team office dog Picabo. In the video above, multiple styles are combined in real-time and the resulting style is applied using a single style transfer network. The user is provided with a set of 13 different painting styles and adjusts their relative strengths in the final style via sliders. In this demonstration, the user is an active participant in producing the pastiche. A Quick History of Style Transfer While transferring the style of one image to another has existed for nearly 15 years [1] [2], leveraging neural networks to accomplish it is both very recent and very fascinating. In \u201CA Neural Algorithm of Artistic Style\u201D [3], researchers Gatys, Ecker & Bethge introduced a method that uses deep convolutional neural network (CNN) classifiers. The pastiche image is found via optimization: the algorithm looks for an image which elicits the same kind of activations in the CNN\u2019s lower layers - which capture the overall rough aesthetic of the style input (broad brushstrokes, cubist patterns, etc.) - yet produces activations in the higher layers - which capture the things that make the subject recognizable - that are close to those produced by the content image. From some starting point (e.g. random noise, or the content image itself), the pastiche image is progressively refined until these requirements are met. Content image: The Tübingen Neckarfront by Andreas Praefcke, Style painting: \u201CHead of a Clown\u201D, by Georges Rouault. The pastiches produced via this algorithm look spectacular: Figure adapted from L. Gatys et al. \"A Neural Algorithm of Artistic Style\" (2015).  This work is considered a breakthrough in the field of deep learning research because it provided the first proof of concept for neural network-based style transfer. Unfortunately this method for stylizing an individual image is computationally demanding. For instance, in the first demos available on the web, one would upload a photo to a server, and then still have plenty of time to go grab a cup of coffee before a result was available. This process was sped up significantly by subsequent research [4, 5] that recognized that this optimization problem may be recast as an image transformation problem, where one wishes to apply a single, fixed painting style to an arbitrary content image (e.g. a photograph). The problem can then be solved by teaching a feed-forward, deep convolutional neural network to alter a corpus of content images to match the style of a painting. The goal of the trained network is two-fold: maintain the content of the original image while matching the visual style of the painting. The end result of this was that what once took a few minutes for a single static image, could now be run real time (e.g. applying style transfer to a live video). However, the increase in speed that allowed real-time style transfer came with a cost - a given style transfer network is tied to the style of a single painting, losing some flexibility of the original algorithm, which was not tied to any one style. This means that to build a style transfer system capable of modeling 100 paintings, one has to train and store 100 separate style transfer networks. Our Contribution: Learning and Combining Multiple Styles We started from the observation that many artists from the impressionist period employ similar brush stroke techniques and color palettes. Furthermore, painting by say, Monet, are even more visually similar. Poppy Field (left) and Impression, Sunrise (right) by Claude Monet. Images from Wikipedia We leveraged this observation in our training of a machine learning system. That is, we trained a single system that is able to capture and generalize across many Monet paintings or even a diverse array of artists across genres. The pastiches produced are qualitatively comparable to those produced in previous work, while originating from the same style transfer network. Pastiches produced by our single network, trained on 32 varied styles. These pastiches are qualitatively equivalent to those created by single-style networks: Image Credit: (from top to bottom) content photographs by Andreas Praefcke, Rich Niewiroski Jr. and J.-H. Janßen, (from left to right) style paintings by William Glackens, Paul Signac, Georges Rouault, Edvard Munch and Vincent van Gogh. The technique we developed is simple to implement and is not memory intensive. Furthermore, our network, trained on several artistic styles, permits arbitrary combining multiple painting styles in real-time, as shown in the video above. Here are four styles being combined in different proportions on a photograph of Tübingen: Unlike previous approaches to fast style transfer, we feel that this method of modeling multiple styles at the same time opens the door to exciting new ways for users to interact with style transfer algorithms, not only allowing the freedom to create new styles based on the mixture of several others, but to do it in real-time. Stay tuned for a future post on the Magenta blog, in which we will describe the algorithm in more detail and release the TensorFlow source code to run this model and demo yourself. We also recommend that you check out Nat & Lo\u2019s fantastic video explanation on the subject of style transfer. References [1] Efros, Alexei A., and William T. Freeman. Image quilting for texture synthesis and transfer (2001). [2] Hertzmann, Aaron, Charles E. Jacobs, Nuria Oliver, Brian Curless, and David H. Salesin. Image analogies (2001). [3] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. A Neural Algorithm of Artistic Style (2015). [4] Ulyanov, Dmitry, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture Networks: Feed-forward Synthesis of Textures and Stylized Images (2016). [5] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. Perceptual Losses for Real-Time Style Transfer and Super-Resolution (2016). * This work was done during an internship with the Google Brain Team. Vincent is currently a Ph.D. candidate at MILA, Université de Montréal.↩ Google Labels: Google Brain , Neural Networks , Style Transfer , TensorFlow    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"three\tResearch"}
{"author":"crowsonkb","created":"Thu Nov 24 11:53:29 EST 2016","text":"http:\/\/style-transfer.kath.io\n\nIt's optimizer-based (not feed-forward) and there is a visualization of each iterate allowing one to see the optimizer's progress. The optimizer-based method of stylization renders higher quality images IMHO. There's limited interactivity, in that one can change parameters like the image size or the weights of each layer in the loss function, without restarting. (It will just clear the L-BFGS memory or the Adam moment estimations and continue on using the current image as the initial state.)","flair":"four\tProject"}
{"author":"pmigdal","created":"Sat Oct 15 14:24:07 EDT 2016","text":"Sarcasm is almost impossible for computers to spot. A mathematical approach to linguistics could change that. Back in 1970, the social activist Irina Dunn scribbled a slogan on the back of a toilet cubicle door at the University of Sydney. It said: \u201CA woman needs a man like a fish needs a bicycle.\u201D The phrase went viral and eventually became a famous refrain for the growing feminist movement of the time.  The phrase is also an example of sarcasm. The humor comes from the fact that a fish doesn\u2019t need a bicycle. Most humans have little trouble spotting this. But while various advanced machine learning techniques have helped computers spot other forms of humor, sarcasm still largely eludes them. These other forms of humor can be spotted by looking for, say, positive verbs associated with negative or undesirable situation. And some researchers have used this approach to look for sarcasm. But sarcasm is often devoid of sentiment. The phrase above is a good example\u2014it contains no sentiment-bearing words. So a new strategy is clearly needed if computers are ever to spot this kind of joke. Today, Aditya Joshi at the Indian Institute of Technology Bombay in India, and a few pals, say they\u2019ve hit on just such a strategy. They say their new approach dramatically improves the ability of computers to spot sarcasm. Their method is relatively straightforward. Instead analyzing the sentiment in a sentence, Joshi and co analyze the similarity of the words. The do this by studying the way words relate to each other in a vast database of Google News stories containing some three million words. This is known as the Word2Vec database. This database has been analyzed extensively to determine how often words appear next to each other. This allows them to be represented as vectors in in a high dimensional space. It turns out that similar words can be represented by similar vectors and that vector space mathematics can capture simple relationships between them. For example, \u201Cking \u2013 man + woman = queen.\u201D Although there are clear differences between the words \u201Cman\u201D and \u201Cwoman,\u201D they occupy similar parts of the vector space. However, the words bicycle and fish occupy entirely different parts of the space and so are thought of as very different. According to Joshi and co, sentences that contrast similar concepts with dissimilar ones are more likely to be sarcastic. To test this idea, they study the similarity between words in a database of quotes on the Goodreads website. The team chose only quotes that have been tagged \u201Csarcastic\u201D by readers and, as a control, also include quotes tagged as \u201Cphilosophy.\u201D This results in a database of 3,629 quotes, of which 759 are sarcastic. The team then compared the word vectors in each quote looking for similarities and differences. The results make for interesting reading. Joshi and co say this word embedding approach is significantly better than other techniques at spotting sarcasm. \u201CWe observe an improvement in sarcasm detection,\u201D they say. The new approach isn\u2019t perfect, of course. And the errors it makes are instructive. For example, it did not spot the sarcasm in the following quote: \u201CGreat. Relationship advice from one of America\u2019s most wanted.\u201D That\u2019s probably because many of these words have multiple meanings that the Word2Vec embedding does not capture. Another sarcastic sentence it fails to spot is: \u201COh, and I suppose the apple ate the cheese.\u201D In this case, apple and cheese have a high similarity score and none of the words pairs shows a meaningful difference. So this example does not follow the rule that the algorithm is designed to search for. The algorithm also incorrectly identifies some sentences as sarcastic. Joshi and co point to this one, for example: \u201COh my love, I like to vanish in you like a ripple vanishes in an ocean\u2014slowly, silently and endlessly.\u201D Humans had not tagged this as sarcastic. However, it is not hard to imagine this sentence being used sarcastically. Overall, this is interesting work which raises some directions for future research. In particular, it would be fascinating to use this kind of algorithm to create sarcastic sentences and perhaps use human judges to decide whether or not they work in this sense. Beyond that is the task of computational humor itself. That\u2019s an ambitious goal but perhaps one that is not entirely out of reach. Much humor is formulaic so an algorithm ought to be able to apply such a formula with ease. Yeah, right! Ref: arxiv.org\/abs\/1610.00883: Are Word Embedding-based Features Useful for Sarcasm Detection?","flair":"three\tResearch"}
{"author":"inxurgence","created":"Wed Nov 02 22:28:28 EDT 2016","text":" Skip to content Algobeans Layman Tutorials in Analytics Menu Home About All Posts Subscribe Artificial Neural Networks Introduction (Part II) November 3, 2016November 3, 2016 We\u2019ve learned how Artificial Neural Networks (ANN) can be used to recognize handwritten digits in a previous post. In the current post, we discuss additional techniques to improve the accuracy of neural networks. Neural networks have been used successfully to solve problems such as image\/audio recognition and language processing (see Figure 1). Figure 1. Uses of neural networks \u2013 click to enlarge. Despite their potential, neural networks were popular only in recent years due to 3 reasons: Advances in storing and sharing data. With more data available to train on, the performance of neural networks have improved. Increased computing power. Used mainly to display computer images in the past, graphics processing units (GPUs) were discovered to be up to 150 faster than central processing units (CPUs). Now, GPUs enable neural network models to train efficiently on large datasets. Enhanced neural network algorithms. Matching the performance of a human brain is a difficult feat, but techniques have been developed to improve the performance of neural network algorithms, 3 of which are discussed in this post: Distortion (to increase training data) Mini-Batch Gradient Descent (to shorten training time) Dropout (to improve prediction accuracy) Technique 1: Distortion (increase training data) An artificial neural network learns to recognize handwritten digits when it is given more handwritten images to train on, along with labels of those images. Hence, providing a sufficiently large training dataset of labelled images is critical. However, sometimes the number of available labelled images could be limited. One way to overcome data shortage is to create more data. By applying different distortions to existing images, each distorted image could be treated as a new training example, thus vastly expanding the size of our training data. Figure 2. Different distortions applied to the digit \u201C3\u201D. Source: CodeProject The most effective distortions are the ones that are represented in the existing dataset. For example, we could rotate the images to simulate how people write at an angle. Another technique is elastic deformation, which stretches and squeezes an image at certain points to simulate uncontrolled oscillations of hand muscles (see Figure 2). Distortion techniques could also be applied to non-visual data. To train a neural network to recognize speech, background noises could be added to existing audio clips to generate new audio samples. Background noises used should already be present in our dataset. Technique 2: Mini-Batch Gradient Descent (shorten training time) In our previous tutorial, we learned that an artificial neural network comprises neurons, and their activation is governed by a set of rules. Using a mathematical concept called gradient descent, neuron activation rules are tweaked incrementally to improve overall prediction accuracy. To do this however, the neural network cycles through every single training example before determining how best to revise the rules. Hence, while a larger training dataset improves prediction, it will also increase the time taken to process all the training examples. A more efficient solution would be to look at only a small batch of examples each time to approximate the best rule change. This technique is called mini-batch gradient descent. To understand how this works, let\u2019s look at the following analogy (corresponding technical terms are in bold): Imagine you are a ruler of a rich kingdom, and you need to decide how much money to allocate (neurons\u2019 rules) to each government department (neurons). Being a rich ruler, you have an unlimited budget, and you want to take into account the wishes of everyone (training data) in your kingdom. However, everyone has a different idea of how the budget should be allocated. Figure 3. Mini-batch gradient descent analogy. Step 1: You consult individuals on your budget plans. During the interview, each person will state, vaguely, the degree of change (gradient) they want to see in budget allocation. For example, to increase education spending by a bit, and to cut welfare spending by a lot. Step 2: You take an average of everyone\u2019s views (gradient descent) and revise your budget accordingly (back propagation). Because of your citizens\u2019 ambiguous wordings, you are careful not to change the budget too drastically. Instead, you stick to small changes (learning rate). Step 3: You present your budget plan, and your citizens vote on whether they approve or disapprove of it (model accuracy). If you decide that the approval rate is high enough, the budget is passed. Otherwise, you repeat Steps 1 to 3, gradually improving approval rates. This process is slow because you take one step only after speaking to everyone. One way to speed things up is to approximate the overall opinion by consulting a small subset of your citizens (mini-batch gradient descent). The following graphs compare prediction accuracy between a vanilla gradient descent (left) vs. mini-batch gradient descent (right): Figure 4. Performance of a vanilla gradient descent (left) and a mini-batch gradient descent with 10 training examples in a batch (right) in a linear regression model predicting 100 simulated training samples. Using a vanilla gradient descent, prediction error decreased steadily and stabilized after about 250 cycles. On the other hand, a mini-batch gradient descent caused error to decrease with fluctuations and it stabilized only after about 400 cycles. While mini-batch gradient descent needed more cycles to reach the end, each phase cycle was much shorter and thus required less time on the whole. Technique 3: Dropout (improve prediction accuracy) Sometimes a neural network might attempt to adjust its neuron activation rules to fit training examples, to the point where the rules do not generalize well to new examples. This phenomenon is called overfitting, a common problem in machine learning which could lead to poor accuracy when predicting new data. The dropout technique could prevent that. Let\u2019s look at another analogy to describe the intuition behind dropout. Imagine yourself as the manager of a soccer team. You have two players who have developed strong teamwork and chemistry after playing together for many months. While this is advantageous when both players in the game, their over-reliance on each other might impair performance when one player injured. To overcome this problem, you could force these two players to train with other team members more often. In the same way, neurons in a neural network might grow reliant on each other when a few neurons co-adapt to patterns in training examples, causing their rules to change in a similar way during gradient descent. Because they do not work with other neurons, they might overlook intrinsic features of training examples, resulting in less robust predictions for new data. To solve this, we could force different neurons to work together by randomly dropping half the neurons in each cycle of a gradient descent. Figure 5. Fully-connected neural network (left) and neural network with dropped neurons (right). In the dropout, neurons B, D, and F do not transmit signals to other neurons. Neurons which are dropped are completely deactivated and do not send any signals. Hence, they do not affect the activation of neurons in the next layer. Furthermore, their rules remain constant during that cycle, and the entire neural network trains as if these neurons did not exist. The dropout technique thus forces neurons to discover more features in training examples as neurons collaborate in different combinations. Conclusion This wraps up the 3 techniques that could be used to improve the accuracy of artificial neural networks. Did you learn something useful today? We would be glad to inform you when we have new tutorials, so that your learning continues! Sign up below to get bite-sized tutorials delivered to your inbox: Copyright © 2015-Present Algobeans.com. All rights reserved. Be a cool bean. Share this: Share on Facebook (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Reddit (Opens in new window) Click to email (Opens in new window) Like this: Like Loading... Related Posted in: Tutorial | Tagged: AI, distortion, dropout, gradient descent, layman, machine learning, neural networks Post navigation  k-Nearest Neighbors & Anomaly Detection Tutorial Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Search for: Wanna Learn Data Science? Get intuitive explanations focusing on core concepts with no math. Discover applications and pitfalls in concise 1500-word chapters. Follow Us Facebook Twitter RSS About Algobeans Algobeans is the brainchild of two data science enthusiasts, Annalyn (University of Cambridge) and Kenneth (Stanford University). We noticed that while data science is increasingly used to improve workplace decisions, many people know little about the field. Hence, we created Algobeans so that everyone and anyone can learn - be it an aspiring student or enterprising business professional. Each tutorial covers the important functions and assumptions of a data science technique, without any math or jargon. We also illustrate these techniques with real-world data and examples. wanna learn data science? Get intuitive explanations focusing on core concepts with no math. Discover applications and pitfalls in concise 1500-word posts delivered to your inbox. Copyright © 2015-Present Algobeans.com. All rights reserved. Be a cool bean. Credits to Freepik for pretty images. Facebook Twitter RSS Send to Email Address Your Name Your Email Address Cancel Post was not sent - check your email addresses! Email check failed, please try again Sorry, your blog cannot share posts by email. %d bloggers like this: ","flair":"four\tProject"}
{"author":"ebazarov","created":"Sun Oct 23 04:45:54 EDT 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 201 Fork 30 ArdalanM\/pyLightGBM Code Issues 6 Pull requests 1 Projects 0 Pulse Graphs Python binding for Microsoft LightGBM 120 commits 2 branches 2 releases 6 contributors Jupyter Notebook 82.7% Python 17.3% Jupyter Notebook Python Clone or download Clone with HTTPS Use Git or checkout with SVN using the web URL. Download ZIP Find file Branch: master Switch branches\/tags Branches Tags dev master Nothing to show 0.2.6 0.2.5 Nothing to show New pull request Latest commit f1c6397 Nov 16, 2016 ArdalanM committed on GitHub Merge pull request #33 from xujin1982\/master \u2026 Update models.py Permalink Failed to load latest commit information. examples add dart example Nov 12, 2016 notebooks Adding csv support for not sparse data Nov 2, 2016 pylightgbm Update models.py Nov 15, 2016 tests Merge branch 'dev' Nov 13, 2016 .gitignore add ignore Oct 19, 2016 .travis.yml update Nov 5, 2016 LICENSE add license Nov 8, 2016 README.md update readme Nov 14, 2016 pytest.ini add ci Nov 5, 2016 requirements.txt update requirements to allow to run all examples without an error Nov 10, 2016 setup.py version 0.2.6 Nov 12, 2016 README.md pyLightGBM: python binding for Microsoft LightGBM Features: Regression, Classification (binary, multi class) Feature importance (clf.feature_importance()) Early stopping (clf.best_round) Works with scikit-learn: GridSearchCV, cross_val_score, etc... Silent mode (verbose=False) Installation Install lastest verion of Microsoft LightGBM then install the wrapper: pip install git+https:\/\/github.com\/ArdalanM\/pyLightGBM.git Examples Regression: import numpy as np from sklearn import datasets, metrics, model_selection from pylightgbm.models import GBMRegressor # full path to lightgbm executable (on Windows include .exe) exec = \"~\/Documents\/apps\/LightGBM\/lightgbm\" X, y = datasets.load_diabetes(return_X_y=True) clf = GBMRegressor(exec_path=exec, num_iterations=100, early_stopping_round=10, num_leaves=10, min_data_in_leaf=10) x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2) clf.fit(x_train, y_train, test_data=[(x_test, y_test)]) print(\"Mean Square Error: \", metrics.mean_squared_error(y_test, clf.predict(x_test))) Binary Classification: import numpy as np from sklearn import datasets, metrics, model_selection from pylightgbm.models import GBMClassifier # full path to lightgbm executable (on Windows include .exe) exec = \"~\/Documents\/apps\/LightGBM\/lightgbm\" X, Y = datasets.make_classification(n_samples=200, n_features=10) x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2) clf = GBMClassifier(exec_path=exec, min_data_in_leaf=1) clf.fit(x_train, y_train, test_data=[(x_test, y_test)]) y_pred = clf.predict(x_test) print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred)) Grid Search: import numpy as np from sklearn import datasets, metrics, model_selection from pylightgbm.models import GBMClassifier # full path to lightgbm executable (on Windows include .exe) exec = \"~\/Documents\/apps\/LightGBM\/lightgbm\" X, Y = datasets.make_classification(n_samples=1000, n_features=10) gbm = GBMClassifier(exec_path=exec, metric='binary_error', early_stopping_round=10, bagging_freq=10) param_grid = {'learning_rate': [0.1, 0.04], 'bagging_fraction': [0.5, 0.9]} scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True) clf = model_selection.GridSearchCV(gbm, param_grid, scoring=scorer, cv=2) clf.fit(X, Y) print(\"Best score: \", clf.best_score_) print(\"Best params: \", clf.best_params_) Notebooks Using pyLightGBM for Kaggle competition (Allstate Claims Severity) Bayesian global optimization with pyLightGBM using data from Kaggle competition (Allstate Claims Severity) Available parameters (default values): application=\"regression\" num_iterations=10 learning_rate=0.1 num_leaves=127 tree_learner=\"serial\" num_threads=1 min_data_in_leaf=100 metric='l2' is_training_metric=False feature_fraction=1. feature_fraction_seed=2 bagging_fraction=1. bagging_freq=0 bagging_seed=3 metric_freq=1 early_stopping_round=0 max_bin=255 is_unbalance=False num_class=1 boosting_type='gbdt' min_sum_hessian_in_leaf=10 drop_rate=0.01 drop_seed=4 max_depth=-1 lambda_l1=0. lambda_l2=0. min_gain_to_split=0. verbose=True model=None Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"zhongwenxu","created":"Mon Oct 31 05:32:45 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.09027 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes Authors: Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, Timothy P Lillicrap (Submitted on 27 Oct 2016) Abstract: Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\\!000\\times$ faster and with $3,\\!000\\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer. Comments: in 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain Subjects: Learning (cs.LG) Cite as: arXiv:1610.09027 [cs.LG]   (or arXiv:1610.09027v1 [cs.LG] for this version) Submission history From: Jonathan Hunt [view email] [v1] Thu, 27 Oct 2016 22:38:05 GMT (2240kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"downtownslim","created":"Thu Oct 20 20:23:10 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1602.05179 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1602 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Benjamin Scellier Yoshua Bengio Bookmark (what is this?) Computer Science > Learning Title: Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation Authors: Benjamin Scellier, Yoshua Bengio (Submitted on 16 Feb 2016 (v1), last revised 26 Sep 2016 (this version, v4)) Abstract: We introduce Equilibrium Propagation (e-prop), a learning algorithm for energy-based models. This algorithm involves only one kind of neural computation both for the first phase (when the prediction is made) and the second phase (after the target is revealed) of training. Contrary to backpropagation in feedforward networks, there is no need for special computation in the second phase of our learning algorithm. Equilibrium Propagation combines features of Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: the algorithm computes the exact gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of e-prop corresponds to only nudging the first-phase fixed point towards a configuration that has lower cost value. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The theory developed in this paper shows that the signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to the objective function. Thus, this work makes it more plausible that a mechanism similar to backpropagation could be implemented by brains. Subjects: Learning (cs.LG) Cite as: arXiv:1602.05179 [cs.LG]   (or arXiv:1602.05179v4 [cs.LG] for this version) Submission history From: Benjamin Scellier [view email] [v1] Tue, 16 Feb 2016 20:46:51 GMT (300kb,D) [v2] Wed, 24 Feb 2016 11:13:08 GMT (301kb,D) [v3] Tue, 20 Sep 2016 16:15:26 GMT (436kb,D) [v4] Mon, 26 Sep 2016 09:55:15 GMT (439kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"c_y_smith","created":"Tue Oct 11 17:30:38 EDT 2016","text":"This is a TensorFlow implementation of several techniques described in the papers: Additionally, techniques are presented for semantic segmentation and multiple style transfer. The Neural Style algorithm synthesizes a pastiche by separating and combining the content of one image with the style of another image using convolutional neural networks (CNN). Below is an example of transferring the artistic style of The Starry Night onto a photograph of an African lion: Transferring the style of various artworks to the same content image produces qualitatively convincing results: Here we reproduce Figure 3 from the first paper, which renders a photograph of the Neckarfront in Tübingen, Germany in the style of 5 different iconic paintings The Shipwreck of the Minotaur, The Starry Night, Composition VII, The Scream, Seated Nude: The relative weight of the style and content can be controlled. Here we render with an increasing style weight applied to Red Canna: More than one style image can be used to blend multiple artistic styles. Top row (left to right): The Starry Night + The Scream, The Scream + Composition VII, Seated Nude + Composition VII Bottom row (left to right): Seated Nude + The Starry Night, Oversoul + Freshness of Cold, David Bowie + Skull When using multiple style images, the degree of blending between the images can be controlled. Top row (left to right): content image, .2 The Starry Night + .8 The Scream, .8 The Starry Night + .2 The Scream Bottom row (left to right): .2 Oversoul + .8 Freshness of Cold, .5 Oversoul + .5 Freshness of Cold, .8 Oversoul + .2 Freshness of Cold The color scheme of the original image can be preserved by including the flag . Colors are transferred using either the YUV, YCrCb, CIE L*a*b*, or CIE L*u*v* color spaces. Here we reproduce Figure 1 and Figure 2 in the third paper using luminance-only transfer: Left to right: content image, stylized image, stylized image with the original colors of the content image The algorithm is not constrained to artistic painting styles. It can also be applied to photographic textures to create pareidolic images. Style can be transferred to semantic segmentations in the content image. Multiple styles can be transferred to the foreground and background of the content image. Note: The masking is done during synthesis; not as a post-processing step. Animations can be rendered by applying the algorithm to each source frame. For the best results, the gradient descent is initialized with the previously stylized frame warped to the current frame according to the optical flow between the pair of frames. Loss functions for temporal consistency are used to penalize pixels excluding disoccluded regions and motion boundaries. Top row (left to right): source frames, ground-truth optical flow visualized Bottom row (left to right): disoccluded regions and motion boundaries, stylized frames The initialization of the gradient descent is controlled using for single images and or for video frames. White noise allows an arbitrary number of distinct images to be generated. Whereas, initializing with a fixed image always converges to the same output. Here we reproduce Figure 6 from the first paper: Top row (left to right): Initialized with the content image, the style image, white noise (RNG seed 1) Bottom row (left to right): Initialized with white noise (RNG seeds 2, 3, 4) The feature complexities and receptive field sizes increase down the CNN heirarchy. Here we reproduce Figure 3 from the original paper: Rows: increasing subsets of CNN layers; i.e. 'conv4_1' means using 'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1'. Columns: alpha\/beta ratio of the the content and style reconstruction (see Content \/ Style Tradeoff). Note: Paths to images should not contain the character to represent your home directory; you should instead use a relative path or the absolute path. To use multiple style images, pass a space-separated list of the image names and image weights like this: Note: When using you must have previously computed the backward and forward optical flow between the frames. See and By default, uses the NVIDIA cuDNN GPU backend for convolutions and L-BFGS for optimization. These produce better and faster results, but can consume a lot of memory. You can reduce memory usage with the following: All images were rendered on a machine with: The implementation is based on the projects: Artistic images were created by the modern artists: Artistic images were created by the popular historical artists: Bash shell scripts for testing were created by my brother Sheldon Smith. If you find this code useful for your research, please cite:","flair":"four\tProject"}
{"author":"Mandrathax","created":"Mon Nov 07 10:23:23 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01491 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1611 Change to browse by: cond-mat cond-mat.dis-nn cs cs.AI cs.CC stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Understanding Deep Neural Networks with Rectified Linear Units Authors: Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee (Submitted on 4 Nov 2016 (v1), last revised 11 Nov 2016 (this version, v2)) Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial (in data size and circuit size) time algorithm to train a ReLU DNN with one hidden layer and a single input to global optimality. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\\mathbb{R}^n \\to \\mathbb{R}$ function is representable by a ReLU DNN if and only if it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN.We follow up with gap results showing that there is a smoothly parameterized family of $\\mathbb{R}\\to \\mathbb{R}$ \"hard\" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\\mathbb{R}^n\\to \\mathbb{R}$ functions for $n\\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of \"hard\" functions in the neural nets literature. Comments: In this updated version of the paper we incorporate a suggestion by Adam Klivans which now makes our training algorithm run in time polynomial in the data size as well as the circuit size Subjects: Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Computational Complexity (cs.CC); Machine Learning (stat.ML) Cite as: arXiv:1611.01491 [cs.LG]   (or arXiv:1611.01491v2 [cs.LG] for this version) Submission history From: Anirbit Mukherjee [view email] [v1] Fri, 4 Nov 2016 18:54:50 GMT (545kb,D) [v2] Fri, 11 Nov 2016 20:25:56 GMT (546kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"J_Baur136","created":"Thu Oct 06 11:07:21 EDT 2016","text":"Hello all, I'm not sure if this is the best place to post this and if someone could point me in a better direction I would gladly take this elsewhere. I am a student in college and I have to interview someone in a career similar to the one I would like to be in. I am looking into a career in the artificial intelligence\/machine learning field, so if any of you have a career in that or very similar to that I would love to ask you about 10 questions. If you are willing leaving a comment that I could PM you would be amazing. Thanks you for your time, have a great day!","flair":"null\tnull"}
{"author":"clbam8","created":"Thu Nov 17 17:36:52 EST 2016","text":" Home Research Publications AlphaGo DQN Applied DeepMind Health DeepMind for Google News & Blog About Us Careers Research Highlighted Research AlphaGo DQN Publications Latest Research News Reinforcement learning with unsupervised auxiliary tasks Applied DeepMind Health Streams DeepMind for Google Latest Applied News Working with the NHS to build lifesaving technology Careers Home News & Blog About Us Press Terms and Conditions Privacy Policy Reinforcement learning with unsupervised auxiliary tasks Our primary mission at DeepMind is to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be taught how. Our reinforcement learning agents have achieved breakthroughs in Atari 2600 games and the game of Go. Such systems, however, can require a lot of data and a long time to learn so we are always looking for ways to improve our generic learning algorithms. Our recent paper \u201CReinforcement Learning with Unsupervised Auxiliary Tasks\u201D introduces a method for greatly improving the learning speed and final performance of agents. We do this by augmenting the standard deep reinforcement learning methods with two main additional tasks for our agents to perform during training.  A visualisation of our agent in a Labyrinth maze foraging task can be seen below. The first task involves the agent learning how to control the pixels on the screen, which emphasises learning how your actions affect what you will see rather than just prediction. This is similar to how a baby might learn to control their hands by moving them and observing the movements. By learning to change different parts of the screen, our agent learns features of the visual input that are useful for playing the game and getting higher scores. In the second task the agent is trained to predict the onset of immediate rewards from a short historical context. In order to better deal with the scenario where rewards are rare we present the agent with past rewarding and non-rewarding histories in equal proportion. By learning on rewarding histories much more frequently, the agent can discover visual features predictive of reward much faster. The combination of these auxiliary tasks, together with our previous A3C paper is our new UNREAL agent (UNsupervised REinforcement and Auxiliary Learning). We tested this agent on a suite of 57 Atari games as well as a 3D environment called Labyrinth with 13 levels. In all the games, the same UNREAL agent is trained in the same way, on the raw image output from the game, to produce actions to maximise the score or reward of the agent in the game. The behaviour required to get game rewards is incredibly varied, from picking up apples in 3D mazes to playing Space Invaders - the same UNREAL algorithm learns to play these games often to human level and beyond. Some results and visualisations can be seen in the video below. UNREAL agent playing Labyrinth In Labyrinth, the result of using the auxiliary tasks - controlling the pixels on the screen and predicting when reward is going to occur - means that UNREAL is able to learn over 10x faster than our previous best A3C agent, and reaches far better performance. We can now achieve 87% of expert human performance averaged across the Labyrinth levels we considered, with super-human performance on a number of them. On Atari the agent now achieves on average 9x human performance. We hope that this work will allow us to scale up our agents to ever more complex environments. Read the full paper here. Share Article LinkedIn WhatsApp SMS Reddit Authors Thursday, 17 November 2016 Max Jaderberg Research Scientist, DeepMind Vlad Mnih Research Scientist, DeepMind Wojciech Marian Czarnecki Research Engineer, DeepMind Show all results Follow Research Research Applied Applied News & Blog News & Blog About Us About Us Careers Careers Press Terms and Conditions Privacy Policy Alphabet Inc © 2016 DeepMind Technologies Limited DeepMind.com uses cookies to help give you the best possible user experience. Find Out More ","flair":"three\tResearch"}
{"author":"UltraMarathonMan","created":"Tue Sep 27 15:59:06 EDT 2016","text":"The talks at the Deep Learning School on September 24\/25, 2016 were amazing. I clipped out individual talks  from the full live streams and provided links to each below in case that's useful for people who want to watch specific talks several times (like I do). Please check out the official website (http:\/\/www.bayareadlschool.org) and full live streams below.\n\nHaving read, watched, and presented deep learning material over the past few years, I have to say that this is one of the best collection of introductory deep learning talks I've yet encountered. Here are links to the individual talks and the full live streams for the two days:\n\n1. Foundations of Deep Learning (Hugo Larochelle, Twitter) - https:\/\/youtu.be\/zij_FTbJHsk\n2. Deep Learning for Computer Vision (Andrej Karpathy, OpenAI) - https:\/\/youtu.be\/u6aEYuemt0M\n3. Deep Learning for Natural Language Processing (Richard Socher, Salesforce) - https:\/\/youtu.be\/oGk1v1jQITw\n4. TensorFlow Tutorial (Sherry Moore, Google Brain) - https:\/\/youtu.be\/Ejec3ID_h0w\n5. Foundations of Unsupervised Deep Learning (Ruslan Salakhutdinov, CMU) - https:\/\/youtu.be\/rK6bchqeaN8\n6. Nuts and Bolts of Applying Deep Learning (Andrew Ng) - https:\/\/youtu.be\/F1ka6a13S9I\n7. Deep Reinforcement Learning (John Schulman, OpenAI) - https:\/\/youtu.be\/PtAIh9KSnjo\n8. Theano Tutorial (Pascal Lamblin, MILA) - https:\/\/youtu.be\/OU8I1oJ9HhI\n9. Deep Learning for Speech Recognition (Adam Coates, Baidu) - https:\/\/youtu.be\/g-sndkf7mCs\n10. Torch Tutorial (Alex Wiltschko, Twitter) - https:\/\/youtu.be\/L1sHcj3qDNc\n11. Sequence to Sequence Deep Learning (Quoc Le, Google) - https:\/\/youtu.be\/G5RY_SUJih4\n12. Foundations and Challenges of Deep Learning (Yoshua Bengio) - https:\/\/youtu.be\/11rsu_WwZTc\n\nFull Day Live Streams:\nDay 1: https:\/\/youtu.be\/eyovmAtoUx0\nDay 2: https:\/\/youtu.be\/9dXiAecyJrY\n\nGo to http:\/\/www.bayareadlschool.org for more information on the event, speaker bios, slides, etc. Huge thanks to the organizers (Shubho Sengupta et al) for making this event happen.","flair":"null\tnull"}
{"author":"AlexeyKurakin","created":"Wed Nov 16 00:47:48 EST 2016","text":"Links to the code, checkpoint and accuracy of the checkpoint could be found in \"Pre-trained Models\" section at \nhttps:\/\/github.com\/tensorflow\/models\/blob\/master\/slim\/README.md#pre-trained-models","flair":"null\tnull"}
{"author":"Buck-Nasty","created":"Mon Oct 03 22:27:04 EDT 2016","text":" Google Research Blog The latest news from Research at Google How Robots Can Acquire New Skills from Their Shared Experience Monday, October 03, 2016 Posted by Sergey Levine (Google Brain Team), Timothy Lillicrap (DeepMind), Mrinal Kalakrishnan (X) The ability to learn from experience will likely be a key in enabling robots to help with complex real-world tasks, from assisting the elderly with chores and daily activities, to helping us in offices and hospitals, to performing jobs that are too dangerous or unpleasant for people. However, if each robot must learn its full repertoire of skills for these tasks only from its own experience, it could take far too long to acquire a rich enough range of behaviors to be useful. Could we bridge this gap by making it possible for robots to collectively learn from each other\u2019s experiences? While machine learning algorithms have made great strides in natural language understanding and speech recognition, the kind of symbolic high-level reasoning that allows people to communicate complex concepts in words remains out of reach for machines. However, robots can instantaneously transmit their experience to other robots over the network - sometimes known as \"cloud robotics\" - and it is this ability that can let them learn from each other. This is true even for seemingly simple low-level skills. Humans and animals excel at adaptive motor control that integrates their senses, reflexes, and muscles in a closely coordinated feedback loop. Robots still struggle with these basic skills in the real world, where the variability and complexity of the environment demands well-honed behaviors that are not easily fooled by distractors. If we enable robots to transmit their experiences to each other, could they learn to perform motion skills in close coordination with sensing in realistic environments? We previously wrote about how multiple robots could pool their experiences to learn a grasping task. Here, we will discuss new experiments that we conducted to investigate three possible approaches for general-purpose skill learning across multiple robots: learning motion skills directly from experience, learning internal models of physics, and learning skills with human assistance. In all three cases, multiple robots shared their experiences to build a common model of the skill. The skills learned by the robots are still relatively simple -- pushing objects and opening doors -- but by learning such skills more quickly and efficiently through collective learning, robots might in the future acquire richer behavioral repertoires that could eventually make it possible for them to assist us in our daily lives. Learning from raw experience with model-free reinforcement learning. Perhaps one of the simplest ways for robots to teach each other is to pool information about their successes and failures in the world. Humans and animals acquire many skills by direct trial-and-error learning. During this kind of \u2018model-free\u2019 learning -- so called because there is no explicit model of the environment formed -- they explore variations on their existing behavior and then reinforce and exploit the variations that give bigger rewards. In combination with deep neural networks, model-free algorithms have recently proved to be surprisingly effective and have been key to successes with the Atari video game system and playing Go. Having multiple robots allows us to experiment with sharing experiences to speed up this kind of direct learning in the real world. In these experiments we tasked robots with trying to move their arms to goal locations, or reaching to and opening a door. Each robot has a copy of a neural network that allows it to estimate the value of taking a given action in a given state. By querying this network, the robot can quickly decide what actions might be worth taking in the world. When a robot acts, we add noise to the actions it selects, so the resulting behavior is sometimes a bit better than previously observed, and sometimes a bit worse. This allows each robot to explore different ways of approaching a task. Records of the actions taken by the robots, their behaviors, and the final outcomes are sent back to a central server. The server collects the experiences from all of the robots and uses them to iteratively improve the neural network that estimates value for different states and actions. The model-free algorithms we employed look across both good and bad experiences and distill these into a new network that is better at understanding how action and success are related. Then, at regular intervals, each robot takes a copy of the updated network from the server and begins to act using the information in its new network. Given that this updated network is a bit better at estimating the true value of actions in the world, the robots will produce better behavior. This cycle can then be repeated to continue improving on the task. In the video below, a robot explores the door opening task. With a few hours of practice, robots sharing their raw experience learn to make reaches to targets, and to open a door by making contact with the handle and pulling. In the case of door opening, the robots learn to deal with the complex physics of the contacts between the hook and the door handle without building an explicit model of the world, as can be seen in the example below: Learning how the world works by interacting with objects. Direct trial-and-error reinforcement learning is a great way to learn individual skills. However, humans and animals don\u2019t learn exclusively by trial and error. We also build mental models about our environment and imagine how the world might change in response to our actions. We can start with the simplest of physical interactions, and have our robots learn the basics of cause and effect from reflecting on their own experiences. In this experiment, we had the robots play with a wide variety of common household objects by randomly prodding and pushing them inside a tabletop bin. The robots again shared their experiences with each other and together built a single predictive model that attempted to forecast what the world might look like in response to their actions. This predictive model can make simple, if slightly blurry, forecasts about future camera images when provided with the current image and a possible sequence of actions that the robot might execute: Top row: robotic arms interacting with common household items. Bottom row: Predicted future camera images given an initial image and a sequence of actions. Once this model is trained, the robots can use it to perform purposeful manipulations, for example based on user commands. In our prototype, a user can command the robot to move a particular object simply by clicking on that object, and then clicking on the point where the object should go: The robots in this experiment were not told anything about objects or physics: they only see that the command requires a particular pixel to be moved to a particular place. However, because they have seen so many object interactions in their shared past experiences, they can forecast how particular actions will affect particular pixels. In order for such an implicit understanding of physics to emerge, the robots must be provided with a sufficient breadth of experience. This requires either a lot of time, or sharing the combined experiences of many robots. An extended video on this project may be found here. Learning with the help of humans. So far, we discussed how robots can learn entirely on their own. However, human guidance is important, not just for telling the robot what to do, but also for helping the robots along. We have a lot of intuition about how various manipulation skills can be performed, and it only seems natural that transferring this intuition to robots can help them learn these skills a lot faster. In the next experiment, we provided each robot with a different door, and guided each of them by hand to show how these doors can be opened. These demonstrations are encoded into a single combined strategy for all robots, called a policy. The policy is a deep neural network which converts camera images to robot actions, and is maintained on a central server. The following video shows the instructor demonstrating the door-opening skill to a robot: Next, the robots collectively improve this policy through a trial-and-error learning process. Each robot attempts to open its own door using the latest available policy, with some added noise for exploration. These attempts allow each robot to plan a better strategy for opening the door the next time around, and improve the policy accordingly: Not surprisingly, we find that robots learn more effectively if they are trained on a curriculum of tasks that are gradually increasing in difficulty. In our experiment, each robot starts off by practicing the door-opening skill on a specific position and orientation of the door that the instructor had previously shown it. As it gets better at performing the task, the instructor starts to alter the position and orientation of the door to be just a bit beyond the current capabilities of the policy, but not so difficult that it fails entirely. This allows the robots to gradually increase their skill level over time, and expands the range of situations they can handle. The combination of human-guidance with trial-and-error learning allowed the robots to collectively learn the skill of door-opening in just a couple of hours. Since the robots were trained on doors that look different from each other, the final policy succeeds on a door with a handle that none of the robots had seen before: In all three of the experiments described above, the ability to communicate and exchange their experiences allows the robots to learn more quickly and effectively. This becomes particularly important when we combine robotic learning with deep learning, as is the case in all of the experiments discussed above. We\u2019ve seen before that deep learning works best when provided with ample training data. For example, the popular ImageNet benchmark uses over 1.5 million labeled examples. While such a quantity of data is not impossible for a single robot to gather over a few years, it is much more efficient to gather the same volume of experience from multiple robots over the course of a few weeks. Besides faster learning times, this approach might benefit from the greater diversity of experience: a real-world deployment might involve multiple robots in different places and different settings, sharing heterogeneous, varied experiences to build a single highly generalizable representation. Of course, the kinds of behaviors that robots today can learn are still quite limited. Even basic motion skills, such as picking up objects and opening doors, remain in the realm of cutting edge research. In all of these experiments, a human engineer is still needed to tell the robots what they should learn to do by specifying a detailed objective function. However, as algorithms improve and robots are deployed more widely, their ability to share and pool their experiences could be instrumental for enabling them to assist us in our daily lives. The experiments on learning by trial-and-error were conducted by Shixiang (Shane) Gu and Ethan Holly from the Google Brain team, and Timothy Lillicrap from DeepMind. Work on learning predictive models was conducted by Chelsea Finn from the Google Brain team, and the research on learning from demonstration was conducted by Yevgen Chebotar, Ali Yahya, Adrian Li, and Mrinal Kalakrishnan from X. We would also like to acknowledge contributions by Peter Pastor, Gabriel Dulac-Arnold, and Jon Scholz. Articles about each of the experiments discussed in this blog post can be found below: Deep Reinforcement Learning for Robotic Manipulation. Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine. [video] Deep Visual Foresight for Planning Robot Motion. Chelsea Finn, Sergey Levine. [video] [data] Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search. Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, Sergey Levine.  [video] Path Integral Guided Policy Search. Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, Sergey Levine. [video] Google Labels: DeepMind , Google Brain , Machine Learning , Robotics    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"cjmcmurtrie","created":"Tue Nov 01 09:52:21 EDT 2016","text":"I have implemented models that are very good at generating handwriting, and models that are very good at predicting MNIST digits - but I am failing at producing anything that can produce a legible translation of real handwritten documents.\n\nI have tried various recurrent networks with some serious proprocessing hacks - I use heuristics to split documents images into lines, and feed columns of pixels into a recurrent network. I set up training data to align these with output characters (in itself a significant ordeal). But output is still far below threshold for real applications.\n\nHas anyone had better luck? Are there any pre-trained models online that perform this function, or any methods that are known to produce results?","flair":"one\tDiscussion"}
{"author":"short_vix","created":"Fri Oct 28 09:59:30 EDT 2016","text":"Apologies in advance if this is the wrong place for this sort of post. If you guys are like me, you shift through hundreds of PDFs and haven't found a satisfactory research tool. I've been looking for quite some time and have not found one with the following set of features.\n\n * ability to search by author or title and edit the meta data( you usually have to correct this ).\n * ability to add multiple tags and search by tags\n * web, mobile or cross platform (I prefer tablet)\n * ability to annotate and share annotations with others\n * collaboration, ability to share work spaces, PDFs, notes exc.. exc..\n * drop box integration\n\nIs there something out there that has the following set of features? The closest tool I found is LiquidText PDF for the iPad however it's missing a quite a few features from the above set.","flair":"one\tDiscussion"}
{"author":"timgasper","created":"Thu Nov 03 12:24:45 EDT 2016","text":"Recently, we made our Bitfusion Deep Learning  AMIs available on the newly announced AWS P2 instances. Naturally, one of the first questions that arises is, how does the performance of the new P2 instances compare to that of the the previous generation G2 instances. In this post we take a a quick look at single-GPU performance across a variety of convolutional neural networks. To keep things consistent we start each EC2 instance with the exact same AMI, thus keeping the driver, cuda, cudnn, and framework the same across the instances. To evaluate TensorFlow performance we utilized the Bitfusion TensorFlow AMI along with the convnet-benchmark to measure for forward and backward propagation times for some of the more well known convolutional neural networks including AlexNet, Overfeat, VGG, and GoogleNet. Because of the much larger GPU memory of 12 GiB, the P2 instances can accommodate much larger batch sizes than the G2 instances. For the purpose of the benchmarks below, the batch sizes were selected for each network type such that they could run on the G2 as well as on the P2 instances. The Tables below summarize the results obtained for G2 and P2 instances: Averaging the speedup across all four types of networks, the results show an approximate ~2.42x improvement in performance \u2013 not bad for an instance which is only ~1.39 more expensive on an hourly on demand basis. We have several other Deep learning AMIs available in the AWS Marketplace including Caffe, Chainer, Theano, Torch, and Digits.  If you are interested in seeing GPU Performance benchmarks for any of the above drop us a note.","flair":"three\tResearch"}
{"author":"nickl","created":"Sat Oct 29 06:14:49 EDT 2016","text":" 1 Cross-Modal Scene Networks Yusuf Aytar*, Lluis Castrejon*, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba Abstract\u2014People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality. Index Terms\u2014cross-modal perception, domain adaptation, scene understanding. F 1 INTRODUCTION CAN you recognize the scenes in Figure 1, even thoughthey are depicted in different modalities? Most people have the capability to perceive a concept in one modality, but represent it independently of the modality. This cross- modal ability enables people to perform some important abstraction tasks, such as learning in different modalities (cartoons, stories) and applying them in the real-world. Unfortunately, visual representations do not yet have this cross-modal capability. Standard approaches typically learn a separate representation for each modality, which works well when operating within the same modality. However, the representations learned are not aligned across modalities, which makes cross-modal transfer difficult. Two modalities are strongly aligned if, for two images from each modality, we have paired data and correspon- dence at the level of objects. In contrast, weak alignment is if we have only unpaired data and a coarse global label that is shared across both images. For instance, if we have a picture of a bedroom and a line drawing of a different bedroom, the only thing that we know is shared across these two images is the scene type. However, they will differ in the objects and viewpoint inside. In this paper, our goal is to learn a representation for scenes that has strong alignment without using paired data. We seek to learn representations that will connect objects (such as bed, car) across modalities (e.g., a picture of a car, a line drawing of a car, and the word \u201Ccar\u201D) without ever specifying that such a correspondence exists. To investigate this, we assembled a new cross-modal scene dataset, which captures hundreds of natural scene types in five different modalities, and we show a few ex- amples in Figure 1. Using this dataset and only annotations of scene categories, we propose to learn an aligned cross- modal scene representation. Y Aytar, C Vondrick, A Torralba are with Massachusetts Institute of Technol- ogy, 77 Massachusetts Ave, Cambridge, MA 02139 USA. L Castrejon is with the Department of Computer Science, University of Toronto, Ontario, Canada. H Pirsiavash is with the University of Maryland Baltimore County, 1000 Hilltop Cir, ITE 342, Baltimore, MD 21250 USA Manuscript submitted October 14, 2016 We present two approaches to regularize cross-modal convolutional networks so that the intermediate represen- tations are aligned across modalities, even when only weak alignment of scene categories is available during training. Figure 2 visualizes the representation that our full method learns. Notice that our approach learns hidden units that activate on the same object, regardless of the modality. Although the only supervision is the scene category, our approach enables alignment to emerge automatically. Our approach builds on a foundation of domain adap- tation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision. However, our focus is learn- ing cross-modal representations when the modalities are significantly different (e.g., text and natural images) and with minimal supervision. In our approach, the only su- pervision we give is the scene category, and no alignments nor correspondences are annotated. To our knowledge, the adaptation of intermediate representations across several extremely different modalities with minimal supervision has not yet been extensively explored. We believe cross-modal representations can have a large impact on several computer vision applications. For exam- ple, data in one modality may be difficult to acquire for privacy, legal, or logistic reasons (eg, images in hospitals), but may be abundant in other modalities, allowing us to train models using accessible modalities. In search, users may wish to retrieve similar natural images given a query in a modality that is simpler for a human to produce (eg, drawing or writing). Additionally, some modalities may be more effective for human-machine communication. Our experiments suggest our network is learning an aligned cross-modal representation without paired data. We show four main results. Firstly, we show that our method enables better representations for cross-modal retrieval than a fine-tuning approach. Secondly, we experimented with zero-shot recognition and retrieval using our representation, and results suggest our approach can perform well even when training data for that category is missing for some modalities. Thirdly, we visualize the internal activations of our network, and we demonstrate that units automatically emerge that activate on high-level concepts agnostic of the 2 B e d ro o m K in d e rg a rt e n c la s sr o o m There is a bed with a striped bedspread. Beside this is a nightstand with a drawer. There is also a tall dresser and a chair with a blue cushion. On the dresser is a jewelry box and a clock. I am inside a room surrounded by my favorite things. This room is filled with pillows and a comfortable bed. There are stuffed animals everywhere. I have posters on the walls. My jewelry box is on the dresser. Real Clip art Sketches Spatial text Descriptions There are brightly colored wooden tables with little chairs. There is a rug in one corner with ABC blocks on it. There is a bookcase with picture books, a larger teacher's desk and a chalkboard. The young students gather in the room at their tables to color. They learn numbers and letters and play games. At nap time they all pull out mats and go to sleep. Fig. 1: Can you recognize scenes across different modalities? Above, we show a few examples of our new cross-modal scene dataset. In this paper, we investigate how to learn cross-modal scene representations. modality. Finally, we show that our learned representation enables us to reconstruct images across other modalities. The remainder of this paper describes and analyzes our cross-modal representations in detail. In section 2, we first discuss related work that our work builds upon. In section 3, we introduce our new cross-modal scene dataset. In section 4, we present two complementary approaches to regularize convolutional networks so that intermediate representations are aligned across modalities. In section 5, we present our visualizations and experiments in cross-modal transfer. 2 RELATED WORK Domain Adaptation: Domain adaptation techniques ad- dress the problem of learning models on some source data distribution that generalize to a different target distribu- tion. [36] proposes a method for domain adaptation using metric learning. In [16] this approach is extended to work on unsupervised settings where one does not have access to target data labels, while [39] uses deep CNNs instead. [38] shows the biases inherent in common vision datasets and [21] proposes models that remain invariant to them. [26] learns an aligned representation for domain adaptation using CNNs and the MMD metric. Our method differs from these works in that it seeks to find a cross-modal representations between highly different modalities instead of modelling close domain shifts. One-Shot\/Zero-Shot Learning: One-shot learning tech- niques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3]. In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.g. by using additional knowledge in the form of textual descriptions or attributes. The goal of our method is to learn aligned representations across domains, which could be used for zero-shot learning. Cross-modal content retrieval and multi-modal embed- dings: Large unannotated image collections are difficult to explore, and retrieving content given fine-grained queries might be a difficult task. A common solution to this issue is to use query examples from a different modality in which it is easy to express a concept (such as a clip art images, text or a sketches) and then rank the images in the collection according to their similarity to the input query. Matching can be done by establishing a similarity metric between content from different domains. [8] focuses on recovering semantically related natural images to a given sketch query and [42] uses query sketches to recover 3D shapes. [19] uses an MRF of topic models to retrieve images using text, while [34] models the correlations between visual SIFT features and text hidden topic models to retrieve media across both domains. CCA [17] and variants [35] are commonly em- ployed methods in cross-modal content retrieval. Another possibility is to learn a joint embedding for images and text in which nearest neighbors are semantically related. [13], [30] learn a semantic embedding that joins representations from a CNN trained on ImageNet and distributed word rep- resentations. [22], [44] extend them to include a decoder that maps common representations to captions. [37] maps visual features to a word semantic embedding. Our method learns a joint embedding for many different modalities, including different visual domains and text. Another group of works incorporate sound as another modality [29], [31]. Our joint representation is different from previous works in that it is initially obtained from a CNN and sentence embeddings are mapped to it. Furthermore, we do not require explicit one- to-one correspondences across modalities. Learning from Visual Abstraction: [47] introduced cli- part images for visual abstraction. The idea is to learn concepts by collecting data in the abstract world rather than the natural images so that we are not affected by mistakes in mid-level recognition e.g. object detectors. [12] learns dynamics and [48] learns sentence phrases in this abstract world and transfer them to natural images. Our work can complement this effort by learning models in a representation space that is invariant to modality. 3 Shared'Cross+Modal' Representation religious,*church,*plants,* impressive,*monks plants,*fruits,*basil, land,*mint Natural**Images Sketches Clip*Art Spatial*Text Descriptions Unit*13: Tower*Roof Unit*241: Plants Modal Specific'CNNs pool5 (MLP) Fig. 2: We learn low-level representations specific for each modality (white and grays) and a high-level representation that is shared across all modalities (red). Above, we also show masks of inputs that activate specific units the most [45]. Interestingly, although the network is trained without aligned data, units emerge in the shared representation that tend to fire on the same objects independently of the modality. 3 CROSS-MODAL PLACES DATASET We assembled a new dataset1 to train and evaluate cross- modal scene recognition models called CMPlaces. It covers five different modalities: natural images, line drawings, cartoons, text descriptions, and spatial text images. We show a few samples from these modalities in Figure 1. Each example in the dataset is annotated with a scene label. We use the same list of 205 scene categories as Places [46], which is one of the largest scene datasets available today. Hence, the examples in our dataset span a large number of natural situations. Note that the examples in our dataset are not paired between modalities since our goal is to learn strong alignments from weakly aligned data. Furthermore, this design decision eased data collection. We chose these modalities for two reasons. Firstly, since our goal is to study transfer across significantly different modalities, we seek modalities with different statistics to those of natural images (such as line drawings and text). Secondly, these modalities are easier to generate than real images, which is relevant to applications such as image retrieval. For each modality we select 10 random examples in each of the 205 categories for the validation set and the rest for the training set, except for natural images for which we employ the training and validation splits from [46] containing 2.5 million images. Natural Images: We use images from the Places 205 Database [46] to form the natural images modality. Line Drawings: We collected a new database of sketches organized into the same 205 scene categories through Ama- zon Mechanical Turk (AMT). The workers were presented with the WordNet description of a scene and were asked to draw it with their mouse. We instructed workers to not write text that identifies the scene (such as a sign). We collected 14,830 training examples and 2,050 validation examples. Descriptions: We also built a database of scene descrip- tions through AMT. We once again presented users with the WordNet definition of a scene, but instead we asked them 1. Dataset is available at http:\/\/projects.csail.mit.edu\/cmplaces\/ to write a detailed description of the scene that comes to their mind after reading the definition. We specifically asked the users to avoid using trivial words that could easily give away the scene category (such as writing \u201Cthis is a bed- room\u201D), and we encouraged them to write full paragraphs. We split our dataset into 9,752 training descriptions and 2,050 validation descriptions. We believe Descriptions is a good modality to study as humans communicate easily in this modality and allows to depict scenes with great detail, making it an interesting but challenging modality to transfer between. Clip Art: We assembled a dataset of clip art images for the 205 scene categories defined in Places205. Clip art images were collected from image search engines by using queries containing the scene category and then manually filtered. This dataset complements other cartoon datasets [47], but focuses on scenes. We believe clip art can be an interesting modality because they are readily available on the Internet and depict everyday situations. We split the dataset into 11,372 training and 1,954 validation images (some categories had less than 10 examples). Spatial Text: Finally, we created a dataset that combines images and text. This modality consists of an image with words written on it that correspond to spatial locations of objects. We automatically construct this dataset using images from SUN [43] and its annotated objects. We created 456,300 training images and 2,050 validation images. This modality has an interesting application for content retrieval. By learning a cross-modal representation with this modality, users could use a user interface to write the names of objects and place them in the image where they want them to appear. Then, this query can be used to retrieve a natural image with a similar object layout. 4 CROSS-MODAL SCENE REPRESENTATION In this section we describe our approach for learning cross- modal scene representations. Our goal is to learn a strongly aligned representation for the different modalities in CM- Places. Specifically, we want to learn a representation in http:\/\/projects.csail.mit.edu\/cmplaces\/ 4 which different scene parts or concepts are represented in- dependently of the modality. This task is challenging partly because our training data is only annotated with scene labels instead of having one-to-one correspondences, meaning that our approach must learn a strong alignment from weakly aligned data. 4.1 Cross-Modal Scene Networks We extend single-modality classification networks [24] in order to handle multiple modalities. The main modifications we introduce are that we a) have one network for each modality and b) enforce higher-level layers to be shared across all modalities. The motivation is to let early layers specialize to modality specific features (such as edges in natural images, shapes in line drawings, or phrases in text), while higher layers are meant to capture higher- level concepts (such as objects) in a representation that is independent of the modality . We show this network topology in Figure 3 with modal- specific layers (white) and shared layers (red). The modal- specific layers each produce a convolutional feature map (pool5), which is then fed into the shared layers (fc6 and fc7). For visual modalities, we use the same convolu- tional network architecture (Figure 3a), but different weights across modalities. However, since text cannot be fed into a CNN (descriptions are not images), we instead encode each description into skip thought vectors [23] and use a multiple layer perceptron to map them into a representation with the same dimensionaly as pool5 (Figure 3b). Note that, in contrast to siamese networks [5], our architecture allows learning alignments without paired data. We could train these networks jointly end-to-end to categorize the scene label while sharing weights across modalities in higher layers. Unfortunately, we empirically discovered that this method by itself does not learn a robust cross-modal representation. This approach encourages units in the later layers to emerge that are specific to a modality (e.g., fires only on cartoon cars). Instead, our goal is to have a representation that is independent the modality (e.g., fires on cars in all modalities). In the rest of this section, we address this problem with two complementary ideas. Our first idea modifies the popular fine-tuning procedure, but applies it on modalities instead. Our second idea is to regularize the activations in the network to have common statistics. We finally discuss how these methods can be combined. 4.2 Method A: Modality Tuning Our first approach is inspired by finetuning, which is a pop- ular method for transfer learning with deep architectures [6], [15], [46]. The conventional approach for finetuning is to replace the last layer of the network with a new layer for the target task. The intuition behind fine-tuning is that the earlier layers can be shared across all vision tasks (which may be difficult to learn otherwise without large amounts of data in the target task), while the later layers can specialize to the target task. We propose a modification to the fine-tuning procedure for cross-modal alignment. Rather than replacing the last layers of the network (which are task specific), we can Specific'to' Modality Shared'Across'All' Modalities SceneImage (a) Images Shared'Across'All' Modalities Specific to'Text Scene Skip' Thought' Vector (b) Descriptions Fig. 3: Scene Networks: We use two types of networks. a) For pixel based modalities, we use a CNN based off [46] to produce pool5. b) When the input is a description, we use an MLP on skip-thought vectors [23] to produce pool5 (as text cannot be easily fed into the same CNN). Fig. 4: Statistical Regularization. We illustrate this regu- larization with an example. Above, the feature distribution p(xi) learned from Places network is modeled with a GMM, and on incorporated as a prior on xi while optimizing the deep model in line drawings modality. instead replace the earlier layers of the network (which are modality specific). By freezing the later layers in the network, we transfer a high level representation to other modalities. This approach can be viewed as finetuning the network for a modality rather than a task. To do this, we must first learn a source representation that will be utilized for all five modalities. We use the Places-CNN network as our initial representation. Places is a reasonable representation to start with because [45] shows that high-level concepts (objects) emerge in the later layers. We then train each modal-specific network to categorize scenes in its modality while holding the shared higher layers fixed. Consequently, each network will be forced to produce an aligned intermediate representation so that the higher layers will categorize the correct scene. Since the higher level layers were originally trained with only one modality (natural images), they did not have a chance to adapt to the other modalities. After we train the networks for each modality for a fixed number of iterations, we can unfreeze the later layers, and train the full network jointly, allowing the later layers to accommodate informa- tion from the other modalities without overfitting to modal- specific representations. Our approach is a form of curriculum learning [4]. If 5 we train this multi-modal network with the later layers unfrozen from the beginning, units tend to specialize to a particular modality, which is undesirable for cross-modal transfer. By enforcing a curriculum to learn high level concepts first, then transfer to modalities, we can obtain representations that are more modality-invariant. 4.3 Method B: Statistical Regularization Our second approach is to encourage intermediate layers to have similar statistics across modalities. Our approach builds upon [14], [1] who transfer statistical properties across object detection tasks. Here, we instead transfer sta- tistical properties of the activations across modalities. Let xn and yn be a training image and the scene label respectively, which we use to learn the network parameters w. We write hi(xn;w) to refer to the hidden activations for the ith layer given input xn, and z(xn;w) is the output of the network. During learning, we add a regularization term over hidden activations h: min w X n L(z(xn;w), yn) + X n,i �i · Ri (hi(xn;w)) (1) where the first term L is the standard softmax objective and the second term R is a regularization over the activations.2 The importance of this regularization is controlled by the hyperparameter �i 2 R. The purpose of R is to encourage activations in the intermediate hidden layers to have similar statistics across modalities. Let Pi(h) be a distribution over the hidden activations in layer i. We then define R to be the negative log likelihood: Ri(h) = � logPi(h; ✓i) (2) Since Pi is unknown we learn it by assuming it is a paramet- ric distribution and estimating its parameters with a large training set. To that goal, we use activations in the hidden layers of Places-CNN to estimate Pi for each layer. The only constraint on Pi is that its log likelihood is differentiable with respect to hi, as during learning we will optimize Eqn.1 via backpropagation. While there are a variety of types of distributions we could use, we explore two: Multivariate Gaussian (B-Single). We consider model- ing Pi with a normal distribution: Pi(h;µ,⌃) ⇠ N (µ,⌃). By taking the negative log likelihood, we obtain the regular- ization term Ri(h) for this choice of distribution: Ri(h;µi,⌃i) = 1 2 (h� µi)T⌃i�1(h� µi) (3) where we have omitted a constant term that does not affect the optimum of the objective. Notice that the derivatives �Ri �h can be easily computed, allowing us to back-propagate this cost through the network. Gaussian Mixture (B-GMM). We also consider using a mixture of Gaussians to parametrize Pi, which is more flexible than a single Gaussian distribution. Under this model, the negative log likelihood is: Ri(h;↵, µ,⌃) = � log KX k=1 ↵k · Pk(h;µk,⌃k) (4) 2. We omitted the weight decay from the objective for clarity. In practice, we also use weight decay. such that Pk(h;µ,⌃) ⇠ N (µ,⌃) and P k ↵k = 1 for ↵k � 0 8k. Note that we have dropped the layer subscript i for clarity, however it is present on all parameters. Since �Ri�h can be analytically computed, we can efficiently incorporate this cost into our objective during learning with backpropa- gation. To reduce the number of parameters, we assume the covariances ⌃k are diagonal. We fit a separate distribution for each of the regularized layers in our experiments (pool5, fc6, fc7). During learn- ing, the optimization will favor solutions that categorize the scene but also have an internal shared representation that is likely under Pi. Since Pi is estimated using Places-CNN, we are enforcing each modality network to have similar higher layers statistics to those of Places-CNN. 4.4 Method C: Joint Method The two proposed methods (A and B) operate on com- plementary principles and may be jointly applied while learning the networks. We combine both methods by first fixing the shared layers for a given number of iterations. Then, we unfreeze the weights of the shared layers, but now train with the regularization of method B to encourage activations to be statistically similar across modalities and avoid overfitting to a specific modality. 4.5 Implementation Details We implemented our network models using Caffe [20]. Both our methods build on top of the model described in [24], with the modification that the activations from layers pool5 onwards are shared across modalities, and layers before are modal-specific. Architectures for method A only use standard layer types found in the default version of the framework. In contrast, for model B we implemented a layer to perform regularization given the statistics of a GMM as explained in the previous sections. In our experiments the GMM models are composed by K = 100 different single gaussians. For each model we have a separate CNN initialized using the weights of Places-CNN [46]. The weights in the lower-layers can adapt independently for each modality, while we impose restrictions in the higher layer weights as explained for each method. Because CNNs start training from a good initialization, we set up the learning rate to lr = 1e �3 (higher learning rates made our models diverge). We train the models using Stochastic Gradient Descent. To adapt textual data to our models we use the network architecture described here. First, we represent descriptions by average-pooling the Skip-thought [23] representations of each sentence in a given description (a description con- tains multiple sentences). To adapt this input to our shared representation we employ a 2-layer MLP. The layer size is constant and equal to 4800 units, which is the same dimensionality as that of a Skip-thought vector, and we use ReLU non-linearities. The weights of these layers are initialized using a gaussian distribution with std = 0.1. This choice is important as the statistics of the Skip-thought representations are quite different to those of images and inadequate weight initializations prevent the network from adapting textual descriptions to the shared representation. Finally, the output layer of the MLP is fully-connected to the first layer (pool5) of our shared representation. 6 Cross Modal Query NAT CLP SPT LDR DSC Mean Retrieval Target CLP SPT LDR DSC NAT SPT LDR DSC NAT CLP LDR DSC NAT CLP SPT DSC NAT CLP SPT LDR mAP BL-Individual 17.9 11.9 10.0 1.3 12.2 10.3 9.2 1.3 7.0 9.1 5.2 1.1 5.7 8.8 5.4 1.2 0.9 1.4 1.5 1.2 6.1 BL-Shared-Upper-Scratch 7.0 7.8 4.1 10.9 5.5 5.0 3.2 9.2 5.2 4.5 2.7 8.9 3.1 3.0 3.0 5.2 5.8 5.1 6.3 3.2 5.4 BL-Shared-Upper 10.4 12.4 4.5 14.6 9.1 7.2 3.7 10.1 6.8 5.5 3.0 8.9 3.3 3.8 3.6 4.6 4.3 4.8 6.6 3.3 6.5 A: Tune 13.3 11.3 6.7 21.9 10.1 8.5 5.7 15.8 6.3 4.8 3.4 11.4 5.4 5.2 4.5 9.5 8.9 5.5 9.0 3.6 8.5 A: Tune (Free) 14.0 16.0 7.9 20.6 9.6 8.1 4.7 14.8 11.3 8.0 5.2 18.0 5.2 4.6 4.5 8.7 7.7 4.2 9.4 3.4 9.3 B: StatReg (Gaussian) 17.3 11.9 10.1 1.6 12.6 8.9 9.7 1.3 6.6 8.6 4.9 1.4 5.4 8.0 5.3 1.2 1.2 1.8 1.8 1.6 6.1 B: StatReg (GMM) 18.2 11.3 10.5 1.2 14.5 10.7 10.1 1.2 7.0 7.9 4.9 1.2 7.9 9.9 6.5 1.0 0.8 1.0 1.2 1.0 6.4 C: Tune + StatReg (GMM) 13.2 16.9 7.2 24.5 10.9 10.4 5.7 16.5 10.1 8.3 5.0 18.8 5.7 5.7 6.0 8.8 19.5 15.8 21.4 8.0 11.9 TABLE 1: Cross-Modal Retrieval mAP: We report the mean average precision (mAP) on retrieving images across modalities using fc7 features. Each column shows a different query-target pair. On the far right, we average over all pairs. For comparison, chance obtains 0.73 mAP. Best performances in each column are highlighted as bold in both this table and the others. Our methods perform better on average than the finetuning baselines with method C performing the best. Cross Modal Query NAT CLP SPT LDR DSC Mean Retrieval Target CLP SPT LDR DSC NAT SPT LDR DSC NAT CLP LDR DSC NAT CLP SPT DSC NAT CLP SPT LDR PR@10 BL-Individual 17.8 12.0 10.4 0.5 22.9 10.2 9.8 0.6 12.3 8.8 5.3 0.4 10.1 8.4 5.1 0.5 0.7 0.7 0.8 0.7 6.9 BL-Shared-Upper-Scratch 7.1 7.6 4.7 10.4 11.1 4.9 3.4 8.4 9.7 4.3 2.7 8.1 5.4 2.9 2.8 4.6 10.3 5.8 6.3 3.1 6.2 BL-Shared-Upper 11.1 12.6 4.9 14.2 16.8 7.0 4.1 9.9 12.0 6.1 2.9 8.1 5.9 3.6 3.4 3.8 5.9 4.9 6.4 3.3 7.4 A: Tune 14.3 10.6 7.8 20.7 18.1 8.2 6.1 14.5 9.6 4.8 3.4 10.4 8.8 5.1 3.7 8.4 14.8 5.5 8.6 3.8 9.4 A: Tune (Free) 15.0 16.4 8.9 19.8 16.8 8.1 4.9 13.8 21.1 9.0 5.6 17.4 8.4 4.6 4.3 8.1 12.2 4.5 9.8 3.9 10.6 B: StatReg (Gaussian) 16.9 11.6 10.8 0.9 22.8 9.1 10.4 0.6 12.1 8.6 5.0 0.7 9.5 7.7 5.1 0.6 1.4 1.3 1.3 1.3 6.9 B: StatReg (GMM) 18.2 10.8 11.3 0.5 23.9 9.9 10.4 0.5 11.0 7.4 4.7 0.5 13.0 9.1 6.2 0.5 0.7 0.5 0.7 0.6 7.0 C: Tune + StatReg (GMM) 14.1 16.6 7.9 23.2 17.8 10.0 6.1 15.1 18.1 8.7 5.2 17.7 8.8 5.4 5.4 7.9 33.5 17.1 20.9 9.2 13.4 TABLE 2: Cross-Modal Retrieval PR@10: We report the precision at top 10 retrieval of images across modalities using fc7 features. Each column shows a different query-target pair. On the far right, we average over all pairs. Our methods perform better on average than the finetuning baselines with method C performing the best. Cross-Modal Retrieval vs Layers pool5 fc6 fc7 BL-Individual 2.0 4.0 6.1 BL-Shared-Upper-Scratch 1.5 3.8 5.4 BL-Shared-Upper 1.6 3.2 6.5 A: Tune 4.2 8.4 8.5 A: Tune (Free) 4.1 8.4 9.3 B: StatReg (Gaussian) 2.0 4.2 6.1 B: StatReg (GMM) 2.0 5.5 6.4 C: Tune + StatReg (GMM) 3.4 11.1 11.9 TABLE 3: Mean Cross-Modal Retrieval mAPs across Lay- ers: Note that the baseline results decrease drastically as we go lower levels (e.g. fc7 to fc6) in the deep network. However the alignment approaches are much less affected. 5 EXPERIMENTAL RESULTS Our goal in this paper is to learn a representation that is aligned across modalities. We show four main results that evaluate how well our methods address this problem. First, we perform cross-modal retrieval of semantically- related content. Secondly, we analyze the network\u2019s abil- ity to recognize scene categories which are absent from modality. Thirdly, we show visualizations of the learned representations that give a qualitative measure of how this alignment is achieved. Finally, we show we can reconstruct natural images from other modalities using the features in the aligned representation as a qualitative measure of which semantics are preserved in our cross-modal representation. 5.1 Cross-Modal Retrieval In this experiment we test the performance of our models to retrieve content depicting the same scene across modalities. Our hypothesis is that, if our representation is strongly aligned, then nearest neighbors in this common represen- tation will be semantically related and similar scenes will be retrieved. We proceed by first extracting features for the valida- tion set of each modality from the shared layers of our cross-modal representation. Then, for every modality, we randomly sample a query image and compute the cosine distance to the extracted feature vectors of all content in the other modalities. We rank the documents according to the distances and compute the Average Precision (AP) when using the scene labels. We repeat this procedure 1000 times and report the obtained mean APs for cross-modality re- trieval in Table 1. For completeness, we also show examples of retrievals in Figure 5. We compare our results against finetuning baselines: Finetuning individual networks (BL-Individual): In this baseline we finetune a separate CNN for each of the modalities. The CNNs follow the AlexNet [24] architecture and are initialized with the weights of Places-CNN. We then finetune each one of them using the training set from the corresponding modality. This is the current standard approach employed in the computer vision community, but it does not enforce the representations in higher CNN layers to be aligned across modalities. Finetuning with shared upper layers (BL-Shared- Upper): similarly to our method A, we force networks 7 Fig. 5: Cross-Modality Retrieval : An example of cross-modal retrieval given a query from each of the modalities. For each row, the leftmost column depicts the query example, while the rest of the columns show the top 2 ranked results in each modalitiy. for each modality to share layers from pool5 onwards. However, as opposed to our method, in this baseline we do not fix the weights in the shared layers and instead let them be updated by backpropagation. CCA approaches are common for cross-modal retrieval, however past approaches were not directly comparable to our method. Standard CCA requires sample-level align- ment, which is missing in our dataset. Cluster CCA [35] works for class-level alignments, but the formulation is intended for only two modalities. On the other hand, Gener- alized CCA [17] does work for multiple modalities but still requires sample-level alignments. Concurrent work with ours extends CCA to multi-label settings [33]. As displayed in Table 1 both method A and B improve over all baselines, suggesting that the proposed methods have a better semantic alignment in fc7. Furthermore, method C outperforms all other reported methods. Partic- ularly, we can observe how method C is able to obtain a comparable performance for retrievals using descriptions to method A, while retaining the superior performance of method B for the other modalities. Note that in our experiments the baseline methods perform similarly to our method in all modalities except for descriptions, as they were not able to align the textual and visual data very well. Also note that the performance gap between our method and the baselines increases as modalities differ from each other (see SPT and DSC results). For statistical regularization, using GMM instead of a single Gaussian also notably improves the performance, arguably because of the increased complexity of the model. 5.2 Zero-Shot Recognition and Retrieval One important application of cross-modal transfer is learn- ing in one modality (e.g., natural images), but leveraging it Cross-Modal Transfer Classification CLP SPT LDR DSC BL-PlacesNet 29.1 2.2 7.1 2.2 BL-Shared-Upper-Scratch 17.3 16.0 12.6 31.9 BL-Shared-Upper 22.1 22.0 14.9 43.4 A: Modality Tuning 18.6 20.0 14.6 51.0 B: StatReg (Gauss) 50.5 20.9 24.8 4.2 B: StatReg (GMM) 32.8 23.3 20.4 2.2 C: Tune + StatReg (GMM) 16.3 21.1 13.3 49.7 TABLE 4: Zero-Shot Scene Classification: We hold out 55 scene categories during training for the clip art, spatial text, line drawings, and text descriptions modalities, and evaluate the network\u2019s ability to still classify them on the validation set. Since categories were not removed from the natural images, the network can still solve the scene classification task by finding a strong alignment between modalities. Our results suggest that our approach enables better scene classification with missing data, suggesting the network is learning a more robust alignment. in a different modality (e.g., sketches or text). For example, some domains may be easier to acquire training data (due to privacy or cost), but the model will eventually be tested in a different domain. Here, we experiment using our approach for scene recognition when some modalities lack training data of some categories. We train our models the same way as before, except we remove some categories from the clip art, sketches, spatial text, and textual description modalities. To do this, we randomly chose 55 scene categories to remove from these modalities\u2019 training data. Hence, only the natural image modality has access to all 205 categories Although most modalities lack any training data for some categories, our hope is the network\u2019s alignment 8 Unit 31 (Fountain) Unit 50 (Arcade) Unit 81 (Ring) Unit 86 (Car) Unit 104 (Castle) Unit 115 (Bed) we, water, fishes, you, drink, formed, greek, would, ball, have play, children, there, equipment, are, for, train, hole, games, path ropes, recess, seats, dug, that, square, down, each, fight, it bed, nightstand, window, gas, shampoo, you, tallest, rock, i, my church, priest, sermon, religious, he, impressive, large, stared, fountain, gas ice, terrain, plane, cold, i, nightstand, inside, beds, two, movement Real Clip art Sketches Spatial text Descriptions Fig. 6: Visualizing Unit Activations: We visualize pool5 in our cross-modal representation above by finding masks of images\/descriptions that activate a specific unit the most [45]. Interestingly, the same unit learns to detect the same concept across modalities, suggesting that it may has learned to generalize across these modalities. Cross Modal Query CLP SPT LDR DSC Mean Retrieval Target SPT LDR DSC CLP LDR DSC CLP SPT DSC CLP SPT LDR mAP BL-Shared-Upper-Scratch 5.4 6.0 10.9 6.1 5.6 10.5 6.2 4.6 8.4 7.3 6.3 6.1 6.9 BL-Shared-Upper 5.6 6.1 11.8 6.7 5.7 11.4 6.0 5.0 8.8 6.9 7.5 5.7 7.3 A: Tune (Free) 5.1 5.9 14.5 4.9 5.2 10.6 4.9 6.0 11.7 5.5 6.5 5.8 7.2 B: StatReg (Gauss) 8.3 11.1 3.9 9.0 6.9 3.8 11.9 5.9 3.7 3.9 4.2 3.9 6.4 B: StatReg (GMM) 11.3 10.8 3.6 9.3 8.2 3.7 11.5 8.8 3.5 3.7 4.4 3.3 6.8 C: Tune+StatReg (GMM) 7.0 6.7 12.3 6.1 6.0 11.1 6.2 6.9 9.7 12.3 12.5 9.7 8.9 TABLE 5: Zero-Shot Scene Retrieval: We hold out 55 scene categories during training for the clip art, spatial text, line drawings, and text descriptions modalities, and evaluate the network\u2019s ability to still retrieve these categories. Our results suggest that our approach outperforms baselines even when the retrievals are done with missing training data. between data-limited modalities and natural images will be robust enough that it can still recognize the removed categories at inference time. Table 4 shows classification accuracy on the held-out categories for the data-limited modalities. For visual modalities, statistical regularization outperforms the baseline non-regularized network, suggest- ing this approach helps an alignment to emerge that is useful for classification. However, for textual descriptions, modality tuning provides a better alignment. We believe this is because text is a significantly different modality from images, making it harder to align. We also show perfor- mance using the pre-trained PlacesCNN, which never saw the other modalities during training. Our approach tends to outperform the PlacesCNN on the modalities that are very different to natural images. We also experimented with cross-modal retrieval on the held-out categories. Table 5 shows mean average precision for retrieval on these missing categories. While modality tuning provides a slight improvement over baselines on average, combining both of our approaches yields better retrievals in the absence of missing categories. Table 4 and Table 5 both suggest our network is starting to learn an alignment even the absence of categories for some modalities. However, they also suggest a trade-off that depends on the task. If the task is classification, then our experiments suggest one of statistical regularization or modality tuning is better. However, if the task is retrieval, then combining both methods is better. We believe this is because the both our methods can be seen as a cross-modal regularization. Stronger regularization on the internal ac- tivations helps retrieval performance because it helps the features to be more specific to instances. Nevertheless, such regularization also adds more constraints during learning that may hurt classification performance. 5.3 Hidden Unit Visualizations We now investigate what input data activates units in our shared representation. For visual data, we use a visualiza- tion similar to [45]. For textual descriptions, we compute the paragraphs that maximally activate each filter, and then we employ tf-idf features to determine the most common relevant words in these paragraphs. Figure 6 shows, for some of the 256 filters in pool5, the images in each visual modality that maximally activated the filter with their mask superimposed, as well as the most common words in the paragraphs that maximally activated the units. We can observe how the same concept can be detected across modalities without having explicitly aligned 9 Fig. 7: t-SNE Embedding of Cross-Modal Representation: We visualize the embedding for fc7 of representations from different networks using t-SNE [27]. Colors correspond to the modality. If the representation is agnostic to the modality, then the features should not cluster by modality. These visualizations suggest that our full method does a better job at discarding modality information than baselines. training data. These results suggest that our method is learning some strong alignments across modality only using weak labels coming from the scene categories. To quantify this observation, we set up an experiment. We showed human subjects activations of 100 random units from pool5. These activations included the top five re- sponses in each modality with their mask. The task was to select, for each unit, those images that depicted a common concept if it existed. Activations could be generated from either the baseline BL-Ind or from our method A, but this information is hidden from the subjects. After running the experiment, we selected those results in which at least 4 images for the real modality were selected. This ensured that the results were not noisy and were produced using units with consistent activations, as we empirically found this to be a good indicator of whether a unit represented an aligned concept. We then computed the number of times subjects selected at least one image in each of the other modalities. With our method, 33% of the times this process selected at least one image from each modality, whereas for the baseline this only happened 25% of the times. Furthermore, 19% of the times we selected at least two images for each modality as opposed to only 14% for the baseline. These results suggest that, when a unit is detecting a clear concept, our method outperforms the best finetuning method and can strongly align the different modalities. 5.4 Analyzing Modality Invariance A representation is invariant to modality if the feature vector does not store information about the origin modality. Since modality invariant representations would be useful cross- modal transfer, we wish to analyze the degree to which modality-specific information is contained in the represen- tation. Using examples from the validation set, Figure 7 shows a two-dimensional embedding of the representation from our networks using t-SNE [27]. To do this, we ran- domly sample 1, 000 examples from each modality and compute t-SNE of the fc7 features. We then color each point by the modality. The visualization shows that the baseline network (without any cross-modal regularization) clearly separates the representation by modality, which is undesirable. Statistical regularization offers some invariance to modality, except for text. While our representation is not completely invariant to modality, the visualization suggests the full approach tends to be better at discarding modality information than baselines. 5.5 Feature Reconstructions Here we investigate if we can generate images in differ- ent modalities given a query. The motivation is to gain some visual understanding of which concepts are preserved across modalities and which information is discarded [40]. We use the reconstruction approach from [7] out-of-the-box, but we train the network using our features. We learn an inverting network for each modality that learns a mapping from features in the shared pool5 layer to downsampled reconstructions of the original images. We refer readers to [7] for full details. We employ pool5 features as opposed to fc7 features because the amount of compression of the input image in the latter produces worse reconstructions. If concepts in our representation are correctly aligned, our hypothesis is that the reconstruction network will learn to generate images that capture the statistics of the data in the output modality and while show same concepts across modalities in similar spatial locations. Note that one limitation of these inversions is that output images are blurry, even when reconstructing images within a same modality, due to the data compression in pool5. However, our reconstructions have similar quality to those in [7] when reconstructing from pool5 features within a modality. Figure 8 shows some successful examples of reconstruc- tions. We observed this is a hard, arguably because the statistics of the activations in the common representation are very different across modalities despite the alignment, which might be due to the reduced amount of information in some of the modalities (i.e. clipart and spatial text im- ages contain much less information that natural images). However, we note that in the examples the trained model is capable of reproducing the statistics of the output modal- ity. Moreover, the reconstructions usually depict the same concepts present in the original image, indicating that our representation is aligning and preserving scene information across modalities. 6 CONCLUSION Humans are able to leverage knowledge and experiences independently of the modality they perceive it in, and a 10 Fig. 8: Inverting features across modalities: We visualize some of the generated images by our inverting network trained on real images. Top row: reconstructions from real images. These preserve most of the details of the original image but are blurry because of the low dimensionality of the pool5 representation. Second row: reconstructions from line drawings, where the network adds colors to the reconstructions while preserving the original scene composition. Third row: inversions from the spatial text modality. Reconstructions are less detailed but roughly preserve the location, shape and colors of the different parts of the input scene. Fourth row: inversions from the clip-art modality; and inversions from natural image to line drawing modality. similar capability in machines would enable several im- portant applications in retrieval and recognition. In this paper, we proposed an approach to learn aligned cross- modal representations without paired data. Interestingly, our experiments suggest that our approach encourages alignment to emerge in the representation automatically across modalities, even when the training data is unaligned. Acknowledgements We thank TIG for managing our computer cluster. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. This work was supported by NSF grant IIS-1524817, by a Google faculty research award to A.T and by a Google Ph.D. fellowship to C.V. REFERENCES [1] Y. Aytar and A. Zisserman. Part level transfer regularization for enhancing exemplar svms. In Computer Vision and Image Understanding, 2015. 5 [2] J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional neural networks using textual descrip- tions. arXiv, 2015. 2 [3] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature replacement. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 672\u2013679. IEEE, 2005. 2 [4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009. 4 [5] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Säckinger, and R. Shah. Signature verification using a siamese time delay neural network. International Journal of Pattern Recogni- tion and Artificial Intelligence, 7(04):669\u2013688, 1993. 4 [6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013. 4 [7] A. Dosovitskiy and T. Brox. Inverting convolutional networks with convolutional networks. arXiv, 2015. 9 [8] M. Eitz, K. Hildebrand, T. Boubekeur, and M. Alexa. Sketch- based image retrieval: Benchmark and bag-of-features descriptors. TVCG, 2011. 2 [9] M. Elhoseiny, B. Saleh, and A. Elgammal. Write a classifier: Zero- shot learning using purely textual descriptions. In ICCV, 2013. 2 [10] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. TPAMI, 2006. 2 [11] M. Fink. Object classification from a single example utilizing class relevance metrics. NIPS, 2005. 2 [12] D. F. Fouhey and C. L. Zitnick. Predicting object dynamics in scenes. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2027\u20132034. IEEE, 2014. 2 [13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013. 1, 2 [14] T. Gao, M. Stark, and D. Koller. What makes a good detector?\u2013 structured priors for learning from few examples. In Computer Vision\u2013ECCV 2012, pages 354\u2013367. Springer, 2012. 5 [15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hi- erarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 4 [16] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In ICCV, 2011. 1, 2 [17] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural computation, 16(12):2639\u20132664, 2004. 2, 7 [18] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context from a single image. In Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 1, pages 654\u2013661. IEEE, 2005. 2 [19] Y. Jia, M. Salzmann, and T. Darrell. Learning cross-modality similarity for multinomial data. In ICCV, 2011. 2 [20] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675\u2013678. ACM, 2014. 5 [21] A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and A. Torralba. Undoing the damage of dataset bias. In ECCV, 2012. 2 [22] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying visual- semantic embeddings with multimodal neural language models. arXiv, 2014. 2 11 [23] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skip-thought vectors. arXiv, 2015. 4, 5 [24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classifica- tion with deep convolutional neural networks. In NIPS, 2012. 4, 5, 6 [25] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR, 2009. 2 [26] M. Long and J. Wang. Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. 2 [27] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008. 9 [28] K. Murphy and W. Freeman. Contextual models for object detec- tion using boosted random fields. NIPS, 2004. 2 [29] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 689\u2013696, 2011. 2 [30] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning by convex combi- nation of semantic embeddings. arXiv, 2013. 1, 2 [31] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson, and W. T. Freeman. Visually indicated sounds. arXiv preprint arXiv:1512.08512, 2015. 2 [32] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. Zero- shot learning with semantic output codes. In NIPS, 2009. 2 [33] V. Ranjan, N. Rasiwasia, and C. Jawahar. Multi-label cross-modal retrieval. In Proceedings of the IEEE International Conference on Computer Vision, pages 4094\u20134102, 2015. 7 [34] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanck- riet, R. Levy, and N. Vasconcelos. A new approach to cross-modal multimedia retrieval. In ICM, 2010. 2 [35] N. Rasiwasia, D. Mahajan, V. Mahadevan, and G. Aggarwal. Cluster canonical correlation analysis. In AISTATS, pages 823\u2013831, 2014. 2, 7 [36] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV, 2010. 1, 2 [37] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot learning through cross-modal transfer. In Advances in neural information processing systems, pages 935\u2013943, 2013. 1, 2 [38] A. Torralba, A. Efros, et al. Unbiased look at dataset bias. In CVPR, 2011. 2 [39] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068\u20134076, 2015. 2 [40] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. Hoggles: Visualizing object detection features. In Proceedings of the IEEE International Conference on Computer Vision, pages 1\u20138, 2013. 9 [41] C. Vondrick, H. Pirsiavash, A. Oliva, and A. Torralba. Learning visual biases from human imagination. In Advances in Neural Information Processing Systems, 2015. 2 [42] F. Wang, L. Kang, and Y. Li. Sketch-based 3d shape retrieval using convolutional neural networks. arXiv, 2015. 2 [43] J. Xiao, J. Hays, K. Ehinger, A. Oliva, A. Torralba, et al. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 3 [44] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015. 2 [45] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. arXiv, 2014. 3, 4, 8 [46] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014. 3, 4, 5 [47] C. L. Zitnick and D. Parikh. Bringing semantics into focus using visual abstraction. In CVPR, 2013. 2, 3 [48] C. L. Zitnick, D. Parikh, and L. Vanderwende. Learning the visual interpretation of sentences. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1681\u20131688. IEEE, 2013. 2 Yusuf Aytar is a post-doctoral research associate at Massachusetts Institute of Technology (MIT) since October 2014. He received his D.Phil. degree from University of Oxford. As a Fulbright scholar, he obtained his M.Sc. degree from University of Central Florida (UCF). His research is mainly concentrated on computer vision, machine learning, and transfer learning. Lluis Castrejon received the BS degree in Computer Science and BS degree in Telecommuncations Engineering from the Universitat Po- litecnica de Catalunya, Spain in 2015. Prior to that, he conducted his undergraduate thesis at the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT). He was awarded a La Caixa Fellowship to pursue graduate studies in North America in 2015. He is currently a MS student in Machine Learning and Computer Vision at the University of Toronto. Carl Vondrick is a doctoral candidate at the Massachusetts Institute of Technology (MIT) where his research studies computer vision and ma- chine learning. He received his bachelors degree in computer science from the University of California, Irvine in 2011, graduating summa cum laude. His research has been awarded the National Science Foundation Graduate Fellowship and the Google PhD Fellowship. Hamed Pirsiavash is an assistant professor at the University of Mary- land Baltimore County (UMBC) since August 2015. Prior to that, he was a postdoctoral research associate at MIT and he obtained his PhD at the University of California Irvine. He does research in the intersection of computer vision and machine learning. Antonio Torralba received the degree in telecommunications engineer- ing from Telecom BCN, Barcelona, Spain, in 1994 and the Ph.D. degree in signal, image, and speech processing from the Institut National Poly- technique de Grenoble, France, in 2000. From 2000 to 2005, he spent postdoctoral training at the Brain and Cognitive Science Department and the Computer Science and Artificial Intelligence Laboratory, MIT. He is now a Professor of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology (MIT). Prof. Torralba is an Associate Editor of the International Journal in Computer Vision, and has served as program chair for the Computer Vision and Pattern Recognition conference in 2015. He received the 2008 National Science Foundation (NSF) Career award, the best student paper award at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2009, and the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition (IAPR). ","flair":"three\tResearch"}
{"author":"highlife159","created":"Fri Nov 11 13:24:01 EST 2016","text":" ACCELERATED COMPUTING Cuda zone menu Downloads Training Ecosystem Forums Search CUDA Zone Register Now Login Main menu Skip to primary content Skip to secondary content Features CUDACasts CUDA Pro Tips CUDA Spotlights ← Previous Image Segmentation Using DIGITS 5 Share: Posted on November 10, 2016 by Greg Heinrich 10 Comments Tagged Computer Vision, Deep Learning, DIGITS, Image Segmentation Today we\u2019re excited to announce NVIDIA DIGITS 5. DIGITS 5 comes with a number of new features, two of which are of particular interest for this post: a fully-integrated segmentation workflow, allowing you to create image segmentation datasets and visualize the output of a segmentation network, and the DIGITS model store, a public online repository from which you can download network descriptions and pre-trained models. In this post I will explore the subject of image segmentation. I\u2019ll use DIGITS 5 to teach a neural network to recognize and locate cars, pedestrians, road signs and a variety of other urban objects in synthetic images from the SYNTHIA dataset. Figure 1 shows a preview of what you will learn to do in this post. Figure 1: Sample visualizations of image segmentation using DIGITS 5.0 showing alternately the input image, an overlay of FCN-Alexnet predictions, an overlay of FCN-8s predictions and the ground truth. From Image Classification to Image Segmentation Suppose you want to design image understanding software for self-driving cars. Chances are you\u2019ve heard about Alexnet [1], GoogLeNet [2], VGG-16 [3] and other image classification neural network architectures, so you might want to start from there. Image classification is the process by which a computer program is able to tell you that a picture of a dog is a picture of a dog. The output of an image classification model is a discrete probability distribution: one number between 0 and 1\u2014a probability\u2014for each class the model is trained to recognise. Figure 2 illustrates an example classification of an image of a cat using Alexnet in DIGITS. The result is spectacularly good: remember that Alexnet was trained on 1000 different classes of objects from categories including animals, musical instruments, vegetables, vehicles and many others. I find it humbling to see that with over 99% confidence, a machine is able to correctly classify the subject of the image as feline. I, for one, would be at a loss to tell whether that particular cat is an Egyptian, tabby or tiger cat. Figure 2: Alexnet classification of an image of a cat from the PASCAL VOC dataset. But what happens if you classify an image of a cat and a dog? Common sense might lead you to believe that the neural network would assign an equal share of the probability distribution to our two favorite pet species. Let\u2019s try it: Figure 3 shows the outcome. There is a mix of cats and dogs in the predictions, but AlexNet doesn\u2019t deliver the hoped-for 50\/50 split. In the middle picture there are in fact no cats within the Top 5 predictions. This is disappointing, but on the other hand Alexnet was trained on a \u201Csmall\u201D world of 1.2 million images in which there is only ever one object to see, so one can\u2019t reasonably expect it to perform well in the presence of multiple objects. Figure 3: Alexnet classifications of images of cats and dogs from the PASCAL VOC dataset. Another limitation of classification networks is their inability to tell the location of objects in images. Again, this is understandable, considering that they were not trained to do this, but this is nonetheless a major hindrance in computer vision: if a self-driving car is unable to determine the location of the road it will probably not travel a long way! Image segmentation addresses some of these shortcomings. Instead of predicting a single probability distribution for the whole image, the image is divided into a number of blocks and each block is assigned its own probability distribution. In the most common embodiment, images are divided down to pixel level and each pixel is classified: for every pixel in the image, the network is trained to predict which class that particular pixel is part of. This allows the network to not only identify several object classes in each image, but also to determine the location of objects. Image segmentation typically generates a label image the same size as the input whose pixels are color-coded according to their classes. Figure 4 shows an example that segments  four different classes in a single image: table, chair, sofa and potted-plant. Figure 4: Image Segmentation example from the PASCAL VOC dataset (white areas mark undefined pixels like object contours and unclassified objects). In a further refinement of image segmentation named Instance-aware Image Segmentation (IAIS) the network learns to identify the contours of every object in the image. This is particularly useful in applications that must be able to uniquely identify every occurrence of a class, even when there is no clear separation between them, such as in Figure 5: the middle image is the image segmentation label, while the rightmost image is the IAIS label (notice how the color codes uniquely identify each person). I won\u2019t go deeper into the subject of IAIS and I will focus on instance segmentation;but I encourage you to check out Facebook\u2019s SharpMask work on IAIS. Figure 5: Image segmentation (middle) vs. Instance-aware Image Segmentation (right). Images from the PASCAL VOC dataset. Let\u2019s look at how we can design a network that is capable of segmenting an image. From CNN to FCN The previous section distinguished image classification models that make one probability distribution prediction per image from image segmentation models that predict one probability distribution per pixel. In principle, this sounds rather similar and you might expect the same techniques to apply to both problems. After all, it merely adds a spatial dimension to the problem. In this post I will show you that just a few minor adjustments are enough to turn a classification neural network into a semantic segmentation neural network. I\u2019ll employ techniques first introduced in this paper [4] (which I\u2019ll refer to as the FCN paper. Before I get started, some terminology: I will refer to typical classification networks like Alexnet as Convolutional Neural Networks (CNN). This is slightly abusive since convolutional neural networks serve many purpose besides image classification but it is a common approximation. In a CNN it is common practice to split the network into two parts:in the first part, the feature extractor, the data goes through several convolutional layers to extract progressively more complex and abstract features. Convolutional layers are typically interspersed with non-linear transfer functions and pooling layers. Each convolutional layer can be seen as a set of image filters that trigger a high response on a particular pattern. For example, Figure 6 shows a representation of the filters from the first convolutional layer in Alexnet and the activations (the outputs) on a dummy image that contains simple shapes (amusingly, AlexNet classifies the image as a wall clock!). Those filters trigger a high response on shapes like horizontal and vertical edges and corners. For instance, have a look at the filter at the bottom left corner which looks like black-and-white vertical stripes. Now look at the corresponding activations and the high response on vertical lines. Similarly, the next filter immediately at the right shows a high response on oblique lines. Further convolutional layers down the network will be able to trigger a high response on more elaborate shapes like polygons and eventually learn to detect textures and various constituents of natural objects. In a convolutional layer, every output is computed by applying each filter to a window (also known as the receptive field) in the input, sliding the window by the layer stride until the full input has been processed. The receptive field has the same size as the filters. See Figure 7 for an illustration of this behavior. Note that the input window spans across all channels of the input image. Figure 6: AlexNet conv1 layer, as represented in DIGITS. From top to bottom: data layer (input); visualization of the filters of conv1; activations (output) of conv1. Figure 7: Left: An example input volume in red and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input. Right: The neurons still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially. Source: Stanford CS231 course. In the second and final part of a CNN, the classifier consists of a number of fully-connected layers, the first of which receives its inputs from the feature extractor. These layers learn complex relationships between features to endow the network with a high-level understanding of the image contents. For example the presence of big eyes and fur might have the network lean towards a cat. How exactly the network makes sense of these features is somewhat magical and another trait of the pure beauty of deep learning. This lack of explainability is sometimes criticized but it is not unlike the way the human brain functions: would you be able to explain how you know that an image of a cat is not an image of a dog? Fully Convolutional Networks (FCN), as their name implies, consist of only convolutional layers and the occasional non-parametric layers mentioned above. How can eliminating fully-connected layers create a seemingly more powerful model? To answer this question let\u2019s ponder another. Figure 8: Input, weights and activations of Alexnet\u2019s first fully-connected layer (fc6), as represented in DIGITS. The question is: what is the difference between a fully-connected layer and a convolutional layer? Well that\u2019s simple: in a fully-connected layer, every output neuron computes a weighted sum of the values in the input. In contrast, in a convolutional layer, every filter computes a weighted sum of the values in the receptive field. Wait, isn\u2019t that exactly the same thing? Yes, but only if the input to the layer has the same size as the receptive field. If the input is larger than the receptive field then the convolutional layer slides its input window and computes another weighted sum. This process repeats until the input image has been scanned left to right, top to bottom. In the end, each filter generates a matrix of activations; each such matrix is called a feature map. This provides a clue: to replace a fully-connected layer with an equivalent convolutional layer, just set the size of the filters to the size of the input to the layer, and use as many filters as there are neurons in the fully-connected layer. I\u2019ll demonstrate this on the first fully-connected layer in Alexnet (fc6): see Figure 8 for a DIGITS visualization of the layers of interest. You can see that fc6 receives its input from pool5 and the shape of the input is a 256-channel 6×6 image. Besides, the activations at fc6 are a 4096-long vector, which means that fc6 has 4096 output neurons. It follows that if I want to replace fc6 with an equivalent convolutional layer, all I have to do is set the filter size to 6×6 and the number of output feature maps to 4096. As a small digression, how many trainable parameters do you think this layer would have? For every filter there is one bias term plus one weight per number in the receptive field. The receptive field has a depth of 256 and a size of 6×6 therefore there are 256x6x6+1=9217 parameters per filter. Since there are 4096 filters, the total number of parameters for this layer is 37,752,832. That is exactly the number of parameters that DIGITS says fc6 has. All is well so far. In practice, Replacing the layer is simple. If you are using Caffe, just replace the definition on the left in Table 1 with the definition on the right. Table 1: Left: fc6 definition, Right: equivalent conv6 definition with a kernel size of 6 because the input to fc6 is a 6×6 image patch. layer { name: \"fc6\" type: \"InnerProduct\" bottom: \"pool5\" top: \"fc6\" inner_product_param { num_output: 4096 } } layer { name: \"conv6\" type: \"Convolution\" bottom: \"pool5\" top: \"conv6\" convolution_param { num_output: 4096 kernel_size: 6 } } Armed with this knowledge, you can now proceed to converting all the fully-connected layers in Alexnet with their corresponding convolutional layers. Note that you don\u2019t have to use DIGITS to figure out the shapes of the input to those layers; you could calculate them manually. As fun as that may sound, I assure you that you will run out of patience if you need to do this for the 16 layers (plus intervening pooling layers) in VGG-16. Not to mention the fact that you will inevitably lose the scratchpad you used to scribble your notes. Besides, as a Deep Learning fan, you should be comfortable with the idea of letting a machine do the work for you. So let DIGITS do the work for you. The resulting FCN has exactly the same number of learnable parameters, the same expressivity and the same computational complexity as the base CNN. Given the same input, it will generate the same output. You might wonder: why go through the trouble of converting the model? Well, \u201Cconvolutionalizing\u201D the base CNN introduces a great amount of flexibility. The model is no longer constrained to operate on a fixed input size (224×224 pixels in Alexnet). It can process larger images by scanning through the input as if sliding a window, and instead of producing a single probability distribution for the whole input, the model generates one per 224×224 window. The output of the network is a tensor with shape KxHxW where K is the number of classes, H is the number of sliding windows along the vertical axis and W is the number of sliding windows along the horizontal axis. A note on computational efficiency: in theory you could implement the sliding window naively by repeatedly selecting patches of an image and feeding them to a CNN for processing. In practice, this would be computationally very inefficient: as you slide the window incrementally, there is only a small number of new pixels to see at each step. Yet, each patch would need to be fully processed by the CNN, even in the presence of a large overlap between successive patches. You would therefore end up processing each pixel many times. In an FCN, since those computations are all happening within the network, only the minimum number of operations gets to execute so the whole process is orders of magnitude faster. In summary, that brings us to our first milestone: adding two spatial dimensions to the output of the classification network. In the next section I\u2019ll show you how to further refine the model. Image Segmentation FCN The previous section showed how to design an FCN that predicts one class probability distribution per window. Obviously, the number of windows depends on the size of the input image, the size of the window and the step size used between windows when scanning the input image. Ideally, an image segmentation model will generate one probability distribution per pixel in the image. How can you do this in practice? Here again I will employ a method from the FCN paper. When the input image traverses the successive layers of the \u201Cconvolutionalized\u201D Alexnet, the pixel data at the input is effectively compressed into a set of coarser, higher-level feature representations. In image segmentation, the aim is to interpolate those coarse features to reconstruct a fine classification, for every pixel in the input. It turns out that this is easily done with deconvolutional layers. These layers perform the inverse operation of their convolutional counterparts: given the output of the convolution, a deconvolutional layer finds the input that would have generated the output, given the definition of the filter. Remember that the stride in a convolutional layer (or a pooling layer) defines how far the window slides when processing the input and is therefore a measure of how down-sampled the output is. Conversely, the stride in a deconvolutional layer is a measure of how up-sampled the output is. Choose a stride of 4 and the output is 4 times bigger! The next question is: how do I determine how much to up-sample the activations of the final convolutional layer in the model to obtain an output that is the same size as the input image? I need to inspect every layer and carefully write down its scaling factor. Once I have done that for all layers, I just multiply scaling factors together. Let\u2019s look at the first convolutional layer in Alexnet. layer { name: \"conv1\" type: \"Convolution\" bottom: \"data_preprocessed\" top: \"conv1\" convolution_param { num_output: 96 kernel_size: 11 stride: 4 } } The stride of conv1 is 4, therefore the scaling factor is ¼. Repeating this for all layers, I determine that the total scaling factor in the model is 1\/32, as summarised in Table 2. Consequently, the stride I need for the deconvolutional layer is 32. layer { name: \"upscore\" type: \"Deconvolution\" bottom: \"score\" top: \"upscore\" convolution_param { num_output: 12 # set this to number of classes kernel_size: 63 stride: 32 } } For the sake of completeness, I have to say that it is not completely true that a convolutional layer with stride yields an output that is times the size of the input in all spatial dimensions. In practice, adding padding to the input will increase the number of activations. Conversely, using kernels with size will knock activations off the input. At the limit, if you provide the layer with an infinitely long input, the input\/output size ratio will indeed be in all (spatial) dimensions. In reality, the output of every convolutional (or pooling) layer is shifted by . See Table 2 for a summary of those computations. Table 2: scaling factors and offsets across successive layers in a \u201Cconvolutionalized\u201D Alexnet. Layer Stride Pad Kernel size Scaling factor (1\/S) Cumulative scaling factor Offset (P-(K-1)\/2)\/S Cumulative Offset conv1 4 100 11 1\/4 1\/4 23 23 pool1 2 0 3 1\/2 1\/8 -1 22 conv2 1 2 5 1 1\/8 0 22 pool2 2 0 3 1\/2 1\/16 -1 21 conv3 1 1 3 1 1\/16 0 21 conv4 1 1 3 1 1\/16 0 21 conv5 1 1 3 1 1\/16 0 21 pool5 2 0 3 1\/2 1\/32 -1 20 conv6 1 0 6 1 1\/32 -2 18 conv7 1 0 1 1 1\/32 0 18 Table 2 shows that the output of the network is shifted by 18 pixels. The last trick I need in the segmentation model is a layer to crop the network output and remove the 18 extra pixels on each border. This is easily done in Caffe with a Crop layer, defined in the following listing. layer { name: \"score\" type: \"Crop\" bottom: \"upscore\" bottom: \"data\" top: \"score\" crop_param { axis: 2 offset: 18 } } You might notice that in this version of Alexnet there is a bit more padding than you are used to seeing in that conv1 layer. There are two reasons behind this: one reason is to generate a large initial shift, so the offsets incurred by successive layers do not eat into the image. The main reason, however, is to have the network process the borders of the input image in such a way that they hit the center of the network\u2019s receptive field, approximately. At last, I now have everything I need to replicate the FCN-Alexnet model from the FCN paper. Let\u2019s relax for a minute and look at some refreshing images from the SYNTHIA dataset. The SYNTHIA Dataset The SYNTHIA dataset was originally published in this this paper [5]. See Figure 9 for sample images from the SYNTHIA dataset. These images show synthetically generated urban scenes with various object classes such as buildings, roads, cars and pedestrians under varying conditions such as day and night. Amusingly, the images look realistic enough that one sometimes feels intrigued by them: hmmm, that man reading a newspaper in the middle of the road on that first picture is acting strangely, he\u2019s certainly up to no good! Figure 9: Samples from the SYNTHIA dataset. Left: images to segment. Right: ground truth. In DIGITS 5.0, creating an image segmentation dataset is as simple as pointing to the input and ground-truth image folders and clicking the \u201CCreate\u201D button. DIGITS supports various label formats such as palette images (where pixel values in label images are an index into a color palette) and RGB images (where each color denotes a particular class). After you have created your dataset in DIGITS you can explore the databases to visually inspect their contents, as in Figure 10. Figure 10: Database exploration in DIGITS. Top: input images. Bottom: labels. Training the Model A dataset and a network description are all you need to start training a model in DIGITS. If you thought the process of convolutionalizing Alexnet was somewhat complicated or time consuming then fret not: DIGITS 5.0 comes with a model store and as you can imagine, FCN-Alexnet can be retrieved from the DIGITS model store! If, however you decided to go the hard way and create the model description yourself, you will want to use a suitable weight initialization scheme like the Kaiming [5] (also known as MSRA) method, which is state of the art in the presence of Rectified Linear Units. This is easily done in Caffe by adding a weight_filler { type: \"msra\" } directive to your parametric layers. If you train your model this way in DIGITS you will probably end up with a curve that resembles the one in Figure 11. As you can see the performance is less than satisfactory. Validation accuracy plateaus at 35% (meaning that only 35% of pixels in the validation set are correctly labeled). The training loss is in line with the validation loss, indicating that the network is underfitting the training set. Figure 11: Training\/validation loss and validation accuracy when training FCN-Alexnet on Synthia using random weight initialization in DIGITS. You can try your luck on a sample image and ask DIGITS for a visualization of the image segmentation. You will see something like Figure 12, where you can see that the network is indiscriminately classifying everything as building. It turns out that building is the most represented object class in SYNTHIA and the network has just lazily learnt to achieve 35% accuracy by labeling everything as building. What are commonly accepted ways to deal with a network that underfits the training set? Train for longer: looking at the loss curves, this is unlikely to help as training seems to have hit a plateau. The network has entered a local minimum and is unable to get out of it. Increase the learning rate, and reduce the batch size: this could encourage a network that is trapped in a local minimum to explore beyond its immediate surroundings, although this increases the risk that the network diverges. Increase the size of the model: this could increase the expressivity of the model. Another method that I found to work extremely well in computer vision is transfer learning. Read on to find out more! Figure 12: Sample visualization of image segmentation in DIGITS when training FCN-Alexnet on SYNTHIA dataset using random weight initialization. The network has classified everything as Building. Transfer Learning You don\u2019t have to start from randomly initialized weights to train a model. In a lot of cases, it helps to reuse knowledge that a network learned when training on another dataset. This is particularly true in Computer Vision through CNNs since a lot of low-level features (lines, corners, shapes, textures) immediately apply to any dataset. Since image segmentation does classification at the pixel level it makes sense to transfer learning from image classification datasets such as ILSVRC2012. This turns out to be rather straightforward when using Caffe\u2014with one or two gotchas of course! Remember that in Alexnet\u2019s fc6, the weights have a shape of 4096×9216. In FCN-Alexnet\u2019s conv6, the weights have a shape of 4096x256x6x6. This is exactly the same number of weights, but since the shapes are different, Caffe will be unable to automatically carry the weights over to FCN-Alexnet. This operation may be performed using a net surgery script, an example of which can be found in the DIGITS repository on Github. The function of the net surgery script is to transplant parameters from fully-connected layers to their convolutional counterparts. You may however find it easier to simply download the pretrained model from the public DIGITS Model Store! Figure 13 shows  a preview of the model store: click \u201CImport\u201D next to \u201CFCN-Alexnet\u201D and DIGITS will download the pre-trained model. Figure 13: The DIGITS Model Store. Another related concern that you might have is how to initialize the upsampling layer added earlier in this post, since this layer isn\u2019t part of the original Alexnet model: in the FCN paper the recommendation is to randomly initialize the corresponding weights and have the network learn them. The authors of the paper later realized that it is just as simple to initialize these weights in such a way that the layer just acts as a magnifying glass, by doing bilinear interpolation. In Caffe this is done by adding a weight_filler {type: \"bilinear\"} directive to the layer. Using a pre-trained FCN-Alexnet model, you will notice that the accuracy quickly exceeds 90% and when testing individual images such as in Figure 14 the result will be a much more convincing image segmentation, with detections for 9 different object classes. You might, however, be slightly disappointed to see that object contours are very coarse. Read on to the next and final section to discover how to further improve the precision and accuracy of our segmentation model. Figure 14: Sample visualization of image segmentation in DIGITS when training FCN-Alexnet on SYNTHIA dataset using an ILSVRC2012-pretrained Alexnet. Enabling Finer Segmentation Remember that the new upsampling layer added to FCN-Alexnet magnifies the output of conv7 by a factor of 32. In practice, this means that the network makes a single prediction per 32×32 pixel block, which explains why object contours are so coarse. The FCN paper introduces another great idea for addressing this limitation: skip connections are added to directly redirect the output of pool3 and pool4 towards the output of the network. Since those pooling layers are further back in the network, they operate on lower-level features and are able to capture finer details. In a network architecture called FCN-8s the FCN paper introduces a VGG-16-based network in which the final output is an 8x upsampling of the sum of pool3, a 2x upsampling of pool4 and a 4x upsampling of conv7, as Figure 15 illustrates. This leads to a network that is able to make predictions at a much finer grain, down to 8×8 pixel blocks. Figure 15: Illustration of FCN-8s skip connections \u2013 source: FCN paper. For your convenience, a pre-trained FCN-8s can be downloaded from the public DIGITS model store. (You don\u2019t want to manually convolutionalize VGG-16!) If you train FCN-8s on SYNTHIA using DIGITS, you should see that the validation accuracy exceeds 95% after only a couple of epochs. Most importantly, when you test a sample image and observe DIGITS\u2019 superb image segmentation visualization, you will see much sharper object contours, as in Figure 16. Figure 16: Sample visualization of image segmentation in DIGITS when training FCN-8s on SYNTHIA dataset. Now It\u2019s Your Turn! After reading this post, you should have the information you need to get started with image segmentation. The DIGITS 5 general release will be available in the first week of December. Visit the DIGITS page to learn more and sign up for the NVIDIA Developer program to be notified when it is ready for download. DIGITS is an open-source project on GitHub. If you\u2019d like to start experimenting with image segmentation right away, head over to the DIGITS GitHub project page where you can get the source code. We look forward to your feedback and contributions on Github as we continue to develop it. Please let us know how you are doing by commenting on this post! Acknowledgements I would like to thank Mark Harris for his insightful comments and suggestions. References [1] Krizhevsky, A., Sutskever, I. and Hinton, G. E. \u201CImageNet Classification with Deep Convolutional Neural Networks\u201D. NIPS Proceedings. NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. 2012. [2]  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. \u201CGoing Deeper With Convolutions\u201D. CVPR 2015. [3] Simonyan, Karen, and Andrew Zisserman. \u201CVery deep convolutional networks for large-scale image recognition.\u201D arXiv technical report arXiv:1409.1556. 2014. [4] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. \u201CFully convolutional networks for semantic segmentation.\u201D Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015: 3431-3440. [5] Ros, German, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez; \u201CThe SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes.\u201D Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016: 3234-3243. [6] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun \u201CDelving deep into rectifiers: Surpassing human-level performance on imagenet classification.\u201D Proceedings of the IEEE International Conference on Computer Vision 2015: 1026-1034. Related Posts Exploring the SpaceNet Dataset Using DIGITS Deep Learning for Object Detection with DIGITS DetectNet: Deep Neural Network for Object Detection in DIGITS CUDACasts Episode 21: Porting a simple OpenCV sample to the Jetson TK1 GPU ∥∀ Share: About Greg Heinrich Greg Heinrich is a Software Engineer in the Accelerated Computing team at Nvidia. He is one of the main contributors to the DIGITS open source software. View all posts by Greg Heinrich → MichaelSB Any plans to support Theano or TensorFlow? Greg We would love to support more frameworks in the future. DIGITS is open-source, feel free to contribute! Vikram Meena Can we use this for SAR images segmentation. Greg Hello, I have found FCN models to work very well for very different datasets (natural images, synthetic images, medical images). I suggest you try and please let us know the result! Nerea Hi! I have also worked with FCN-8s for image segmentation with good results. The only problem is the high time required for both training and inference. Any idea how to attack this without loosing accuracy? Greg Hello Nerea, technology is moving fast so chances are the newer generations of GPUs would meet your processing time requirements. Did you try any of the GPUs from the Pascal family? Also, Fully Convolutional Networks scale extremely well to multi-GPU training so you might want to consider adding GPUs to reduce training time. For inference you can use TensorRT (https:\/\/developer.nvidia.com\/tensorrt) to reduce test time. Another option would be to base your network on a different CNN: FCN-8s is based on VGG-16 but you might want to stick to e.g. Alexnet and add skip connections, if what you\u2019re interested in is a finer grain in the predictions. Alternatively, you could increase the stride of the first few convolutional layers to increase their receptive field and reduce the number of activations in the network thus reducing the computational complexity. Yet another option is to reduce the image size if you feel this won\u2019t destroy too much content. These are some thoughts but there are probably a million ways to tackle this problem. mladefer I\u2019m having trouble understanding what is exactly meant by offset in Table 2 and why is it calculated as (P \u2013 (K \u2013 1)\/2) \/ S. I understand that the new feature map size could be calculated as (W + 2P \u2013 K) \/ S + 1 where W is the input size. So for example, conv 1 with W = 224, P = 100, K = 11 and S = 4 would result in size 104. How does that relate to offset as defined above? Greg Consider conv1 for example: because this layer has 100-pixel padding on each side its output is larger than if there was no padding at all. Therefore if you were to upscale the output of conv1 to reconstitute an image of the original input you would have to crop the upscaled output. In a lot of cases, a center crop will do but if you want to calculate the offset incurred by each conv or pooling layer you can use the (P-(K-1)\/2)\/S formula. In practice you need to do this since the Caffe \u201Ccrop\u201D layer requires an offset and a shape, it won\u2019t do the center crop automatically. Interestingly your question made me realize I had made a mistake in the offset calculations. The number is correct for each layer however the offset does not add up the way I showed. Amazingly the final answer is correct. I will fix this in the article but in the meantime I suggest you have a look at https:\/\/github.com\/BVLC\/caffe\/blob\/25b9ef95f35a4de766500c9c70a18d839a5f7c70\/python\/caffe\/coord_map.py#L172-L185 mladefer I figured out the general intention, but I couldn\u2019t get the numbers to match and it is still not clear to me. In this example, 224×224 image with padding 100 on each side would result in 104×104 feature maps after conv1 (as given by W\u2019 = (W + 2P \u2013 K) \/ S + 1). If we upscale with deconvolution using stride 4, I am expecting a 416×416 image. With offset 23 in Table 2 and cropping 23 pixels at each side, this gives 370×370. What am I missing here? When I think about it some more, the actual size after deconvolution should also depend on used kernel size, but I\u2019m not sure if I could just use the above formula for size with known W\u2019 (104) and solve for W. Greg Let\u2019s assume your input is 224*224. The intrinsic offset of conv1 is (P-(K-1)\/2)\/S is 23.75. The output of conv1 has size 104*104. In the article I omitted to say that a deconvolution layer yields an intrinsic offset of (K-1)\/2-P so in your example if you want to upscale conv1 using stride S=4 and kernel size K=7 this would be 3. The size of the output of the upscale layer would be (W-1)*S-2P+K=(104-1)*4-0+7=419 (for each spatial dimension). The update I need to make in the article is to fix the recipe for composing those offsets across layers. We can\u2019t simply add them up. We need to \u201Cback propagate\u201D the offset, from the top of the graph to the bottom of the graph. The composition of a layer L1 with a layer L2 (i.e. L2 is a bottom of L1 in Caffe terminology), with offsets O1 and O2 respectively, yields an offset of O2\/F+O1, where F is the cumulative scaling factor of L2. So now in our example we have: \u2013 offset of upscale layer: 3 \u2013 offset of conv1 layer: 23.75 \u2013 scaling factor of conv1: 1\/4 \u2013 total offset of composition of upscale with conv1: 23.75\/(1\/4)+3=98 This means that you need to take 98 pixels off each border of the output of the upscale layer => you end up with 419-98*2=223 pixels. Adjusting for rounding errors due to integer kernel sizes this is exactly what you need. Subscribe: RSS Email Connect: Follow @gpucomputing X Enter your email address: Subscribe Resources About Parallel Forall NVIDIA Developer Forums Accelerated Computing Newsletter Recent Posts Image Segmentation Using DIGITS 5 New Compiler Features in CUDA 8 Deep Learning in Aerial Systems Using Jetson Mixed-Precision Programming with CUDA 8 The Intersection of Large-Scale Graph Analytics and Deep Learning My TweetsRecent Comments Badri Gopalan on EGL Eye: OpenGL Visualization without an X Server Greg on Image Segmentation Using DIGITS 5 mladefer on Image Segmentation Using DIGITS 5 Benjamin Hernandez on EGL Eye: OpenGL Visualization without an X Server Alp Alt on DetectNet: Deep Neural Network for Object Detection in DIGITS ACCELERATED COMPUTING GAMEWORKS EMBEDDED COMPUTING DESIGNWORKS GET STARTED About CUDA Parallel Computing CUDA Toolkit CUDACast LEARN MORE Training and Courseware Tools and Ecosystem Academic Collaboration Documentation GET INVOLVED Forums Parallel Forall Blog Developer Program Contact Us Copyright © 2016 NVIDIA Corporation   |   Legal Information   |   Privacy Policy Close Parallel Forall Features CUDACasts CUDA Pro Tips CUDA Spotlights Search for: Accelerated Computing Accelerated Computing Downloads Training Ecosystem Forums Register Now Login ","flair":"two\tNews"}
{"author":"bronzestick","created":"Mon Oct 10 00:03:11 EDT 2016","text":"In the existing literature of deep learning, is there any work that deals with capturing uncertainty like Bayesian models do? Like, for example, can they result in a predictive distribution that has low variance if the test point is similar to training data and has high variance if the test point differs greatly from the training data (think of being far in the input data space)","flair":"one\tDiscussion"}
{"author":"inejc","created":"Wed Nov 02 17:36:35 EDT 2016","text":"Below is a link to the Github repository that contains my solution for the competition alongside a brief description of the approaches I've used and plans for future work.\n\nhttps:\/\/github.com\/inejc\/painters","flair":"four\tProject"}
{"author":"Ungerwhere","created":"Tue Sep 27 14:52:27 EDT 2016","text":"Last time we wrote an image classifier using TensorFlow for Poets. This time, we\u2019ll write a basic one using TF.Learn. To make it easier for you to try this out, I wrote a Jupyter Notebook for this episode -- https:\/\/goo.gl\/NNlMNu -- and I\u2019ll start with a quick screencast of installing TensorFlow using Docker, and serving the notebook. This is a great way to get all the dependencies installed and properly configured. I've linked some additional notebooks below you can try out, too. Next, I\u2019ll start introducing a linear classifier. My goal here is just to get us started. I\u2019d like to spend a lot more time on this next episode, if there\u2019s interest? I have a couple alternate ways of introducing them that I think would be helpful (and I put some exceptional links below for you to check out to learn more, esp. Colah's blog and CS231n - wow!). Finally, I\u2019ll show you how to reproduce those nifty images of weights from TensorFlow.org's Basic MNIST\u2019s tutorial.Jupyter Notebook: https:\/\/goo.gl\/NNlMNuDocker images: https:\/\/goo.gl\/8fmqVWMNIST tutorial: https:\/\/goo.gl\/GQ3t7nVisualizing MNIST: http:\/\/goo.gl\/ROcwpR (this blog is outstanding)More notebooks: https:\/\/goo.gl\/GgLIh7More about linear classifiers: https:\/\/goo.gl\/u2f2NEMuch more about linear classifiers: http:\/\/goo.gl\/au1PdG (this course is outstanding, highly recommended)More TF.Learn examples: https:\/\/goo.gl\/szki63Thanks for watching, and have fun! For updates on new episodes, you can find me on Twitter at www.twitter.com\/random_forests","flair":"null\tnull"}
{"author":"morgangiraud","created":"Wed Nov 09 08:01:37 EST 2016","text":"This repo is the result of some work done in the Startup Weekend AI in Paris. It contains two models: DISCLAIMER This repo does not contains the trainings\/dev\/test sets due to proprietary concerns. Completely blowing up previous bees larvae detections i know of using OpenCV, and this was achieved thanks to only 2000 training samples which is a very small dataset. Also, it took less than 10 minutes to train it This validate again and gain the fact that deep learning is very well suited to handle real life data and its variability. The training phase is interesting in terms of overfitting: We can see that we reach 0% () error on the training set which means we completely overfit the data, yet the generalization on the dev set keeps improving until no learning is possible anymore. This is a clear indicator that more data would improve even more the accuracy, also we probably can simplify it even further and improve performance for this simple binary classifier. You can test both models by running script. And finally you can even export a frozen model using . if you want to use it in production with TensorFlow in a more convenient way.","flair":"four\tProject"}
{"author":"RankLord","created":"Wed Nov 16 08:21:37 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1606.01299 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1606 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Yaniv Romano John Isidoro Peyman Milanfar Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: RAISR: Rapid and Accurate Image Super Resolution Authors: Yaniv Romano, John Isidoro, Peyman Milanfar (Submitted on 3 Jun 2016 (v1), last revised 4 Oct 2016 (this version, v3)) Abstract: Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect. Comments: Supplementary material can be found at this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1606.01299 [cs.CV]   (or arXiv:1606.01299v3 [cs.CV] for this version) Submission history From: Yaniv Romano [view email] [v1] Fri, 3 Jun 2016 22:56:49 GMT (11016kb,D) [v2] Sat, 13 Aug 2016 08:39:18 GMT (9005kb,D) [v3] Tue, 4 Oct 2016 21:22:51 GMT (9333kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"lfotofilter","created":"Sun Nov 06 07:08:13 EST 2016","text":"Last week I read Abadi and Andersen\u2019s recent paper [1], Learning to Protect Communications with Adversarial Neural Cryptography. I thought the idea seemed pretty cool and that it wouldn\u2019t be too tricky to implement, and would also serve as an ideal project to learn a bit more Theano. This post describes the paper, my implementation, and the results. The authors set up their experiment as follows. We have three neural networks, named Alice, Bob, and Eve. Alice wishes to communicate an N bit message P to Bob. Alice and Bob also share a key (which you can think of as a password) of N bits. Alice takes the message and the key, and encrypts the message, producing a communication C of N bits. Bob receives this communication, and then attempts to decrypt it, producing PBob. Unfortunately for Bob and Alice, Eve intercepts Alice\u2019s communication C. She then decrypts this message herself, producing her attempted recovery of P, which is called PEve. Figure 1: The adversarial network setup diagram given in [1]. As mentioned, Alice, Bob and Eve are all neural networks. All three of these networks are quite similar. Alice (Figure 2) takes as input the message and key vectors, concatenated into one long vector of length 2N. This then passes through a single fully-connected hidden layer of size 2N. It then passes through what I will refer to as the standard convolutional setup, which takes this 2N-length vector, passes it through a number of 1D convolution filters, and eventually outputs an N-length vector. This is the communication C that gets sent to Bob. Bob\u2019s network (Figure 3) is identical to Alice\u2019s, except that his input is the concatenation of the communication and the key. Eve\u2019s network is also quite similar to Bob and Alice\u2019s. However her input is just the communication C. She also has an additional fully-connected hidden layer of size 2N prior to the standard convolutional setup: the authors wanted to make Eve a bit more complex in order to give her a better chance of figuring out how to decrypt C. The following table gives the output dimensionality of each layer in each network: The next ingredient are the loss functions: Eve\u2019s loss function is simply the absolute difference between the message vector P and her attempt at decrypting it PEve. This loss is backpropagated through the weights in the Eve network to update her weights. Bob and Alice\u2019s loss function is also the absolute decryption error, plus an additional term that signifies how well Eve is currently decrypting the message. This additional term is: This is minimised when Eve gets half the bits correct, on average, in her decryption, which indicates that she is doing no better than random guessing. The gradient of this loss function is used to update the weights in both the Bob and Alice networks. Putting these together, we have: Training then proceeds in an adversarial way: we train Bob and Alice for a while until they are communicating effectively, then we train Eve for some time, until she learns to decrypt the message. Then we train Bob and Alice again, who figure out how to fool Eve\u2019s current decryption method. Then we train Eve again, who cracks the improved encryption, and so on. The authors find that after a while, Bob and Alice\u2019s encryption becomes too strong for Eve to be able to learn to crack it. The implementation is fairly straightforward. I have used some custom classes, and for adding standard NN layers. You can find these in the file. We first need to define our batch size and N up front. I have added the possibility to have different lengths for the key, message and communication, however I have not tested this; changing it might cause issues. For Alice and Bob we just create a fairly straightforward sequential NN: Eve is similarly implemented. We just need to use in her inputs. Here we just implement the loss equations described in the previous section. Note that the additional term in Bob\u2019s loss function is a bit simpler than the equation described above. Things have been set up such that a mean error of 1 means that half the bits were correctly decrypted (as bits are input as either -1 or 1, so a single error = 2). Hence the N\/2 terms can be dropped from the implementation. The only tricky-ish thing here is making sure that the training function for Alice and Bob updates all their parameters, while Eve\u2019s only updates her parameters. I use for an implementation of the Adam SGD optimiser. I put the functions in dictionaries for ease of use in adversarial training. Since it is used in all three networks, I made a custom class for the standard convolutional setup. It stores all the parameters and tensors relavent to all of the convolutional layers in the model. I have tried to match the description of the convolution setup described in the paper: To perform the adversarial training, I made a function that would train either Alice and Bob or Eve for some time. We then just iterate between calling this function on Alice and Bob, and then for Eve. The function generates random message and key pairs. We train according to the loss, but for plotting we just store the decryption error for the party that is currently being trained. I trained both Alice and Bob, and then Eve, for up to 2000 iterations at a time (early stopping occurred if the decryption error was below 0.01 for a while). I did 60 overall repetitions of this adversarial training setup. I then plotted the minimum decryption error achieved by Bob and by Eve in each of these 60 runs (Figure 5). So, it seems to work. After a few adversarial rounds, Bob and Alice figure out a way to effectively scramble the communication such that Eve cannot learn how to decrypt it. I also tested the setup without the four convolutional layers, instead replacing this with an additional 2N in, 1N out hidden layer (Figure 6). Figure 6: Bob and Eve\u2019s decryption errors over 60 adversarial training iterations, with the convolutional phase of the network excluded. This seems to suggest that the convolution layers helps, but perhaps it is still possible to achieve the goals of this experiment without it - Eve still isn\u2019t able to perfectly recover the message in this setup either. I should note that this paper didn\u2019t receive much love when it was posted on the Reddit MachineLearning forum. And I have to say I kind of agree with the points made in that discussion: really the fact that this works doesn\u2019t mean it has created good encryption. Rather it more just speaks to the weakness of the Eve network in its ability to decrypt the message. This is sort of reflected by the fact that this setup still seems to work without the convolution layers (Figure 6). Still, it is an interesting idea, and I don\u2019t think I\u2019m in a position to judge its academic merit. Thanks for reading - thoughts, comments or questions are welcome!","flair":"four\tProject"}
{"author":"zxzhijia","created":"Wed Oct 05 20:04:35 EDT 2016","text":"This topics course aims to present the mathematical, statistical and computational challenges of building stable representations for high-dimensional data, such as images, text and audio. We will delve into selected topics of Deep Learning, discussing recent models from both supervised and unsupervised learning. Special emphasis will be on convolutional architectures, invariance learning, unsupervised learning and non-convex optimization.","flair":"null\tnull"}
{"author":"NYDreamer","created":"Wed Sep 28 19:36:59 EDT 2016","text":"The new module , which groups together the functionalities of formerly , and , introduces new possibilities such as nested cross-validation and better manipulation of parameter searches with Pandas. Many things will stay the same but there are some key differences. Read below to know more about the changes. The new cross-validation splitters, defined in the , are no longer initialized with any data-dependent parameters such as . Instead they expose a method that takes in the data and yields a generator for the different splits. This change makes it possible to use the cross-validation splitters to perform nested cross-validation, facilitated by model_selection.GridSearchCV and model_selection.RandomizedSearchCV utilities. The new attribute (of model_selection.GridSearchCV and model_selection.RandomizedSearchCV) introduced in lieu of the attribute is a dict of 1D arrays with elements in each array corresponding to the parameter settings (i.e. search candidates). The dict can be easily imported into as a for exploring the search results. The arrays include scores for each cross-validation split (with keys such as ), as well as their mean () and standard deviation (). The ranks for the search candidates (based on their mean cross-validation score) is available at . The parameter values for each parameter is stored separately as numpy masked object arrays. The value, for that search candidate, is masked if the corresponding parameter is not applicable. Additionally a list of all the parameter dicts are stored at . Some parameter names have changed: The parameter in new , model_selection.GroupKFold (see below for the name change), and model_selection.StratifiedKFold is now renamed to . The parameter in model_selection.ShuffleSplit, the new class model_selection.GroupShuffleSplit and model_selection.StratifiedShuffleSplit is now renamed to . Rename of splitter classes which accepts group labels along with data The cross-validation splitters , , and have been renamed to model_selection.GroupKFold, model_selection.GroupShuffleSplit, model_selection.LeaveOneGroupOut and model_selection.LeavePGroupsOut respectively. Note the change from singular to plural form in model_selection.LeavePGroupsOut. The parameter in the method of the newly renamed splitters model_selection.GroupKFold, model_selection.LeaveOneGroupOut, model_selection.LeavePGroupsOut, model_selection.GroupShuffleSplit is renamed to following the new nomenclature of their class names. also includes the training scores for each cross-validation split (with keys such as ), as well as their mean () and standard deviation (). To avoid the cost of evaluating training score, set . Additionally the mean and standard deviation of the times taken to split, train and score the model across all the cross-validation splits is available at the key and respectively.","flair":"null\tnull"}
{"author":"yoav_hollander","created":"Thu Nov 24 13:07:22 EST 2016","text":" Skip to content The Foretellix Blog Menu Home About Privacy Policy Terms of Use Why this blog Terminology All posts HVC\u20192016 trip report Posted on November 24, 2016 by Yoav Hollander I attended HVC again this year (see also my HVC\u20192015 trip report). Here is a very subjective description of what I saw and heard \u2013 much of it outside the conference room itself. Note that while HVC is about \u201Call things verification\u201D, much of the audience comes from the chip verification world. Also, the conference historically leans towards the formal. This was still the case this year (with some exceptions, such as the special session on Intelligent Autonomous Systems \u2013 see below). Machine Learning \u2013 the new \u201Chot tech\u201D \u2013 also had a strong presence. My presentation: Verifying Intelligent Autonomous Systems The special session consisted of my presentation, followed by Shahank Pathak\u2019s presentation about Probabilistic Model Checking (PMC \u2013 more on this below).  As I said in a previous post I consider the conference to be slightly too formal-heavy, so to balance that I plan to provide mainly light entertainment, gossip and sweeping generalizations about the current state of this increasingly-important field. There was a lot to go over, so my presentation is indeed somewhat dense (feel free to watch the full 55-minute video, including Q&A). Here are some related links if you want to dig further: Related posts in my blog: AV testing, Tesla crash, mostly-autonomous systems, spec bugs, Checking probabilistic components, fault trees, ML verification, explainable AI Other related blogs: Brad Templeton\u2019s blog, Philip Koopman\u2019s blog Regular readers of this blog will probably not find much new in the presentation, except for that PMC angle: Probabilistic Model Checking (PMC) and all that I talked before (see links above) about how hard it is to compute the probability of failures of heterogeneous systems, and why fault trees, while important, have lots of issues. Well, for the special case where we can assume that the system behaves like a Markov chain, there is a pretty reasonable solution: PMC. This special case is important enough to warrant a few more words: A Markov chain, as you guys may remember, is simply a state machine with probabilities assigned to transitions. This does not sound all that exciting, but it turns out that many systems behave like that, for instance systems based on reinforcement learning (RL). Most RL systems assume that the rest-of-the-world (the thing being controlled by the RL system) can be modeled as a Markov Decision Process (MDP) or even a Partially Observable Markov Decision Process (POMDP). POMDPs are not for the faint of heart: Not only are you unsure about the result of executing a specific action in a specific state, but you are never quite sure which state you are in. Still, this anxiety-inducing world turns out to be a good approximation for the world we live in, so RL systems are increasingly popular. Training an RL system amounts to deciding on a \u201Cpolicy\u201D (i.e. which action to choose at any point, so as to best push the MDP to \u201Crewarding\u201D states). Combining the resulting policy with the MDP gets us a Markov chain, and thus makes PMC possible. This is fortunate, because RL-based systems are doing more and more things (from playing Go to controlling robots). Essentially, PMC does for Markov chains what normal model checking does for state machines. Consider the following slide from my HVC presentation: If the light-blue box can indeed be modeled as a Markov chain, one can use PMC to get the exact probability that some \u201Cbad sequence\u201D will happen. Shashank\u2019s presentation explained all that (see related paper), and described another important area where PMC is helpful (belief space planning). Shashank and myself agree, however, that even in the above example PMC cannot be the whole answer, because: This is just one module (the full syoostem may have, say, 100M code lines, some of which may indirectly influence the ML-based system) Training set may be deficient (e.g. missing rare, high-impact things like black ice) Other issues Thus one still needs something like CDV for full-system verification. Still, PMC seems to be pretty useful, and it would be interesting to see how it could be better combined with fault trees and with CDV. Shield synthesis and ML safety How do we make sure that an ML-based system (trained, say, via RL) behaves safely, in the sense that it does not do dangerous things? I am talking here about the design question, not about the related verification question. One way, as hinted above, is to make sure the training set includes all the rare-but-dangerous things we are trying to guard against. But this has problems: In your effort to go through all the things that could go wrong, you could skew the distribution of the training set. Also, defining the \u201Cright\u201D reward signal (which also penalizes dangerous stuff) is tricky. I mentioned here an interview with a Tesla director who said about ML-based systems (paraphrased): Because there is such a long tail of corner cases, you can\u2019t just \u201Cturn the crank\u201D on the ML algorithm to fix them. Rather, those corner cases require \u201Cspecific tuning of specific algorithms\u201D. So Tesla, apparently, adds that final \u201Csafety\u201D touch via hand-tuning some SW \u2013 perhaps by \u201Cfixing\u201D dangerous outputs of the ML system. Well, there is at least one more way, which Bettina Könighofer from Graz University of Technology (Austria) presented (an older related paper is here). What they do is called \u201Cshield synthesis\u201D, and while it is currently not ML-specific, here is how it would work for ML: Write the safety assertions in some suitable temporal language Synthesize those assertions into a SW module (the \u201Cshield\u201D) to which you feed both the inputs and the outputs of the ML-based system If the shield observes that the output is safe, it lets it through, otherwise it blocks it and supplies its own, safe output Alternatively, you can put the shield before the ML system, allowing the ML system to choose only from safe outputs In any case, the resulting output is fed back into the training process of the ML-based system: This is important for better training-in-the-face-of-safety, and for making sure that subsequent, safe outputs will \u201Cflow smoothly\u201D This is hopefully somewhat better than the Tesla approach (as I imagined it above), since the SW module is automatically generated from the safety assertions. Note, however, that the fact that the module is \u201Ccorrect by construction\u201D does not mean that you do not have to verify it, hopefully via CDV (see a discussion of that topic here). Misc stuff Here are a bunch of other things which caught my attention: Is formal growing, relatively? Formal Verification people always say \u201CFV is finally taking over\u201D, so I learned to tune that out (see also this post). However, in this HVC I heard this more, so maybe it is growing (in the HW module verification domain). Creating a formal testbench: One group who said \u201CFV is growing\u201D was some (normally trustworthy) folks from Mellanox. They also explained how it is very easy to create formal-ready module-level testbenches: You simply write the testbench in RTL (the format used for describing the HW itself), and then you express any non-determinism via simple a simple \u201Cif\u201D statement. For instance, if both behavior X and Y are legal in some situation, you write \u201Cif my_signal then X else Y\u201D, and (crucially) you do not assign anything to my_signal \u2013 this lets the formal engine set it to any value so as to try and contradict the various assertions. In hindsight, this is obviously a good way to write such testbenches \u2013 it just never occurred to me before. Using ML to discover specifications: People often don\u2019t write specs, and yet it is hard to test functions without specs. One way around that, discussed by Swarat Chaudhuri of Rice U in a talk called \u201CGuiding formal methods with discovered knowledge\u201D (which contained much else) uses an ML-based approach to discover the spec. This is not as crazy as it sounds. Essentially, they mine lots of open-source repositories for functions, then create (symbolic) traces from them, and finally try to extract specs from those traces (e.g. \u201COne should always open a file before writing to it\u201D). This is all done probabilistically using ML: Essentially, they learn the \u201Cjoint distribution\u201D of functions, traces and specs. Then, given a new function, they see if its traces match the (probable) spec. In addition, there were several presentations about automatic debugging, and some of them talked about combining logical (e.g. SAT-based) techniques with ML-based techniques (used for instance to cluster similar failures or to help debug via anomaly detection). It seems that a common theme of this HVC (and several of my posts lately, come to think of it) is that important-but-uneasy interface between the hard, logical stuff and the softer, probabilistic stuff. Notes I\u2019d like to thank Shashank Pathak, Bettina Könighofer and Amiram Yehudai for commenting on earlier drafts of this post.   Share this: Twitter Facebook Google Like this: Like Loading... Post navigation Misc stuff: The verification gap, ML training and more Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (required) (Address never made public) Name (required) Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. Notify me of new posts via email. Search for: Archives Archives Select Month November 2016  (1) October 2016  (1) September 2016  (3) August 2016  (1) July 2016  (4) June 2016  (3) May 2016  (1) April 2016  (2) March 2016  (1) January 2016  (2) December 2015  (1) November 2015  (2) October 2015  (2) September 2015  (1) August 2015  (3) July 2015  (10) Recent Posts HVC\u20192016 trip report Misc stuff: The verification gap, ML training and more Verification implications of the new US AV policy Using Machine Learning to verify Machine Learning? Machine Learning for Coverage Maximization Machine Learning verification and Explainable AI About faults and bugs Checking probabilistic components Future of verification: Better ways to predict behavior Recent Comments Yoav Hollander on Misc stuff: The verification g\u2026 Gil Amid on Misc stuff: The verification g\u2026 Steve Ross on Verification implications of t\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Using Machine Learning to veri\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Verification implications of t\u2026 Steve Ross on Verification implications of t\u2026 Mugur Tatar on Checking probabilistic compone\u2026 Recent Posts HVC\u20192016 trip report Misc stuff: The verification gap, ML training and more Verification implications of the new US AV policy Using Machine Learning to verify Machine Learning? Machine Learning for Coverage Maximization Machine Learning verification and Explainable AI About faults and bugs Checking probabilistic components Future of verification: Better ways to predict behavior Recent Comments Yoav Hollander on Misc stuff: The verification g\u2026 Gil Amid on Misc stuff: The verification g\u2026 Steve Ross on Verification implications of t\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Using Machine Learning to veri\u2026 Gil Amid on Using Machine Learning to veri\u2026 Yoav Hollander on Verification implications of t\u2026 Steve Ross on Verification implications of t\u2026 Mugur Tatar on Checking probabilistic compone\u2026 Archives Archives Select Month November 2016  (1) October 2016  (1) September 2016  (3) August 2016  (1) July 2016  (4) June 2016  (3) May 2016  (1) April 2016  (2) March 2016  (1) January 2016  (2) December 2015  (1) November 2015  (2) October 2015  (2) September 2015  (1) August 2015  (3) July 2015  (10) Meta Register Log in Entries RSS Comments RSS WordPress.com Follow Blog via Email Enter your email address to follow this blog and receive notifications of new posts by email. Join 64 other followers Follow The Foretellix Blog on WordPress.com Blog at WordPress.com. %d bloggers like this: ","flair":"null\tnull"}
{"author":"kevinzakka","created":"Wed Sep 28 04:12:50 EDT 2016","text":"- I was wondering if leveraging the sparse fft algorithm (for specific cases) to speed up train-time convolutions is something that has been explored? If so, do any of the libraries today use it?\n- What are some cases where this would be applicable? (I'm thinking audio like in the new Wavenet paper, or specific set of images?)\n","flair":"null\tnull"}
{"author":"MarkusDeNeutoy","created":"Thu Oct 27 23:40:22 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.07647 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1610 Change to browse by: cs cs.NE stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: Learning to Reason With Adaptive Computation Authors: Mark Neumann, Pontus Stenetorp, Sebastian Riedel (Submitted on 24 Oct 2016 (v1), last revised 10 Nov 2016 (this version, v2)) Abstract: Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. Comments: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems Subjects: Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) Cite as: arXiv:1610.07647 [cs.CL]   (or arXiv:1610.07647v2 [cs.CL] for this version) Submission history From: Mark Neumann [view email] [v1] Mon, 24 Oct 2016 20:48:04 GMT (278kb,D) [v2] Thu, 10 Nov 2016 19:35:11 GMT (278kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"jayjaymz","created":"Thu Nov 24 13:14:17 EST 2016","text":"Hello, \n\nI am working on a character sequence prediction problem. I have created a (stateful as is defined in Keras) 3 layer RNN in Keras that takes sequences of equal length and tries to predict the next character. I'm training it on a 80million character dataset. I'm using mini-batches and my loss function is (categorical) cross entropy.\n\nThe definition of perplexity I'm refering to can be found [here](http:\/\/cs224d.stanford.edu\/lecture_notes\/LectureNotes4.pdf).\n\nWhat I can not understand is if and how you can calculate perplexity given a single batch's loss, since I'm trying in mini batches.\n\n        loss = training_model.train_on_batch(x, y)\n\nIs this cross entropy error I'm getting the same as in the definition of entropy? Especially seeing how x and y are of size(batch_size, seq_length, vocab_size) can I actually use this error to calculate perplexity?\n\nCode for my implementation is here: https:\/\/github.com\/AuthEceSoftEng\/rnn2source\/blob\/master\/src\/char-rnn.py\n\nFeel free to ask for clarifications, thanks!\n","flair":"one\tDiscussion"}
{"author":"clbam8","created":"Wed Nov 16 16:37:39 EST 2016","text":" Home Moments Search query Search Twitter Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Language: English Bahasa Indonesia Bahasa Melayu Català Čeština Dansk Deutsch English UK Español Filipino Français Hrvatski Italiano Magyar Nederlands Norsk Polski Português Română Slovenčina Suomi Svenska Tiếng Việt Türkçe Ελληνικά Български език Русский Српски Українська мова עִבְרִית العربية فارسی मराठी हिन्दी বাংলা ગુજરાતી தமிழ் ಕನ್ನಡ ภาษาไทย 한국어 日本語 简体中文 繁體中文 Have an account? Log in Have an account? Remember me · Forgot password? New to Twitter? Sign up dannysullivan's profile Danny Sullivan Verified account @dannysullivan Danny SullivanVerified account @dannysullivan Founding Editor, @MarketingLand & @SEngineLand, covering Google, Facebook, Twitter, SEO, SEM & all things digital marketing, search marketing and search. dannysullivan.com Joined March 2007 © 2016 Twitter About Help Terms Privacy Cookies Ads info Dismiss Close Previous Next Close Go to a person's profile Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Close Retweet this to your followers? Optional comment for Retweet   Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @   140 Retweet Tweet Close Are you sure you want to delete this Tweet? Cancel Delete Close Promote this Tweet Close Block Cancel Block Add a location to your Tweets When you tweet with a location, Twitter stores that location. You can switch location on\/off before each Tweet and always have the option to delete your location history. Learn more Turn location on Not now Close Profile summary Close Your lists Close Create a new list List name Description Under 100 characters, optional Privacy Public · Anyone can follow this list Private · Only you can access this list Save list Close Close Copy link to Tweet Here's the URL for this Tweet. Copy it to easily share with friends. Close Embed this Tweet Embed this Video Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? Include parent Tweet Include media Preview Close Log in to Twitter Remember me · Forgot password? Don't have an account? Sign up » Close Sign up for Twitter Not on Twitter? Sign up, tune into the things you care about, and get updates as they happen. Sign up Have an account? Log in » Close Two-way (sending and receiving) short codes: Country Code For customers of United States 40404 (any) Canada 21212 (any) United Kingdom 86444 Vodafone, Orange, 3, O2 Brazil 40404 Nextel, TIM Haiti 40404 Digicel, Voila Ireland 51210 Vodafone, O2 India 53000 Bharti Airtel, Videocon, Reliance Indonesia 89887 AXIS, 3, Telkomsel, Indosat, XL Axiata Italy 4880804 Wind 3424486444 Vodafone » See SMS short codes for other countries Close Confirmation Close   Close Close Buy Now Close Buy Now Hmm... Something went wrong. Please try again. Skip all Welcome home! This timeline is where you\u2019ll spend most of your time, getting instant updates about what matters to you. Tweets not working for you? Hover over the profile pic and click the Following button to unfollow any account. Say a lot with a little When you see a Tweet you love, tap the heart \u2014 it lets the person who wrote it know you shared the love. Spread the word The fastest way to share someone else\u2019s Tweet with your followers is with a Retweet. Tap the icon to send it instantly. Join the conversation Add your thoughts about any Tweet with a Reply. Find a topic you\u2019re passionate about, and jump right in. Learn the latest Get instant insight into what people are talking about now. Get more of what you love Follow more accounts to get instant updates about topics you care about. Find what's happening See the latest conversations about any topic instantly. Never miss a Moment Catch up instantly on the best stories happening as they unfold. Back Next Next Tweet from user Previous Tweet Next Tweet Follow Following Unfollow Blocked Unblock Pending Cancel Danny Sullivan Verified account \u200F@dannysullivan Nov 15 New leaders of Google Cloud Machine Learning, both women https:\/\/martechtoday.com\/google-machine-learning-192476 \u2026pic.twitter.com\/oAhEpVBjR6 Retweets 335 Likes 475 10:48 AM - 15 Nov 2016 0 replies 335 retweets 475 likes Reply Retweet 335 Retweeted 335 Like 475 Liked 475 More Copy link to Tweet Embed Tweet GB \u200F@recrdx Nov 20 @dannysullivan @balajis in 2008 it was said that 80% of the uses for GPS hadn't been discovered. That's #ML today. But 99% 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet suncho \u200F@suncho Nov 15 @dannysullivan @tsanova @martech_today I find the origin to be more remarkable than the gender. Then again, I work in Chinese company, we have many smart female managers here. 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet aeiaiai \u200F@aeiaiai Nov 21 @dannysullivan @balajis @nikita_maia 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet 霜秋中的大松鼠 \u200F@shuangqiu Nov 16 Israel @dannysullivan both Chinese\u2026 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Mark Rogowsky \u200F@maxrogo Nov 15 @dannysullivan @BenedictEvans Something in the Li DNA? (Yes, I get they aren't related) 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet Back to top ↑ Loading seems to be taking a while. Twitter may be over capacity or experiencing a momentary hiccup. Try again or visit Twitter Status for more information. Promoted Tweet false © 2016 Twitter About Help Terms Privacy Cookies Ads info ","flair":"two\tNews"}
{"author":"jaromiru","created":"Mon Nov 07 09:22:55 EST 2016","text":" ヤロミル about AI Skip to content Let\u2019s make a DQN: Double Learning and Prioritized Experience Replay November 7, 2016DQNヤロミル This article is part of series Let\u2019s make a DQN. 1. Theory 2. Implementation 3. Debugging 4. Full DQN 5. Double DQN and Prioritized experience replay Introduction Last time we implemented a Full DQN based agent with target network and reward clipping. In this article we will explore two techniques, which will help our agent to perform better, learn faster and be more stable \u2013 Double Learning and Prioritized Experience Replay. Double Learning One problem in the DQN algorithm is that the agent tends to overestimate the Q function value, due to the max in the formula used to set targets: To demonstrate this problem, let\u2019s imagine a following situation. For one particular state there is a set of actions, all of which have the same true Q value. But the estimate is inherently noisy and differs from the true value. Because of the max in the formula, the action with the highest positive error is selected and this value is subsequently propagated further to other states. This leads to positive bias \u2013 value overestimation. This severe impact on stability of our learning algorithm1. A solution to this problem was proposed by Hado van Hasselt (2010)2 and called Double Learning. In this new algorithm, two Q functions \u2013 and \u2013 are independently learned. One function is then used to determine the maximizing action and second to estimate its value. Either or is updated randomly with a formula: or It was proven that by decoupling the maximizing action from its value in this way, one can indeed eliminate the maximization bias2. When thinking about implementation into the DQN algorithm, we can leverage the fact that we already have two different networks giving us two different estimates and (target network). Although not really independent, it allows us to change our algorithm in a really simple way. The original target formula would change to: Translated to code, we only need to change one line to get the desired improvements: t[a] = r + GAMMA * pTarget_[i][ numpy.argmax(p_[i]) ] The Deep Reinforcement Learning with Double Q-learning1 paper reports that although Double DQN (DDQN) does not always improve performance, it substantially benefits the stability of learning. This improved stability directly translates to ability to learn much complicated tasks. When testing DDQN on 49 Atari games, it achieved about twice the average score of DQN with the same hyperparameters. With tuned hyperparameters, DDQN achieved almost four time the average score of DQN. Summary of the results is shown in a table in the next section. Prioritized Experience Replay One of the possible improvements already acknowledged in the original research2 lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay3 (PER) is one strategy that tries to leverage this fact by changing the sampling distribution. The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world \u2013 if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits. We can define an error of a sample S = (s, a, r, s\u2019) as a distance between the Q(s, a) and its target T(S):\u2028 For DDQN described above, T it would be:\u2028 We will store this error in the agent\u2019s memory along with every sample and update it with each learning step. One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula: Epsilon is a small positive constant that ensures that no transition has zero priority. Alpha, , controls the difference between high and low error. It determines how much prioritization is used. With we would get the uniform case. Priority is translated to probability of being chosen for replay. A sample i has a probability of being picked during the experience replay determined by a formula: The algorithm is simple \u2013 during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them. Initialization and new transitions The original paper says that new transitions come without a known error3, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. It\u2019s also effective, because high value transitions are discovered immediately. Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple: The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising. Efficient implementation So how do we store the experience and effectively sample from it? A naive implementation would be to have all samples in an array sorted according to their priorities. A random number s, , would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution. But this would have a terrible efficiency: O(n log n) for insertion and update and O(n) for sampling. A first important observation is that we don\u2019t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability. This releases the need for sorting, improving the algorithm to O(1) for insertion and update. But the O(n) for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree \u2013 a binary tree data structure where the parent\u2019s value is the sum of its children. The samples themselves are stored in the leaf nodes. Update of a leaf node involves propagating a value difference up the tree, obtaining O(log n). Sampling follows the thought process of the array case, but achieves O(log n). For a value s, , we use the following algorithm (pseudo code): def retrieve(n, s): if n is leaf_node: return n if n.left.val &gt;= s: return retrieve(n.left, s) else: return retrieve(n.right, s - n.left.val) Following picture illustrates sampling from a tree with s = 24: With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples. For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on GitHub. Results Tests performed on 49 Atari games showed that PER really translates into faster learning and higher performance3. What\u2019s more, it\u2019s complementary to DDQN. Following table summarizes the results for 49 Atari games benchmark. Values are taken from Schaul et al. (2015)3 and rescaled. Because the DQN+PER value for proportional PER was not available, the provided value is for similar, but different rank-based PER. For specifics of how these tests were performed, look into the paper3. DQN DQN+PER DDQN DDQN+PER 100% 291% 343% 451% Implementation An implementation of DDQN+PER for an Atari game Seaquest is available on GitHub. It\u2019s an improvement over the DQN code presented in last chapter and should be easy to understand. The DQN architecture from the original paper4 is implemented, although with some differences. In short, the algorithm first rescales the screen to 84×84 pixels and extracts luminance. Then it feeds last two screens as an input to the neural network. This ensures that the algorithm is also aware of a direction of movement of the game elements, something which would not be possible with only a current screen as input. Experience is stored in a sum tree with capacity of 200 000 samples. Neural network uses three convolutional layers and one dense hidden layer with following parameters: Convolution2D(32, 8, 8, subsample=(4,4), activation='relu') Convolution2D(64, 4, 4, subsample=(2,2), activation='relu') Convolution2D(64, 3, 3, activation='relu') Dense(output_dim=512, activation='relu') Dense(output_dim=actionCount, activation='linear') These hyperparameters were used: Parameter Value memory capacity 200000 batch size 32 γ 0.99 exploration εmax 1.00 exploration εmin 0.10 final exploration frame 500000 PER α 0.6 PER ε 0.01 RMSprop learning rate 0.00025 target network update frequency 10000 It is possible to run this program on a regular computer, however it is very resource demanding. It takes about 12 GB of RAM and fully utilizes one core of CPU and whole GPU to slowly improve. In my computer it runs around 20 FPS. After about 12 hours, 750 000 steps and 700 episodes, it reached an average reward of 263 (mean reward of a random agent is 87). You can see it in action here: To get better results, you have to run it for at least tens of millions of steps. The following graph shows that the improvement is indeed very slow: Conclusion We addressed Double Learning and Prioritized Experience Replay techniques that both substantially improve the DQN algorithm and can be used together to make a state-of-the-art algorithm on the Atari benchmark (at least as of 18 Nov 2015 \u2013 the day Prioritized Experience Replay3 article was published). This articles finishes the Let\u2019s make a DQN series. It was meant as a simplified tutorial for those who don\u2019t want to read whole research articles but still want to understand what DQN is and what it does. I also hope that it sparkled your interest in this interesting direction of AI and that you will want to learn even more now. I hope you enjoyed these articles at least as me writing them and that you learned something. If you have any questions or have anything to add, please feel free to leave a comment or contact me at author@jaromiru.com. About the author: Jaromír Janisch has graduated in 2012 as a Software Engineer from University of West Bohemia. Until 2016 he worked in several software companies, mainly as a developer. He\u2019s been always passionate about progress in artificial intelligence and decided to join the field as a researcher. He is currently preparing to start his Ph.D. studies. In this blog he strives to extract main points from sometimes complicated scientific research, reformulate them in his own words and make it more accessible for people outside the field. Hado van Hasselt, Arthur Guez, David Silver \u2013 Deep Reinforcement Learning with Double Q-learning, arXiv:1509.06461, 2016 ↩ ↩ Hado van Hasselt \u2013 Double Q-learning, Advances in Neural Information Processing Systems, 2010 ↩ ↩ ↩ Schaul et al. \u2013 Prioritized Experience Replay, arXiv:1511.05952, 2015 ↩ ↩ ↩ ↩ ↩ ↩ Mnih et al. \u2013 Human-level control through deep reinforcement learning, Nature 518, 2015 ↩ Share this article: Share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Click to share on Twitter (Opens in new window) Post navigation ← Let\u2019s make a DQN: Full DQN 2 comments Vincent says: November 9, 2016 at 6:37 pm Very interesting read! This makes it very easy to understand. Are you planning. Are you planning on writing posts about more RL subjects? I would love to read more intuitive articles about policy gradients methods. Good luck with your PhD! Reply ヤロミール says: November 10, 2016 at 12:48 am Thank you! More articles are definitely possible, although not in the very near future. Leave a Reply Cancel reply Enter your comment here... Fill in your details below or click an icon to log in: Email (Address never made public) Name Website You are commenting using your WordPress.com account. ( Log Out \/ Change ) You are commenting using your Twitter account. ( Log Out \/ Change ) You are commenting using your Facebook account. ( Log Out \/ Change ) You are commenting using your Google+ account. ( Log Out \/ Change ) Cancel Connecting to %s Notify me of new comments via email. ","flair":"four\tProject"}
{"author":"purpledazy","created":"Sun Oct 16 02:27:14 EDT 2016","text":"I am having a problem formalising my data questions as probabilistic programming\/graphical models, for some reason the gap between examples provided in various books and real-life scenarios seems too far to bridge. \n\nFor example -\nIn one case my goal is to predict the number of months left for a building construction to be completed.\nThe data I have available is a list of buildings in a region with those features:\n\n* When did the construction of each building started\n\n* The number of workers that worked\/still working on its construction - per month since the construction started\n\n* Number of floors the building has\/planned to have\n\n* Has the construction finished or not? (in the form of finish date or null)\n\n\nMy \u2018progress\u2019 so far:\n\n* I think the model\u2019s output of interest would be a distribution of \u2018time it takes to complete a building' dependent on the number of floors, number of workers per month, and how many months have passed since the construction started.\n\n* The prior for that would be the current distribution months it took to complete the existing completed buildings (so filtering out all non-completed buildings from the dataset)\n\n* The number of floors parameter seems like it should be modelled as a hierarchical component (either each number of floors or a subgroup will have a different distribution of \u2018number of workers per month\u2019) - that\u2019s based on a hunch that buildings with more floors require more workers but in a non-linear way.\n\nTwo things that I feel I am stuck with are: \n\n* How to model the \u2018number of workers\u2019 data since it requires two parameters - time (e.g., month index since construction started) and the number of workers at that period\u2026 the distributions in the examples I\u2019ve seen model a single \u2018value\u2019. I can break this into a fixed array of multiple distributions (one per month) but for some reason that doesn\u2019t seem right\u2026 (how will all those distributions be tied to each other? see next point)\n\n* How to tie the number of floors and distribution of workers to target distribution?\n\nMany thanks for reading all this, really not sure how to get help to go beyond the text book examples in probabilistic programming\/graph models\u2026 quite frustrating, since I am really keen on applying it in my work.","flair":"one\tDiscussion"}
{"author":"belsnickel4ever","created":"Sat Nov 26 02:15:35 EST 2016","text":"Hi, anyone interested in maintaining\/contributing to a repo for your custom utility functions(Callbacks, Data processing, Training monitoring, code examples etc.) and hacks that you use in your day-to-day work in Keras. \n\nPlug- https:\/\/github.com\/ishank26\/Kutils  \n\nEdit: My work is tiny as compared to other third party libraries for Keras. fchollet maintains a list of keras related projects- https:\/\/github.com\/fchollet\/keras-resources. IMO since keras development is distributed and the source has many issues, it will be better to have a curated list or a repo for hacks.  ","flair":"one\tDiscussion"}
{"author":"egrefen","created":"Thu Nov 10 05:39:51 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.02554 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1611 Change to browse by: cs cs.AI cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: The Neural Noisy Channel Authors: Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefenstette, Tomas Kocisky (Submitted on 8 Nov 2016) Abstract: We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use. Comments: ICLR 2017 submission Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1611.02554 [cs.CL]   (or arXiv:1611.02554v1 [cs.CL] for this version) Submission history From: Lei Yu [view email] [v1] Tue, 8 Nov 2016 15:18:44 GMT (27kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"ankeshanand","created":"Tue Nov 15 08:25:30 EST 2016","text":"A Tensorflow Flow implementation of Google Brain's recent paper (Learning to Protect Communications with Adversarial Neural Cryptography.) Two Neural Networks, Alice and Bob learn to communicate secretly with each other, in presence of an adversary Eve. First, ensure you have the dependencies installed. To train the neural networks, run the script.","flair":"four\tProject"}
{"author":"givdwiel","created":"Mon Oct 24 06:46:45 EDT 2016","text":"This repository contains an innovative algorithm that constructs an ensemble using well-known decision tree induction algorithms such as CART, C4.5, QUEST and GUIDE combined with bagging and boosting. Then, this ensemble is converted to a single, interpretable decision tree in a genetic fashion. For a certain number of iterations, random pairs of decision trees are merged together by first converting them to sets of k-dimensional hyperplanes and then calculating the intersection of these two sets (a classic problem from computational geometry). Moreover, in each iteration, an individual is mutated with a certain probabibility. After these iterations, the accuracy on a validation set is measured for each of the decision trees in the population and the one with the highest accuracy (and lowest number of nodes in case of a tie) is returned. Example.py has run code for all implemented algorithms and returns their average predictive performance, computational complexity and model complexity on a number of dataset An install.sh script is provided that will install all required dependencies A nicely looking documentation page is available in the doc\/ directory. Download the complete directory and open index.html A wrapper is written around Orange C4.5, sklearn CART, GUIDE and QUEST. The returned object is a Decision Tree, which can be found in . Moreover, different methods are available on this decision tree: classify new, unknown samples; visualise the tree; export it to string, JSON and DOT; etc. A wrapper is written around the well-known state-of-the-art ensemble techniques XGBoost and Random Forests A wrapper written around the R package inTrees and an implementation of ISM can be found in the constructors package. A new dataset can easily be plugged in into the benchmark. For this, a function must be written in You can contact me at givdwiel.vandewiele at ugent.be for any questions, proposals or if you wish to contribute. Please refer to my work when you use it. A reference to this github or to the following (yet unpublished) paper: @article{Vandewiele, abstract = {Models obtained by decision tree induction techniques excel in being interpretable. However, they can be prone to overfitting, which results in a low predictive per-formance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.}, author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and Hoecke, Sofie Van}, title = {{GENESIM: genetic extraction of a single, interpretable model}}, year={2016}, note={available at; \\url{https:\/\/github.com\/IBCNServices\/GENESIM}} }","flair":"three\tResearch"}
{"author":"ajmooch","created":"Tue Nov 01 01:36:30 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.09585 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.CV stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Conditional Image Synthesis With Auxiliary Classifier GANs Authors: Augustus Odena, Christopher Olah, Jonathon Shlens (Submitted on 30 Oct 2016) Abstract: Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. Subjects: Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1610.09585 [stat.ML]   (or arXiv:1610.09585v1 [stat.ML] for this version) Submission history From: Augustus Odena [view email] [v1] Sun, 30 Oct 2016 00:29:31 GMT (29025kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"JColl1","created":"Tue Sep 27 19:29:05 EDT 2016","text":"In this post, I\u2019ll introduce you to the Julia programming language and a couple long-term projects of mine: Plots for easily building complex data visualizations, and JuliaML for machine learning and AI. After short introductions to each, we\u2019ll quickly throw together some custom code to build and visualize the training of an artificial neural network. Julia is fast, but you\u2019ll see that the real speed comes from developer productivity. Since the original post there have been several API changes, and the code in this post has either been added to packages (such as MLPlots) or will not run due to name changes. I have created a Jupyter notebook with updated code. Julia is a fantastic, game-changing language. I\u2019ve been coding for 25 years, using mainstays like C, C++, Python, Java, Matlab, and Mathematica. I\u2019ve dabbled in many others: Go, Erlang, Haskel, VB, C#, Javascript, Lisp, etc. For every great thing about each of these languages, there\u2019s something equally bad to offset it. I could never escape the \u201Ctwo-language problem\u201D, which is when you must maintain a multi-language code base to deal with the deficiencies of each language. C can be fast to run, but it certainly isn\u2019t fast to code. The lack of high-level interfaces means you\u2019ll need to do most of your analysis work in another language. For me, that was usually Python. Now\u2026 Python can be great, but when it\u2019s not good enough\u2026 ugh. Python excels when you want high level and your required functionality already exists. If you want to implement a new algorithm with even minor complexity, you\u2019ll likely need another language. (Yes\u2026 Cython is another language.) C is great when you just want to move some bytes around. But as soon as you leave the \u201Csweet spot\u201D of these respective languages, everything becomes prohibitively difficult. Julia is amazing because you can properly abstract exactly the right amount. Write pseudocode and watch it run (and usually fast!) Easily create strongly-typed custom data manipulators. Write a macro to automate generation of your boilerplate code. Use generated functions to produce highly specialized code paths depending on input types. Create your own mini-language for domain-specificity. I often find myself designing solutions to problems that simply should not be attempted in other languages. I won\u2019t waste time going through Julia basics here. For the new users, there are many resources for learning. The takeaway is: if you\u2019re reading this post and you haven\u2019t tried Julia, drop what you\u2019re doing and give it a try. With services like JuliaBox, you really don\u2019t have an excuse. Plots (and the JuliaPlots ecosystem) are modular tools and a cohesive interface, which let you very simply define and manipulate visualizations. One of its strengths is the varied supported backends. Choose text-based plotting from a remote server or real-time 3D simulations. Fast, interactive, lightweight, or complex\u2026 all without changing your code. Massive thanks to the creators and developers of the many backend packages, and especially to Josef Heinen and Simon Danisch for their work in integrating the awesome GR and GLVisualize frameworks. However, more powerful than any individual feature is the concept of recipes. A recipe can be simply defined as a conversion with attributes. \u201CUser recipes\u201D and \u201Ctype recipes\u201D can be defined on custom types to enable them to be \u201Cplotted\u201D just like anything else. For example, the type in my AtariAlgos package will capture the current screen from an Atari game and display it as an image plot with the simple command : \u201CSeries recipes\u201D allow you to build up complex visualizations in a modular way. For example, a histogram recipe will bin data and return a bar plot, while a bar recipe can in turn be defined as a bunch of shapes. The modularity greatly simplifies generic plot design. Using modular recipes, we are able to implement boxplots and violin plots, even when a backend only supports simple drawing of lines and shapes: To see many more examples of recipes in the wild, check out StatPlots, PlotRecipes, and more in the wider ecosystem. For a more complete introduction of Plots, see my JuliaCon 2016 workshop and read through the documentation JuliaML (Machine Learning in Julia) is a community organization that was formed to brainstorm and design cohesive alternatives for data science. We believe that Julia has the potential to change the way researchers approach science, enabling algorithm designers to truly \u201Cthink outside the box\u201D (because of the difficulty of implementing non-conventional approaches in other languages). Many of us have independently developed tools for machine learning before contributing. Some of my contributions to the current codebase in JuliaML are copied-from or inspired-by my work in OnlineAI. The recent initiatives in the Learn ecosystem (LearnBase, Losses, Transformations, Penalties, ObjectiveFunctions, and StochasticOptimization) were spawned during the 2016 JuliaCon hackathon at MIT. Many of us, including Josh Day, Alex Williams, and Christof Stocker (by Skype), stood in front of a giant blackboard and hashed out the general design. Our goal was to provide fast, reliable building blocks for machine learning researchers, and to unify the existing fragmented development efforts. Time to code! I\u2019ll walk you through some code to build, learn, and visualize a fully connected neural network for the MNIST dataset. The steps I\u2019ll cover are: Get the software (use on a package for the latest features): Start up Julia, then load the packages: A custom type to simplify the creation of trace plots (which Build a neural net with softplus activations for the inner layers and softmax output for classification: Note: the method is a very simple convenience constructor for transformations. It\u2019s pretty easy to construct the transformation yourself for more complex models. This is what is constructed on the call to (note: this is the output of , not runnable code): Create an objective function to minimize, adding an Elastic (combined L1\/L2) penalty\/regularization. Note that the cross-entropy loss function is inferred automatically for us since we are using softmax output: Add a method for computing the loss and accuracy on a subsample of test data: Our custom trace method which will be called after each minibatch: Finally, we build our learner and learn! We\u2019ll use the Adadelta method with a learning rate of 0.05. Notice we just added our custom tracer to the list of parameters\u2026 we could have added others if we wanted. The method is just a convenience to optionally construct a with some common sub-learners. In this case we add a sub-learner to stop the optimization after 50000 iterations. We will train on randomly-sampled minibatches of 5 observations at a time, and update our parameters using the average gradient: After a little while we are able to predict ~97% of the test examples correctly. The heatmap (which represents the \u201Cimportance\u201D of each pixel according to the outgoing weights of our model) depicts the important curves we have learned to distinguish the digits. The performance can be improved, and I might devote future posts to the many ways we could improve our model, however model performance was not my focus here. Rather I wanted to highlight and display the flexibility in learning and visualizing machine learning models. There are many approaches and toolsets for data science. In the future, I hope that the ease of development in Julia convinces people to move their considerable resources away from inefficient languages and towards Julia. I\u2019d like to see Learn.jl become the generic interface for all things learning, similar to how Plots is slowly becoming the center of visualization in Julia. If you have questions, or want to help out, come chat. For those in the reinforcement learning community, I\u2019ll probably focus my next post on Reinforce, AtariAlgos, and OpenAIGym. I\u2019m open to many types of collaboration. In addition, I can consult and\/or advise on many topics in finance and data science. If you think I can help you, or you can help me, please don\u2019t hesitate to get in touch.","flair":"null\tnull"}
{"author":"deepmind2016","created":"Sun Oct 16 13:08:35 EDT 2016","text":"In this video(https:\/\/www.youtube.com\/watch?v=TNZk8lo4e-Q) at 25:22, Nando multiplies approximated integral(which has been approximated using law of large numbers) with delta function(which counts the number of samples in dtheta) in importance sampling. What's the role of delta function?, we have already approximated integral and have probability distribution over theta. Why we need delta function?","flair":"null\tnull"}
{"author":"methodmissin","created":"Wed Nov 02 11:17:42 EDT 2016","text":"Several years ago I saw Jeff Hawkins present on the NuPIC platform for on-line machine learning.\n\nLast week I was thinking about our company's Service Oriented Architecture and all our pipes and collectors for monitoring and stats and logs and so forth, and how unwieldy and difficult to wrangle it is, and it occurred to me that perhaps ML can assist here.\n\nI reason that I could set up a simple NN that took as input the current stream of scalar data from a single monitor, and output [FAULT, NO-FAULT].\n\nNO-FAULT should be the default state, and emits as long as the input follows predicted traffic.\n\nFAULT emits when the input goes some percentage outside of the prediction.\n\nDeploy independent ML for every input, or pipe several related inputs into one?\n\nRegardless, the goal would be to eliminate the need for an expert human to interpret 'good' data vs 'problem' data and set discrete alerting thresholds. Eliminate the need to have deployment-specific configurations; all hosts get the same treatment and the company benefits from better alerting.\n\nThis was demonstrated on HTM for monitoring energy consumption of an office building.\n\nIs HTM still the best model for training machines to do this, or have other tools\/techniques emerged?\n","flair":"one\tDiscussion"}
{"author":"lagastic","created":"Thu Sep 29 12:00:05 EDT 2016","text":"In contrast to the usual approach to operating self-driving cars, we did not program any explicit object detection, mapping, path planning or control components into this car. Instead, the car learns on its own to create all necessary internal representations necessary to steer, simply by observing human drivers. The car successfully navigates the construction site while freeing us from creating specialized detectors for cones or other objects present at the site. Similarly, the car can drive on the road that is overgrown with grass and bushes without the need to create a vegetation detection system. All it takes is about twenty example runs driven by humans at different times of the day. Learning to drive in these complex environments demonstrates new capabilities of deep neural networks. The car also learns to generalize its driving behavior. This video includes a clip that shows a car that was trained only on California roads successfully driving itself in New Jersey .Learn more about NVIDIA DRIVE technology: http:\/\/nvda.ws\/2cBewNI","flair":"null\tnull"}
{"author":"AutoModerator","created":"Wed Oct 05 11:52:17 EDT 2016","text":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n","flair":"null\tnull"}
{"author":"Kiuhnm","created":"Thu Nov 10 07:05:48 EST 2016","text":"I asked this question both on Stack Overflow and on Theano's newsgroup (Yes, I'm using Theano), but I got no answer, which makes me think that what I'm trying to do is quite unusual.\n\nBasically, I need to solve a linear system as part of the computation of a neural network. Ideally I'd like to solve a linear system, but inverting the matrix shouldn't be a problem since it's small (100x100) and well conditioned.\n\nThe problem with Theano is that it needs [Cula](http:\/\/www.culatools.com\/downloads\/dense\/) to work on Gpu, but, as you can see, the website has some problems.\n\nCan you think of a workaround? I googled quite a bit but it seems that my needs are unusual. Is there any other library\/framework that will do what I need (on GPU)?\n\nI'm also OK with doing the inversion\/solve on CPU and the rest of the computations on GPU, but I don't think one can mix CPU and GPU code in Theano and I think that breaking the computation into many functions would interfere with automatic differentiation.\n\n---\n\nUpdate: This [PR](https:\/\/github.com\/Theano\/Theano\/pull\/4917) solves the problem!","flair":"one\tDiscussion"}
{"author":"scardax88","created":"Thu Oct 27 07:57:36 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.07448 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: A Framework for Parallel and Distributed Training of Neural Networks Authors: Simone Scardapane, Paolo Di Lorenzo (Submitted on 24 Oct 2016) Abstract: The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach. Comments: Preprint submitted to Neural Networks (Elsevier) Subjects: Machine Learning (stat.ML); Learning (cs.LG) Cite as: arXiv:1610.07448 [stat.ML]   (or arXiv:1610.07448v1 [stat.ML] for this version) Submission history From: Simone Scardapane [view email] [v1] Mon, 24 Oct 2016 14:58:56 GMT (501kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"cavedave","created":"Mon Oct 10 12:18:08 EDT 2016","text":"Together with Tor, we have worked a lot on bandit problems in the past and developed a true passion for them. At the pressure of some friends and students (and a potential publisher), and also just to have some fun, we are developing a new graduate course devoted to this subject. This semester, I am teaching this course at the University of Alberta (UofA) and next semester Tor will do the same at the Indiana University where he just moved after finishing his post-doc at the UofA. The focus of the course will be on understanding the core ideas, mathematics and implementation details for current state-of-the-art algorithms. As we go, we plan to update this site on a weekly basis, describing what was taught in the given week \u2014 stealing the idea from Seb (e.g., see his excellent posts about convex optimization). The posts should appear around Sunday. Eventually, we hope that the posts will also form the basis of a new book on bandits that we are very excited about. For now, we would like to invite everyone interested in bandit problems to follow this site, give us feedback by commenting on these pages, ask questions, make suggestions for other topics, or criticize what we write. In other words, we wish to leverage the wisdom of crowd in this adventure to help us to make the course better. So this is the high level background. Today, in the remainder of this post I will first briefly motivate why anyone should care about bandits and look at where the name comes from. Next, I will introduce the formal language that we will use later and finish by peeking into what will happen in the rest of the semester. This is pretty basic stuff. To whet your appetite, next week, we will continue with a short review of probability theory and concentration results, including a fuss-free crash course on measure-theoretic probability in 30 minutes or so. These topics form the necessary background as we will first learn about the so-called stochastic bandit problems where one can get lost very easily without proper mathematical foundations. The level of discussion will be intended for anyone with undergraduate training in probability. By the end of the week, we will learn about the explore-then-exploit strategies and the upper-confidence bound algorithm. Why should we care about bandit problems? Decision making in the face of uncertainty is a significant challenge in machine learning. Which drugs should a patient receive? How should I allocate my study time between courses? Which version of a website will generate the most revenue? What move should be considered next when playing chess\/go? All of these questions can be expressed in the multi-armed bandit framework where a learning agent sequentially takes actions, observes rewards and aims to maximise the total reward over a period of time. The framework is now very popular, used in practice by big companies, and growing fast. In particular, google scholar reports 1000, 2500, and 7700 papers when searching for the phrase bandit algorithm for the periods of 2001-2005, 2006-2010, and 2011- present (see the figure below), respectively. Even if these numbers are somewhat overblown, they indicate that the field is growing rapidly. This could be a fashion or maybe there is something interesting happening here? We think that the latter is true! Fine, so maybe you decided to care about bandit problems. But what are they exactly? Bandit problems were introduced by William R. Thompson (one of our heroes, whose name we will see popping up again soon) in a paper in 1933 for the so-called Bayesian setting (that we will also talk about later). Thompson was interested in medical trials and the cruelty of running a trial \u201Cblindly\u201D, without adapting the treatment allocations on the fly as the drug appears more or less effective. Clinical trials is thus one of the first intended applications. But why would anyone call problems like optimizing drug allocation a \u201Cbandit-problem\u201D? The name comes from the 1950s when Frederick Mosteller and Robert Bush decided to study animal learning and ran trials on mice and then on humans. The mice faced the dilemma of choosing to go left or right, after starting in the bottom of a T-shaped maze, not knowing each time at which end they will find food. To study a similar learning setting in humans, a \u201Ctwo-armed bandit\u201D machine was commissioned where humans could choose to pull either the left or the right arm of the machine, each giving a random payoff with the distribution of payoffs for each arm unknown to the human player. The machine was called a \u201Ctwo-armed bandit\u201D in homage to the one-armed bandit, an old-fashioned name for a lever operated slot machine (\u201Cbandit\u201D because they steal your money!). Now, imagine that you are playing on this two-armed bandit machine and you already pulled each lever 5 times, resulting in the following payoffs: The left arm appears to be doing a little better: The average payoff for this arm is 4 (say) dollars per round, while that of the right arm is only 2 dollars per round. Let\u2019s say, you have 20 more trials (pulls) altogether. How would you pull the arms in the remaining trials? Will you keep pulling the left arm, ignoring the right (owing to its initial success), or would you pull the right arm a few more times, thinking that maybe the initial poorer average payoff is due to \u201Cbad luck\u201D only? This illustrates the interest in bandit problems: They capture the fundamental dilemma a learner faces when choosing between uncertain options. Should one explore an option that looks inferior or should one exploit one\u2019s knowledge and just go with the currently best looking option? How to maintain the balance between exploration and exploitation is at the heart of bandit problems. There are many real-world problems where a learner repeatedly chooses between options with uncertain payoffs with the sole goal of maximizing the total gain over some period of time. However, the world is a messy place. To gain some principled understanding of the learner\u2019s problem, we will often make extra modelling assumptions to restrict how the rewards are generated. To be able to talk about these assumptions, we need a more formal language. In this formal language we are talking about the learner as someone (or something) that interacts with an environment as shown in the figure below. On the figure the learner is a represented by a box on the left, the environment is represent by a box on the right. They are interconnected by two arrows connecting the two boxes. These arrows represent the interaction between the learner and the environment. What the figure fails to capture is that the interaction happens in a sequential fashion over a number of discrete rounds. In particular, the specification starts with the following interaction protocol: For rounds $t=1,2,\u2026,n$: 1. Learner chooses an action $A_t$ from a set $\\cA$ of available actions. The chosen action is sent to the environment; 2. The environment generates a response in the form of a real-valued reward $X_t\\in \\R$, which is sent back to the learner. The goal of the learner is to maximize the sum of rewards that it receives, $\\sum_{t=1}^n X_t$. A couple of things need to be clarified that are hidden by the above short description: When in step 2 we say that the reward is sent back to the learner, we mean that the learner gets to know this number and can thus use it in the future rounds when it needs to decide which action to take. Thus, in step 1 of round $t$, the learner\u2019s decision is based on the history of interaction up to the end of round $t-1$, i.e., on $H_{t-1} = (A_1,X_1,\\dots,A_{t-1},X_{t-1})$. Our goal, is to equip the learner with a learning algorithm to maximize its reward. Most of the time, the word \u201Calgorithm\u201D will not be taken too seriously in the sense that a \u201Clearning algorithm\u201D will be viewed as a mapping of possible histories to actions (possibly randomized). Nevertheless, throughout the course we will keep an eye on discussing whether such maps can be efficiently implemented on computers (justifying the name \u201Calgorithms\u201D). The next question to discuss is how to evaluate a learner (to simplify language, we identify learners and learning algorithms)? One idea is to measure learning speed by what is called the regret. The regret of a learner acting in an environment is relative to some action. We will start with a very informal definition (and hope you forgive us). The reason is that there a number of related definitions \u2013 all quite similar \u2013 and we do not want to be bogged down with the details in this introductory post. We often calculate the worst-case regret over the possible actions $a$, though in stochastic environments, we may first take expectations for each action and then calculate the worst-case over the actions (later we should come back to discussing the difference between taking the expectation first and then the maximum, versus taking the maximum first and then the expectation). For a fixed environment, the worst-case regret is just a shift and a sign change of the total reward: Maximizing reward is the same as minimizing regret. A good learner achieves sublinear worst-case regret: That is, if $R_n$ is the regret after $n$ rounds of interaction of a learner, $R_n = o(n)$. In other words, the average per round regret, $R_n\/n$ converges to zero as $n\\to\\infty$. Of major interest will be to see the rate of growth of regret for various classes of environments, as well as for various algorithms, and in particular, seeing how small the regret can be over some class of environment and what algorithms can achieve such regret lower bounds. The astute reader may notice that regret compares payoffs of fixed actions to the total payoff of the learner: In particular, in a non-stationary environment, a learner that can change what action it uses over time may achieve negative worst-case regret! In other words, the assumption that the world is stationary (i.e., there are no seasonality or other patterns in the environment, there is no payoff drift, etc.) is built into the above simple concept of regret. The notion of regret can and has been extended to make it more meaningful in non-stationary environments, but as a start, we should be content with the assumption of stationarity. Another point of discussion is whether in designing a learning algorithm we can use the knowledge of the time horizon $n$ or not.  In some learning settings assuming this is perfectly fine (say, businesses can have natural business cycles, say, weeks, months, etc.), but sometimes assuming the knowledge of the time horizon is unnatural. In those cases we will be interested in algorithms that work well in lack of the knowledge of the time horizon, in other words, also known as anytime algorithms. As we will see, sometimes the knowledge of time horizon will make a difference when it comes to designing learning algorithms, though usually we find that the difference will be small. Let\u2019s say we are fine with stationarity. Still, why should one use (worst-case) regret as opposed to simply resorting to comparing learners by the total reward they collected, especially since we said that minimizing the regret is the same as maximizing total reward? Why complicate matters with talking about regret then? First, regret facilitates comparisons across environments by doing a bit of normalization: One can shift rewards by an arbitrary constant amount in some environment and the regret does not change, whereas total reward would be impacted by this change (note that scale is still an issue: multiplying the rewards by a positive constant number, the regret also gets multiplied, so usually we assume some fixed scale, or we need to normalize regret with the scale of rewards). More important however is that regret is well aligned with the intuition that learning should be easy in environments where either (i) all actions pay similar amounts (all learners do well in such environments and all of them will have low regret), and also in environments where (ii) some actions pay significantly more than others (because in such environments it is easy to find the high-paying action or actions, so many learners will have small regret). As a result, the worst-case regret over various classes of environment is a meaningful quantity, as opposed to worst-case total reward which is vacuous. Generally, regret allows one to study learning in worst-case settings. Whether this leads to conservative behavior is another interesting point for further discussion (and is an active topic of research). With this, let me turn to discussing the typical restrictions on the bandit environments. A particularly simple, yet appealing and rich problem setting is that of stochastic, stationary bandits. In this case the environment is restricted to generate the reward in response to each action from a distribution that is specific to that action, and independently of the previous action choices (of the possibly randomizing learner) and rewards. Sometimes, this is also called \u201Cstochastic i.i.d.\u201D bandits since for a given action, any reward for that action is independent of all the other rewards of the same action and they are all generated from identical distributions (that is, there is one distribution per action). Since \u201Cstochastic, stationary\u201D and \u201Cstochastic i.i.d.\u201D are a bit too long, in the future, we will just refer to this setting as that of \u201Cstochastic bandits\u201C. This is the problem setting that we will start discussing next week. For some applications the assumption that the rewards are generated in a stochastic and stationary way may be too restrictive. In particular, stochastic assumptions can be hard to justify in the real-world: The world, for most of what we know about it, is deterministic, if it is hard to predict and often chaotic looking. Of course, stochasticity has been enormously successful to explain mass phenomenon and patterns in data and for some this may be sufficient reason to keep it as the modeling assumption. But what if the stochastic assumptions fail to hold? What if they are violated for a single round? Or just for one action, at some rounds? Will all our results become suddenly vacuous? Or will the algorithms developed be robust to smaller or larger deviations from the modeling assumptions?  One approach, which is admittedly quite extreme, is to drop all the assumptions on how rewards are assigned to arms. More precisely, some minimal assumptions are kept, like that the rewards lie in a bounded interval, or that the reward assignment is done before the interaction begins, or simultaneously with the choice of the learner\u2019s action. In any case, if these are the only assumptions, we get what is called the setting of adversarial bandits. Can we still say meaningful things about the various algorithms? Can learning algorithms achieve nontrivial results? Can their regret be sublinear? It turns out, that, perhaps surprisingly, this is possible. At a second thought though this may not be that surprising: After all, sublinear regret requires only that the learner uses the single best action (best in hindsight) for most of the time, but the learner can still use any other actions for a smaller (and vanishing) fraction of time. What saves learners then is that the identity of the  action with the highest total payoff in hindsight cannot change too abruptly and too often as averages cannot be changed too much as long as the summands in the average are bounded. Let\u2019s close with what the course will cover. The course will have three major blocks. In the first block, we will discuss finite-armed bandits, both stochastic and adversarial as these help the best to develop our basic intuition of what makes a learning algorithm good. In the next block we will discuss the beautiful developments in linear bandits, again, both stochastic and adversarial, their connection to contextual bandits, and also various combinatorial settings, which are of major interest in various applications. The reason to spend much time on linearity is because it is the simplest structure that permits one to address large scale problems, and is a very popular and successful modelling assumption. In the last block, we will discuss partial monitoring and learning and acting in Markovian Decision Processes, that is, topics which go beyond bandit problems. With this little excursion outside of bandit land we hope to give a better glimpse for everyone on how exactly bandits fit the bigger picture of online learning under uncertainty. The paper by Thompson that was the first to consider bandit problems is the following: Besides Thompson\u2019s seminal paper, there are already a number of books on bandits that may serve as useful additional reading. The most recent (and also most related) is by Sebastien Bubeck and Nicolo Cesa-Bianchi and is freely available online. This is an excellent book and is warmly recommended. The book largely overlaps the topics we will cover, though we are also planning to cover developments that did not fit the page limit of Seb and Nicolo\u2019s book, as well as newer developments. There are also two books that focus mostly on the Bayesian setting, which we will address only a little. Both are based on relatively old material, but are still useful references for this line of work and are well worth reading.","flair":"four\tProject"}
{"author":"dataoverflow","created":"Fri Oct 07 05:44:41 EDT 2016","text":" Nanodegree Catalog Sign In My Classroom Get Started Join our list for the latest Udacity news and offers Nanodegree Catalog Sign In My Classroom Get Started An Open Source Self-Driving Car Udacity is building an open source self-driving car, and we want your help! Join the effort to create the world's first open source autonomous vehicle. We've broken down the problem into multiple complex challenges, and you or a team can compete to have your solution run in a real self-driving car. Learn More Join Slack Challenge 1 3D Model for Camera Mount View Challenge Details Challenge 2 Using Deep Learning to Predict Steering Angles View Challenge Details Challenge 3 Image-Based Localization View Challenge Details Challenge 4 Self-Driving Car Android Dashboard View Challenge Details Challenge 5 Coming Soon... View Challenge Details Home About Catalog FAQ Terms Featured Nanodegree Programs Self-Driving Car Engineer VR Developer Android Developer Full Stack Web Developer Become an iOS Developer Front-End Web Developer Machine Learning Engineer Student Resources Blog Career Resource Center Help & FAQ Catalog Student Success Veteran Programs Mobile Udacity About In the News Jobs @ Udacity Udacity Self-Driving Car Georgia Tech Udacity for Business Hire Graduates Inquiries Contact Us Developer API Legal Service Status Course Guides Site Map Brazil China Germany India United States \"Nanodegree\" is a registered trademark of Udacity. © 2011\u20132016 Udacity, Inc. Udacity is not an accredited university and we don't confer traditional degrees. Udacity Nanodegree programs represent collaborations with our industry partners who help us develop our content and who hire many of our program graduates. Udacity 现已提供中文版本！ A Udacity tem uma página em português para você! There's a local version of Udacity for you! 将此设置为 Udacity 默认主页 Tornar esta a página padrão da Udacity Always make this my Udacity homepage 前往优达学城中文网站 Ir para a página brasileira Go to Indian Site or continue to Global Site ","flair":"null\tnull"}
{"author":"yoyosarian","created":"Sun Nov 20 21:41:20 EST 2016","text":"I have been playing around with compressing atari games on to small 2048 lstms [here](https:\/\/github.com\/loliverhennigh\/Compressing-Dynamical-Systems-Atari). This is by no means an original idea and has been tried [here](http:\/\/cs231n.stanford.edu\/reports2016\/116_Report.pdf). I am just wondering if this is a promising idea. It seems in the paper listed they were unable to get very good results.","flair":"one\tDiscussion"}
{"author":"brand0x","created":"Sun Oct 02 17:55:50 EDT 2016","text":"Hello! We're putting together an e-zine about the intersection of machine learning and investigative journalism. There are a few of us writing articles right now, some from professional machine learning backgrounds and some from nonprofit investigative journalism. I figured I'd see if anyone wanted to contribute to it in here.\n\nOur goal is to contribute something back to investigative journalists that could possibly be used a an experimental roadmap on how things could be done better and more efficiently using machine learning.","flair":"null\tnull"}
{"author":"internet_ham","created":"Tue Nov 15 16:40:33 EST 2016","text":"A selection of interesting pairings that we found. You can start here and dive into some surprising or amusing aspects of how the machine learning algorithm perceives art. What is the connection between a 4000 year old clay figure and Van Gogh's Starry Night? How do you get from Bruegel's Tower of Babel to the street art of Rio de Janeiro? What links an African mask to a Japanese wood cut?","flair":"four\tProject"}
{"author":"hardmaru","created":"Tue Oct 11 12:02:53 EDT 2016","text":"300 lines of python code to demonstrate DDPG with Keras This is the second blog posts on the reinforcement learning. In this project we will demonstrate how to use the Deep Deterministic Policy Gradient algorithm (DDPG) with Keras together to play TORCS (The Open Racing Car Simulator), a very interesting AI racing game and research platform. (Change the flag train_indicator=1 in ddpg.py if you want to train the network) As a typical child growing up in Hong Kong, I do like watching cartoon movies. One of my favorite movies is called GPX Cyber Formula. It is an anime series about Formula racing in the future, a time when the race cars are equipped with super-intelligent AI computer system called \u201CCyber Systems\u201D. The AI can communicate with humans interactively and it can assist drivers to race in various extreme situations. Although we are still far away from building super-intelligent AI system, the latest development in the computer vision and deep learning has created an exciting era for me to fulfill my little dream \u2013 to create a cyber system called \u201CAsurada\u201D. I think it is important to study TORCS because: In the previous blog post Using Keras and Deep Q-Network to Play FlappyBird we demonstrate using Deep Q-Network to play FlappyBird. However, a big limitation of Deep Q-Network is that the outputs\/actions are discrete while the action like steering are continuous in car racing. An obvious approach to adapt DQN to continuous domains is to simply discretize the action space. However, we encounter the \u201Ccurse of dimensionality\u201D problem. For example, if you discretize the steering wheel from -90 to +90 degrees in 5 degrees each and acceleration from 0km to 300km in 5km each, your output combinations will be 36 steering states times 60 velocity states which equals to 2160 possible combinations. The situation will be worse when you want to build robots to perform something very specialized, such as brain surgery that requires fine control of actions and naive discretization will not able to achieve the required precision to do the operations. Google Deepmind has devised a new algorithm to tackle the continuous action space problem by combining 3 techniques together 1) Deterministic Policy-Gradient Algorithms 2) Actor-Critic Methods 3) Deep Q-Network called Deep Deterministic Policy Gradients (DDPG) The original paper in 1) is not easy for non-machine learning expert to digest so I will sketch the proof here. If you are already familiar with the algorithm you can directly go to Keras code session. First, we are going to define a policy network that implements our AI-driver. This network will take the state of the game (for example, the velocity of the car, the distance between the car and the track axis etc) and decide what we should do (steer left or right, hit the gas pedal or hit the brake). It is called Policy-Based Reinforcement Learning because we will directly parametrize the policy here, s is the state , a is the action and Please note that there are 2 types of the policies: Why do we need stochastic policies in addition to a deterministic policy? It is easy to understand a deterministic policy. I see a particular state input, then I take a particular action. But sometimes deterministic policy won\u2019t work, like in the example of GO, where your first state is the empty board like below: If you use same deterministic strategy, your network will always place the stone in a \u201Cparticular\u201D position which is a highly undesirable behaviour since it makes you predictable by your opponent. In that situation, a stochastic policy is more suitable than deterministic policy. So how can I find An intuitive policy objective function will be the expectation of the total discount reward where the expectations of the total reward R is calculated under some probability distribution If you recall our previous blog that we have introduced the Q-function, which is maximum discounted future reward if we choose action a in state s now in the continuous case we can use the SARSA formula therefore, we can write the gradient of a deterministic policy Silver el at. (2014) proved that this is the policy gradient, i.e. you will get the maximum expected reward as long as you update your model parameters following the gradient formula above. The Actor-Critic Algorithm is essentially a hybrid method to combine the policy gradient method and the value function method together. The policy function is known as the actor, while the value function is referred to as the critic. Essentially, the actor produces the action Going back to the previous equations, we can use the trick of the Deep-Q Network where we replace the Q-function as a neural network Follow the previous DQN blog post, we could use an iterative method to solve for the Q-function, where we can setup the Loss function the Q-value can be used to estimate the values of the current actor policy. The following figure shows the actor-critic architecture from Sutton\u2019s Book [2] Let\u2019s first talk about how to build the Actor Network in Keras. Here we used 2 hidden layers with 300 and 600 hidden units respectively. The output consist of 3 continuous actions, Steering, which is a single unit with tanh activation function (where -1 means max right turn and +1 means max left turn). Acceleration, which is a single unit with sigmoid activation function (where 0 means no gas, 1 means full gas). Brake, another single unit with sigmoid activation function (where 0 means no brake, 1 bull brake) We have used Keras function called Merge to combine 3 outputs together. Smart readers may ask why not using traditional Dense function like this There is a reason for that. First using 3 different Dense() function allows each continuous action have different activation function, for example, using tanh() for acceleration doesn\u2019t make sense as tanh are in the range [-1,1] while the acceleration is in the range [0,1] Please also noted that in the final layer we used the normal initialization with The construction of the Critic Network is very similar to the Deep-Q Network in the previous post. The only difference is that we used 2 hidden layers with 300 and 600 hidden units. Also, the critic network takes both the states and the action as inputs. According to the DDPG paper, the actions were not included until the 2nd hidden layer of Q-network. Here we used the Keras function Merge to merge the action and the hidden layer together It is a well-known fact that directly implementing Q-learning with neural networks proved to be unstable in many environments including TORCS. Deepmind team came up the solution to the problem is to use a target network, where we created a copy of the actor and critic networks respectively, that are used for calculating the target values. The weights of these target networks are then updated by having them slowly track the learned networks: It is extremely easy to implement target networks in Keras: After we finished the network setup, Let\u2019s go through the example in ddpg.py, our main code The code simply does the following: In the TORCS there are 18 different types of sensor input, the details can be found here Simulated Car Racing Championship : Competition Software Manual. So which sensor input should we use? After some trial-and-error, I found the following inputs are useful: Please note that we have normalized some of those value before feed into the neural network and some sensor inputs are not exposed in gym_torcs. The Advanced user needs to amend gym_torcs.py to change the parameters. [checkout the function make_observaton()] Now we can use the inputs above to feed into the neural network. The code is actually very simple: However, we immediately run into the two issues. First, how do we decide the reward? Second, how do we do exploration in the continuous action space? In the original paper, they used the reward function which is equal to the velocity of the car projected along the track direction However, I found that the training is not very stable as reported in the original paper. I believe the reason is that in the original policy the AI will try to accelerate the gas pedal very hard (to get maximum reward) and it hits the edge and the episode terminated very quickly. Therefore, the neural network stuck in a very poor local minimum. The new proposed reward function is below: In plain English, we want to maximum longitudinal velocity (first term), minimize transverse velocity (second term), and we also penalize the AI if it constantly drives very off center of the track (third term) I found the new reward function greatly improves the stability and the learning time of TORCS. Another issue is how to design a right exploration algorithm in the continuous domain. In the previous blog post, we used What is Ornstein-Uhlenbeck process? In simple English it is simply a stochastic process which has mean-reverting properties. Basically, the most important parameters are the My finding is that the AI can learn a reasonable policy on the simple track if using a sensible exploration policy and revised reward function, like within ~200 episode. Similar to the FlappyBird case, we also used the Experience Replay to saved down all the episode Please note that when we calculated the target_q_values we do use the output of target-Network instead of the model instead. The used of the slow-varying target-Network will reduce the oscillations of the Q-value estimation, which greatly improve the stability of the learning. The actual training of the neural network is very simple, only contains 6 lines of code: In plain English, we first update the critic by minimizing the loss Then the actor policy is updated using the sampled policy gradient therefore, it can be written as The last 2 lines of the code update the target network In order to test the policy, I choose a slightly difficult track called Aalborg as my training dataset. The figure below shows the layout of the track: I trained the neural network with 2000 episodes and allowed Ornstein-Uhlenbeck process decay linearly in 100000 frames. (i.e. no more exploitation is applied after 100000 frames). I also validate my neural network by allowing the agent to drive on a much longer track called Alpine 1 (3 times longer). It is important to test the AI agents in other tracks to make sure the AI do not simply \u201Cmemorize the track\u201D, aka overfitting. The first video shows the result of the Aalborg track, our training dataset. I used the software avconv to capture my video output. (My computer is a bit slow therefore the Audio output is not very smooth). As you can see the agent learned a decent policy. (I took the video after the agent finished 2 loops) The second video shows the result of the Alpine 1 track, our validation dataset. The agent managed to drive for 3 minutes before the car spin and stop there. Since the AI agent can drive on Alpine 1 track which is much longer than Aalborg track, we can say that the Neural Network did not overfit on the testing dataset. However, you can see the policy is still not optimal yet as the agent didn\u2019t use the brake much. It turns out that asking AI to learn how to brake is much harder than steering or acceleration. The reason is that the velocity of the car slow down when the brake is applied, therefore, the reward function is reduced and the AI agent is not keen to hit the brake at all. Also, if I allow the AI to hit the brake and the acceleration at the same time during the exploration phase, the AI will often hit the brake hard therefore we are stuck in very poor local minimum (as the car is not moving and no reward is received) So how to solve this problem? I recalled myself when I first learnt driving in Princeton many years ago, my teacher told me do not hit the brake too hard and also try to feel the brake. I apply this idea into TORCS with a stochastic brake : During the exploration phase, I hit the brake 10% of the time (feel the brake) while 90% of time I don\u2019t hit the brake. Since I only hit the brake 10% of the time the car can get some velocity therefore it won\u2019t stuck in a poor local minimum (car not moving) while at the same time it can learn how to brake. The third video shows that the \u201Cstochastic brake\u201D allows AI agent accelerate very fast in a straight line and brake properly before the turn. I like this driving action as it is much more closer to human behaviour. (Update on 25-Oct-2016) The fourth video shows that staying in the middle of the track is not a necessary requirement in the reward function. The AI agent can learn to find the apexes of turns. In this work, we manage to demonstrate using Keras and DDPG algorithm to play TORCS. Although the DDPG can learn a reasonable policy, I still think it is quite different from how humans learn to drive. For example, we used Ornstein-Uhlenbeck process to perform the exploration. However, when the number of actions increase, the number of combinations increase and it is not obvious to me how to do the exploration. Consider using DDPG algorithm to drive a Boeing 747: I am almost sure that you cannot fly the plane by turning the switch randomly :) However, having said that the algorithm is quite powerful because we have a model-free algorithm for continuous control, which is very important in robotics. 1) To try different tracks, you need to type sudo torcs \u2013> Race \u2013> Practice \u2013> Configure Race 2) To turn off the engine sound during training you can type sudo torcs \u2013> Options \u2013> Sound \u2013> Disable sound 3) Installation of the TORCS requires openCV. I have some hard time to install correctly as it crashed my NVIDIA Titan-X driver. I strongly suggest you download a copy of the NVIDIA driver in the local drive first. You can restore your video driver by installing the video card driver from the text mode in case if your video driver is crashed 4) The following blog may be useful to you Installing OpenCV 3.0.0 on Ubuntu 14.04 5) This forum may be helpul if you experience Segmentation faults in TORCS.Torcs Segfaults on Launch 6) To test if your TORCS is installed correctly : 1) Open a terminal, type torcs \u2013> Race \u2013> Practice \u2013> New Race \u2013> Then you should see a blue screen said \u201CInitializing Driver scr_server1\u201D. Then 2) Open another terminal, type python snakeoil3_gym.py, you should see the car demo immediately. 7) snakeoil3_gym.py is the python script to communicate with TORCS server. I thank to Naoto Yoshida, the author of the gym_torcs and his prompt reply on various TORCS setup issue. I also thank to @karpathy his great post Deep Reinforcement Learning: Pong from Pixels which really helps me to understand policy gradient. I thank to @hardmaru and @flyyufelix for their comments and suggestions.","flair":"four\tProject"}
{"author":"syllogism_","created":"Mon Oct 31 22:26:44 EDT 2016","text":"This directory contains an implementation of the entailment prediction model described by Parikh et al. (2016). The model is notable for its competitive performance with very few parameters. The model is implemented using Keras and spaCy. Keras is used to build and train the network. spaCy is used to load the GloVe vectors, perform the feature extraction, and help you apply the model at run-time. The following demo code shows how the entailment model can be used at runtime, once the hook is installed to customise the method of spaCy's and objects: I'm working on a blog post to explain Parikh et al.'s model in more detail. I think it is a very interesting example of the attention mechanism, which I didn't understand very well before working through this paper. There are lots of ways to extend the model. First, install Keras, spaCy and the spaCy English models (about 1GB of data): You'll also want to get keras working on your GPU. This will depend on your set up, so you're mostly on your own for this step. If you're using AWS, try the NVidia AMI. It made things pretty easy. Once you've installed the dependencies, you can run a small preliminary test of the Keras model: This compiles the model and fits it with some dummy data. You should see that both tests passed. You can run the directory as a script, which executes the file keras_parikh_entailment\/__main__.py. The first thing you'll want to do is train the model: Training takes about 300 epochs for full accuracy, and I haven't rerun the full experiment since refactoring things to publish this example \u2014 please let me know if I've broken something. You should get to at least 85% on the development data. The other two modes demonstrate run-time usage. I never like relying on the accuracy printed by methods. I never really feel confident until I've run a new process that loads the model and starts making predictions, without access to the gold labels. I've therefore included an mode. Finally, there's also a little demo, which mostly exists to show you how run-time usage will eventually look. We should have the blog post explaining the model ready before the end of the week. To get notified when it's published, you can either the follow me on Twitter, or subscribe to our mailing list.","flair":"four\tProject"}
{"author":"EvanVanNess","created":"Fri Sep 30 01:28:59 EDT 2016","text":"In this post, I will talk about our recent paper called [1609.09106] HyperNetworks. I worked on this paper as a Google Brain Resident - a great research program where we can work on machine learning research for a whole year, with a salary and benefits! The Brain team is now accepting applications for the 2017 program: see g.co\/brainresidency. A Dynamic Hypernetwork Generating Handwriting. The weight matrices of the LSTM are changing over time. Most modern neural network architectures are either a deep ConvNet, or a long RNN, or some combination of the two. These two architectures seem to be at opposite ends of a spectrum. Recurrent Networks can be viewed as a really deep feed forward network with the identical weights at each layer (this is called weight-tying). A deep ConvNet allows each layer to be different. But perhaps the two are related somehow. Every year, the winning ImageNet models get deeper and deeper. Think about a deep 110-layer, or even 1001-layer Residual Network architectures we keep hearing about. Do all 110 layers have to be unique? Are most layers even useful? People have already thought of forcing a deep ConvNet to be like an RNN, i.e. with identical weights at every layer. However, if we force a deep ResNet to have its weight tied, the performance would be embarrassing. In our paper, we use HyperNetworks to explore a middle ground - to enforce a relaxed version of weight-tying. A HyperNetwork is just a small network that generates the weights of a much larger network, like the weights of a deep ResNet, effectively parameterizing the weights of each layer of the ResNet. We can use hypernetwork to explore the tradeoff between the model\u2019s expressivity versus how much we tie the weights of a deep ConvNet. It is kind of like applying compression to an image, and being able to adjust how much compression we want to use, except here, the images are the weights of a deep ConvNet. While neural net compression is a nice topic, I am more interested to do things that are a bit more against the grain, as you know from reading my previous blog posts. There are many algorithms that already take a fully trained network, and then apply compression methods to the weights of the pre-trained network so that it can be stored with fewer bits. While these approaches are useful, I find it much more interesting to start from a small number of parameters, and learn to construct larger and complex representations from them. Many beautiful complex patterns in the natural world can be constructed using a small set of simple rules. Digital artists have also designed beautiful generative works based on this concept. It is this type of complex-abstraction concept that I want to explore in my machine learning research. In my view, neural network decompression is a more interesting problem than compression. In particular, I also want to explore decompressing the weights of an already compressed neural net, i.e., the weights a recurrent network. The more exciting work is in the second part of my paper where we apply Hypernetworks to Recurrent Networks. The weights of an RNN are tied at each time-step, limiting its expressivity. What if we can have a way to allow the weights of an RNN to be different at each time step (like a deep convnet), and also for each individual input sequence? The main result in the paper is to challenge the weight-sharing paradigm of Recurrent Nets. We do this by embedding a small Hypernetwork inside a large LSTM, so that the weights used by the main LSTM can be modified accordingly by the Hypernetwork, whenever it feels like modifying the weights. In this instance, the Hypernetwork is also another LSTM, just a smaller version, and we will give it the power to modify the weights of the main LSTM, at each timestep, and also for each input sequence. In the process, we achieve state-of-the-art results for character level language modelling tasks for Penn Treebank and Wikipedia datasets. More importantly, we explore what happens when our models are able to generate generative models. The resulting HyperLSTM model looks and feels like a normal generic TensorFlow RNN cell. Just like how some people among us have super-human powers, in the HyperLSTM model, if the main LSTM is a human brain, then the HyperLSTM is some weird intelligent creature controlling the brain from within. The concept of using a neural network to generate the weights of a much larger neural network originated from Neuroevolution. While genetic algorithms are easy and fun to use, it is difficult to get them to directly find solutions for a really large set of model parameters. Ken Stanley came up with a brilliant method called HyperNEAT to address this problem. He came up with this method while trying to use an algorithm called NEAT to create beautiful neural network generated art. NEAT is an evolved neural network that can be used to generate art by giving it the locations of each pixel and reading from its output the colour of that pixel. What if we get NEAT to paint the weights of a weight matrix instead? HyperNEAT attempts to do this. It uses a small NEAT network to generate all the weight parameters of a large neural network. The small NEAT network usually consists of less than a few thousand parameters, and its architecture evolved to produce the weights of a large network, given a set of virtual coordinate information about each weight connection. While the concept of parameterizing a large set of weights into a small number of parameters is indeed very useful in Neuroevolution, some other researchers thought that HyperNEAT can be a bit of an overkill. Writing and debugging NEAT can be a lot of work, and selective laziness can go a long way in research. Schmidhuber\u2019s group decided to try an alternative method, and just use Discrete Cosine Transform to compress a large weight matrix so that it can be approximated by a small set of coefficients (this is how JPEG compression works). They then use genetic algorithms to solve for the best set of coefficients so that the weights of a recurrent network is good enough to drive a virtual car around the tracks inside the TORCS simulation. They basically performed JPEG compression on the weight matrix of a self-driving car. Some people, including my colleagues at Deepmind have also played around with the idea of using HyperNEAT to evolve a small weight-generating network, but use back propagation instead of genetic algorithms to solve for the weights for the small network. They summarized some cool results in their paper about DPPNs. Personally, I\u2019m more interested to explore another aspect of neural network weight generation. I like to view a neural net as a powerful type of computing device, and the weights of the neural net are kind of like the machine-level instructions for this computer. The larger the number of neurons, the more expressive the neural net becomes. But unlike machine instructions of a normal computer, neural network weights are also robust, so if we add noise to some part of the neural network, it can still function somewhat. For this reason I find the concept of expressing a large set of weights with a small number of parameters fascinating. This form of weight-abstraction is sort of like coming up with a beautiful abstract higher level language (like LISP or Python) that gets compiled into raw machine-level code (the neural network weights). Or from a biological viewpoint, it is also like how large complex biological structures can be expressed at the genotype level. In the paper, I decided to explore the concept of having a network to generate weights for a larger network, and try to develop this concept a bit further. However, I took a slightly different approach from HyperNEAT and DPPNs. As mentioned above, these methods take a set of virtual input coordinates as input to a small network to generate the weights of a large network. I played around with this concept extensively (See Appendix Section A of the paper), but it just didn\u2019t work well for modern architectures like deep ConvNets and LSTMs. While HyperNEAT and DCT can produce good looking weight matrices, due to the prior enforced by smooth functions such as sinusoids, this artistic smoothness is also its limitation for many practical applications. A good looking weight matrix is not useful if it doesn\u2019t work. Look at this picture of the weights of a typical Deep Residual Network: Figure: Images of a 16x16x3x3, and 32x32x3x3 Weight Kernels in typical Residual Network trained on CIFAR-10. While I won\u2019t want to hang pictures of ResNet weights in my living room wall, they work really well, and I want to generate pictures of these weights with less parameters. I took an approach that is simpler and more in the fashion of VAE or GAN-type approaches. More modern generative models like GANs and VAEs take in a smallish embedding vector Z, of say 64 numbers, and from these 64 values, try to generate realistic images of cats or other cool things. Why not also try to generate weight matrices for a ResNet? So the approach we take is also to train a simple 2-layer network to generate the 16x16x3x3 weight kernels with an embedding vector of 64 numbers. The larger weight kernels will just be constructed by tiling small versions together (ie, the one on the right will require 256 numbers to generate). We will use the same 2-layer network to generate each and every kernel of a deep ResNet. When we train the ResNet to do image classification, rather than training the ResNet weights directly, we will be training the set of Z\u2019s and the parameters of this 2-layer network instead. Figure: Images of generated 16x16x3x3, and 32x32x3x3 Weight Kernels for a ResNet trained on the same task. We experimented with a typical off-the-shelf ResNet (the \u201CWRN-40-2\u201D configuration from this nice ResNet variation called Wide Residual Networks), of which 36 layers are these types of weight kernels. The best test classification accuracy on CIFAR-10 at the time of writing is ~ 96% using tens of millions of parameters. This particular ResNet uses only ~ 2.2 million parameters and can be trained to get ~ 94% accuracy on CIFAR-10, which I think is quite good. Our version of this ResNet that uses hypernet-generated weights used merely ~ 150k parameters, while the accuracy is still respectable. Our model got ~ 93% test accuracy. Figure: Structure of the Wide ResNet Family. We used N=6 and k=2. These results makes me think about these super deep 1001 layer ResNets that perform really well on ImageNet contests. Perhaps most of their weights are not that useful, but actually having the weights there, as a kind of placeholder, so that a large number of neurons can compute is the useful bit of why they are so good. If you would like to see an implementation, please see this IPython Notebook prepared to demonstrate this weight-generation concept outlined in the MNIST experiment in the paper. The notebook can be extended and combined with the Residual Network model in TensorFlow. As mentioned in the Introduction, we also tried to apply Hypernetworks on Recurrent Networks, and I feel this is the main contribution of the research. One of the insights from working with hypernetworks on ResNets is that while we get to use much fewer parameters in the model, we see a reduction of accuracy as a tradeoff. So what if we go the other way instead? If we can use a hypernetwork to allow us to relax the weight-sharing constraints of an RNN, and allow the weight matrix to change at each unrolled timestep, it would look closer like a deep ConvNet, so maybe we can squeeze better results from them. Figure: The HyperRNN system. The black system represents the main RNN while the orange system represents the weight-generating HyperRNN cell. Our approach is to put a small LSTM cell (called the HyperLSTM cell) inside a large LSTM cell (the main LSTM). The HyperLSTM cell will have its own hidden units and its own input sequence. The input sequence for the HyperLSTM cell will be constructed from 2 sources: the previous hidden states of the main LSTM concatenated with the actual input sequence of the main LSTM. The outputs of the HyperLSTM cell will be the embedding vector Z that will then be used to generate the weight matrix for the main LSTM. Unlike the Static Hypernetwork, the weight-generating embedding vectors are not kept constant, but will be dynamically generated by the HyperLSTM cell. This allows our model to generate a new set of weights at each time step and for each input example. In the paper I discuss many practicalities and more computationally and memory efficient approach of generating the weights from the embedding vector to simplify and reduce computation constraints of this approach. One thing that I learned is that while it is important to dream up new types of algorithms and new approaches in research, at the end of the day it is important to keep things practical and make stuff work. It is also important to make it easy for other people to use your work. For our implementation of Dynamic Hypernetworks, we made it so that we can just plug our HyperLSTM cell into any TensorFlow code written to use objects, since the HyperLSTM inherited from this abstract class. This makes it easy to plug my research code to existing code that was designed to use the vanilla LSTM cell. For example, when I was experimenting with our HyperLSTM cell on the wikipedia dataset, I just used char-rnn-tensorflow and plugged the research model right in for training and inference. Here is a passage that generated with our HyperLSTM model after training on the wikipedia dataset: Figure: Generated text, along with levels of weight-changing activity of the main LSTM\u2019s weight matrices. Somehow HyperLSTM learned to put Soviet, Facism, a computer company, and an all-important type of machine in one sentence. In the above figure, in addition to just displaying the generated text, we can also visualise how the weights of the main LSTM are being modified by the HyperLSTM cell. I chose to visualise the changes of the four hidden-to-gate weight matrices of the LSTM over time in four different colours, to represent each of the four input, candidate, forget and output gates of the LSTM (see this blog post for a great explanation). We can interpret high intensity regions as instances where the HyperLSTM cell just made large changes to the weights of the main LSTM, before the main LSTM is used to generate each character. A low intensity means the HyperLSTM cell is taking a break, so the weights of the main LSTM are not being changed that much during these breaks. Below is another example passage: An interesting thing to note is during the less active periods of the HyperLSTM cell, the types of words seem to be more predictable. For example, from the first example, was generated by more or less a static network after . In the second example, was generated by a relatively constant main LSTM, but right after the 1980s, the HyperLSTM cell suddenly woke up and decided to give the main LSTM model a bit of a shake, before it went to discuss savage employment concerns. In a way, the HyperLSTM cell is generating the generative model as the generative model is generating the sequence. This meta-ability to dynamically generate the generative model seems to be very powerful, and in fact our HyperLSTM model was able to beat previous state-of-the-art character-level prediction dataset benchmarks such as Character-Level Penn Treebank and Hutter Prize Wikipedia (). Our model got 1.25 bpc and 1.38 bpc respectively (as of 27-Sep-2016) without using dynamic evaluation, beating previous records of 1.27 and 1.40 (as of 10-Sep-2016). Someone else will probably beat these state-of-the-art numbers in a few weeks given the fast pace of the machine learning research field, and also the fact that ICLR 2017 deadlines are just around the corner. In fact, I don\u2019t really think beating the state-of-the-art on some text dataset is as important as exploring the concept of this multi-level dynamic model within a dynamic model abstraction. I think in the future, people might focus less on architecture design, and the focus will move towards two directions, either towards the application side, or more towards the fundamental building block side. The thing I like about our approach is we effectively created a building block called the HyperLSTM, which from the TensorFlow user\u2019s point of view, looks and feels like exactly like a normal LSTM cell. It is just as easy to plug-and-play the HyperLSTM into existing TensorFlow code, as changing between RNN, GRU, and LSTM cells, since we made HyperLSTM to be just an instance of, called (which contains the full system, not to be confused with the HyperLSTM Cell). I also experimented HyperLSTM to perform the handwriting generation task. In a previous post, I explored Alex Graves\u2019 approach of getting an LSTM to generate a random handwriting sequence. The way this generation works is to model the Figure: Handwriting sampled from a 2D mixture Gaussian distribution, and the Bernoulli distribution, using the vanilla LSTM model. Both the Gaussian and Bernoulli probability distributions change over time. During handwriting, the parameters of these two distributions will change over time, and also depend on each other. For example, as you are finish writing a word, the probability that your pen leave the paper increases, and the next location of the pen will likely be further away from where it is now and have a much higher variance. We can get an LSTM to output the parameters for both the mixture Gaussian and the Bernoulli distribution, and have the values of these parameters change at each timestep depending on the LSTM\u2019s internal states. You can visualize how the Gaussian distribution changes over time by looking at the red bubbles in the above figure, which indicate the location and size of the Gaussian distributions for next pen stroke. We can sample from this time-varying distribution and hence sample an entire fake handwriting passage by connecting the sampled points. I view this type of model to be similar to Dynamic Hypernetworks, since an LSTM is dynamically changing the parameters of a generative distribution (the Gaussian and the Bernoulli) over time, and by training the entire model on a handwriting dataset, it can perform a formidable job of generating handwriting samples. In our paper, we applied Dynamic Hypernetworks to extend this approach. We will replace in my code with . In this approach, the weight matrices and biases of the LSTM will be modified over time by a smaller LSTM. By doing this simple change, we extend this model-generating-model approach by another level, by having a small LSTM dynamically generate a larger LSTM model at each time step, and for the generated large LSTM model to generate parameters for the Gaussian and Bernoulli distributions also at each time step. So model-generating-model becomes model-generating-model-generating-model. Similar to text-generation earlier, this newer approach achieves much better scores compared to the normal, and even multi-layer LSTM\u2019s, with a similar number of training parameters. I modified write-rnn-tensorflow to replicate the exact experiment done in section 4.2 of Graves\u2019 paper, and checked that the Log Loss results when using is similar enough to the previously published results. After tying out with the previous historical results, we can switch to and rerun the experiments. But before doing that, we tried to improve our baseline method first. It is important show the baseline technique some respect, and give face to them, since they led way to our research. We found that applying black magic techniques like data-augmentation and dropout, we can already improve the scores for the baseline 1-Layer from -1026 nats to -1055 nats. After switching on our model, the got the Log Loss score all the way down to -1162 nats, a large and meaningful improvement. Along with the quantitative results are various actual generated samples from various models you can check out in the paper (in the Appendix section). To wrap up this post, I made a small demo showing how the handwriting generation process works with HyperLSTMs. I want to show how the weights of the main LSTM is being modified by the HyperLSTM cell with this demo. Unlike character-generation, the time-axis doesn\u2019t really correspond exactly to the x-axis for handwriting, so I found it easier to visualise this process as a web-demo, since you can\u2019t really do animations for .pdf papers submitted to arXiv. In the future, more science will move more towards web-posts rather than static .pdf files for journals and conferences. My colleagues @ch402 and @shancarter also recently created a platform called distill.pub to encourage the more modern web-based publication format. I will start using this platform in the future. You can see the handwriting being generated as well as changes being made to the LSTM\u2019s 4 hidden-to-gate weight matrices. I chose to only visualize the changes made to I learned a lot from reading open-source code and tutorials, and for practical Recurrent Networks in TensorFlow I highly recommend Denny Britz\u2019s blog, the mysterious r2rt super-blog, this blog post on Recurrent Batch Norm, and also TensorFlow With The Latest Papers Implemented. This local implementation of is based on the Layer Norm implementation by LeavesBreathe and Batch Norm code by OlavHN. You can also try to plug into char-rnn-tensorflow, or use in other interesting tasks. It works, but currently is not as fast as vanilla LSTM, but over time I expect to see many improvements in core TensorFlow that allow for more speedy optimisations. I tried to redo the Character Penn Treebank experiment on a local desktop equipped with a Titan X GPU with a clean open source development stack. I used the following setup: a hidden unit size of 1000 for the main LSTM, batch sizes of 128, sequence length of 100, learning rate of 0.001, and dropout keep probabilities of 0.90 for training the model on the training set. The hypernetwork has 128 units and generate embedding vectors of size 4 (these two parameters are optional parameters for ). I trained the model for two days and recorded results for the validation set at intervals of 500 steps. For evaluation on the validation set, I used batch sizes of 20 and sequence lengths of 2000. The best model on the validation set occurred after training for 37k steps (after a day of training) and this achieved a validation score of 1.282 bpc. Using this model on the test set, on a single batch of the full sequence length of 449945 (no mini-batches, so this takes a while in CPU mode), the result on the test set was 1.249256 bpc. The model only sees the test set once, unlike methods that use dynamic evaluation, where some people decide to train their models on the test set, and report results on the test set (note: this is not cool). I tested on TensorFlow 0.90. Later on, I\u2019ll try to release the patches I made for that does the train\/validation\/test split properly once I clean it up a bit. This should make it easier for others to conduct research on character level language models in TensorFlow, and to beat our results in the future. The model generates text that look like the below. Can you tell the difference between the generated text and the actual dataset? Maybe we can still tell the difference. Updated versions will be at https:\/\/github.com\/hardmaru\/supercell\/ \"\"\" # HyperLSTM # 27-Sep-2016 # https:\/\/arxiv.org\/abs\/1609.09106 # # latest at https:\/\/github.com\/hardmaru\/supercell # # derived with help from # https:\/\/github.com\/OlavHN\/bnlstm # https:\/\/github.com\/LeavesBreathe\/tensorflow_with_latest_papers \"\"\" \"\"\" Vanilla LSTM with ortho initializer, and also recurrent dropout without memory loss (https:\/\/arxiv.org\/abs\/1603.05118) derived from https:\/\/github.com\/OlavHN\/bnlstm https:\/\/github.com\/LeavesBreathe\/tensorflow_with_latest_papers \"\"\" # Keep W_xh and W_hh separate here as well to use different initialization methods #output for mean and variance should be [batch_size] \"\"\" Layer-Norm, with Ortho Initialization and Recurrent Dropout without Memory Loss. https:\/\/arxiv.org\/abs\/1607.06450 - Layer Norm https:\/\/arxiv.org\/abs\/1603.05118 - Recurrent Dropout without Memory Loss derived from https:\/\/github.com\/OlavHN\/bnlstm https:\/\/github.com\/LeavesBreathe\/tensorflow_with_latest_papers \"\"\" \"\"\"Initialize the Layer Norm LSTM cell. Args: num_units: int, The number of units in the LSTM cell. forget_bias: float, The bias added to forget gates (default 1.0). use_recurrent_dropout: float, Whether to use Recurrent Dropout (default False) dropout_keep_prob: float, dropout keep probability (default 0.90) \"\"\" # no bias, since there's a bias thing inside layer norm \"\"\" HyperLSTM, with Ortho Initialization, Layer Norm and Recurrent Dropout without Memory Loss. https:\/\/arxiv.org\/abs\/1609.09106 http:\/\/blog.otoro.net\/2016\/09\/28\/hyper-networks\/ \"\"\" \"\"\"Initialize the Layer Norm HyperLSTM cell. Args: num_units: int, The number of units in the LSTM cell. input_size: int, The size of the input. batch_size: int, Batch size of model. forget_bias: float, The bias added to forget gates (default 1.0). use_recurrent_dropout: float, Whether to use Recurrent Dropout (default False) dropout_keep_prob: float, dropout keep probability (default 0.90) use_layer_norm: boolean. (default True) Controls whether we use LayerNorm layers in main LSTM and HyperLSTM cell. hyper_num_units: int, number of units in HyperLSTM cell. (default is 128, recommend experimenting with 256 for larger tasks) hyper_embedding_size: int, size of signals emitted from HyperLSTM cell. (default is 4, recommend trying larger values but larger is not always better) hyper_use_recurrent_dropout: boolean. (default False) Controls whether HyperLSTM cell also uses recurrent dropout. (Not in Paper.) Recommend turning this on only if hyper_num_units becomes very large (>= 512) \"\"\" # concatenate the input and hidden states for hyperlstm input # bias is to be broadcasted.","flair":"null\tnull"}
{"author":"pmfcdb","created":"Fri Nov 04 18:29:21 EDT 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose CNTK Image Detection FastRCNN Pascal VOC 2007 Dataset Paulo Brito SubscribeSubscribedUnsubscribe88 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Statistics 271 views 1 Like this video? Sign in to make your opinion count. Sign in 2 1 Don't like this video? Sign in to make your opinion count. Sign in 2 Loading... Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 4, 2016 CNTK - Deep learning framework from Microsoft used in Image Detection - Pascal VOC 2007 Dataset - FastRCNN algorithm 04:10 Visualization Input ROIs 06:58 (mAP) at around 0.4565 07:13 Visualization Output ROIs Category Science & Technology License Standard YouTube License Show more Show less Loading... Advertisement Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Code Swarm for CNTK - Duration: 3:21. Landon Wilkins 108 views 3:21 CNTK Image Detection FastRCNN Grocery Dataset - Duration: 11:54. Paulo Brito 265 views 11:54 Microsoft Machine Learning & Data Science Summit 2016 CNTK Microsoftâ\u20AC™s Open Source Deep Learning Too - Duration: 51:51. 25msr 623 views 51:51 Evolution of CNTK (Gource Visualization) - Duration: 1:32. Landon Wilkins 223 views 1:32 Dan Does Data: Microsoft CNTK new Deep Learning library! - Duration: 1:03:42. Dan Van Boxel 3,632 views 1:03:42 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 1) - Duration: 54:25. KDD2016 video 330 views 54:25 CNTK: Microsoft's Open-Source Deep-Learning Toolkit - Duration: 53:43. Microsoft Research 5,273 views 53:43 Unlock deeper learning with the new Microsoft Cognitive Toolkit - Duration: 1:46. Microsoft Research 24,130 views 1:46 Share Your Science: Deep Learning at Microsoft Research - Duration: 2:13. NVIDIADeveloper 1,556 views 2:13 Facial expression ASM - Duration: 0:41. Paulo Brito 455 views 0:41 Deep learning Image Classification on Pascal Voc Dataset - Duration: 1:15. Mustafa ihssan 1,821 views 1:15 Tutorial: Deep Learning - Duration: 2:29:35. Microsoft Research 6,744 views 2:29:35 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 2) - Duration: 1:09:13. KDD2016 video 72 views 1:09:13 Noqui movie - Duration: 11:11. Paulo Brito 39 views 11:11 #DebugNews - CNTK - Duration: 6:26. Debugasse 84 views 6:26 Noki development update 04 - Duration: 3:24. Paulo Brito 29 views 3:24 Google Deepmind DQN plays Atari Breakout | NickysChannel13 - Duration: 2:51. NickysChannel13 107 views 2:51 Forklift Simulation Training - Duration: 1:03. Paulo Brito 133 views 1:03 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 3) - Duration: 44:57. KDD2016 video 35 views 44:57 Project TOK - BLE communication : debug mode - Duration: 1:11. Paulo Brito 25 views 1:11 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"two\tNews"}
{"author":"richardweiss","created":"Wed Oct 05 05:51:36 EDT 2016","text":"One of the things I like about deep neural networks and all the accompanying tools, are their flexibility. There\u2019s so much more to them than just classification! All it takes is a set of mental shortcuts, that change the way you look at a neural network, and suddenly you start to see many new ways to approach deep learning problems. The shortcut I\u2019ll talk about in this post is embeddings. Embeddings let you take discrete items (like movies or books or words or whatever), and transform them into vectors. These vectors are generally more meaningful than the items themselves. We\u2019ll be creating embeddings for movies and users, and using them to predict the rating a user would have given to a particular movie. I\u2019ll also try to include some of the things that didn\u2019t work, and some of the things I did to debug the system. Those tricks are perhaps more useful than the final technique, and they don\u2019t often come up in finished papers. For these experiments I\u2019m using the movielens 1M dataset. The dataset is basically people\u2019s ratings of movies they\u2019ve watched, from one to five stars. It also includes a few other bits of information that I\u2019m not using (yet), such as genre of movie or viewer profession. As far as I know, the most common way to approach these sorts of problems are as matrix decomposition factorization problems. In matrix decomposition problems, you start with the fat user-movie matrix. Every user has a cell for every movie, and where they\u2019ve rated a film, that cell will have their rating, otherwise it\u2019ll be zero. You take this fat user-movie matrix, and you decompose it into two skinny matrices: one for users and one for movies. The user one will have an entry for every user, and the movie one will have an entry for every movie. Each entry, in either matrix, will have a certain, pre-specified length, called the inner dimension. There will most likely be some conditions on the two matrices. In Nonnegative matrix factorization, for example, you optimize the squared error between the inner product of the skinny matrices, and the fat matrix. You do so, while requiring that the two decomposed matrices may only have positive or zero entries. The approach I took blends deep learning and matrix decomposition methods. To my mind, the disadvantage of matrix decompositions are: Instead of doing a matrix decomposition, I\u2019ll be learning an embedding for both users and movies, as well as a deep-net that combines them into an answer. It\u2019s still a classification problem (classifying how many stars a movie got), but there\u2019s a whole lot of interesting stuff in the embedding. My favorite thing about deep learning is that it\u2019s flexibility means there are so may ways to think about a problem. The three mental frameworks I generally use are: Anyway, this post deals entirely with frameworks one and two. The reward structure isn\u2019t too hard, there are a couple of options: In the end I tried both, and the second worked a little bit better. The network structure is a little more interesting. I used keras\u2019 embedding layer to produce vectors for both movies and users, and concatenated them together and fed them into a standard neural network. In no particular order: So my number one tip here: build a test version of your problem that is so simple that your network will be able to solve it. Trust me, it\u2019ll help. In the end, I used the following code to build my network: As you can see, I take and . Both are single numbers, representing the user and movie ID. These are both turned into vectors with the keras layer. Both vectors have 32 entries. Next, I smush them both together with a merge (notice that it\u2019s with a small m). Then I put them through a simple dense neural network, with dropout and batch normalization. Finally I take the result with softmax, and compile it all. You also check out the full notebook It does pretty well! I used mean absolute error as my metric, so I could easily compare it to these stats. Their best result is 0.668 MAE, and after ten rounds of training and a 25% holdout set, I managed to reach 0.669, a little bit above their best effort. Of course, MyMediaLite has one critical advantage: speed. Their system is built to allow for very fast lookups, whereas my one needs to run a complex neural network for every single movie. That\u2019d be fine for this application - there aren\u2019t too many movies, but take youtube for example: they recently explained their recommender, and mention having millions of videos and a billion users. Too big for a simple tool like mine. I learned a lot from building this, hopefully you did as well. The main takeaways are: I\u2019ll experiment a little more, and try to re-visit some of the issues and improvements, to see what improvements I can make, and what more I can learn from the problem.","flair":"null\tnull"}
{"author":"tuan3w","created":"Tue Oct 18 10:21:11 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.05256 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1610 Change to browse by: cs References & Citations NASA ADS 1 blog link (what is this?) Bookmark (what is this?) Computer Science > Computation and Language Title: Achieving Human Parity in Conversational Speech Recognition Authors: W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig (Submitted on 17 Oct 2016) Abstract: Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training. Subjects: Computation and Language (cs.CL) Report number: MSR-TR-2016-71 Cite as: arXiv:1610.05256 [cs.CL]   (or arXiv:1610.05256v1 [cs.CL] for this version) Submission history From: Geoffrey Zweig [view email] [v1] Mon, 17 Oct 2016 18:40:50 GMT (85kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"thecity2","created":"Tue Oct 04 19:05:56 EDT 2016","text":"The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. \u201CBig data,\u201D \u201Cdata science,\u201D and \u201Cmachine learning\u201D have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on a journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories \u2013 Bayesian, frequentist, Fisherian \u2013 individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The book integrates methodology and algorithms with statistical inference, and ends with speculation on the future direction of statistics and data science.","flair":"null\tnull"}
{"author":"jjrob13","created":"Thu Oct 27 00:23:54 EDT 2016","text":"I am doing some work in the music information retrieval domain and was wondering if the weights for any of the speech to text networks were available or if any other trained networks for musical feature extraction were open sourced.","flair":"one\tDiscussion"}
{"author":"dhammack","created":"Tue Oct 25 17:16:00 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1609.04309 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1609 Change to browse by: cs cs.LG References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Edouard Grave Armand Joulin Moustapha Cissé David Grangier Hervé Jégou Bookmark (what is this?) Computer Science > Computation and Language Title: Efficient softmax approximation for GPUs Authors: Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou (Submitted on 14 Sep 2016) Abstract: We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. Subjects: Computation and Language (cs.CL); Learning (cs.LG) Cite as: arXiv:1609.04309 [cs.CL]   (or arXiv:1609.04309v1 [cs.CL] for this version) Submission history From: Edouard Grave [view email] [v1] Wed, 14 Sep 2016 15:15:08 GMT (116kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"sour_losers","created":"Sat Nov 19 03:32:05 EST 2016","text":" Elad Blog Friday, November 18, 2016 Facebook Must Really Suck At Machine Learning Facebook recently claimed it is hard[1] to differentiate between fake news[2] and real news. Given how similar fake news detection is to related problems such as search index spam, ads landing page spam, social networking bots, and porn detection this suggests one of two things: (1) Facebook really sucks at machine learning or (2) Facebook does not want to address the problem. Lets look at each of these: 1. Facebook Sucks At Machine Learning? Over the course of my career I worked on, amongst other things, Google mobile products (including mobile search index and looking at items like porting Google News to mobile), Google ads targeting to pages across the web, and Twitter Search (I was Director of Search Product for a time). At both Google and Twitter, the companies had to deal with large number of ambiguous signals including: Ambiguous content on web pages. Google classifies these pages for search results, but also to determine the right set of ads to target to the pages. This included semantic analysis of the pages as well as a look at page \"quality\" scores. Fake landing pages for ads. Google needs to make sure ads and the web pages the ads pointed too were legitimate. Spam tweets. There were (and are) a lot of bots and spam tweets on Twitter. The service was continuously removing poorly ranked tweets from search results.  In all cases, the important thing to do was to understand the content of a tweet, web page, or other content unit, and then to rank the relative quality and importance of that content. Similar problems also exist in areas like Google web index spam and porn detection. In all cases, there are a lot of shades of grey - i.e. there is a fine line between porn and not-porn, or a spammy tweet and a silly or satiric tweet. Facebook has developed a number of technologies to rank its news feed, to target ads, and to classify its users. However, the claim from Facebook has been that fake news is a complex area, and this complexity makes it difficult to address. Intriguingly, a group of undergrads at Princeton were able to build a quick and dirty fake news classifier during a 36 hour hackathon. It is possible these Princeton students a set of once-in-a-generation geniuses. Or, perhaps, fake news is actually tractable as a problem using existing techniques Facebook already has in house. 2. Facebook Does Not Want To Address The Problem? Facebook's CEO recently posted that 99% of news post on Facebook are not fake (see below for chart of Facebook user engagement with fake versus non-fake news). Facebook has also been under fire by the right for its \"liberal bias\". This prompted Facebook to hold a meeting with leading conservative members of the GOP to discuss its newsfeed and how to play a non-partisan role in content. Fake news is not a partisan issue. It is about ensuring that people are helped to understand what is real and what are lies. A lack of willingness to tackle the issue of fake news is a willingness to accept a lack of truth in our society at mass scale. Other Companies You Can Work At Instead Of Facebook Great engineers want to work with other great engineers. If Facebook lacks the talent to address the fake news problem, do you really want to join an organization so poor at machine learning? Alternatively, if Facebook simply lacks the will to address this issue, it might be something worth taking into account as well. A number of talented engineers are also immigrants - a group much maligned in fake news posts. If you are a talented machine learning or AI engineer, there are a number of companies you can work at instead of Facebook. Some potential ideas: Google. Lots of amazing Machine learning problems across search, ads, AGI, virtual assistants, etc. Uber. Work on intelligent routing and optimization, self driving cars, and other technologies. Microsoft. Microsoft is working on applying machine learning to health care problems like cancer. Tesla. Self-driving cars. Wish. Large scale commerce platform powered by data analytics and ML. Stripe. Payment fraud and other areas that power our global payments infrastructure. Netflix. Media recommendations. Apple. While less known for machine learning, Apple has been applying it to areas around privacy as well as apps like Siri. Amazon. Amazon has been doing cool things in voice recognition technology with Alexa\/Echo. In addition, I would not be surprised if AWS extended its efforts around GPU clusters as well. Dozens of AI startups. There are lots of Deep Learning, AI, and ML companies that have been funded recently. There are lots of cool things for you to work on instead. If you work on machine learning or data science and want to work somewhere other then Facebook - feel free to drop me a line. I am happy to refer you to a few dozen companies as alternatives. Notes [1] Exact quote from Zuckerberg is: \"This is an area where I believe we must proceed very carefully though. Identifying the \"truth\" is complicated. While some hoaxes can be completely debunked, a greater amount of content, including from mainstream sources, often gets the basic idea right but some details wrong or omitted. An even greater volume of stories express an opinion that many will disagree with and flag as incorrect even when factual. I am confident we can find ways for our community to tell us what content is most meaningful, but I believe we must be extremely cautious about becoming arbiters of truth ourselves.\" This \"grey area\" argument is made all the time. Yet machine learning classifiers work incredibly well for porn and other areas that have lots of grey. Similarly, getting rid of the 80% easy to spot, most egregious stuff is a good starting point. This argument strikes me as a red herring. [1] \"Fake news\" is a nice way to say lies and propaganda.  Posted by Elad Gil at 11:32 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Older Post Home Subscribe To Posts Atom Posts Comments Atom Comments Search This Blog About Me Elad Gil San Francisco, CA, United States I am a serial entrepreneur obsessed with technology and startups. My last startup Mixer Labs was acquired by Twitter. I am now working on something new. www.linkedin.com\/in\/eladgil View my complete profile Blog Archive ▼  2016 (6) ▼  November (1) Facebook Must Really Suck At Machine Learning ►  August (1) ►  July (2) ►  January (2) ►  2015 (5) ►  October (1) ►  May (1) ►  March (1) ►  January (2) ►  2014 (18) ►  December (3) ►  September (1) ►  August (2) ►  July (4) ►  February (3) ►  January (5) ►  2013 (9) ►  November (1) ►  June (2) ►  May (1) ►  April (1) ►  March (1) ►  February (2) ►  January (1) ►  2012 (17) ►  November (2) ►  October (4) ►  September (3) ►  August (1) ►  July (1) ►  April (2) ►  March (1) ►  February (2) ►  January (1) ►  2011 (29) ►  December (2) ►  November (3) ►  September (2) ►  August (3) ►  June (2) ►  May (2) ►  April (2) ►  March (5) ►  February (3) ►  January (5) ►  2010 (33) ►  December (3) ►  November (1) ►  October (6) ►  September (5) ►  August (3) ►  July (4) ►  June (2) ►  May (1) ►  March (2) ►  February (5) ►  January (1) ►  2009 (9) ►  December (1) ►  October (1) ►  August (1) ►  June (3) ►  March (1) ►  January (2) ►  2008 (3) ►  December (1) ►  October (1) ►  June (1) ►  2007 (3) ►  December (1) ►  November (1) ►  August (1) EG Picture Window template. Powered by Blogger. ","flair":"one\tDiscussion"}
{"author":"egrefen","created":"Wed Oct 12 13:59:19 EDT 2016","text":" Home Research Publications AlphaGo DQN Applied DeepMind Health DeepMind for Google News & Blog About Us Careers Research Highlighted Research AlphaGo DQN Publications Latest Research News Reinforcement learning with unsupervised auxiliary tasks Applied DeepMind Health Streams DeepMind for Google Latest Applied News Working with the NHS to build lifesaving technology Careers Home News & Blog About Us Press Terms and Conditions Privacy Policy Differentiable neural computers In a recent study in Nature, we introduce a form of memory-augmented neural network called a differentiable neural computer, and show that it can learn to use its memory to answer questions about complex, structured data, including artificially generated stories, family trees, and even a map of the London Underground. We also show that it can solve a block puzzle game using reinforcement learning. Plato likened memory to a wax tablet on which an impression, imposed on it once, would remain fixed. He expressed in metaphor the modern notion of plasticity \u2013 that our minds can be shaped and reshaped by experience. But the wax of our memories does not just form impressions, it also forms connections, from one memory to the next. Philosophers like John Locke believed that memories connected if they were formed nearby in time and space. Instead of wax, the most potent metaphor expressing this is Marcel Proust\u2019s madeleine cake; for Proust, one taste of the confection as an adult undammed a torrent of associations from his childhood. These episodic memories (event memories) are known to depend on the hippocampus in the human brain. Today, our metaphors for memory have been refined. We no longer think of memory as a wax tablet but as a reconstructive process, whereby experiences are reassembled from their constituent parts. And instead of a simple association between stimuli and behavioural responses, the relationship between memories and action is variable, conditioned on context and priorities. A simple article of memorised knowledge, for example a memory of the layout of the London Underground, can be used to answer the question, \u201CHow do you get from Piccadilly Circus to Moorgate?\u201D as well as the question, \u201CWhat is directly adjacent to Moorgate, going north on the Northern Line?\u201D. It all depends on the question; the contents of memory and their use can be separated. Another view holds that memories can be organised in order to perform computation. More like lego than wax, memories can be recombined depending on the problem at hand. Neural networks excel at pattern recognition and quick, reactive decision-making, but we are only just beginning to build neural networks that can think slowly \u2013 that is, deliberate or reason using knowledge. For example, how could a neural network store memories for facts like the connections in a transport network and then logically reason about its pieces of knowledge to answer questions? In a recent paper, we showed how neural networks and memory systems can be combined to make learning machines that can store knowledge quickly and reason about it flexibly. These models, which we call differentiable neural computers (DNCs), can learn from examples like neural networks, but they can also store complex data like computers. In a normal computer, the processor can read and write information from and to random access memory (RAM). RAM gives the processor much more space to organise the intermediate results of computations. Temporary placeholders for information are called variables and are stored in memory. In a computer, it is a trivial operation to form a variable that holds a numerical value. And it is also simple to make data structures \u2013 variables in memory that contain links that can be followed to get to other variables. One of the simplest data structures is a list \u2013 a sequence of variables that can be read item by item. For example, one could store a list of players\u2019 names on a sports team and then read each name one by one. A more complicated data structure is a tree. In a family tree for instance, links from children to parents can be followed to read out a line of ancestry. One of the most complex and general data structures is a graph, like the London Underground network. When we designed DNCs, we wanted machines that could learn to form and navigate complex data structures on their own. At the heart of a DNC is a neural network called a controller, which is analogous to the processor in a computer. A controller is responsible for taking input in, reading from and writing to memory, and producing output that can be interpreted as an answer. The memory is a set of locations that can each store a vector of information. A controller can perform several operations on memory. At every tick of a clock, it chooses whether to write to memory or not. If it chooses to write, it can choose to store information at a new, unused location or at a location that already contains information the controller is searching for. This allows the controller to update what is stored at a location. If all the locations in memory are used up, the controller can decide to free locations, much like how a computer can reallocate memory that is no longer needed. When the controller does write, it sends a vector of information to the chosen location in memory. Every time information is written, the locations are connected by links of association, which represent the order in which information was stored. As well as writing, the controller can read from multiple locations in memory. Memory can be searched based on the content of each location, or the associative temporal links can be followed forward and backward to recall information written in sequence or in reverse. The read out information can be used to produce answers to questions or actions to take in an environment. Together, these operations give DNCs the ability to make choices about how they allocate memory, store information in memory, and easily find it once there. Illustration of the DNC architecture. The neural network controller receives external inputs and, based on these, interacts with the memory using read and write operations known as 'heads'. To help the controller navigate the memory, DNC stores 'temporal links' to keep track of the order things were written in, and records the current 'usage' level of each memory location. To the non-technical reader, it may seem a bit odd that we have repeatedly used phrases like \u201Cthe controller can\u201D or \u201Cdifferentiable neural computers \u2026 make choices\u201D. We speak like this because differentiable neural computers learn how to use memory and how to produce answers completely from scratch. They learn to do so using the magic of optimisation: when a DNC produces an answer, we compare the answer to a desired correct answer. Over time, the controller learns to produce answers that are closer and closer to the correct answer. In the process, it figures out how to use its memory. We wanted to test DNCs on problems that involved constructing data structures and using those data structures to answer questions. Graph data structures are very important for representing data items that can be arbitrarily connected to form paths and cycles. In the paper, we showed that a DNC can learn on its own to write down a description of an arbitrary graph and answer questions about it. When we described the stations and lines of the London Underground, we could ask a DNC to answer questions like, \u201CStarting at Bond street, and taking the Central line in a direction one stop, the Circle line in a direction for four stops, and the Jubilee line in a direction for two stops, at what stop do you wind up?\u201D Or, the DNC could plan routes given questions like \u201CHow do you get from Moorgate to Piccadilly Circus?\u201D DNC was trained using randomly generated graphs (left). After training it was tested to see if it could navigate the London Underground (right). The (from, to, edge) triples used to define the graph for the network are shown below, along with examples of two kinds of task: 'traversal', where it is asked to start at a station and follow a sequence of lines; and 'shortest path' where it is asked to find the quickest route between two stations. In a family tree, we showed that it could answer questions that require complex deductions. For example, even though we only described parent, child, and sibling relationships to the network, we could ask it questions like \u201CWho is Freya\u2019s maternal great uncle?\u201D We also found it possible to analyse how DNCs used their memories by visualising which locations in memory were being read by the controller to produce what answers. Conventional neural networks in our comparisons either could not store the information, or they could not learn to reason in a way that would generalise to new examples. DNC answering a question about a family tree We could also train a DNC by reinforcement learning. In this framework, we let the DNC produce actions but never show it the answer. Instead, we score it with points when it has produced a good sequence of actions (like the children\u2019s game \u201Chot or cold\u201D). We connected a DNC to a simple environment with coloured blocks arranged in piles. We would give it instructions for goals to achieve: \u201CPut the light blue block below the green; the orange to the left of the red; the purple below the orange; the light blue to the right of the dark blue; the green below the red; and the purple to the left of the green\u201D. DNC solving a moving block puzzle We could establish a large number of such possible goals and then ask the network to execute the actions that would produce one or another goal state on command. In this case, again like a computer, the DNC could store several subroutines in memory, one per possible goal, and execute one or another. The question of how human memory works is ancient and our understanding still developing. We hope that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems. For more information about DNC, please read our paper and an opinion piece by Herbert Jaeger about deep neural reasoning. An artist's impression of a DNC. The glowing controller network is visualised as the operator of a factory where information is delivered in the form of zeros and ones, manipulated by three robotic arms reading, writing and erasing on a scroll that represents the memory, and finally exported as a newly minted binary sequence. Share Article LinkedIn WhatsApp SMS Reddit Authors Wednesday, 12 October 2016 Alexander Graves Research Scientist, DeepMind Greg Wayne Research Scientist, DeepMind Show all results Follow Research Research Applied Applied News & Blog News & Blog About Us About Us Careers Careers Press Terms and Conditions Privacy Policy Alphabet Inc © 2016 DeepMind Technologies Limited DeepMind.com uses cookies to help give you the best possible user experience. Find Out More ","flair":"three\tResearch"}
{"author":"3eyedravens","created":"Mon Nov 07 04:46:05 EST 2016","text":"This work was carried out at the University of Oxford Computer Science Department by Yannis Assael, Brendan Shillingford, Prof Shimon Whiteson and Prof Nando de Freitas. We thank Google DeepMind, CIFAR, and NVIDIA for financial support. We also thank University of Sheffield, Jon Barker, Martin Cooke, Stuart Cunningham and Xu Shao for the GRID corpus dataset; Aine Jackson, Brittany Klug and Samantha Pugh for helping us measure the experienced lipreader baseline; Mitko Sabev for his phonetics guidance; Odysseas Votsis for his video production help; and Alex Graves and Oiwi Parker Jones for helpful comments.LipNet is doing lipreading using Machine Learning, aiming to help those who are hard of hearing and can revolutionise speech recognition.LipNet: Sentence-level Lipreading[Yannis M. Assael, Brendan Shillingford], Shimon Whiteson, Nando de Freitas[https:\/\/arxiv.org\/abs\/1611.01599]Abstract:Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.","flair":"four\tProject"}
{"author":"downtownslim","created":"Thu Nov 17 11:24:23 EST 2016","text":"Lip Reading Sentences in the Wild - Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zissermanhttps:\/\/arxiv.org\/abs\/1611.05358Abstract:The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem \u2013 unconstrained natural language sentences, and in the wild videos.Our key contributions are: (1) a \u2018Watch, Listen, Attend and Spell\u2019 (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a \u2018Lip Reading Sentences\u2019 (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.","flair":"three\tResearch"}
{"author":"stefan_kurcubic","created":"Sat Oct 08 10:25:56 EDT 2016","text":"Hi, i recently got interested in machine learning \n\narticles kept poping up and i wanted to give it a try but it seems to be veeeeery very very very deep and hard field to get into and that it would take a lot of time to make anything practical and even that would require a lot of math\n\nAre there any people here who studied ML for fun and possibly got job in that?\nWhat was the most fun project that you made\/are making?\n\n\n","flair":"null\tnull"}
{"author":"xternalz","created":"Thu Sep 29 23:54:56 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1609.09106 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1609 Change to browse by: cs References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex David Ha Andrew M. Dai Quoc V. Le Bookmark (what is this?) Computer Science > Learning Title: HyperNetworks Authors: David Ha, Andrew Dai, Quoc V. Le (Submitted on 27 Sep 2016 (v1), last revised 28 Oct 2016 (this version, v3)) Abstract: This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters. Subjects: Learning (cs.LG) Cite as: arXiv:1609.09106 [cs.LG]   (or arXiv:1609.09106v3 [cs.LG] for this version) Submission history From: David Ha [view email] [v1] Tue, 27 Sep 2016 05:57:00 GMT (4158kb,D) [v2] Thu, 27 Oct 2016 02:04:56 GMT (2538kb,D) [v3] Fri, 28 Oct 2016 00:28:32 GMT (2538kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"pmfcdb","created":"Fri Nov 04 18:29:21 EDT 2016","text":" Skip navigation Sign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose CNTK Image Detection FastRCNN Pascal VOC 2007 Dataset Paulo Brito SubscribeSubscribedUnsubscribe88 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Statistics 271 views 1 Like this video? Sign in to make your opinion count. Sign in 2 1 Don't like this video? Sign in to make your opinion count. Sign in 2 Loading... Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 4, 2016 CNTK - Deep learning framework from Microsoft used in Image Detection - Pascal VOC 2007 Dataset - FastRCNN algorithm 04:10 Visualization Input ROIs 06:58 (mAP) at around 0.4565 07:13 Visualization Output ROIs Category Science & Technology License Standard YouTube License Show more Show less Loading... Advertisement Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Code Swarm for CNTK - Duration: 3:21. Landon Wilkins 108 views 3:21 CNTK Image Detection FastRCNN Grocery Dataset - Duration: 11:54. Paulo Brito 265 views 11:54 Microsoft Machine Learning & Data Science Summit 2016 CNTK Microsoftâ\u20AC™s Open Source Deep Learning Too - Duration: 51:51. 25msr 623 views 51:51 Evolution of CNTK (Gource Visualization) - Duration: 1:32. Landon Wilkins 223 views 1:32 Dan Does Data: Microsoft CNTK new Deep Learning library! - Duration: 1:03:42. Dan Van Boxel 3,632 views 1:03:42 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 1) - Duration: 54:25. KDD2016 video 330 views 54:25 CNTK: Microsoft's Open-Source Deep-Learning Toolkit - Duration: 53:43. Microsoft Research 5,273 views 53:43 Unlock deeper learning with the new Microsoft Cognitive Toolkit - Duration: 1:46. Microsoft Research 24,130 views 1:46 Share Your Science: Deep Learning at Microsoft Research - Duration: 2:13. NVIDIADeveloper 1,556 views 2:13 Facial expression ASM - Duration: 0:41. Paulo Brito 455 views 0:41 Deep learning Image Classification on Pascal Voc Dataset - Duration: 1:15. Mustafa ihssan 1,821 views 1:15 Tutorial: Deep Learning - Duration: 2:29:35. Microsoft Research 6,744 views 2:29:35 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 2) - Duration: 1:09:13. KDD2016 video 72 views 1:09:13 Noqui movie - Duration: 11:11. Paulo Brito 39 views 11:11 #DebugNews - CNTK - Duration: 6:26. Debugasse 84 views 6:26 Google Deepmind DQN plays Atari Breakout | NickysChannel13 - Duration: 2:51. NickysChannel13 107 views 2:51 Noki development update 04 - Duration: 3:24. Paulo Brito 29 views 3:24 Forklift Simulation Training - Duration: 1:03. Paulo Brito 133 views 1:03 CNTK - Microsoftâ\u20AC™s open-source deep-learning toolkit (Part 3) - Duration: 44:57. KDD2016 video 35 views 44:57 Project TOK - BLE communication : debug mode - Duration: 1:11. Paulo Brito 25 views 1:11 Loading more suggestions... Show more Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"two\tNews"}
{"author":"roboticrabbitsmasher","created":"Mon Oct 03 19:30:32 EDT 2016","text":"It is getting easier and easier to do deep learning (DL). There exist papers, blogs, frameworks, books, courses, newsletters, conferences, and many more resources. If you don\u2019t want to implement it yourself, there are machine learning (ML) API services on AWS, GCE, Azure and companies like Clarifai and Bonsai, to name a few. Thinking back to a few years ago, neural nets were sometimes regarded as \u201Cjust a fad\u201D \u2014I can remember skipping this topic in my grad school ML class because the professor didn\u2019t like them and thought they were hyped. So let\u2019s talk about some of things that have happened to the field of DL in the last few years that have shifted this view. While initially most DL papers came from a few academics, most of the new papers, results, tools, and datasets now come from big companies. And much of the current state of everything can be attributed to big companies. So the natural question is \u201CWhat\u2019s in it for them?\u201D Why would Google open source something like Tensorflow, or why would Microsoft publish a paper detailing how to make state-of-the-art vision algorithms? On the surface it seems like developing this technology provides a competitive advantage, so it doesn\u2019t make sense why you might want to make it free or widely available. As it turns out, this actually represents some pretty nifty business strategy. First, let\u2019s profile a big company to see what components it has, and to see things from its perspective. Let\u2019s start by thinking of Amazon, what they do and where they make money. Let\u2019s start with what they do: In the process of doing business, they are generating one more critical resource, data. The data is super important because not only does this help to make Amazon better, but it is also proprietary, so other web sellers can\u2019t get better. Amazon can analyze the data to figure out better recommendations, predict who will want what and preship items, figure out consumption patterns, A\/B test, and the list goes on and on. Then, when Amazon is doing a better job as a marketplace, more people will buy and sell, and they will collect more data, creating a positive feedback loop. This isn\u2019t unique to Amazon. The more Amazon sells, the better it gets at selling. The more searches Google provides, the better searches it can provide. The more movies Netflix recommends, the better recommendations it can make. So effectively by collecting data, you're raising the barrier to entry for competition. This is what makes data super duper valuable. You can think of Microsoft, Google, and Facebook, all working roughly in the same way. They all have some version of AWS and some software cash-cow that makes money with the help of data. Their business models are thus roughly the same. Now, the bulk of Amazon\u2019s profit comes from AWS, and less from their retail division. For Google, Facebook, and Microsoft, it is the complete opposite. Most of Google and Facebook\u2019s money comes from ads, and Microsoft\u2019s from selling software. The two largest money makers consist of some sort of software and of the computing infrastructure. In contrast, a small portion of money comes from companies selling data indirectly, via machine learning APIs. Now that we have an idea of the rough profile of one of these companies, let\u2019s talk about deep learning, but as a collection of products. The argument I am about to present takes its roots from this highly recommended blog post by Joel Spolsky. The relevant takeaways are: Let\u2019s now think about DL as related to five different products: data, compute power, research, DL tools, and software (that could benefit from DL \u2014 for example, Netflix recommendation). Let\u2019s analyze these in terms of their relationship with each other. Let\u2019s talk about this chart a bit. As you can see, most of these products complement each other pretty well \u2014 cheaper GPUs make it so you can do more research, deploy more DL helped software, crunch more data, etc. Basically, more of any one of these components makes the other ones better. If you talk to people about why \u201CAI has taken off in the last five years,\u201D they will invariably mention it is because of Moore\u2019s Law. I argue that having more computation and cheaper computation is not in itself the propelling force; rather, it provides the necessary momentum to accelerate data, research, tools, and \u2014 most importantly \u2014 profitability. It makes sense that the diagonal in the above chart would be mostly substitutes. If AWS is cheaper the GCE, then I will use AWS since these are commodities. The one exception to this is research. Research complementing other research makes sense (the more research is widely disseminated, the more focused it can be). Furthermore, releasing research is beneficial even between competitors, since data \u2014 the main fuel for ML \u2014 is proprietary. With this in mind, think of where big companies make money from: Think of what the big companies give out for free: By giving these away for free, companies want to commoditize tools and research (aka the difficult parts), and monetize the resources needed for ML (which is a large capital investment that creates a high barrier of entry). Given where companies make money, I\u2019d predict that DL research is going to continue to look at more resource intensive models so that really only big companies can take advantage of it. Finally, I want to thank my friends Eric Bakan, Stedman Hood, and Corina Grigore for helping me flesh out this idea.","flair":"null\tnull"}
{"author":"zhongwenxu","created":"Fri Oct 21 20:11:32 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.06258 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG cs.NE stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Using Fast Weights to Attend to the Recent Past Authors: Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu (Submitted on 20 Oct 2016 (v1), last revised 27 Oct 2016 (this version, v2)) Abstract: Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \"fast weights\" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. Subjects: Machine Learning (stat.ML); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1610.06258 [stat.ML]   (or arXiv:1610.06258v2 [stat.ML] for this version) Submission history From: Jimmy Ba [view email] [v1] Thu, 20 Oct 2016 01:03:20 GMT (153kb,D) [v2] Thu, 27 Oct 2016 19:53:07 GMT (154kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"sybilckw","created":"Mon Oct 17 09:39:51 EDT 2016","text":"DataGenCARS is a complete Java-based synthetic dataset generator for the evaluation of Context-Aware Recommendation Systems (CARS).  The generator presents a high flexibility in the obtaining of datasets in several scenarios of items recommendation (with context or without context). Some key features of Data-GenCARS are listed below: - Realistic generation of ratings and attributes of items. - The automatic mapping between item schemas and Java classes. - The possibility to mix real and synthetic datasets and of replicating existing real datasets. Java documentation:  The Java documentation generated by DataGenCARS can be downloaded here.","flair":"three\tResearch"}
{"author":"xplkqlkcassia","created":"Tue Nov 15 02:33:01 EST 2016","text":"As a chronic time-waster and someone who likes to play around with Google Translate, I found that the Translate team seems to have rolled out other NMT-based language pairs (I haven't heard news about it anywhere). These all appear work both ways - instead of SMT where phrase-blocks are highlighted, the entire sentence is highlighted:\n\n* English -&gt; French\n* English -&gt; German\n* English -&gt; Spanish\n* English -&gt; Chinese\n* English -&gt; Portuguese\n* English -&gt; Japanese\n* English -&gt; Korean\n\nSome things to note: \n\n* It can fuse together words in German. German grammar allows you to fuse nouns together into a single word. SMT can't do this, NMT does it spectacularly, forming words that have never been formed before. It even does this with words that are very rare, or words that I made up, which is pretty impressive.\n\n * **The pancake dog is ready!** -&gt; *Der Pfannkuchenhund ist fertig!*\n * **The jam holder mechanism is durable.** -&gt; *Der Stauhaltermechanismus ist langlebig.*\n * **The dark grey road cleaning machine is in the wax cupboard.** -&gt; *Die dunkelgraue Straßenreinigungsmaschine befindet sich im Wachsschrank.*\n\n* It can make reasonable guesses as to the gender of a non-word in languages which use genders (French, Spanish, German, Portuguese), and will always capitalise it appropriately in German. For example, I made up the word \"an olutura\", which translated as \"una olutura\", whereas \"a pakank\" translated as \"un pakank\".\n\n * **When I was walking in the park yesterday, I saw an olutura and a pakank lying on the ground.** -&gt; *Cuando estaba caminando en el parque ayer, vi una olutura y un pakank tendido en el suelo.*\n\nI've [linked](https:\/\/www.reddit.com\/r\/linguistics\/comments\/5d17vy\/google_translate_recently_implemented_new_neural\/) an example of this to \/r\/linguistics, and I'm hoping they'll do some destructive testing to more fully figure out whether the new algorithm is able to capture the quirks of those particular languages.\n","flair":"one\tDiscussion"}
{"author":"ill-logical","created":"Sun Oct 30 19:52:27 EDT 2016","text":" Home Moments Search query Search Twitter Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Language: English Bahasa Indonesia Bahasa Melayu Català Čeština Dansk Deutsch English UK Español Filipino Français Hrvatski Italiano Magyar Nederlands Norsk Polski Português Română Slovenčina Suomi Svenska Tiếng Việt Türkçe Ελληνικά Български език Русский Српски Українська мова עִבְרִית العربية فارسی मराठी हिन्दी বাংলা ગુજરાતી தமிழ் ಕನ್ನಡ ภาษาไทย 한국어 日本語 简体中文 繁體中文 Have an account? Log in Have an account? Remember me · Forgot password? New to Twitter? Sign up wikileaks's profile WikiLeaks Verified account @wikileaks WikiLeaksVerified account @wikileaks We open governments. Contact: https:\/\/wikileaks.org\/#submit  PGP: A04C 5E09 ED02 B328 03EB 6116 93ED 732E 9231 8DBA Also: @WLTaskForce @WikiLeaksShop wikileaks.org Joined October 2008 © 2016 Twitter About Help Terms Privacy Cookies Ads info Dismiss Close Previous Next Close Go to a person's profile Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @ Close Retweet this to your followers? Optional comment for Retweet   Saved searches Remove In this conversation Verified account @ Suggested users Verified account @ Verified account @   140 Retweet Tweet Close Are you sure you want to delete this Tweet? Cancel Delete Close Promote this Tweet Close Block Cancel Block Add a location to your Tweets When you tweet with a location, Twitter stores that location. You can switch location on\/off before each Tweet and always have the option to delete your location history. Learn more Turn location on Not now Close Profile summary Close Your lists Close Create a new list List name Description Under 100 characters, optional Privacy Public · Anyone can follow this list Private · Only you can access this list Save list Close Close Copy link to Tweet Here's the URL for this Tweet. Copy it to easily share with friends. Close Embed this Tweet Embed this Video Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? Include parent Tweet Include media Preview Close Log in to Twitter Remember me · Forgot password? Don't have an account? Sign up » Close Sign up for Twitter Not on Twitter? Sign up, tune into the things you care about, and get updates as they happen. Sign up Have an account? Log in » Close Two-way (sending and receiving) short codes: Country Code For customers of United States 40404 (any) Canada 21212 (any) United Kingdom 86444 Vodafone, Orange, 3, O2 Brazil 40404 Nextel, TIM Haiti 40404 Digicel, Voila Ireland 51210 Vodafone, O2 India 53000 Bharti Airtel, Videocon, Reliance Indonesia 89887 AXIS, 3, Telkomsel, Indosat, XL Axiata Italy 4880804 Wind 3424486444 Vodafone » See SMS short codes for other countries Close Confirmation Close   Close Close Buy Now Close Buy Now Hmm... Something went wrong. Please try again. Skip all Welcome home! This timeline is where you\u2019ll spend most of your time, getting instant updates about what matters to you. Tweets not working for you? Hover over the profile pic and click the Following button to unfollow any account. Say a lot with a little When you see a Tweet you love, tap the heart \u2014 it lets the person who wrote it know you shared the love. Spread the word The fastest way to share someone else\u2019s Tweet with your followers is with a Retweet. Tap the icon to send it instantly. Join the conversation Add your thoughts about any Tweet with a Reply. Find a topic you\u2019re passionate about, and jump right in. Learn the latest Get instant insight into what people are talking about now. Get more of what you love Follow more accounts to get instant updates about topics you care about. Find what's happening See the latest conversations about any topic instantly. Never miss a Moment Catch up instantly on the best stories happening as they unfold. Back Next Next Tweet from user Previous Tweet Next Tweet Follow Following Unfollow Blocked Unblock Pending Cancel WikiLeaks Verified account \u200F@wikileaks Oct 30 Leaked email: Hillary Clinton worried as early as Jan 2015 that Facebook open-sourcing Artificial Intelligence might \"affect our plans\"pic.twitter.com\/UzusTqdVoq Retweets 7,899 Likes 8,575 3:19 PM - 30 Oct 2016 0 replies 7,899 retweets 8,575 likes Reply Retweet 7.9K Retweeted 7.9K Like 8.6K Liked 8.6K More Copy link to Tweet Embed Tweet  💡 \u200F@mydoghatesme Oct 30 @wikileaks have you no dirt on trump or does it suit to have a dumb puppet win the election? 0 replies 3 retweets 34 likes Reply Retweet 3 Retweeted 3 Like 34 Liked 34 More Copy link to Tweet Embed Tweet View other replies JST \u200F@JSTCOLIND Oct 30 @mydoghatesme @wikileaks \"We Open Governments\".. Why is that so hard to understand? If Hillary wasn't such a crook, do you think there would be vast amounts of this? 0 replies 13 retweets 188 likes Reply Retweet 13 Retweeted 13 Like 188 Liked 188 More Copy link to Tweet Embed Tweet View other replies  💡 \u200F@mydoghatesme Oct 30 @JSTCOLIND @wikileaks bullshit can you imagine being a fly on the wall of trumps backroom!! 0 replies 0 retweets 8 likes Reply Retweet Retweeted Like 8 Liked 8 More Copy link to Tweet Embed Tweet View other replies Afro Infidel \u200F@Snoozemayne Oct 30 @mydoghatesme @JSTCOLIND @wikileaks the shit on Trump has already been leaked, turns out that there isn't much to it, maybe next time sheep 0 replies 11 retweets 167 likes Reply Retweet 11 Retweeted 11 Like 167 Liked 167 More Copy link to Tweet Embed Tweet View other replies  💡 \u200F@mydoghatesme Oct 30 @Snoozemayne @JSTCOLIND @wikileaks sexual assault isn't much? 0 replies 1 retweet 5 likes Reply Retweet 1 Retweeted 1 Like 5 Liked 5 More Copy link to Tweet Embed Tweet View other replies Afro Infidel \u200F@Snoozemayne Oct 30 @mydoghatesme @JSTCOLIND @wikileaks when, where and whom did he assault , or are you STILL talking about that tape from 11 years ago 0 replies 0 retweets 44 likes Reply Retweet Retweeted Like 44 Liked 44 More Copy link to Tweet Embed Tweet View other replies  💡 \u200F@mydoghatesme Oct 30 @Snoozemayne @JSTCOLIND @wikileaks do you own a tv?!? 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet View other replies Afro Infidel \u200F@Snoozemayne Oct 30 @mydoghatesme @JSTCOLIND @wikileaks now your resorting to sounding like a literal cuck, and I hate using that wordpic.twitter.com\/vE5s6nWc0A 0 replies 6 retweets 43 likes Reply Retweet 6 Retweeted 6 Like 43 Liked 43 More Copy link to Tweet Embed Tweet View other replies Show more Cynthia Johnston \u200F@GonzoRider Oct 30 @wikileaks So what? Leak Donald's taxes for Chrissakes. 0 replies 3 retweets 27 likes Reply Retweet 3 Retweeted 3 Like 27 Liked 27 More Copy link to Tweet Embed Tweet View other replies PatriotNation \u200F@Patsaholic Oct 30 @GonzoRider @wikileaks Really? HRC owns the media, disrupts rallies,& has been far from honest w\/ email & you think DT's taxes are relevant? 0 replies 13 retweets 76 likes Reply Retweet 13 Retweeted 13 Like 76 Liked 76 More Copy link to Tweet Embed Tweet View other replies Lynetta Trump \u200F@suzannemc1967 Oct 31 @Patsaholic @KimHarm63660098 @GonzoRider @wikileaks they won't give up on the taxes. Taxes WERE leaked, remember? They can't face HRC OVER 0 replies 2 retweets 5 likes Reply Retweet 2 Retweeted 2 Like 5 Liked 5 More Copy link to Tweet Embed Tweet Kim \u200F@KimHarm63660098 Oct 31 @suzannemc1967 @Patsaholic @GonzoRider @wikileaks Who gives a damn about taxes they didn't kill anyone or hand over USA to foreign govts 0 replies 6 retweets 13 likes Reply Retweet 6 Retweeted 6 Like 13 Liked 13 More Copy link to Tweet Embed Tweet Unified Patriot \u200F@GoodsoulO Oct 30 @wikileakspic.twitter.com\/DEYJpY3baD 0 replies 71 retweets 157 likes Reply Retweet 71 Retweeted 71 Like 157 Liked 157 More Copy link to Tweet Embed Tweet View other replies Currie Dobson \u200F@Ventuckyspaz Oct 30 @GoodsoulO @wikileaks This made me laugh so hard. Fucking sick what times we live in. #WienerSuicided #StopHRCMurders #WhereIsHuma #WhereIsJulian 0 replies 1 retweet 4 likes Reply Retweet 1 Retweeted 1 Like 4 Liked 4 More Copy link to Tweet Embed Tweet Unified Patriot \u200F@GoodsoulO Oct 30 @Ventuckyspaz for you.pic.twitter.com\/W60qjn0KrN 0 replies 0 retweets 2 likes Reply Retweet Retweeted Like 2 Liked 2 More Copy link to Tweet Embed Tweet Dr. Roy Schestowitz \u200F@schestowitz Oct 30 @wikileaks Right now #facebook helps by silencing Conservative voices. I guess one can be co-opted one way or another (quid-pro-quo) 0 replies 15 retweets 110 likes Reply Retweet 15 Retweeted 15 Like 110 Liked 110 More Copy link to Tweet Embed Tweet View other replies Currie Dobson \u200F@Ventuckyspaz Oct 30 @schestowitz @wikileaks @schestowitz trust me not just conservatives. Facebook has been attacking and blocking @DrJillStein it's whoever opposes HRC, anybody. 0 replies 3 retweets 15 likes Reply Retweet 3 Retweeted 3 Like 15 Liked 15 More Copy link to Tweet Embed Tweet Show more James Just James \u200F@purpleidea Oct 30 @wikileaks What's the context on this one? What plans? 0 replies 2 retweets 21 likes Reply Retweet 2 Retweeted 2 Like 21 Liked 21 More Copy link to Tweet Embed Tweet View other replies Marcus Drop \u200F@mucusdrop Oct 30 @purpleidea @wikileaks Certain employees at Facebook tried to control which news stories people would see as well as censor Trump comments. 0 replies 8 retweets 41 likes Reply Retweet 8 Retweeted 8 Like 41 Liked 41 More Copy link to Tweet Embed Tweet View other replies James Just James \u200F@purpleidea Oct 30 @mucusdrop @wikileaks Oh! Is there any more evidence actually linking these two things? 0 replies 0 retweets 0 likes Reply Retweet Retweeted Like Liked More Copy link to Tweet Embed Tweet View other replies Marcus Drop \u200F@mucusdrop Oct 30 @purpleideahttp:\/\/hothardware.com\/news\/is-facebook-heavily-biased-toward-hilary-clinton \u2026 0 replies 2 retweets 6 likes Reply Retweet 2 Retweeted 2 Like 6 Liked 6 More Copy link to Tweet Embed Tweet View other replies James Just James \u200F@purpleidea Oct 30 @mucusdrop These are good links, but they don't directly show that this wikileaks email is referring to that as \"the plans\". 0 replies 0 retweets 1 like Reply Retweet Retweeted Like 1 Liked 1 More Copy link to Tweet Embed Tweet View other replies Marcus Drop \u200F@mucusdrop Oct 30 @purpleidea I'm fairly certain it has to do with Facebook switching to AI to promote stories instead of people. 0 replies 2 retweets 8 likes Reply Retweet 2 Retweeted 2 Like 8 Liked 8 More Copy link to Tweet Embed Tweet Back to top ↑ Loading seems to be taking a while. Twitter may be over capacity or experiencing a momentary hiccup. Try again or visit Twitter Status for more information. Promoted Tweet false © 2016 Twitter About Help Terms Privacy Cookies Ads info ","flair":"two\tNews"}
{"author":"Max_Poole","created":"Thu Oct 13 19:51:50 EDT 2016","text":"Debugging machine learning code can be tough because a lot of times your model may not be deterministic. The obvious step is to make sure your code is correct, but beyond that what are some steps that you take when you train a model and the results you get are not what you expect?","flair":"one\tDiscussion"}
{"author":"edderic","created":"Wed Oct 12 08:09:54 EDT 2016","text":" Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 11 Fork 14 Edderic\/udacity-machine-learning-nanodegree Code Issues 0 Pull requests 0 Projects 0 Pulse Graphs Permalink Branch: master Switch branches\/tags Branches Tags master Nothing to show Nothing to show Find file Copy path udacity-machine-learning-nanodegree\/p5-student-demand-prediction\/report.pdf 1f6fb0a Oct 12, 2016 Edderic copy p5 to here so that all udacity projects in one place 1 contributor Users who have contributed to this file Edderic Download History 1.06 MB Sorry, something went wrong. Reload? Sorry, we cannot display this file. Sorry, this file is invalid so it cannot be displayed. Viewer requires iframe. Jump to Line Go Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help You can't perform that action at this time. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. ","flair":"four\tProject"}
{"author":"gmkim90","created":"Thu Oct 13 01:47:52 EDT 2016","text":"In \"Improved Techniques for Training GANs\", I am surprised by performance of semi-supervised learning presented in paper.\n\nBut I am not sure how semi-supervised learning perform better than others such as Auxiliary Deep Generative Model(ADGM) or Ladder network. \n\nAs I understand, unlabeled data is discriminated from negative class (class# = K+1), and does not learn which class it could be. \nHowever, ADGM seems trying to infer class of unlabeled data. \n\nI really appreciate any comments\/insight how it performs well.﻿","flair":"one\tDiscussion"}
{"author":"alexbotev","created":"Sun Oct 02 11:37:07 EDT 2016","text":"Hi, so I actually subimted it as an issue to the github repo, but maybe someone here can clarify some of the things if I do not understand them correctly, as I do not know if the authors would reply any time soon. The link to the description is here: https:\/\/github.com\/casperkaae\/LVAE\/issues\/1","flair":"null\tnull"}
{"author":"downtownslim","created":"Thu Oct 06 12:58:02 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1609.08913 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1609 Change to browse by: cs cs.IT cs.LG math stat References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm Authors: George D. Montanez (Submitted on 28 Sep 2016) Abstract: No Free Lunch theorems show that the average performance across any closed-under-permutation set of problems is fixed for all algorithms, under appropriate conditions. Extending these results, we demonstrate that the proportion of favorable problems is itself strictly bounded, such that no single algorithm can perform well over a large fraction of possible problems. Our results explain why we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters. Subjects: Machine Learning (stat.ML); Information Theory (cs.IT); Learning (cs.LG) Cite as: arXiv:1609.08913 [stat.ML]   (or arXiv:1609.08913v1 [stat.ML] for this version) Submission history From: George Montañez [view email] [v1] Wed, 28 Sep 2016 13:52:17 GMT (396kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"spruceabtuse","created":"Tue Nov 15 15:34:36 EST 2016","text":"This interview took place at the RE\u2022WORK Deep Learning Summit in Boston, on 12-13 May 2016. https:\/\/www.re-work.co\/events\/deep-le...View further videos from the summit here: http:\/\/videos.re-work.co\/events\/1Nervana Systems: AI techniques are rivaling human performance and will be the foundation of the next revolution in computing. At Nervana, we are making scalable AI accessible and enabling disruptive new applications across a wide range of industry verticals.View further information here: https:\/\/www.nervanasys.com\/","flair":"two\tNews"}
{"author":"Pandemonii","created":"Wed Oct 05 09:06:02 EDT 2016","text":"This is the code for the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution Justin Johnson, Alexandre Alahi, Li Fei-Fei To appear at ECCV 2016 The paper builds on A Neural Algorithm of Artistic Style by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge by training feedforward neural networks that apply artistic styles to images. After training, our feedforward networks can stylize images hundreds of times faster than the optimization-based method presented by Gatys et al. This repository also includes an implementation of instance normalization as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. This simple trick significantly improves the quality of feedforward style transfer models. Stylizing this image of the Stanford campus at a resolution of 1200x630 takes 50 milliseconds on a Pascal Titan X: In this repository we provide: If you find this code useful for your research, please cite All code is implemented in Torch. First install Torch, then update \/ install the following packages: If you have an NVIDIA GPU, you can accelerate all operations with CUDA. First install CUDA, then update \/ install the following packages: When using CUDA, you can use cuDNN to accelerate convolutions. First download cuDNN and copy the libraries to . Then install the Torch bindings for cuDNN: Download all pretrained style transfer models by running the script This will download ten model files (~200MB) to the folder . The style transfer models we used in the paper will be located in the folder . Here are some example results where we use these models to stylize this image of the Chicago skyline with at an image size of 512: As discussed in the paper Instance Normalization: The Missing Ingredient for Fast Stylization by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, replacing batch normalization with instance normalization significantly improves the quality of feedforward style transfer models. We have trained several models with instance normalization; after downloading pretrained models they will be in the folder . These models use the same architecture as those used in our paper, except with half the number of filters per layer and with instance normalization instead of batch normalization. Using narrower layers makes the models smaller and faster without sacrificing model quality. Here are some example outputs from these models, with an image size of 1024: The script lets you use a trained model to stylize new images: You can run the same model on an entire directory of images like this: You can control the size of the output images using the flag. By default this script runs on CPU; to run on GPU, add the flag specifying the GPU on which to run. The full set of options for this script is described here. You can use the script to run one or more models in real-time off a webcam stream. To run this demo you need to use instead of : You can run multiple models at the same time by passing a comma-separated list to the flag: With a Pascal Titan X you can easily run four models in realtime at 640x480: The webcam demo depends on a few extra Lua packages: You can install \/ update these packages by running: The full set of options for this script is described here. You can find instructions for training new models here. The script is similar to the original neural-style, and uses the optimization-based style-transfer method described by Gatys et al. This script uses the same code for computing losses as the feedforward training script, allowing for fair comparisons between feedforward style transfer networks and optimization-based style transfer. Compared to the original neural-style, this script has the following improvements: The full set of options for this script is described here. Free for personal or research use; for commercial use please contact me.","flair":"null\tnull"}
{"author":"adagrad","created":"Mon Oct 31 15:08:33 EDT 2016","text":"There's quite a lot of recent work in energy-based generative models (for example, energy-based [directed generative models](https:\/\/arxiv.org\/abs\/1606.03439), [energy-based GANs](https:\/\/arxiv.org\/abs\/1609.03126), etc.) as well as a lot of work out of Yoshua Bengio's group on links between energy-based models and biology (e.g. [e-prop](https:\/\/arxiv.org\/abs\/1609.03126)). What are some interesting open research questions in this field?\n","flair":"one\tDiscussion"}
{"author":"evc123","created":"Wed Nov 02 04:28:42 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.00020 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CL < prev | next > new | recent | 1611 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computation and Language Title: Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision Authors: Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao (Submitted on 31 Oct 2016 (v1), last revised 3 Nov 2016 (this version, v3)) Abstract: Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge. Comments: Fix the Latex compilation problem. In case the problem still exists, we also hosted the PDF version on another link: this https URL Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Learning (cs.LG) Cite as: arXiv:1611.00020 [cs.CL]   (or arXiv:1611.00020v3 [cs.CL] for this version) Submission history From: Chen Liang [view email] [v1] Mon, 31 Oct 2016 20:07:23 GMT (248kb,D) [v2] Wed, 2 Nov 2016 05:25:19 GMT (378kb,D) [v3] Thu, 3 Nov 2016 16:24:24 GMT (246kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"JustFinishedBSG","created":"Fri Oct 14 10:41:20 EDT 2016","text":"Hello,\n\nI'm interested in knowing how stochastic calculus could be used in machine learning. I have no idea where to start looking. It would seem to me that exploiting time continuity in some problems could bring a lot of very very powerful tools.\n\nIf anybody has pointers to the right direction.\n\nThanks\n\n( i have tried to googling my question, only find unrelated articles about quants. Some links on variational inference too, would that be a direction ?)","flair":"one\tDiscussion"}
{"author":"hardmaru","created":"Mon Oct 31 03:52:46 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > stat > arXiv:1610.09033 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: stat.ML < prev | next > new | recent | 1610 Change to browse by: cs cs.LG stat stat.CO stat.ME References & Citations NASA ADS Bookmark (what is this?) Statistics > Machine Learning Title: Operator Variational Inference Authors: Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei (Submitted on 27 Oct 2016 (v1), last revised 31 Oct 2016 (this version, v2)) Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. Comments: Appears in Neural Information Processing Systems, 2016 Subjects: Machine Learning (stat.ML); Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME) Cite as: arXiv:1610.09033 [stat.ML]   (or arXiv:1610.09033v2 [stat.ML] for this version) Submission history From: Dustin Tran [view email] [v1] Thu, 27 Oct 2016 23:32:25 GMT (150kb,D) [v2] Mon, 31 Oct 2016 23:58:43 GMT (150kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"frustrated_lunatic","created":"Wed Oct 19 20:12:18 EDT 2016","text":"In Liu CiXin\u2019s novel, \u201CThe Dark Forest\u201D he devises a solution to Fermi\u2019s paradox based on three axioms of extraterrestrial contact. With minimal adaptation, the axioms apply to adversarial machine learning. Namely: The result is a Dark Forest equilibrium for machine learning. For a foundational treatment of adversarial game theory, please refer to John Nash\u2019s thesis (yes, it\u2019s that short). Some implications are summarized. The world of ML is subject to the Dark Forest situation. Web crawlers as employed by hyperscale web companies (Google, Baidu, Yahoo (maybe), Tencent, etc.) become hunters in cyberspace searching for ML models. When discovering ML models, their reasonable course of action is to adversarially mine information from the ML model to increase the robustness of one\u2019s own model. There are two solutions to \u2018survive\u2019 the Dark Forest situation. The obvious solution is through isolation, i.e. for each company an internal ML model may be deployed. The second solution is the traveler\u2019s model, i.e. to continuously adjust parameters through active learning (retraining) of the network. A cogent description of the Dark Forest solution to Fermi\u2019s paradox is here http:\/\/philosophy.stackexchange.com\/questions\/18127\/dark-forest-postulate-used-to-explain-the-fermi-paradox. This blog post is solely my own and does not necessarily represent the views of NVIDIA corp. PS. Ian Goodfellow notes that the analogy is imperfect as not every model is in conflict (ecological niche addendum?).","flair":"one\tDiscussion"}
{"author":"acanai","created":"Tue Nov 15 09:51:11 EST 2016","text":" Skip navigation UploadSign in Search Loading... Close Yeah, keep it Undo Close This video is unavailable. Watch Queue Queue Watch QueueQueue Remove all Disconnect The next video is startingstop Loading... Watch Queue Queue __count__\/__total__ Find out whyClose Coresets for Bayesian Logistic Regression - NIPS 2016 spotlight video Jonathan Huggins SubscribeSubscribedUnsubscribe22 Loading... Loading... Working... Add to Want to watch this again later? Sign in to add this video to a playlist. Sign in Share More Report Need to report the video? Sign in to report inappropriate content. Sign in Transcript Statistics 384 views Like this video? Sign in to make your opinion count. Sign in Don't like this video? Sign in to make your opinion count. Sign in Loading... Loading... Transcript The interactive transcript could not be loaded. Loading... Loading... Rating is available when the video has been rented. This feature is not available right now. Please try again later. Published on Nov 11, 2016 NIPS 2016 spotlight video for \"Coresets for Bayesian Logistic Regression\" by Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Full paper available at: https:\/\/arxiv.org\/abs\/1605.06423 Category Science & Technology License Standard YouTube License Show more Show less Comments are disabled for this video. Autoplay When autoplay is enabled, a suggested video will automatically play next. Up next Response to Tamara Broderick, Diana Cai, Trevor Campbell Edge Exchangeability NIPS paper - Duration: 13:24. Harry Crane 2,001 views New 13:24 Bayesian Dynamic Modeling: Sharing Information Across Time and Space - Duration: 52:51. UWTV 8,169 views 52:51 MAD-Bayes and SDA-Bayes: Towards Scalability for Bayesian Nonparametrics - Duration: 44:42. Stanford Autonomous Systems Laboratory 381 views 44:42 NIPS 2016 Spotlight: Learning User Perceived Clusters with Feature-Level Supervision - Duration: 2:33. Shan-Hung Wu 414 views 2:33 103 videos Play all Studio Ghibli OSTìµœí�¬ì£¼ NIPS 2016 Spotlight: Full-Capacity Unitary Recurrent Neural Networks - Duration: 2:58. Thomas Powers 32 views 2:58 Interpretable Nonlinear Dynamic Modeling of Neural Trajectories [NIPS 2016 spotlight] - Duration: 3:04. Brain is (not) a computer 69 views 3:04 Big Data and Bayesian Nonparametrics - Duration: 1:29:50. Microsoft Research 67 views 1:29:50 Bayesian Inference Part I - Zoubin Ghahramani - MLSS 2015 TÃ¼bingen - Duration: 1:32:21. Max Planck Institute for Intelligent Systems TÃ¼bingen 861 views 1:32:21 Data Programming NIPS 2016 Spotlight Video - Duration: 3:24. HazyResearch 220 views 3:24 NIPS 2016 spotlight video - CMICOT - Duration: 3:00. Kate Gladkikh 249 views 3:00 Scan Order in Gibbs Sampling (NIPS 2016 Spotlight Video) - Duration: 2:15. HazyResearch 48 views 2:15 NIPS 2016: Stochastic Structured Prediction under Bandit Feedback - Duration: 2:34. statnlp-hdu 345 views 2:34 Bayesian Optimization for Probabilistic Programs (NIPS 2016 Spotlight) - Duration: 3:11. Tom Rainforth 932 views 3:11 NIPS 2016 Spotlight: Multimodal Residual Learning for Visual QA - Duration: 3:01. Jin-Hwa Kim 219 views 3:01 Collaborative Recurrent Autoencoder for Recommender Systems - NIPS 2016 spotlight video - Duration: 2:56. Wang Hao 15 views 2:56 CMU Machine Learning Lecture Sep 17, 2012 - Duration: 1:02:34. cmuTV 488 views 1:02:34 Language: English Content location: United States Restricted Mode: Off History Help Loading... Loading... Loading... About Press Copyright Creators Advertise Developers +YouTube Terms Privacy Policy & Safety Send feedback Try something new! Loading... Working... Sign in to add this to Watch Later Add to Loading playlists... ","flair":"three\tResearch"}
{"author":"hardmaru","created":"Thu Oct 06 23:37:28 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.01945 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF PostScript Other formats (license) Current browse context: cs.LG < prev | next > new | recent | 1610 Change to browse by: cs stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Computer Science > Learning Title: Connecting Generative Adversarial Networks and Actor-Critic Methods Authors: David Pfau, Oriol Vinyals (Submitted on 6 Oct 2016 (v1), last revised 7 Oct 2016 (this version, v2)) Abstract: Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities. Comments: Fixed typos and clarified some sections Subjects: Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1610.01945 [cs.LG]   (or arXiv:1610.01945v2 [cs.LG] for this version) Submission history From: David Pfau [view email] [v1] Thu, 6 Oct 2016 17:00:54 GMT (116kb) [v2] Fri, 7 Oct 2016 16:15:48 GMT (117kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"rakesajar","created":"Mon Oct 03 04:59:53 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1609.00843 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF only (license) Current browse context: cs.LG < prev | next > new | recent | 1609 Change to browse by: cs cs.AI cs.NE References & Citations NASA ADS DBLP - CS Bibliography listing | bibtex Meng Joo Er Rajasekar Venkatesan Ning Wang Bookmark (what is this?) Computer Science > Learning Title: An Online Universal Classifier for Binary, Multi-class and Multi-label Classification Authors: Meng Joo Er, Rajasekar Venkatesan, Ning Wang (Submitted on 3 Sep 2016) Abstract: Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. Comments: 6 pages, 6 tables Subjects: Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1609.00843 [cs.LG]   (or arXiv:1609.00843v1 [cs.LG] for this version) Submission history From: Rajasekar Venkatesan [view email] [v1] Sat, 3 Sep 2016 17:03:14 GMT (845kb) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"null\tnull"}
{"author":"DrPharael","created":"Mon Nov 21 12:29:10 EST 2016","text":" Menu Search Follow Us Facebook Instagram Twitter Youtube Flipboard LinkedIn Google+ RSS More Youtube Flipboard LinkedIn Google+ RSS Got a tip? Let us know. News Channels Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Topics All Galleries All Timelines Video Shows Apps News Crunch Report Disrupt SF 2016 Gadgets Reviews Interviews TC Features All Shows All Videos Events TechCrunch Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store News About Mobile World Congress CES All Events Crunchbase Trending Tesla Google Facebook News Startups Mobile Gadgets Enterprise Social Europe Message Us Search TechCrunch Search TechCrunch Search Search × Hi! You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you. Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot. Thanks, TC Team Cyber Monday SaleGet A $200 Holiday Gift Guide Item Free With Disrupt London Early Bird Ticket Purchase Today Only - Get Yours Now Artificial Intelligence Relax, artificial intelligence isn\u2019t coming for your job Japan looks to create a superfast supercomputer for deep learning Google\u2019s AI translation tool seems to have invented its own secret internal language Browse more... Google opens new AI lab and invests $3.4M in Montreal-based AI research Posted Nov 21, 2016 by Darrell Etherington (@etherington) 0 SHARES Next Story Spectacles finally arrive in NYC with a Central Park Snapbot store Google has invested a total of $4.5 million CAD ($3.4M US) in AI research in Montreal\u2019s Institute for Learning Algorithms, with an academic fund covering three years that will help pay for seven faculty members across various Montreal academic institutions, including the University of Montreal and McGill. The investment is also continued backing for deep learning expert Yoshua Bengio\u2019s work, and is part of Google\u2019s continued bet on Canada\u2019s strong expertise in machine learning and AI research, both of which are becoming increasingly important to its core business. To that end, along with the investment, Google is also opening a brand new deep learning and AI research group in Montreal at its existing office in the city. The new team will be a remote arm of its Google Brain team based in Mountain View, and will be led locally by Hugo Larochelle, a deep learning expert who\u2019s returning home to Montreal from a role with Twitter in Boston specifically for the new position. Google notes that its total investment in academic research in Canada to date now amounts to around $13 million Canadian over the past 10 years, and it hopes that the new investment will help with the ongoing formation of an AI supercluster in Montreal, which is becoming a hotbed for AI startups as well as academic research. Google also has significant presence in Canada via a Waterloo engineering office, and works with AI expert and University of Toronto CS professor Geoffrey Hinton on a lot of its deep learning research. 0 SHARES Advertisement Advertisement Newsletter Subscriptions The Daily Crunch Get the top tech stories of the day delivered to your inbox TC Weekly Roundup Get a weekly recap of the biggest tech stories Crunchbase Daily The latest startup funding announcements Enter Address Subscribe Latest Crunch Report Facebook Builds a Censorship Tool | Crunch Report Watch More Episodes Artificial Intelligence Popular Posts Featured Stories Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 | Sarah Perez Siren Care makes a \u201Csmart\u201D sock to track diabetic health Nov 25, 2016 | Sarah Buhr Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 | Connie Loizos Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 | Ingrid Lunden Latest From TechCrunch Disrupting the world of science publishing 8 hours ago | Bérénice Magistretti Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago | Ron Miller We\u2019re all screwed, but let\u2019s not be nihilists 13 hours ago | Jon Evans, Columnist Technology as a force for division \u2014 and unification \u2014 in politics yesterday | Richard Muirhead Comment moderation powered by Up Next Spectacles finally arrive in NYC with a Central Park Snapbot store Posted Nov 21, 2016 CrunchBoard Job Listings CRM Berater (m\/f) at eGym GmbH (München, Deutschland) C++ Developer - Qt Product Development (m\/f) at eGym GmbH (München, Deutschland) Full Stack Engineer at FactorChain at The Sourcery (Los Altos, CA, United States) UI Engineering @ FactorChain at The Sourcery (Los Altos, CA, United States) Senior Backend Engineer @ MeetMe at The Sourcery (San Francisco, CA, United States) More from CrunchBoard Advertisement TechCrunch News Video Events Crunchbase TechCrunch Store About Staff Contact Us Advertise With Us Send Us A Tip International China Europe Japan Follow TechCrunch Facebook Twitter Google+ LinkedIn Youtube Pinterest Tumblr Instagram StumbleUpon Feed TechCrunch Apps iOS Android Windows 8 Subscribe to The Daily Crunch Latest headlines delivered to you daily Subscribe to <span class=\"no-mobile\">Subscribe to <\/span>The Daily Crunch Enter Email Address Subscribe © 2013-2016 AOL Inc. All rights reserved. Aol Tech Privacy Policy About Our Ads Anti Harassment Policy Terms of Service Powered by WordPress.com VIP Fonts by News Startups Mobile Gadgets Enterprise Social Europe Asia Crunch Network Unicorn Leaderboard Gift Guides All Galleries All Timelines Videos Apps News Crunch Report Disrupt SF 2016 All Shows All Videos Events Disrupt Startup Battlefield Crunchies Meetups International City Events Hackathon Include NFL\u2019s 1ST and Future TechCrunch Store All Events Crunchbase Message Us Most Popular RAWR: Samsung Canada Wins The Internet With This Custom Galaxy S III Aug 28, 2012 by Matt Burns A Dongle Joke That Spiraled Way Out Of Control Mar 21, 2013 by Kim-Mai Cutler Relax, artificial intelligence isn\u2019t coming for your job 10 hours ago by Ron Miller Black Friday online sales to hit a record-breaking $3 billion, over $1 billion from mobile Nov 25, 2016 by Sarah Perez Disrupting the world of science publishing 8 hours ago by Bérénice Magistretti Payments provider Stripe has raised another $150M at a $9B valuation Nov 25, 2016 by Ingrid Lunden Xiaomi admits it doesn\u2019t make money on smartphone hardware sales Nov 25, 2016 by Darrell Etherington UPDATED: Machine learning can fix Twitter, Facebook, and maybe even America yesterday by Chris Nicholson Peter Thiel taps a principal at Founders Fund for Trump\u2019s transition team Nov 25, 2016 by Connie Loizos ","flair":"two\tNews"}
{"author":"antinucleon","created":"Fri Sep 30 00:21:03 EDT 2016","text":"TL;DR Do something fun, How about build your own TensorFlow with NNVM and Torch7 This is a new interesting era of deep learning, with emergence trend of new system, hardware and computational model. The usecase for deep learning is more heterogeneous, and we need tailored learning system for our cars, mobiles and cloud services. The future of deep learning system is going to be more heterogeneous, and we will find emergence need of different front-ends, backends and optimization techniques. Instead of building a monolithic solution to solve all these problems, how about adopt unix philosophy, build effective modules for learning system, and assemble them together to build minimum and effective systems? We opensourced NNVM library a few weeks ago, as a step to build re-usable modules along multiple deep learning systems. Specifically, NNVM provides a common way to represent and optimize a computation graph and target multiple front-backends. This sounds great, you may say, how about give a concrete example? Of course any reasonable project need example code. Since NNVM is a library to help building deep learning systems, why not provide an example on building something real? So here comes TinyFlow. Let us try something fun in this post and introduce how one can build a new deep learning system with NNVM. To put it short, TinyFlow is an \u201Cexample code\u201D for NNVM. It is of course a fully functioning deep learning system and even has some features that is not currently available in TensorFlow. The entire project takes around 2K lines of code, and can run the example code like the followings. You will find that everything is same as TensorFlow, except that in the first line, we import tinyflow instead. In order to build such a system, there are several major things we need to do: Two major ingredients are used to simplify the development of TinyFlow - NNVM is used to build front-end, and provide common library to do graph optimization, differentiation and memory optimizations - Torch7 is used as a dependency to quickly implement the operators we need. The following is a code snippet we use to register the multiplication operators in TinyFlow. FLuaCompute registers a piece of lua function that uses torch to carry out the real computation of the operator. For a mature deep learning system like Tensorflow, MXNet, Caffe. The operators are usually implemented in C++. This consumes a major amount of effort. Since TinyFlow is a demonstration project on how deep learning system can be built, we choose Torch7 because it provides a rather complete set of operations and can be embedded into C++ backend with low cost. We also intentionally choose to avoid using MXNet as backend, since MXNet already uses NNVM as intermediate layer, and it would be more fun to try something different. The general design itself, however, does not bind the system to be Lua specific, and can execute other types of operators. It is actually also not a bad idea to embed Lua code for customized operators, so that we can reuse common piece that are available in the Torch community. We have provided a Torch plugin in MXNet and it is quite helpful to some of our users. Having operators is only part of the story, we still need to stitch the computation together, and provide code for shape\/type inference and memory sharing. The functions such as FInferShape and FInplaceOption is registered to provide these information, then we reuse the memory optimization and shape\/type inference module in NNVM to write the execution backend. This is what the system stack looks like in TinyFlow. One of the main pain point I had when using TF API is that we have to initialize the variables by giving their shape. This is quite inconvenient especially for deep networks, where the shape of weights in later layers depend on previous inputs. By using the shape inference module in TinyFlow, we provide a simple new API like the automatic variable shape inference provided in MXNet The idea is to provide the shape hints such as number of hidden layers in the operators, enable automatic creation of weight variables in these operators, and use to discover the shape of the variables in a network. All these features are provided by NNVM modules and we only need to expose them via the API. TensorFlow uses a dynamic memory allocator to manage memory because it need to handle too general cases of complicated graphs including loop, condition. Many use-cases in deep learning does not need these, and a fixed computation graph can be optimized in a more static way. TinyFlow uses NNVM\u2019s memory optimization module, which provides inplace manipulation and automatic memory sharing. This leads to similar memory saving behavior as in MXNet which allows training of larger models with limited resources. We could also add more interesting features such as sublinear gradient graph planning as described in https:\/\/arxiv.org\/abs\/1604.06174 to futher reduce the memory cost. We do not build every piece of TinyFlow from scratch. Instead, we explore how we can reuse common modules in deep learning systems such as operators from Torch7 and graph representation\/optimizations from NNVM to quickly build a real deep learning system. We believe that such reusability and modularization in deep learning can help us to advance system faster, and build minimum but powerful system by putting useful parts together. So that the improvements we made to TinyFlow and NNVM not only affect this project, but also all the projects that uses NNVM as intermediate representation layer. We believe more module reuse between deep learning systems like NNVM would happen to move learning system building faster as a whole ecosystem. We intentionally keep the codebase small. However, it still elaborate many key concepts and optimizations that exists in major deep learning systems, such as computation graph, automatic differentiation, shape\/type inference, memory optimization. TinyFlow is a perfect material to explain these concepts and teach students about how to build new deep learning systems.","flair":"null\tnull"}
{"author":"buriburisuri","created":"Thu Nov 24 21:28:16 EST 2016","text":"A tensorflow implementation of speech recognition based on DeepMind's WaveNet: A Generative Model for Raw Audio. (Hereafter the Paper) Although ibab and tomlepaine have already implemented WaveNet with tensorflow, they did not implement speech recognition. That's why we decided to implement it ourselves. Some of Deepmind's recent papers are tricky to reproduce. The Paper also omitted specific details about the implementation, and we had to fill the gaps in our own way. Here are a few important notes. First, while the Paper used the TIMIT dataset for the speech recognition experiment, we used the free VTCK dataset. Second, the Paper added a mean-pooling layer after the dilated convolution layer for down-sampling. We extracted MFCC from wav files and removed the final mean-pooling layer because the original setting was impossible to run on our TitanX GPU. Third, since the TIMIT dataset has phoneme labels, the Paper trained the model with two loss terms, phoneme classification and next phoneme prediction. We, instead, used a single CTC loss because VCTK provides sentence-level labels. As a result, we used only dilated conv1d layers without any causal conv1d layers. Finally, we didn't do quantitative analyses such as WER\/CER\/PER and post-processing by combining a language model due to the time constraints. The final architecture is shown in the following figure. (Some images are cropped from WaveNet: A Generative Model for Raw Audio and Neural Machine Translation in Linear Time) We used only 36,395 sentences in the VCTK corpus with a length of more than 5 seconds to prevent CTC loss errors. VCTK corpus can be downloaded from http:\/\/homepages.inf.ed.ac.uk\/jyamagis\/release\/VCTK-Corpus.tar.gz. After downloading, extract the 'VCTK-Corpus.tar.gz' file to the 'asset\/data\/' directory. to train the network. You can see the result ckpt files and log files in the 'asset\/train' directory. Launch tensorboard --logdir asset\/train\/log to monitor training process. We've trained this model on a single Titan X GPU during 30 hours until 20 epochs and the model stopped at 13.4 ctc loss. If you don't have a Titan X GPU, reduce batch_size in the train.py file from 16 to 4. to transform a speech wave file to the English sentence. The result will be printed on the console. For example, try the following command. The result will be as follows: The ground truth is as follows: As mentioned earlier, there is no language model, so there are some cases where capital letters, punctuations, and words are misspelled. You can transform a speech wave file to English text with the pre-trained model on the VCTK corpus. Extract the following zip file to the 'asset\/train\/ckpt\/' directory. Namju Kim (buriburisuri@gmail.com) at Jamonglabs Co., Ltd. Kyubyong Park (kbpark@jamonglab.com) at Jamonglabs Co., Ltd. If you find this code useful please cite us in your work:","flair":"four\tProject"}
{"author":"rcmalli","created":"Tue Oct 18 19:08:19 EDT 2016","text":"Original models can be used for non-commercial research purposes under Creative Commons Attribution License. The code that provided in this project is under MIT License. If you find this project useful, please include reference link in your work.","flair":"four\tProject"}
{"author":"tensorflow_rb","created":"Tue Sep 27 18:08:12 EDT 2016","text":"Hi Everyone,\nI am the author of [tensorflow.rb](https:\/\/github.com\/somaticio\/tensorflow.rb) the Ruby API for Tensorflow. \nThe Ruby community has been very enthusiastic about developing a Ruby API for tensorflow (more on this [thread](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/50)) and I decided to work on it. \n\nI faced incredible challenges along the way but I do have many interesting findings that I would like to share with you guys. The tensorflow.rb gem can be found on this [link](https:\/\/github.com\/somaticio\/tensorflow.rb). Aside from that, I have written three blog posts where I have given a very brief summary of the work\n \n1. [Introductory blog post](https:\/\/medium.com\/@Arafat.\/introducing-tensorflow-ruby-api-e77a477ff16e#.mhvj9ojlj)\n2. [Developers blog post](https:\/\/medium.com\/@Arafat.\/ruby-tensorflow-for-developers-2ec56b8668c5#.97tng1qqi) (This post is for rubyists and developers of other languages.)\n3. [Image Recognition Tutorial](https:\/\/medium.com\/@Arafat.\/image-recognition-in-ruby-tensorflow-df5d5c05389b#.ty1vygtrg)\n\nThe project still needs a lot of work and contributions are very welcome. I encourage everyone to read the blog posts and then install and play with Tensorflow.rb. Any comments\/suggestions would be nice and you can always post your thoughts on the [gitter channel](https:\/\/gitter.im\/tensorflowrb\/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) or comment below.","flair":"null\tnull"}
{"author":"afeder_","created":"Wed Sep 28 13:23:23 EDT 2016","text":" Google Research Blog The latest news from Research at Google Announcing YouTube-8M: A Large and Diverse Labeled Video Dataset for Video Understanding Research Wednesday, September 28, 2016 Posted by Sudheendra Vijayanarasimhan and Paul Natsev, Software Engineers Many recent breakthroughs in machine learning and machine perception have come from the availability of large labeled datasets, such as ImageNet, which has millions of images labeled with thousands of classes. Their availability has significantly accelerated research in image understanding, for example on detecting and classifying objects in static images. Video analysis provides even more information for detecting and recognizing objects, and understanding human actions and interactions with the world. Improving video understanding can lead to better video search and discovery, similarly to how image understanding helped re-imagine the photos experience. However, one of the key bottlenecks for further advancements in this area has been the lack of real-world video datasets with the same scale and diversity as image datasets. Today, we are excited to announce the release of YouTube-8M, a dataset of 8 million YouTube video URLs (representing over 500,000 hours of video), along with video-level labels from a diverse set of 4800 Knowledge Graph entities. This represents a significant increase in scale and diversity compared to existing video datasets. For example, Sports-1M, the largest existing labeled video dataset we are aware of, has around 1 million YouTube videos and 500 sports-specific classes--YouTube-8M represents nearly an order of magnitude increase in both number of videos and classes. In order to construct a labeled video dataset of this scale, we needed to address two key challenges: (1) video is much more time-consuming to annotate manually than images, and (2) video is very computationally expensive to process and store. To overcome (1), we turned to YouTube and its video annotation system, which identifies relevant Knowledge Graph topics for all public YouTube videos. While these annotations are machine-generated, they incorporate powerful user engagement signals from millions of users as well as video metadata and content analysis. As a result, the quality of these annotations is sufficiently high to be useful for video understanding research and benchmarking purposes. To ensure the stability and quality of the labeled video dataset, we used only public videos with more than 1000 views, and we constructed a diverse vocabulary of entities, which are visually observable and sufficiently frequent. The vocabulary construction was a combination of frequency analysis, automated filtering, verification by human raters that the entities are visually observable, and grouping into 24 top-level verticals (more details in our technical report). The figures below depict the dataset browser and the distribution of videos along the top-level verticals, and illustrate the dataset\u2019s scale and diversity. A dataset explorer allows browsing and searching the full vocabulary of Knowledge Graph entities, grouped in 24 top-level verticals, along with corresponding videos. This screenshot depicts a subset of dataset videos annotated with the entity \u201CGuitar\u201D. The distribution of videos in the top-level verticals illustrates the scope and diversity of the dataset and reflects the natural distribution of popular YouTube videos. To address (2), we had to overcome the storage and computational resource bottlenecks that researchers face when working with videos. Pursuing video understanding at YouTube-8M\u2019s scale would normally require a petabyte of video storage and dozens of CPU-years worth of processing. To make the dataset useful to researchers and students with limited computational resources, we pre-processed the videos and extracted frame-level features using a state-of-the-art deep learning model--the publicly available Inception-V3 image annotation model trained on ImageNet. These features are extracted at 1 frame-per-second temporal resolution, from 1.9 billion video frames, and are further compressed to fit on a single commodity hard disk (less than 1.5 TB). This makes it possible to download this dataset and train a baseline TensorFlow model at full scale on a single GPU in less than a day! We believe this dataset can significantly accelerate research on video understanding as it enables researchers and students without access to big data or big machines to do their research at previously unprecedented scale. We hope this dataset will spur exciting new research on video modeling architectures and representation learning, especially approaches that deal effectively with noisy or incomplete labels, transfer learning and domain adaptation. In fact, we show that pre-training models on this dataset and applying \/ fine-tuning on other external datasets leads to state of the art performance on them (e.g. ActivityNet, Sports-1M). You can read all about our experiments using this dataset, along with more details on how we constructed it, in our technical report. Google Labels: datasets , Video Analysis , YouTube    Labels  accessibility ACL ACM Acoustic Modeling Adaptive Data Analysis ads adsense adwords Africa AI Algorithms Android API App Engine App Inventor April Fools Art Audio Australia Automatic Speech Recognition Awards Cantonese China Chrome Cloud Computing Collaboration Computational Imaging Computational Photography Computer Science Computer Vision conference conferences Conservation correlate Course Builder crowd-sourcing CVPR Data Center data science datasets Deep Learning DeepDream DeepMind distributed systems Diversity Earth Engine economics Education Electronic Commerce and Algorithms electronics EMEA EMNLP Encryption entities Entity Salience Environment Europe Exacycle Expander Faculty Institute Faculty Summit Flu Trends Fusion Tables gamification Gmail Google Books Google Brain Google Cloud Platform Google Docs Google Drive Google Genomics Google Play Apps Google Science Fair Google Sheets Google Translate Google Trips Google Voice Search Google+ Government grants Graph Hardware HCI Health High Dynamic Range Imaging ICLR ICML ICSE Image Annotation Image Classification Image Processing Inbox Information Retrieval internationalization Internet of Things Interspeech IPython Journalism jsm jsm2011 K-12 KDD Klingon Korean Labs Linear Optimization localization Machine Hearing Machine Intelligence Machine Learning Machine Perception Machine Translation MapReduce market algorithms Market Research ML MOOC Multimodal Learning NAACL Natural Language Processing Natural Language Understanding Network Management Networks Neural Networks Ngram NIPS NLP open source operating systems Optical Character Recognition optimization osdi osdi10 patents ph.d. fellowship PhD Fellowship PiLab Policy Professional Development Proposals Public Data Explorer publication Publications Quantum Computing renewable energy Research Research Awards resource optimization Robotics schema.org Search search ads Security and Privacy Semi-supervised Learning SIGCOMM SIGMOD Site Reliability Engineering Social Networks Software Speech Speech Recognition statistics Structured Data Style Transfer Supervised Learning Systems TensorFlow Translate trends TTS TV UI University Relations UNIX User Experience video Video Analysis Vision Research Visiting Faculty Visualization VLDB Voice Search Wiki wikipedia WWW YouTube  Archive      2016 Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2015 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2014 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2013 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2012 Dec Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2011 Dec Nov Sep Aug Jul Jun May Apr Mar Feb Jan     2010 Dec Nov Oct Sep Aug Jul Jun May Apr Mar Feb Jan     2009 Dec Nov Aug Jul Jun May Apr Mar Feb Jan     2008 Dec Nov Oct Sep Jul May Apr Mar Feb     2007 Oct Sep Aug Jul Jun Feb     2006 Dec Nov Sep Aug Jul Jun Apr Mar Feb Feed Googleon Follow @googleresearch Give us feedback in our Product Forums. Company-wide Official Google Blog Public Policy Blog Student Blog Products Android Blog Chrome Blog Lat Long Blog Developers Developers Blog Ads Developer Blog Android Developers Blog Google Privacy Terms ","flair":"null\tnull"}
{"author":"MaxBenChrist","created":"Sun Oct 30 11:24:14 EDT 2016","text":"This repository contains the TSFRESH python package. The abbreviation stands for The package contains many feature extraction methods and a robust feature selection algorithm. Data Scientists often spend most of their time either cleaning data or building features. While we cannot change the first thing, the second can be automated. TSFRESH frees your time spend on building features by extracting them automatically. Hence, you have more time to study the newest deep learning paper, read hacker news or build better models. TSFRESH automatically extracts 100s of features from time series. Those features describe basic characteristics of the time series such as the number of peaks, the average or maximal value or more complex features such as the time reversal symmetry statistic. The set of features can then be used to construct statistical or machine learning models on the time series to be used for example in regression or classification tasks. Time series often contain noise, redundancies or irrelevant information. As a result most of the extracted features will not be useful for the machine learning task at hand. To avoid extracting irrelevant features, the TSFRESH package has a built-in filtering procedure. This filtering procedure evaluates the explaining power and importance of each characteristic for the regression or classification tasks at hand. It is based on the well developed theory of hypothesis testing and uses a multiple test procedure. As a result the filtering process mathematically controls the percentage of irrelevant extracted features. The algorithm is described in the following paper TSFRESH has several selling points, for example If you are interested in the technical workings, go to see our comprehensive Read-The-Docs documentation at http:\/\/tsfresh.readthedocs.io. The algorithm, especially the filtering part are also described in the paper mentioned above. If you have some questions or feedback you can find the developers in the gitter chatroom. We appreciate any contributions, if you are interested in helping us to make TSFRESH the biggest archive of feature extraction methods in python, just head over to our How-To-Contribute instructions.","flair":"four\tProject"}
{"author":"kvfrans","created":"Sun Oct 02 18:48:49 EDT 2016","text":"A few weeks ago I made a post on variational autoencoders, and how they can be applied to image generation. In this post, we'll be taking a look at DRAW: a model based off of the VAE that generates images using a sequence of modifications rather than all at once. In most image generation methods, the actual generation network is a bunch of deconvolution layers. These map from some initial latent matrix of parameters to a bigger matrix, which then maps to an even bigger matrix, and so on. However, there's another way to think about image generation. In the real world, artists don't create paintings instantly. It's a sequence of brush strokes that eventually make up something that looks amazing. DRAW attempt to replicate this behavior. Instead of creating an image instantly, it uses a recurrent neural network as both the encoder and decoder portions of a typical variational autoencoder. Every timestep, a new latent code is passed from the encoder to the decoder. Here's a model of a simplified version of DRAW. If you saw the diagram in my previous VAE post, you would notice that the first column in the recurrent model is exactly the same as a typical variational autoencoder. The difference here is that instead of generating a final image directly, we break up its generation into many iterations. Every iteration, the model improves upon its generated image until the final image (hopefully) looks like the original. Above, the horizontal arrows represent recurrent neural networks. These are fully-connected layers that maintain an internal hidden state along with taking in an input. In practice, we use LSTMs. The uppermost horizontal arrow simply represents the iterative construction of our goal image, as each timestep's image is simply elementwise addition. By itself, this simple recurrent version of a variational autoencoder performs pretty well. We can successfully generate nice-looking MNIST images by iterative improvements. However, artists in real life don't draw by continuously making small changes to the entire canvas. Brush strokes occur only in one portion of the image. In the cast of MNIST: when writing a 5, the typical person does not start from a blob and gradually erase portions until it looks nice. They just make one smooth motion following the shape of the 5. The DRAW model acknowledges this by including an attention gate. This is the more complicated part of DRAW, so I'll start from a high-level explanation and go into detail on how attention is implemented. An attention gate allows our encoder and decoder to focus on specific parts of our image. Let's say we're trying to encode an image of the number 5. Every handwritten number is drawn a little differently: some portions may be thicker or longer than others. Without attention, the encoder would be forced to try and capture all these small nuances at the same time. However, if the encoder could choose a small crop of the image every frame, it could examine each portion of the number one at a time. The same goes for generating the number. The attention unit will determine where to draw the next portion of the 5, while the latent vector passed will determine if the decoder generates a thicker area or a thinner area. In summary, if we think of the latent code in a VAE as a vector that represents the entire image, the latent codes in DRAW can be thought of as vectors that represent a brush stroke. Eventually, a sequence of these vectors creates a recreation of the original image. In the simple recurrent VAE model, the encoder takes in the entire input image at every timestep. Instead of doing this, we want to stick in an attention gate in between the two, so the encoder only receives the portion of our image that the network deems is important at that timestep. We will refer to this first attention gate as the \"read\" attention. There's two parts to this attention gate: choosing the important portion, and cropping the image to get rid of the other parts. We'll start with the first part. In order to determine which part of the image to focus on, we need some sort of observation to make a decision based on. In DRAW, we use the previous timestep's decoder hidden state. Using a simple fully-connected layer, we can map the hidden state to three parameters that represent our square crop: center x, center y, and the scale. Now, instead of encoding the entire image, only a small of the image is encoded. This code is then passed through the system, and decoded back into a small patch. We have a second attention gate after the decoder, that's job is to determine where to place this small patch. It's the same setup as the \"read\" attention gate, except the \"write\" attention gate uses the current decoder instead of the previous timestep's decoder. To review, the read attention gate takes a 9x9 crop from the 28x28 original image. This crop is then passed through the autoencoder, and the write attention gate places the 9x9 crop at its appropriate position in the 28x28 generated image. This process is then repeated for a number of timesteps until the original image is recreated. Describing the attention mechanism as a crop makes sense intuitively. However, in practice, we use a different method. The model structure described above is still accurate, but we use a matrix of gaussian filters instead of a crop. What is a gaussian filter? Imagine that our attention gate consisted of taking a 9x9 crop of the original image, and storing the average grayscale value. Then, when reconstructing the image, the a 9x9 patch of that average grayscale value in added on. A gaussian filter does essentially that, except instead of taking a mean average of the 9x9 area, more influence is placed on the grayscale values near the center. To gain a better understanding, let's think in one dimension first. Let's say we had a vector of 10 random values, say [3,5,2,7,4,9,4,6,1,8]. To find the mean average, we would multiply each value by 0.1, and the sum them up. However, another way to do this is by multiplying each value by its corresponding frequency in a gaussian (otherwise known as normal) distribution, and summing those values up. This would place more of an emphasis on the center values such as 4 and 9, and less on the outer values such as 3 and 8. Furthermore, we can choose the center and spread of our gaussian filter. If we place our filter near the side, values near that area will have more influence. It's important to note that every single value still has some influence, even though it may be tiny. This allows us to pass gradients through the attention gate so we can run backprop. We can extend this into two dimensions by having two gaussian filters, one along the X axis and one along the Y axis. To bring it all together: in the previous section we went over a \"read\" attention gate that chose a center x, center y, and scale for a crop of the original image. We can use those same parameters to define a gaussian filter. The center x and y will determine at what location in the original image to place the filter, and the scale will determine how much influence is spread out or concentrated. However, in the end our gaussian filter only leaves us with one scalar average of a certain portion. Ideally, we would want more information than that. In DRAW, we take an array of gaussian filters, each with their centers spaced apart evenly. For example, we may have a 3x3 array of gaussian filters, with the position of the entire array parameterized by with center x and center y. To account for multiple filte, we need a fourth parameter in our attention gate: stride, or how far apart each filter should be from one another. These multiple filters allow us to capture 3x3 = 9 values of information during each timestep, instead of only one. Finally, just for fun we can consider an extreme situation. If we had 28x28 gaussian filters with a stride of one, and each filter had a spread so small it was only influenced by its center pixel, our attention gate would essentially capture the entire original image. I ran some experiments on generating the MNIST dataset using an implementation of DRAW in tensorflow. First, let's look at the plain recurrent version without any attention. The image starts out like a gray blob, and every frame, it gets a little clearer. Here's the fun part. With attention, the image creation gets a bit crazier. It's kind of hard to tell what the network is doing. While it's not exactly the brushstroke-like drawing behavior we were expecting, at the later timesteps you can see that the network only edits a portion of the image at a time. A thing to note is at the first timestep, the network pays attention to the entire image. Whether this is to create the black background, or to understand what number to draw, is hard to know. Interestingly, there's actually signs of the network changing its mind about what number to draw. Take a look at the second to last row, fourth from the left. The network starts out by drawing the outline of a 4. However, it goes back and changes that 4 into a 7 midway. The code for these results is on my Github, which is a slightly fancier and commented version of ericjang's implementation.","flair":"null\tnull"}
{"author":"PianoMastR64","created":"Wed Nov 02 09:38:59 EDT 2016","text":"Imagine taking an ANN and training it on an emulator rendering a game using the active hex data of the rom (which would contain the player input), the raw pixel output and perhaps the rom file itself. It's objective would be to generate an accurate live video stream using only the rom file and the player's input that's as close to what an emulator would give as possible.\n\nIs this a feasible project?","flair":"one\tDiscussion"}
{"author":"anantzoid","created":"Mon Oct 03 08:45:46 EDT 2016","text":"Recent developments in Deep Learning has paved the way to accomplish tasks involving multimodal learning. Visual Question Answering (VQA) is one such challenge which requires high-level scene interpretation from images combined with language modelling of relevant Q&A. Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. This is a Keras implementation of one such end-to-end system to accomplish the task. The learning architecture behind this demo is based on the model proposed in the VQA paper. The problem is considered as a classification task here, wherein, 1000 top answers are chosen as classes. Images are transformed by passing it through the VGG-19 model that generates a 4096 dimensional vector in the second last layer. The tokens in the question are first embedded into 300 dimensional GloVe vectors and then passed through 2 layer LSTMs. Both multimodal data points are then passed through a dense layer of 1024 units and combined using point-wise multiplication. The new vector serves as input for a fully-connected model having a and a final layer. Preprocessed features provided by VT Vision Lab was used which consisted of images transformed through VGG19 model and indexed tokens. The following packages need to be installed before running the scripts: Then go to the folder and download the requirements given over there. Run along with the following optional parameters: , , . To evaluate the model on validation set, run . Preprocessed features have been used based on these scripts written by the VT vision lab team. These features already consist of transformed image vectors, indexed tokens for text and other metadata, for both the training and validation set. Training was done on g2.2xlarge spot instance of AWS. Mutltiple commuity AMIs can be found having all the required packages pre-installed. g2.2xlarge has a NVIDIA Grid K520 with 4GB memory and takes ~277 seconds\/epoch for a batch size of 256. The model has been trained on 50 epochs and has a accuracy of 45.03% on the validation set. Also, the accuracy started decreasing after 70 epochs. Thus, there is a lot of scope for hyper-parameter tuning here. For details on how to run the demo app, check the docs in folder. If you have any feedback or suggestions, do ping me at anant718@gmail.com","flair":"null\tnull"}
{"author":"kh40tika","created":"Wed Nov 02 08:20:20 EDT 2016","text":"We found training CNN with a fixed untrainable 3x3 convolution followed by a trainable 1x1 convolution(technically just a GEMM) can improve CNN training convergence speed while requiring less computation. Currently this is supported with empirical evidence on small datasets including MNIST and CIFAR10. While the scalability of this method remains to be confirmed. Above experiment was run on same overall model architechture, with only difference being convolution layer. Yeah I know that is not quite close to state of art, but the point is to compare and provide empirical evidence. The code is written to run experiment with MNIST or CIFAR10 dataset. You should modify the code if you want to test on other datasets. For MNIST, go to here and download all 4 .gz files, save them in . Uncompress them, then run in the same folder. MNIST dataset should be ready to go. For CIFAR10, go to here and download the python version, place it in . Just uncompress it and CIFAR10 dataset should be ready to go. To view help on hyperparameters setting. NOTE: CIFAR10 dataset tend to converge much slower than MNIST, you may want longer training like: training\/validation loss curve and model parameters during training will be recorded in file (MATLAB format). Open the file with your favourite tool to analyse. The modified convolution layer takes M image channels and generates 5M image channels via 5 fixed 3x3 convolutions. Then all channels were fed into trainable 1x1 convolution to generate N output channels. 3x3 convolution kernels are following ones: All the above convolution kernels are separable (Note two of them are SOBEL edge detectors). If implemented correctly, this can run much faster than state-of-art generic 3x3 convolution algorithm. We've written a slightly optimized version of this operation with CUDA, in . Thanks to Richard Marko's for doing MNIST preprocessing.","flair":"three\tResearch"}
{"author":"gabrielgoh","created":"Thu Oct 13 16:24:12 EDT 2016","text":"From text translation to video captioning, learning to map one sequence to another is an increasingly active research area in machine learning. Fueled by the success of recurrent neural networks in its many variants, the field has seen rapid advances over the last few years. Recurrent neural networks are typically trained using some form of stochastic gradient descent combined with backpropagation for computing derivatives. The fact that gradient descent finds a useful set of parameters is by no means obvious. The training objective is typically non-convex. The fact that the model is allowed to maintain state is an additional obstacle that makes training of recurrent neural networks challenging. In this post, we take a step back to reflect on the mathematics of recurrent neural networks. Interpreting recurrent neural networks as dynamical systems, we will show that stochastic gradient descent successfully learns the parameters of an unknown linear dynamical system even though the training objective is non-convex. Along the way, we\u2019ll discuss several useful concepts from control theory, a field that has studied linear dynamical systems for decades. Investigating stochastic gradient descent for learning linear dynamical systems not only bears out interesting connections between machine learning and control theory, it might also provide a useful stepping stone for a deeper undestanding of recurrent neural networks more broadly. We focus on time-invariant single-input single-output system. For an input sequence of real numbers $x_1,\\dots, x_T\\in \\mathbb{R}$, the system maintains a sequence of hidden states $h_1,\\dots, h_T\\in \\mathbb{R}^n$, and produces a sequence of outputs $y_1,\\dots, y_T\\in \\mathbb{R}$ according to the following rules: Here $A,B,C,D$ are linear transformations with compatible dimensions, and $\\xi_t$ is Gaussian noise added to the output at each time. In the learning problem, often called system identification in control theory, we observe samples of input-output pairs $((x_1,\\dots, x_T),(y_1,\\dots y_T))$ and aim to recover the parameters of the underlying linear system. Although control theory provides a rich set of techniques for identifying and manipulating linear systems, maximum likelihood estimation with stochastic gradient descent remains a popular heuristic. We denote by $\\Theta = (A,B,C,D)$ the parameters of the true system. We parametrize our model with $\\widehat{\\Theta} = (\\hat{A},\\hat{B},\\hat{C},\\hat{D})$, and the trained model maintains hidden states $\\hat{h}_t$ and outputs $\\hat{y}_t$ exactly as in equation (1). For each given example $(x,y) = ((x_1,\\dots,x_T), (y_1,\\dots, y_t))$, the log-likelihood of model $\\widehat{\\Theta}$ is . The population risk is defined as the expected log-likelihood, Stochastic gradients of the population risk can be computed in time $O(Tn)$ via back-propagation given random samples. We can therefore directly minimize population risk using stochastic gradient descent. The question is just whether the algorithm actually converges. Even though the state transformations are linear, the objective function we defined is not convex. Luckily, we will see that the objective is still close enough to convex for stochastic gradient to make steady progress towards the global minimum. Before we go into the math, let\u2019s illustrate the algorithm with a pressing example that we all run into every morning: hair drying. Imagine you have a hair dryer with a low temperature setting and a high temperature setting. Neither setting is ideal. So every morning you switch between the settings frantically in an attempt to modulate to the ideal temperature. Measuring the resulting temperature (red line below) as a function of the input setting (green dots below), the picture you\u2019ll see is something like this: You can see that the output temperature is related to the inputs. If you set the temperature to high for long enough, you\u2019ll eventually get a high output temperature. But the system has state. Briefly lowering the temperature has little effect on the outputs. Intuition suggests that these kind of effects should be captured by a system with two or three hidden states. So, let\u2019s see how SGD would go about finding the parameters of the system. We\u2019ll initialize a system with three hidden states such that before training its predictions are just the inputs of the system. We then run SGD with a fixed learning rate on the same sequence for 400 steps. The blue line shows the predictions of SGD after 0\/400 gradient updates. Click to advance. Evidently, gradient descent converges just fine on this example. Let\u2019s look at the hair dryer objective function along the line segment between two random points in the domain. The function is clearly not convex, but it doesn\u2019t look too bad either. In particular, from the picture, it could be that the objective function is quasi-convex: Intuitively, quasi-convexity states that the descent direction $-\\nabla f(\\theta)$ is positively correlated with the ideal moving direction $\\theta^* -\\theta$. This implies that the potential function $\\left|\\theta-\\theta ^ * \\right|^2$ decreases in expectation at each step of stochastic gradient descent. This observation plugs nicely into the standard SGD analysis, leading to the following result: The key challenge for us is to understand under what conditions we can prove that the population risk objective is in fact quasi-convex. This requires some background. A linear dynamical system $(A,B,C,D)$ is equivalent to the system $(TAT^{-1}, TB, CT^{-1}, D)$ for any invertible matrix $T$ in terms of the behavior of the outputs. A little thought shows therefore that in its unrestricted parameterization the objective function cannot have a unique optimum. A common way of removing this redundancy is to impose a canonical form. Almost all non-degenerate system admit the controllable canonical form, defined as We will also parametrize our training model using these forms. One of its nice properties is that the coefficients of the characteristic polynomial of the state transition matrix $A$ can be read off from the last row of $A$. That is, Even in controllable canonical form, it still seems rather difficult to learn arbitrary linear dynamical systems. A natural restriction would be stability, that is, to require that the eigenvalues of $A$ are all bounded by $1.$ Equivalently, the roots of the characteristic polynomial should all be contained in the complex unit disc. Without stability, the state of the system could blow up exponentially making robust learning difficult. But the set of all stable systems forms a non-convex domain. It seems daunting to guarantee that stochastic gradient descent would converge from an arbtirary starting point in this domain without ever leaving the domain. We will therefore impose a stronger restriction on the roots of the characteristic polynomial. We call this the Pac-Man condition. You can think of it as a strengthening of stability. Above, we illustrate this condition for a degree 4 system plotting the value of $q_a(z)$ on complex plane for all complex numbers $z$ on the unit circle. We note that Pac-Man condition is satisfied by vectors $a$ with $|a|_1\\le \\sqrt{2}\/2$. Moreover, if $a$ is a random Gaussian vector with expected $\\ell_2$ norm bounded by $o(1\/\\sqrt{\\log n})$, then it will satisfy Pac-Man condition with probability $1-o(1)$. Roughly speaking, the assumption requires the roots of the characteristic polynomial $p_a(z)$ are relatively dispersed inside the unit circle. The Pac-Man condition has three important implications: Relying on the Pac-Man condition, we can show: The theorem sorts out the right dependence on $N$ and $T$. Even if there is only one sequence, we can learn the system provided that the sequence is long enough. Similarly, even if sequences are really short, we can learn provided that there are enough sequences. To establish quasi-convexity under the Pac-Man condition, we will first develop an explicit formula for the population risk in frequency domain. In doing so, we assume that $x_1,\\dots, x_T$ are pairwise independent with mean 0 and variance 1. We also consider the population risk as $T\\rightarrow \\infty$ for simplicity in this post. A simple algebraic manipulation simplifies the population risk with infinite sequence length to The first term, $(\\hat D - D)^2$ is convex and appears nowhere else. We can safely ignore it and focus on the remaining expression instead, which we call the idealized risk: To deal with the sequence $\\hat{C}\\hat{A}^kB$, we take its Fourier transform and obtain that Similarly we take the Fourier transform of $CA^kB$, denoted by $G_{\\lambda}$. Then by Parseval\u2019s Theorem, we obtain the following alternative representation of the population risk, Mapping out $G_\\lambda$ and $\\widehat G_\\lambda$ for all $\\lambda\\in [0, 2\\pi]$ gives the following picture: Left: Target transfer function $G$. Right: Approximation $\\widehat G$ at step 0\/10. Click to advance. Given this pretty representation of the idealized risk objective, we can finally prove our main lemma. The lemma reduces to the following simple claim. The proof simply involves computing the gradients and checking the conditions for quasi-convexity by elementary algebra. We omit a formal proof, but intead show a plot of the function $h(\\hat{u}, \\hat{v}) = (\\hat{u}\/\\hat{v}- 1)^2$ over the reals: To see how the lemma follows from the previous claim we note that quasi-convexity is preserved under composition with any linear transformation. Specifically, $h(z)$ is quasi-convex, then $h(R x)$ is also quasi-convex for any linear map $R$. So, consider the linear map: With this linear transformation, our simple claim about a bivariate function extends to show that $(G_{\\lambda}-\\widehat{G}_{\\lambda})^2$ is quasi-convex when $Re(\\hat{v}\/v) \\ge 0$. In particular, when $\\hat{a}$ and $a$ both satisfy the Pac-Man condition, then $\\hat{v}$ and $v$ both reside in the 90 degree wedge. Therefore they have an angle smaller than 90 degree. This implies that $Re(\\hat{v}\/v) > 0$. We saw conditions under which stochastic gradient descent successfully learns a linear dynamical system. In our paper, we further show that allowing our learned system to have more parameters than the target system makes the problem dramatically easier. In particular, at the expense of slight over-parameterization we can weaken the Pac-Man condition to a mild separation condition on the roots of the characteristic polynomial. This is consistent with empirical observations both in machine learning and control theory that highlight the effectiveness of additional model parameters. More broadly, we hope that our techniques will be a first stepping stone toward a better theoretical understanding of recurrent neural networks.","flair":"three\tResearch"}
{"author":"andrewbarto28","created":"Tue Oct 11 11:35:17 EDT 2016","text":"I have several methods and for each one I am selecting hyperparameters in the same validation set. I know that the accuracies obtained in the validation set will be higher than the real ones, but will ordering of the results in the validation set represent the real ordering? In other words, if I want to show which method is better than the other, is it fine to just compare the validation accuracy of the best hyperparameter for each method?","flair":"one\tDiscussion"}
{"author":"syllogism_","created":"Tue Oct 11 17:07:11 EDT 2016","text":"The State of AI Take the survey About the survey Machine learning is changing the world by greatly extending the range of problems that software can solve. The web is full of predictions about where we might be headed, but where are we now? What\u2019s already here and what\u2019s coming soon? Who\u2019s building these systems and how? The results will be 100% open-source for everyone to access and analyze. Who\u2019s behind this? This survey was produced by Ines Montani and Matthew Honnibal of Explosion AI and Nathan Benaich of Playfair Capital. Our goal is to coordinate a crowdsourced effort to help the community figure out how successful systems are being built, and what challenges teams are facing. If you\u2019re working on AI technologies, we hope you\u2019ll take a few minutes to share your experiences. Get the results! Drop us your email address and we\u2019ll notify you when the data is ready! Spread the word! Illustrations: Agata SasiukInspired by: The State of JSLegal \/ ImprintContact ","flair":"four\tProject"}
{"author":"sybilckw","created":"Thu Nov 10 06:20:34 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.02049 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.RO < prev | next > new | recent | 1611 Change to browse by: cs cs.NE References & Citations NASA ADS Bookmark (what is this?) Computer Science > Robotics Title: Low-effort place recognition with WiFi fingerprints using deep learning Authors: Michał Nowicki, Jan Wietrzykowski (Submitted on 7 Nov 2016) Abstract: Using WiFi signals for indoor localization is the main localization modality of the existing personal indoor localization systems operating on mobile devices. WiFi fingerprinting is also used for mobile robots, as WiFi signals are usually available indoors and can provide rough initial position estimate or can be used together with other positioning systems. Currently, the best solutions rely on filtering, manual data analysis, and time-consuming parameter tuning to achieve reliable and accurate localization. In this work, we propose to use deep neural networks to significantly lower the work-force burden of the localization system design, while still achieving satisfactory results. Assuming the state-of-the-art hierarchical approach, we employ the DNN system for building\/floor classification. We show that stacked autoencoders allow to efficiently reduce the feature space in order to achieve robust and precise classification. The proposed architecture is verified on the publicly available UJIIndoorLoc dataset and the results are compared with other solutions. Subjects: Robotics (cs.RO); Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:1611.02049 [cs.RO]   (or arXiv:1611.02049v1 [cs.RO] for this version) Submission history From: Michal Nowicki [view email] [v1] Mon, 7 Nov 2016 13:47:25 GMT (239kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"sybilckw","created":"Wed Oct 05 11:16:26 EDT 2016","text":"Abstract: Systems for aggregating illustrations require a function for automatically distinguishing illustrations from photographs as they crawl the network to collect images. A previous attempt to implement this functionality by designing basic features that were deemed useful for classification achieved an accuracy of only about 58%. On the other hand, deep neural networks had been successful in computer vision tasks, and convolutional neural networks (CNNs) had performed good at extracting such... Read More","flair":"null\tnull"}
{"author":"allliam","created":"Mon Nov 14 01:55:18 EST 2016","text":"I'm wondering what research has been done on when (and why) gradient descent (in all its forms) fails to learn a random neural network. I'm assuming the structure of the model being trained is the same as the random network we are attempting to learn.  I'm particularly interested in how large and deep a network needs to be before SGD fails to find a good solution. Also, there are a lot of hyper-parameters to [experiment over](https:\/\/arxiv.org\/pdf\/1206.5533v2.pdf), and I would be curious if there are any conclusions about the conditions of a network which would make learning more sensitive to a particular hyper-parameter.\n\nDoes it make sense to use random neural networks as a proxy for the problems we care about? If not, has anyone found regularities of the parameters (edge weights) of DNNs that solve real problems?","flair":"one\tDiscussion"}
{"author":"kevinzakka","created":"Sat Oct 29 16:25:29 EDT 2016","text":"I've decided to try and learn as much as I can on the topic. I want to read the relevant background papers and associated blog posts and source code.\n\nCould anyone point me in the right direction, i.e. papers to read and links?\n\nHere's what I have so far:\n\n* DRCN - [here](https:\/\/arxiv.org\/abs\/1511.04491)\n* Sub-Pixel Convolutional Neural Network - [here](https:\/\/arxiv.org\/abs\/1609.05158)\n* Perceptual Losses - [here](https:\/\/arxiv.org\/abs\/1603.08155)\n* GAN SISR - [here](https:\/\/arxiv.org\/abs\/1609.04802)\n\nNote that I got this list from alexjc's [neural-enhance](https:\/\/github.com\/alexjc\/neural-enhance) README. I guess I'd also need to read the first [GAN paper](https:\/\/arxiv.org\/abs\/1406.2661).\n\nThanks in advance for the help!","flair":"one\tDiscussion"}
{"author":"fhuszar","created":"Thu Nov 24 10:22:41 EST 2016","text":"I have always struggled a bit to grasp DeepMind's motto and mission statement, who want to solve intelligence. Well, that is in fact only step 1, step 2 is using it to solve everything else. What does solving intelligence mean? Do I have to believe in it? And more importantly: How do I use this mission statement to guide me to choose the next topic I work on when I have just finished some work? I recently had conversation with a friend about this very same topic, and it suddenly occurred to me that there is a simple reformulation of solving intelligence mission statement that I can actually identify with: At the end of this machine\/deep learning hype cycle, either of two scenarios could occur: Now the thing is, we can describe the winter scenario much more accurately than the holy shit scenario. To get there we need to prove - not in the mathematical sense - that the current technologies have reached their limits, bar some predictable, incremental progress. Formulating our goal this way also gives a clearer prescriptions in terms of what we should be working on next: We need to find and characterise a few situations that our human intelligence can solve but which appears to be clearly beyond reach for machine learning-based systems. My indirect approach can be just as productive as focussing on 'solving AI'. After all, one of the two scenarios will happen, so if we are effective at speeding up progress towards one of them, we are probably also accelerating our progress towards the other one if that's the one that ends up happening. This was not meant to be a negative comment on DeepMind or anyone else who tries to solve intelligence. To the contrary, it is a reformulation of DeepMind's goal in a way I can actually relate to and can start thinking about. Whether or not you believe we will ultimately solve intelligence, I think it is a lot easier to think about how and why an AI winter would happen than to characterise how general AI suddenly happens. It's like using an indirect proof: you don't actually have to believe the negative premise. In conclusion, as a machine learning researcher, maybe you should be focussed on driving yourself (and all your colleagues) out of your intellectually stimulating jobs, as quickly as you can. Let the next winter come sooner rather than later!","flair":"one\tDiscussion"}
{"author":"amirsaffari","created":"Thu Oct 13 03:07:00 EDT 2016","text":"It is very easy to request new compositions from LnH band. Simply tweet at @lnh_ai with following format: LnH will interpret this tweet as LnH also can choose a good tempo if you miss it, but it\u2019s always fun to experiment with tempo parameter. You can choose between 45 and 320. Depending on how much experimental you want your track to be, you can vary the randomness value (0.0 will be more repetitive while 1.0 would be, let\u2019s say, very experimental). And always remember to include \u201C\/compose\u201D. The song length will be roughly between 1 and 2 minutes depending on your tempo. Once the song is ready, you will receive a tweet back with a link to your song 🙂","flair":"two\tNews"}
{"author":"buriburisuri","created":"Thu Nov 10 04:11:51 EST 2016","text":"A tensorflow implementation of Augustus Odena (at Google Brains) et al's \"Conditional Image Synthesis With Auxiliary Classifier GANs\" paper ) I've already implemented this kind of GAN structure last Sep. (See : Supervised InfoGAN tensorflow implementation) I said that I had added supervised loss(in this paper auxiliary classifier) to InfoGAN structure to achieve the consistency of generated categories and training stability. And the result was promising. It helped the model to generate consistent categories and to converge fast compared to InfoGAN. The paper which propose the same architecture by researchers at Google Brains was published last Oct. They've provided many results from diverse data set and tried to generate big images and explained why this architecture works plausibly. I think this paper is awesome. But they did not provide source codes and I re-introduce this codes. to train the network. You can see the result ckpt files and log files in the 'asset\/train' directory. Launch tensorboard --logdir asset\/train\/log to monitor training process. to generate sample image. The 'sample.png' file will be generated in the 'asset\/train' directory. This image was generated by AC-GAN network. And this image was generated with categorical auxiliary classifier. And this image was generated by continuous auxiliary classifier. You can see the rotation change along the X axis and the thickness change along the Y axis. The following image is the loss chart in the training process. This looks more stable than my original GAN implementation. Namju Kim (buriburisuri@gmail.com) at Jamonglabs Co., Ltd.","flair":"four\tProject"}
{"author":"Thenewcheri","created":"Tue Nov 08 07:35:47 EST 2016","text":"I have been learning Tensor Analysis lately, and realized that the space whose coordinates are the weights of neural network is not flat. Meaning that, gradient descent may not propose the shortest path, geodesic, to a local minima. \nMy question then is, can we speed up the training by making the updates not with the gradients but with covariant derivatives of the cost function?\n\nI am sorry that I could not phrase the question better.","flair":"three\tResearch"}
{"author":"anshaikh","created":"Sun Oct 09 20:58:45 EDT 2016","text":"In decision tree classifier most of the algorithms use Information gain as spiting criterion. We select the feature with maximum information gain to split on.\n\nI think that using accuracy instead of information gain is simpler approach. Is there any scenario where accuracy doesn't work and information gain does?\n\nCan anyone explain what are the advantages of using Information gain over accuracy as splitting criterion?","flair":"one\tDiscusssion"}
{"author":"Faintedsup","created":"Wed Nov 02 15:19:54 EDT 2016","text":"I was searching around for papers about evaluating the performance of players in a particular sport or game, but was unable to find anything except for predictions on game outcome based on data observed during the game.\n\nMore specifically, I'm looking for papers that try to assign a score to individual players in a sport or game, based on their performance during the game. Ideally the data would include visual data of the player, e.g. a player zoning another player in soccer.\n\nI realise this might qualify as a simple question, but seeing as the current question thread is weeks old, I thought I might try making a thread for it instead.","flair":"four\tProject"}
{"author":"skepticforest","created":"Thu Nov 03 22:12:09 EDT 2016","text":"I understand most ML algos pretty well but I haven't ran them a lot. Till now, my experience with ML has been with small datasets in R on my laptop. Which was just simply loading dataset, data processing and calling the specific function. I'm learning deep nets and I'm reading not to use my own laptop as training would degrade it, apparently.\n\nSo what I see from most threads, people use cloud services. I've been reading about using ssh to use AWS\/other cloud services for faster computation. I don't understand what ssh is, how to use AWS etc. Can someone provide a detailed explanation\/links to tutorials on how to start using ML in the cloud for a complete novice, how to ssh and add datasets and ML algos in the different cloud services, what's the difference between the cloud services (which are better).\n\nAnd also can someone explain the role of GPU's in ML and what I need to do in order to use them for ML?\n\nBasically an ELI5 start-to-end tutorial on how to use ML in the cloud and what roles GPU's play in ML","flair":"one\tDiscussion"}
{"author":"hetong_007","created":"Sun Oct 16 15:02:34 EDT 2016","text":"For the detailed experiment scripts and output logs, please refer to this repo. We use 3 data set to conduct our comparison experiments. Details of data are listed in the following table: We use one Linux server as experiment platform, details are listed in the following table: We use xgboost as baseline, and build version is latest version at 27 OCT 2016 016ab89. Both xgboost and LightGBM are built with OpenMP support. We set up total 3 settings for experiments, the parameters of these settings are listed in the following: xgboost grows tree depth-wise and controls model complexity by . LightGBM uses leaf-wise algorithm instead and controls model complexity by . So we cannot compare them in the exact same model setting. For the tradeoff, we use xgboost with , which will have max number leaves to 255, to compare with LightGBM with . And xgboost_approx with will have #bins to 250, which is similar to default(255) in LightGBM. For speed comparison, we only run the training task, which is without any test or metric output. And we don't count the time for IO. The following table is the comparison of time cost: We found LightGBM is faster than xgboost on all experiment data sets. For accuracy comparison, we use the accuracy on test data set to have a fair comparison. We found LightGBM has better accuracy than xgboost on all experiment data sets. We monitor while running training task. And we set (Will increase data-loading time, but reduce peak memory usage, not affect training speed or accuracy) in LightGBM to reduce peak memory usage. LightGBM benefits from its histogram optimization algorithm, so it consumes much lower memory. We use a terabyte click log dataset to conduct parallel experiments. Details are listed in following table: This data contains 13 integer features and 26 category features of 24 days click log. We statistic the CTR and count for these 26 category features from the first ten days, then use next ten days\u2019 data, which had been replaced the category features by the corresponding CTR and count, as training data. The processed training data hava total 1.7 billions records and 67 features. We use 16 windows servers as experiment platform, details are listed in following table: We use data parallel here, since this data is large in #data but small in #feature. From the results, we find LightGBM perform linear speed up in parallel learning.","flair":"one\tDiscussion"}
{"author":"cvikasreddy","created":"Thu Oct 13 05:25:11 EDT 2016","text":"I was training InfoGAN on mnist with one continuous and one categorical variable.\n\nInterestingly the continuous variable is capturing it's category(0-9) and the categorical variable is capturing some other information.\n\nWhat might be the reason?","flair":"one\tDiscussion"}
{"author":"zhongwenxu","created":"Thu Nov 03 21:12:07 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > math > arXiv:1611.01146 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: math.OC < prev | next > new | recent | 1611 Change to browse by: cs cs.DS cs.NE math stat stat.ML References & Citations NASA ADS Bookmark (what is this?) Mathematics > Optimization and Control Title: Finding Approximate Local Minima for Nonconvex Optimization in Linear Time Authors: Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, Tengyu Ma (Submitted on 3 Nov 2016 (v1), last revised 4 Nov 2016 (this version, v2)) Abstract: We design a non-convex second-order optimization algorithm that is guaranteed to return an approximate local minimum in time which is linear in the input representation. The time complexity of our algorithm to find an approximate local minimum is even faster than that of gradient descent to find a critical point. Our algorithm applies to a general class of optimization problems including training a neural network and other non-convex objectives arising in machine learning. Subjects: Optimization and Control (math.OC); Data Structures and Algorithms (cs.DS); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML) Cite as: arXiv:1611.01146 [math.OC]   (or arXiv:1611.01146v2 [math.OC] for this version) Submission history From: Zeyuan Allen-Zhu [view email] [v1] Thu, 3 Nov 2016 19:50:32 GMT (493kb,D) [v2] Fri, 4 Nov 2016 18:38:50 GMT (498kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"AutoModerator","created":"Wed Oct 19 11:52:31 EDT 2016","text":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n","flair":"null\tnull"}
{"author":"zergling103","created":"Fri Oct 28 10:52:09 EDT 2016","text":"So, I've been thinking about how the activation function you use can change how easy it is to train your NN. There are: Sigmoid, tanh, rectifier, leaky rectifier, softplus, etc.\n\nThis had me thinking: Perhaps as part of the training process, one could allow the network to change activation functions on individual neurons to see how this improves or degrades its performance.\n\nOf course, switching from two discrete activation functions abruptly (rectifier to tanh) would likely be too drastic of a change to work, especially with deep networks. So perhaps if you had a more complex activation function that could be parametrically adjusted to take on the form of most simpler activation functions, one could let the network decide what flavour of activation works best by adjusting the parameters in the same way weights are trained.\n\nHas something like this been explored?","flair":"one\tDiscussion"}
{"author":"xternalz","created":"Wed Nov 16 21:09:50 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.05431 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Aggregated Residual Transformations for Deep Neural Networks Authors: Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He (Submitted on 16 Nov 2016) Abstract: We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, codenamed ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. Comments: Tech report Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1611.05431 [cs.CV]   (or arXiv:1611.05431v1 [cs.CV] for this version) Submission history From: Kaiming He [view email] [v1] Wed, 16 Nov 2016 20:34:42 GMT (1073kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Miejuib","created":"Mon Oct 31 17:40:59 EDT 2016","text":"https:\/\/github.com\/Miej\/GoDeeper\n\n\nHey guys, just thought some of you might enjoy this.\n\n\nI've set up an community AMI on aws that I've done a simul-build of a lot of the bleeding-edge releases of some of the most popular machine learning\/deep learning packages\/modules around.\n\n\nSome hilights: \n\np2.xlarge \n\nNVIDIA Tesla K80 \n\nUbuntu 14.04 \n\nCUDA 8.0 \n\ncuDNN 5.1 \n\nOpenCV 3.1 \n\nSpark 2.0 \n\nCaffe 1.0 \n\nTensorflow \n\nTheano \n\nKeras \n\nTorch 7 \n\nMicrosoft CNTK V2 \n\nR \n\nScala \n\nJulia\n\n\nI went through a lot of headaches getting this set up, so hopefully this can help newcomers in the field of ML\/DL get their feet wet with some code in different systems\/languages without any of the hassle of dependency version conflicts and source build seg faults :D\n\n\nEnjoy!","flair":"four\tProject"}
{"author":"quackinspector","created":"Wed Nov 23 17:52:12 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.05118 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs cs.CL References & Citations NASA ADS 1 blog link (what is this?) Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives Authors: Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daumé III, Larry Davis (Submitted on 16 Nov 2016) Abstract: Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \"gutters\" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \"closure\". While computers can now describe the content of natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We collect a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language. Subjects: Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL) Cite as: arXiv:1611.05118 [cs.CV]   (or arXiv:1611.05118v1 [cs.CV] for this version) Submission history From: Mohit Iyyer [view email] [v1] Wed, 16 Nov 2016 02:16:09 GMT (9072kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"quickhook","created":"Tue Nov 22 16:47:36 EST 2016","text":"\"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.","flair":"two\tNews"}
{"author":"code2hell","created":"Tue Oct 11 03:15:03 EDT 2016","text":"I am planning to create a dataset for instance segmentation for a specific purpose. I would really appreciate if someone can post the tools that can be used for creation os such dataset. Thanks.","flair":"one\tDiscussion"}
{"author":"kitfreddura","created":"Sun Nov 27 19:22:20 EST 2016","text":"Hey all,  \n  \nI'm doing some work with applying machine learning to random testing of programs. I have a set of (input, [0,1]) pairs, and I am trying to learn a relationship between the inputs and their labels. The inputs are strings. I could code a neural net with tensorflow, but I imagine there are plenty of pre-made neural nets which would be ideal for this task. Can anyone point me towards such an implementation? Thanks!","flair":"one\tDiscussion"}
{"author":"efavdb","created":"Sat Oct 15 17:36:38 EDT 2016","text":"Here, we provide a brief introduction to reinforcement learning (RL) \u2014 a general technique for training programs to play games efficiently. Our aim is to explain its practical implementation: We cover some basic theory and then walk through a minimal python program that trains a neural network to play the game battleship. Follow @efavdb Follow us on twitter for new submission alerts! Reinforcement learning (RL) techniques are methods that can be used to teach algorithms to play games efficiently. Like supervised machine-learning (ML) methods, RL algorithms learn from data \u2014 in this case, past game play data. However, whereas supervised-learning algorithms train only on data that is already available, RL addresses the challenge of performing well while still in the process of collecting data. In particular, we seek design principles that The reason we particularly want our algorithms to learn fast here is that RL is most fruitfully applied in contexts where training data is limited \u2014 or where the space of strategies is so large that it would be difficult to explore exhaustively. It is in these regimes that supervised techniques have trouble and RL methods shine. In this post, we review one general RL training procedure: The policy-gradient, deep-learning scheme. We review the theory behind this approach in the next section. Following that, we walk through a simple python implementation that trains a neural network to play the game battleship. Our python code can be downloaded from our github page, here. It requires the jupyter, tensorflow, numpy, and matplotlib packages. Policy-gradient, deep RL algorithms consist of two main components: A policy network and a rewards function. We detail these two below and then describe how they work together to train good models. The policy for a given deep RL algorithm is a neural network that maps state values $s$ to probabilities for given game actions $a$. In other words, the input layer of the network accepts a numerical encoding of the environment \u2014 the state of the game at a particular moment. When this input is fed through the network, the values at the output layer correspond to the log probabilities that each of the actions available to us is optimal \u2014 one output node is present for each possible action that we can choose. Note that if we knew with certainty which move we should take, only one output node would have a finite probability. However, if our network is uncertain which action is optimal, more than one output node will have finite weight. To illustrate the above, we present a diagram of the network used in our battleship program below. (For a review of the rules of battleship, see footnote [1].) For simplicity, we work with a 1-d battleship grid. We then encode our current knowledge of the environment using one input neuron for each of our opponent\u2019s grid positions. In particular, we use the following encoding for each neuron \/ index: \\begin{eqnarray} \\label{input} \\tag{1} x_{0,i} = \\begin{cases} -1 & \\text{Have not yet bombed $i$} \\\\ \\ 0 & \\text{Have bombed $i$, no ship} \\\\ +1 & \\text{Have bombed $i$, ship present}. \\end{cases} \\end{eqnarray} In our example figure below, we have five input neurons, so the board is of size five. The first three neurons have value $-1$ implying we have not yet bombed those grid points. Finally, the last two are $+1$ and $0$, respectively, implying that a ship does sit at the fourth site, but not at the fifth. Note that in the output layer of the policy network shown, the first three values are labeled with log probabilities. These values correspond to the probabilities that we should next bomb each of these indices, respectively. We cannot re-bomb the fourth and fifth grid points, so although the network may output some values to these neurons, we\u2019ll ignore them. Before moving on, we note that the reason we use a neural network for our policy is to allow for efficient generalization: For games like Go that have a very large number of states, it is not feasible to collect data on every possible board position. This is exactly the context where ML algorithms excel \u2014 generalizing from past observations to make good predictions for new situations. In order to keep our focus on RL, we won\u2019t review how ML algorithms work in this post (however, you can check out our archives section for relevant primers). Instead we simply note that \u2014 utilizing these tools \u2014 we can get good performance by training only on a representative subset of games \u2014 allowing us to avoid study of the full set, which can be much larger. To train an RL algorithm, we must carry out an iterative game play \/ scoring process: We play games according to our current policy, selecting moves with frequencies proportional to the probabilities output by the network. If the actions taken resulted in good outcomes, we want to strengthen the probability of those actions going forward. The rewards function is the tool we use to formally score our outcomes in past games \u2014 we will encourage our algorithm to try to maximize this quantity during game play. In effect, it is a hyper-parameter for the RL algorithm: many different functions could be used, each resulting in different learning characteristics. For our battleship program, we have used the function \\begin{eqnarray} \\label{rewards} \\tag{2} r(a;t_0) = \\sum_{t \\geq t_0} \\left ( h(t) \u2013 \\overline{h(t)} \\right) (0.5)^{t-t0} \\end{eqnarray} Given a completed game log, this function looks at the action $a$ taken at time $t_0$ and returns a weighted sum of hit values $h(t)$ for this and all future steps in the game. Here, $h(t)$ is $1$ if we had a hit at step $t$ and is $0$ otherwise. In arriving at (\\ref{rewards}), we admit that we did not carry out a careful search over the set of all possible rewards functions. However, we have confirmed that this choice results in good game play, and it is well-motivated: In particular, we note that the weighting term $(0.5)^{t-t0}$ serves to strongly incentivize a hit on the current move (we get a reward of $1$ for a hit at $t_0$), but a hit at $(t_0 + 1)$ also rewards the action at $t_0$ \u2014 with value $0.5$. Similarly, a hit at $(t_0 + 2)$ rewards $0.25$, etc. This weighted look-ahead aspect of (\\ref{rewards}) serves to encourage efficient exploration of the board: It forces the program to care about moves that will enable future hits. The other ingredient of note present in (\\ref{rewards}) is the subtraction of $\\overline{h(t)}$. This is the expected rewards that a random network would obtain. By pulling this out, we only reward our network if it is outperforming random choices \u2014 this results in a net speed-up of the learning process. In order to train our algorithm to maximize captured rewards during game play, we apply gradient descent. To carry this out, we imagine allowing our network parameters $\\theta$ to vary at some particular step in the game. Averaging over all possible actions, the gradient of the expected rewards is then formally, \\begin{eqnarray} \\nonumber \\partial_{\\theta} \\langle r(a \\vert s) \\rangle &\\equiv & \\partial_{\\theta} \\int p(a \\vert \\theta, s) r(a \\vert s) da \\\\ \\nonumber &=& \\int p(a \\vert \\theta, s) r(a \\vert s) \\partial_{\\theta} \\log \\left ( p(a \\vert \\theta, s) \\right) da \\\\ &\\equiv & \\langle r(a \\vert s) \\partial_{\\theta} \\log \\left ( p(a \\vert \\theta, s) \\right) \\rangle. \\tag{3} \\label{formal_ev} \\end{eqnarray} Here, the $p(a)$ values are the action probability outputs of our network. Unfortunately, we usually can\u2019t evaluate the last line above. However, what we can do is approximate it using a sampled value: We simply play a game with our current network, then replace the expected value above by the reward actually captured on the $i$-th move, \\begin{eqnarray} \\hat{g}_i = r(a_i) \\nabla_{\\theta} \\log p(a_i \\vert s_i, \\theta). \\tag{4} \\label{estimator} \\end{eqnarray} Here, $a_i$ is the action that was taken, $r(a_i)$ is reward that was captured, and the derivative of the logarithm shown can be evaluated via back-propagation (aside for those experienced with neural networks: this is the derivative of the cross-entropy loss function that would apply if you treated the event like a supervised-learning training example \u2014 with the selected action $a_i$ taken as the label). The function $\\hat{g}_i$ provides a noisy estimate of the desired gradient, but taking many steps will result in a \u201Cstochastic\u201D gradient descent, on average pushing us towards correct rewards maximization. In summary, then, RL training proceeds iteratively: To initialize an iterative step, we first play a game with our current policy network, selecting moves stochastically according to the network\u2019s output. After the game is complete, we then score our outcome by evaluating the rewards captured on each move \u2014 for example, in the battleship game we use (\\ref{rewards}). Once this is done, we then estimate the gradient of the rewards function using (\\ref{estimator}). Finally, we update the network parameters, moving $\\theta \\to \\theta + \\alpha \\sum \\hat{g}_i$, with $\\alpha$ a small step size parameter. To continue, we then play a new game with the updated network, etc. To see that this process does, in fact, encourage actions that have resulted in good outcomes during training, note that (\\ref{estimator}) is proportional to the rewards captured at the step $i$. Consequently, when we adjust our parameters in the direction of (\\ref{estimator}), we will strongly encourage those actions that have resulted in large rewards outcomes. Further, those moves with negative rewards are actually suppressed. In this way, over time, the network will learn to examine the system and suggest those moves that will likely produce the best outcomes. That\u2019s it for the basics of deep, policy-gradient RL. We now turn to our python example, battleship. Define our network \u2014 a fully connected, three layer system. The code below is mostly tensorflow boilerplate that can be picked up by going through their first tutorials. The one unusual thing is that we have our learning rate in (26) set to the placeholder value (9). This will allow us to vary our step sizes with observed rewards captured below. Next, we define a method that will allow us to play a game using our network. The TRAINING variable specifies whether or not to take the optimal moves or to select moves stochastically. Note that the method returns a set of logs that record the game proceedings. These are needed for training. Our implementation of the rewards function (\\ref{rewards}): Finally, our training loop. Here, we iteratively play through many games, scoring after each game, then adjusting parameters \u2014 setting the placeholder learning rate equal to ALPHA times the rewards captured. Running this last cell, we see that the training works! The following is an example trace from the play_game() method, with the variable TRAINING set to False. This illustrates an intelligent move selection process. Here, the first five lines are the board encodings that the network was fed each step \u2014 using (\\ref{input}). The second to last row presents the sequential grid selections that were chosen. Finally, the last row is the hit log. Notice that the first two moves nicely sample different regions of the board. After this, a hit was recorded at $6$. The algorithm then intelligently selects $7$ and $8$, which it can infer must be the final locations of the ship. The plot below provides further characterization of the learning process. This shows the running average game length (steps required to fully bomb ship) versus training epoch. The program learns the basics quite quickly, then continues to gradually improve over time [2]. In this post, we have covered a variant of RL \u2014 namely, the policy-gradient, deep RL scheme. This is a method that typically defaults to the currently best-known strategy, but occasionally samples from other approaches, ultimately resulting in an iterative improvement in policy. The two main ingredients here are the policy network and the rewards function. Although network architecture design is usually the place where most of the thinking is involved in supervised learning, it is the rewards function that typically requires the most thought in the RL context. A good choice should be as local in time as possible, so as to facilitate training (distant forecast dependence will result in a slow learning process). However, the rewards function should also directly attack the ultimate end of the process (\u201Cwinning\u201D the game \u2014 encouragement of side quests that aren\u2019t necessary can often occur if care is not taken). Balancing these two competing demands can be a challenge, and rewards function design is therefore something of an art form. Our brief introduction here was intended only to illustrate the gist of how RL is carried out in practice. For further details, we can recommend two resources: the text book by Sutton and Barto [3] and a recent talk by John Schulman [4]. [1] Game rules: Battleship is a two-player game. Both players begin with a finite regular grid of positions \u2014 hidden from their opponent \u2014 and a set of \u201Cships\u201D. Each player receives the same quantity of each type of ship. At the start of the game, each player places the ships on their grid in whatever locations they like, subject to some constraints: A ship of length 2, say, must occupy two contiguous indices on the board, and no two ships can occupy the same grid location. Once placed, the ships are fixed in position for the remainder of the game. At this point, game play begins, with the goal being to sink the opponent ships. The locations of the enemy ships are initially unknown because we cannot see the opponent\u2019s grid. To find the ships, one \u201Cbombs\u201D indices on the enemy grid \u2014 with bombing occurs in turns. When an opponent index is bombed, the opponent must truthfully state whether or not a ship was located at the index bombed. Whoever succeeds in bombing all their opponent\u2019s occupied indices first wins the game. Therefore, the problem reduces to finding the enemy ship indices as quickly as possible. [2] One of my colleagues (HC) has suggested that the program likely begins to overfit at some point. However, the 1-d version of the game has so few possible ship locations that characterization of this effect via a training and test set split does not seem appropriate. However, this approach could work were we to move to higher dimensions and introduce multiple ships. [3] Sutton and Barto, (2016). \u201CReinforcement Learning: An Introduction\u201D. Text site, here. [4] John Schulman, (2016). \u201CBay Area Deep Learning School\u201D. Youtube recording of talk available here.","flair":"four\tProject"}
{"author":"urish","created":"Mon Nov 14 17:24:45 EST 2016","text":"I\u2019m still in total shock from the decision my country made last Tuesday. We elected a hateful, bigoted, misogynistic, incompetent demagogue to lead us into a dark and foreboding future. While the internet has been flooded with hot takes about why this happened, I\u2019d like to reflect a bit about why I am so crushed by this outcome. I have been a machine learning researcher for nearly 15 years. I have been enthralled by the promise of data-driven methods to enrich our lives and make the impossible possible. This election is a resounding indictment of the information infrastructures we\u2019ve built to inform ourselves. And I am reaching out to the machine learning community to come to terms with this fact and to do better. There are three major failures of this cycle that are mostly the fault of our infatuation with data. The first is polling. The science of polling was shown to be beyond fallible, with completely incorrect voter screens and projections. While Gelman and others argue that we can learn from a mistake much like we learn from the black box of a crashed plane. But we currently fly tens of thousands of flights per day in our domestic airspace and have had zero fatalities in 2016. This was achieved by rigorous scientific analysis, careful engineering, extensive regulatory oversight, and long training, not simply by reverse-engineering crashed planes, one after another. Statisticians arguing that rare events occur does not provide a way forward to robustify our methods of devoting resources to voter turnout or persuasion. Moreover, we treat polling like BIG DATA with sophisticated polling manipulation and averaging, even though we have less than 10 relevant elections to use to fit our models. There is simply no way to analyze the polls without overfitting. The second major failure is in targeted news on social media \u2013 virality is proving fatal to truth in political discourse. Here, the success of BIG DATA led to a major failure in the democratic process. I\u2019m disheartened to hear that Mark Zuckerberg won\u2019t acknowledge the role Facebook played in spreading disinformation in the 2016 campaign. More than half of the country gets its news from social media, and when that news is targeted it simply feeds into confirmation bias. Our community has developed remarkably effective tools to microtarget advertisements. But if you use ad models to deliver news, that\u2019s propaganda. And just because we didn\u2019t intend to spread rampant misinformation doesn\u2019t mean we are not responsible. And the third major failure has been a general apathy about politics amongst my colleagues here in the Bay Area. When many of the best minds in machine learning have decided that the most existential threat to civilization is the rise of Skynet, we have had a major failure of group think. Many ML researchers are more concerned with trying to bring about The Singularity, than in solving real problems. People are suffering all around us, and many of them are suffering precisely because of our advances in automation. On top of this, 2016 is going to be the warmest year on record. If we devote the majority of our talents and resources to sci-fi navel gazing, then we are gravely failing the world with our neglect. But I think we can be better. I think machine learning can be a powerful tool for social good. I think scientific minds are crucial to moving the world in a more positive direction. But we must now make this decision as a community. I am heartened by Sam Altman\u2019s call to action. But now is the time to put your money and talents where your mouths are. Kevin Drum made some very important points that I want to reiterate and expand upon here: \u201CWe have elected a loudmouth, race-baiting game show host president of the United States.\u201D This man has appointed an openly racist, antisemitic, misogynist as senior advisor. This man has appointed a climate change denier to head his EPA transition. This man has said on national television that he wants to repeal Roe vs Wade. This election was very close. This is not a universal condemnation of progressive values. A few thousand votes in a few places would have resulted in a completely different outcome. And if we want to strive to achieve that outcome, it is time to become more active and vigilant. We have to be better. And we have to be better now. Regardless of our mobilization, there are many people threatened by this new regime in America. Our muslim and Latino friends have been openly targeted. The president-elect has called for nationalizing \u201Cstop-and-frisk.\u201D We have a lot of resources in the machine learning community. Some of us are very wealthy. Others hold positions of influence. We need to use this power to protect those who are threatened by this new regime. We must act, and we must act now. I am hoping that we can put our heads together to work for good in spite of this tremendous set back. I want to write a blogpost about the rise of BIG DATA. About how we used our technical acumen to help each other, protect those endangered, and save our fragile environment. That requires action and mobilization. And it has to happen now. If you are up for a constructive conversation on how to move forward, please leave a comment. I don\u2019t think any of us have a concrete plan yet on how to act, but I hope we can work towards something positive together.","flair":"one\tDiscussion"}
{"author":"ReproducibleResearch","created":"Mon Oct 24 22:58:55 EDT 2016","text":"Just came across this github thread:\n\nhttps:\/\/github.com\/ibab\/tensorflow-wavenet\/issues\/47#issuecomment-255918245\n\nwhich didn't seem to reach much of a conclusion about why the WaveNet results were hard to reproduce. Since this paper made such a big splash with the ML community, has anyone been able to actually create wave forms of the same audio quality as the blog post?\n\nI've come across a lot of open source implementations based on the paper, but none of them have reproduced anything like DeepMind's results.","flair":"one\tDiscussion"}
{"author":"labiuai","created":"Mon Nov 14 14:31:58 EST 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1611.01260 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CV < prev | next > new | recent | 1611 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Computer Vision and Pattern Recognition Title: Learning Identity Mappings with Residual Gates Authors: Pedro H. P. Savarese (Submitted on 4 Nov 2016) Abstract: We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence -- fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model. Subjects: Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG) Cite as: arXiv:1611.01260 [cs.CV]   (or arXiv:1611.01260v1 [cs.CV] for this version) Submission history From: Pedro Henrique Pamplona Savarese [view email] [v1] Fri, 4 Nov 2016 04:34:38 GMT (556kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"Ciuleandra24","created":"Tue Oct 25 11:30:50 EDT 2016","text":"I was wondering what advice you could give about training CNNs for image segmentation or classification. What I would like to know is:\n\n1. How to do hyperparameter tuning? Suppose we have 3000 images for training, on GPU some networks take 12 or more hours to train. I would like to change some parameters but not wait that long each time. Is it relevant to train on a smaller subset? (How small, keep the batch size?)\n\n2. How to choose the batch size? Is it ok to have very small batch size like 2?\n\n3. How to choose the learning rate? Does it depend on image size or on the batch size?\n\n4. How to handle very large images? For example size of 1024 x 2048? I'm concerned that the GPU does not have enough memory for such large images when using very deep architectures.\n\n5. I think that plotting the loss and accuracy would be good. The loss should be going down and the accuracy up. Should I look to something else when analyzing these plots? Should I analyze other parameters?\n\n6. Is it necessary to check the gradients? \n\n7. When to use learning rate decay?\n\nAny advice on what steps to follow when training and how to do hyperparameter tuning would be appreciated. ","flair":"one\tDiscussion"}
{"author":"Pieranha","created":"Thu Oct 13 16:34:46 EDT 2016","text":"The Densely Connected Convolutional Networks paper (https:\/\/arxiv.org\/abs\/1608.06993) is really interesting to me as is the older ResNet paper (https:\/\/arxiv.org\/abs\/1512.03385). However, I'm intrigued by many of their design choices. In particular: \n\n1. Why do they use average-pooling instead of max-pooling at the end of their network? If each neuron is a feature extractor and the final neurons can capture quite complex patterns, then I would assume that you're more interested in whether a single neuron was triggered with that pattern than the average firing of neurons. Has there been any results showing that average-pooling is better than max-pooling?\n\n2. Why do they not use a large fully-connected layer at the end of their network? Suppose that each neuron captures a separate pattern. Then combining these patterns using a fully-connected layer would make sense. This was common for e.g. AlexNet (https:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-)","flair":"one\tDiscussion"}
{"author":"urish","created":"Fri Oct 14 10:07:33 EDT 2016","text":"A popular method for exploring high-dimensional data is something called t-SNE, introduced by van der Maaten and Hinton in 2008. The technique has become widespread in the field of machine learning, since it has an almost magical ability to create compelling two-dimensonal \u201Cmaps\u201D from data with hundreds or even thousands of dimensions. Although impressive, these images can be tempting to misread. The purpose of this note is to prevent some common misreadings. We\u2019ll walk through a series of simple examples to illustrate what t-SNE diagrams can and cannot show. The t-SNE technique really is useful\u2014but only if you know how to interpret it. Before diving in: if you haven\u2019t encountered t-SNE before, here\u2019s what you need to know about the math behind it. The goal is to take a set of points in a high-dimensional space and find a faithful representation of those points in a lower-dimensional space, typically the 2D plane. The algorithm is non-linear and adapts to the underlying data, performing different transformations on different regions. Those differences can be a major source of confusion. A second feature of t-SNE is a tuneable parameter, \u201Cperplexity,\u201D which says (loosely) how to balance attention between local and global aspects of your data. The parameter is, in a sense, a guess about the number of close neighbors each point has. The perplexity value has a complex effect on the resulting pictures. The original paper says, \u201CThe performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.\u201D But the story is more nuanced than that. Getting the most from t-SNE may mean analyzing multiple plots with different perplexities. That\u2019s not the end of the complications. The t-SNE algorithm doesn\u2019t always produce similar output on successive runs, for example, and there are additional hyperparameters related to the optimization process. Let\u2019s start with the \u201Chello world\u201D of t-SNE: a data set of two widely separated clusters. To make things as simple as possible, we\u2019ll consider clusters in a 2D plane, as shown in the lefthand diagram. (For clarity, the two clusters are color coded.) The diagrams at right show t-SNE plots for five different perplexity values. With perplexity values in the range (5 - 50) suggested by van der Maaten & Hinton, the diagrams do show these clusters, although with very different shapes. Outside that range, things get a little weird. With perplexity 2, local variations dominate. The image for perplexity 100, with merged clusters, illustrates a pitfall: for the algorithm to operate property, the perplexity really should be smaller than the number of points. Implementations can give unexpected behavior otherwise. Each of the plots above was made with 5,000 iterations with a learning rate (often called \u201Cepsilon\u201D) of 10, and had reached a point of stability by step 5,000. How much of a difference do those values make? In our experience, the most important thing is to iterate until reaching a stable configuration. The images above show five different runs at perplexity 30. The first four were stopped before stability. After 10, 20, 60, and 120 steps you can see layouts with seeming 1-dimensional and even pointlike images of the clusters. If you see a t-SNE plot with strange \u201Cpinched\u201D shapes, chances are the process was stopped too early. Unfortunately, there\u2019s no fixed number of steps that yields a stable result. Different data sets can require different numbers of iterations to converge. Another natural question is whether different runs with the same hyperparameters produce the same results. In this simple two-cluster example, and most of the others we discuss, multiple runs give the same global shape. Certain data sets, however, yield markedly different diagrams on different runs; we\u2019ll give an example of one of these later. From now on, unless otherwise stated, we\u2019ll show results from 5,000 iterations. That\u2019s generally enough for convergence in the (relatively small) examples in this essay. We\u2019ll keep showing a range of perplexities, however, since that seems to make a big difference in every case. So far, so good. But what if the two clusters have different standard deviations, and so different sizes? (By size we mean bounding box measurements, not number of points.) Below are t-SNE plots for a mixture of Gaussians in plane, where one is 10 times as dispersed as the other. Surprisingly, the two clusters look about same size in the t-SNE plots. What\u2019s going on? The t-SNE algorithm adapts its notion of \u201Cdistance\u201D to regional density variations in the data set. As a result, it naturally expands dense clusters, and contracts sparse ones, evening out cluster sizes. To be clear, this is a different effect than the run-of-the-mill fact that any dimensionality reduction technique will distort distances. (After all, in this example all data was two-dimensional to begin with.) Rather, density equalization happens by design and is a predictable feature of t-SNE. The bottom line, however, is that you cannot see relative sizes of clusters in a t-SNE plot. What about distances between clusters? The next diagrams show three Gaussians of 50 points each, one pair being 5 times as far apart as another pair. At perplexity 50, the diagram gives a good sense of the global geometry. For lower perplexity values the clusters look equidistant. When the perplexity is 100, we see the global geometry fine, but one of the cluster appears, falsely, much smaller than the others. Since perplexity 50 gave us a good picture in this example, can we can always set perplexity to 50 if we want to see global geometry? Sadly, no. If we add more points to each cluster, the perplexity has to increase to compensate. Here are the t-SNE diagrams for three Gaussian clusters with 200 points each, instead of 50. Now none of the trial perplexity values gives a good result. It\u2019s bad news that seeing global geometry requires fine-tuning perplexity. Real-world data would probably have multiple clusters with different numbers of elements. There may not be one perplexity value that will capture distances across all clusters\u2014and sadly perplexity is a global parameter. Fixing this problem might be an interesting area for future research. The basic message is that distances between well-separated clusters in a t-SNE plot may mean nothing. A classic pitfall is thinking you see patterns in what is really just random data. Recognizing noise when you see it is a critical skill, but it takes time to build up the right intuitions. A tricky thing about t-SNE is that it throws a lot of existing intuition out the window. The next diagrams show genuinely random data, 500 points drawn from a unit Gaussian distribution in 100 dimensions. The left image is a projection onto the first two coordinates. The plot with perplexity 2 seems to show dramatic clusters. If you were tuning perplexity to bring out structure in the data, you might think you\u2019d hit the jackpot. Of course, since we know the cloud of points was generated randomly, it has no statistically interesting clusters: those \u201Cclumps\u201D aren\u2019t meaningful. If you look back at previous examples, low perplexity values often lead to this kind of distribution. Recognizing these clumps as random noise is an important part of reading t-SNE plots. There\u2019s something else interesting, though, which may be a win for t-SNE. At first the perplexity 30 plot doesn\u2019t look like a Gaussian distribution at all: there\u2019s only a slight density difference across different regions of the cloud, and the points seem suspiciously evenly distributed. In fact, these features are saying useful things about high-dimensional normal distributions, which are very close to uniform distributions on a sphere: evenly distributed, with roughly equal spaces between points. Seen in this light, the t-SNE plot is more accurate than any linear projection could be. It\u2019s rare for data to be distributed in a perfectly symmetric way. Let\u2019s take a look at an axis-aligned Gaussian distribution in 50 dimensions, where the standard deviation in coordinate i is 1\/i. That is, we\u2019re looking at a long-ish ellipsoidal cloud of points. For high enough perplexity values, the elongated shapes are easy to read. On the other hand, at low perplexity, local effects and meaningless \u201Cclumping\u201D take center stage. More extreme shapes also come through, but again only at the right perplexity. For example, here are two clusters of 75 points each in 2D, arranged in parallel lines with a bit of noise. For a certain range of perplexity the long clusters look close to correct, which is reassuring. Even in the best cases, though, there\u2019s a subtle distortion: the lines are slightly curved outwards in the t-SNE diagram. The reason is that, as usual, t-SNE tends to expand denser regions of data. Since the middles of the clusters have less empty space around them than the ends, the algorithm magnifies them. Sometimes you can read topological information off a t-SNE plot, but that typically requires views at multiple perplexities. One of the simplest topological properties is containment. The plots below show two groups of 75 points in 50 dimensional space. Both are sampled from symmetric Gaussian distributions centered at the origin, but one is 50 times more tightly dispersed than the other. The \u201Csmall\u201D distribution is in effect contained in the large one. The perplexity 30 view shows the basic topology correctly, but again t-SNE greatly exaggerates the size of the smaller group of points. At perplexity 50, there\u2019s a new phenomenon: the outer group becomes a circle, as the plot tries to depict the fact that all its points are about the same distance from the inner group. If you looked at this image alone, it would be easy to misread these outer points as a one-dimensional structure. What about more complicated types of topology? This may be a subject dearer to mathematicians than to practical data analysts, but interesting low-dimensional structures are occasionally found in the wild. Consider a set of points that trace a link or a knot in three dimensions. Once again, looking at multiple perplexity values gives the most complete picture. Low perplexity values give two completely separate loops; high ones show a kind of global connectivity. The trefoil knot is an interesting example of how multiple runs affect the outcome of t-SNE. Below are five runs of the perplexity-2 view. The algorithm settles twice on a circle, which at least preserves the intrinsic topology. But in three of the runs it ends up with three different solutions which introduce artificial breaks. Using the dot color as a guide, you can see that the first and third runs are far from each other. Five runs at perplexity 50, however, give results that (up to symmetry) are visually identical. Evidently some problems are easier than others to optimize. There\u2019s a reason that t-SNE has become so popular: it\u2019s incredibly flexible, and can often find structure where other dimensionality-reduction algorithms cannot. Unfortunately, that very flexibility makes it tricky to interpret. Out of sight from the user, the algorithm makes all sorts of adjustments that tidy up its visualizations. Don\u2019t let the hidden \u201Cmagic\u201D scare you away from the whole technique, though. The good news is that by studying how t-SNE behaves in simple cases, it\u2019s possible to develop an intuition for what\u2019s going on.","flair":"four\tProject"}
{"author":"adagrad","created":"Mon Oct 10 11:58:37 EDT 2016","text":"Zoomout is a convolutional neural network architecture for semantic segmentation. It maps small image elements (pixels or superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by zooming out from the pixel all the way to scene-level resolution. Then, these features are fed into a classifier that outputs a posterior distribution over every pixel in the image. For details, please consult and cite the CVPR paper: http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf The zoomout pipeline requires few image pre-preprocessing steps. Preprocess.lua contains functions to resize training images to a fixed width and height (and to subtract the mean color), including batch and ground truth versions of these functions. Dataset.lua will load PASCAL VOC images, and we supply scripts in coco\/ to also load MS COCO images. Our zoomout architecture was built on top of the VGG-16 model, but you can pass the zoomout constructor any appropriate model (e.g. ResNet). There are several options available to building the zoomout architecture: On top of the zoomout feature extractor, we have a pixel classifier (zoomoutclassifier.lua) that makes the final posterior probability predictions. The default classifier is a 4-layer convolutional neural network. The last layer of the classifier is a bilinear interpolation so that the label predictions match the spatial size of the ground truth. Without a CRF, the current architecture achieves 70% mean intersection-over-union (MIOU) on the PASCAL VOC 2012 challenge. Adding a dense CRF on top (include ref) increases accuracy to 72%. The training steps are as follows: The script for training is included in train.lua, currently we are using stochastic gradient descent with momentum (0.9) but any optimizer should work (e.g. Adam). The only data augmentation used is horizontal flips, each training image is flipped with probability 0.5. The script main.lua does the following: replicates our experimental setup, using VGG-16 as the base classifier and training end-to-end. After about 3-4 epochs, training from scratch should lead to a model with 66% MIOU.","flair":"four\tProject"}
{"author":"brockl33","created":"Wed Oct 19 16:43:59 EDT 2016","text":"Recent evidence has shown that there are paid actors posing as regular citizens in order to achieve political aims:\n\nhttps:\/\/www.youtube.com\/watch?v=U-4w_jf_w0o\n\nhttps:\/\/www.reddit.com\/r\/The_Donald\/comments\/589z26\/domestic_political_terrorist_paid_by_hillarys\/\n\nThis was discovered through manual facial recognition between youtube videos.\n\nCould deep learning be used to detect faces that regularly appear in videos of protests to identify \"regulars\" a.k.a. paid agents that move around the country organizing fake protests?\n\nI want to ask whether deep learning could help sift through the WikiLeaks emails for collusion, but that task seems far away.\n\nVeritas expose https:\/\/youtu.be\/5IuJGHuIkzY\n\nMore info on the manual facial identification https:\/\/www.reddit.com\/r\/The_Donald\/comments\/589x16\/wikileaks_confirm_okeefe_claims_undeniable_email\/","flair":"one\tDiscussion"}
{"author":"acmueller","created":"Wed Nov 23 14:32:14 EST 2016","text":"Nick Foti, Ryan Adams, and I just put a paper on the arxiv about improving variational approximations (short version accepted early to AABI2016). We focused on one problematic aspect of variational inference in practice \u2014 that once the optimization problem is solved, the approximation is set and there isn\u2019t a straightforward way to improve it, even when we can afford some extra compute time. Markov chain Monte Carlo methods have a simple solution \u2014 run the chain for more steps and the posterior approximation will get better and better. Variational inference (VI) methods, on the other hand, typically pre-specify some class of approximating distributions, and then optimize the VI objective. When that pre-specified class of distributions (the variational family) doesn\u2019t include the neighborhood around the target distribution, the resulting VI solution will still be sub-optimal, in that there will be a non-zero KL-divergence between the approximation and the target. This will result in biased posterior estimates. There is a ton of great work toward solving this problem (more examples cited in the paper), including: In the above approaches, the variational family is made to be expressive from the start, and the evidence lower bound (ELBO), or a surrogate, is optimized as usual. In our approach, we first specify a simple class of approximations \u2014 e.g. Gaussians with diagonal covariance \u2014 and then optimize the corresponding variational objective, resulting in the best reachable approximation (in the KL-sense) that a diagonal Gaussian can do. We then expand the variational family in one of two ways: With the new degrees of freedom afforded by the additional covariance or mixture component parameters, we can define a new variational objective and optimize. Mean field variational inference (MFVI) proposes a fully factorized approximate distribution, which trades expressiveness for computational\/algorithmic tractability. This can lead to poor approximations, particularly when the true posterior has some highly correlated dimensions. On the other end of the spectrum, using a full, covariance matrix might be overkill, as some dimensions of the posterior may be (approximately) independent. Furthermore, as the necessary operations are typically , a full covariance matrix may be computationally prohibitive in high dimensions. , which allows necessary covariance manipulations to be done in time. As an example, here is a representative sample of four bivariate marginals from a D = 37-dimensional posterior (resulting from a hierarchical Poisson GLM) \u2014 note that there are a devilish 666 total pairwise covariances to model in this real-data posterior. With a diagonal covariance (D covariance parameters) we see the typical under-estimation of marginal posterior variance (in all images, samples and histograms result from running MCMC for 20k samples) Additional low rank structure introduces the capacity to capture additional correlations. Rank one covariance (2D covariance parameters): Each new rank can \u201Cfind\u201D a direction in parameter space that corresponds to a posterior correlation. Accurately accounting for posterior correlation allows the approximation to obtain more accurate marginal variance estimates. Below is a comparison of MFVI (diagonal covariance) and MCMC (NUTS) on six univariate marginals from the same 37-dimensional posterior. And below are the rank-three covariance univariate marginals. The marginal variances now much better reflect the true posterior\u2019s uncertainty \u2014 and at a much cheaper cost than the full rank parameterization. It is quick and easy to fit this low rank + diagonal variational family to a very general class of posteriors using black-box VI \u2014 details in the paper! Once we\u2019ve settled on a covariance rank, we can further improve the approximation by adding components to form a mixture. This creates a new variational objective \u2014 an objective that is a function of the new mixing weight and the new component parameters. In our paper, we show how to fit these parameters with stochastic gradients obtained via the \u201Cre-parameterization trick\u201D [4]. The idea (illustrated below) is quite simple \u2014 (i) fit an existing approximation; (ii) initialize a new component; (iii) optimize the ELBO as a function of the new component parameters. This allows us to capture non-Gaussianity in the posterior. Below is another real-data example; we plot a bivariate marginal from a 20-dimensional hierarchical model. We found that this variational boosting procedure can be quite sensitive to new component parameter initialization \u2014 our paper discusses an intialization technique that empirically assuages some of this sensitivity. This is definitely an active area of research. We also like to mention independent and concurrent work from Fangjian Guo et al. that was recently submitted to the arxiv; the authors describe a similar procedure for incorporating new mixture components into a variational approximation. They show that this greedy procedure will reduce the KL objective (as a function of the new mixing weight); and they detail a method for fitting new component parameters. Our work focuses more on adapting the reparameterization trick for mixture distributions, and incorporating covariance structure to reduce the number of mixture components required to describe posterior correlation. It\u2019s really interesting and complimentary to our work, and I encourage you to check it out! And on that note, Happy Thanksgiving!","flair":"three\tResearch"}
{"author":"thelibar","created":"Fri Oct 21 09:09:26 EDT 2016","text":"Hi,       \nI have been playing around with neural nets, trying to create one that requires little data and would be able to create logic just from reading random articles on Wikipedia\/comment sections. In my research on the brain I seem to have come upon some attributes of the braing that seem fundamental to how we learn.\n    \nLet me also state that I'm not a classical student of ML so my awareness of the different methods out there is limited. **So my question to you is: How many of these attributes have been implemented in some type of neural net model already?**\n       \nAs you will notice many of the ideas are inspired by processes in the human brain. The reason I think this is a good approach is that most of the information we would like a computer to understand is already encoded for humans, so a model close to the human brain should be effective for making sense out of that information. \n       \n**1. Flow within same layer**\nWhat I mean by flow is transfer of \"charge\" from one neuron withing a layer, to another within the same layer (same level of abstraction).\n      \nAs far as I've seen most neural nets only transfer charge between layers, (through pathways with different weights), never between neurons within the same layer.\n     \nThe reason why I believe this would be beneficial is that by doing that you would come closer to how our brains work (thus need less data for the creation of usable abstracts). For example it is easier to play a song on the guitar from the start, than from the middle. This could be explained by a wave of \"charge\" building up as the charge flows through same level abstractions (chords). In a similar way we often can answer a question more easily if we first replay it in our head (building up a wave of charge) or even repeat the question again out loud. In both cases this accumulating charge flowing from neuron to neuron will increase the likelyhood of a highly connected neuron to trigger. **Example:**\n      \n\"My name is...\" make my brain fill in the dot with \"thelibar\" almost instantaneously. If one would to say \"name is\" or just \"is\" the brain is less likely to give \"thelibar\" as a response since there has been no build up of flow.\n      \n**2. Separate abstractions of data by time pauses.**\nWhen we read, every blankspace, dot and comma is a slight pause in our internal reading of the sentence. My hunch is that we structure information this way because it lets the neurons in the brain \"cool down\". By allowing a minimal pause between each word we assure that letters that are highly related (constitute one word) bind to each other more strongly than letters between different words. For this process to function neurons that have higher charge (were triggered more recently) will also bind more strongly to the currently triggered neuron.\n     \nMy guess is that this is why humans are really bad at reading sentences without blankspaces, or in general process information when it is presented without any intervals to divide the information into discrete chunks (abstracts). \n       \nOf course it would not be time that was passing once this concept is translated to a artificial neural net, but rather it would be a decrease in the charge of a neuron that represents time having passed.\n    \n\n    \nPlease let me know if what I mean is unclear and I will try to explain better.","flair":"one\tDiscusssion"}
{"author":"zhongwenxu","created":"Mon Nov 07 05:25:11 EST 2016","text":"This repo is a TensorFlow implementation of Specifically, we follow the experiments in and try to reproduce the results in Table 1 and Figure 2. The fast weights model can achieve 100% accuracy (0% error rate) on R=50 setting in ~30K iterations. Both trained on GTX 980 Ti, with TensorFlow 0.11rc1. Setting on R=50, using ADAM optimizer with default parameters.","flair":"four\tProject"}
{"author":"Calumnusa","created":"Sat Nov 05 19:27:12 EDT 2016","text":"It seems like all the major players in machine learning are in North America and Europe. The conferences and research all seem to take place solely in those places. Why isn't there much of a presence coming from Asia, both academically and in industry. \n\n","flair":"one\tDiscussion"}
{"author":"nagasgura","created":"Fri Oct 28 12:12:57 EDT 2016","text":" Cornell University Library We gratefully acknowledge support from the Simons Foundation and member institutions arXiv.org > cs > arXiv:1610.06918 All papers Titles Authors Abstracts Full text Help pages (Help | Advanced search) Full-text links: Download: PDF Other formats (license) Current browse context: cs.CR < prev | next > new | recent | 1610 Change to browse by: cs cs.LG References & Citations NASA ADS Bookmark (what is this?) Computer Science > Cryptography and Security Title: Learning to Protect Communications with Adversarial Neural Cryptography Authors: Martín Abadi, David G. Andersen (Google Brain) (Submitted on 21 Oct 2016) Abstract: We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals. Comments: 15 pages Subjects: Cryptography and Security (cs.CR); Learning (cs.LG) Cite as: arXiv:1610.06918 [cs.CR]   (or arXiv:1610.06918v1 [cs.CR] for this version) Submission history From: David Andersen [view email] [v1] Fri, 21 Oct 2016 19:58:29 GMT (181kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) Link back to: arXiv, form interface, contact. ","flair":"three\tResearch"}
{"author":"danijar","created":"Tue Sep 27 14:41:27 EDT 2016","text":"I\u2019ve seen a lot of confusion over the rules of and in TensorFlow. It\u2019s simple: Let\u2019s look at an example. We define a graph with a variable and three operations: always returns the current value of our variable. assigns the initial value of 42 to that variable. assigns the new value of 13 to that variable. On a side note: TensorFlow creates a default graph for you, so we don\u2019t need the first two lines of the code above. The default graph is also what the sessions in the next section use when not manually specifying a graph. To run any of the three defined operations, we need to create a session for that graph. The session will also allocate memory to store the current value of the variable. As you can see, the value of our variable is only valid within one session. If we try to query the value afterwards in a second session, TensorFlow will raise an error because the variable is not initialized there. Of course, we can use the graph in more than one session, we just have to initialize the variables again. The values in the new session will be completely independent from the first one: Hopefully this short workthrough helped you to better understand . Feel free to ask questions in the comments.","flair":"null\tnull"}